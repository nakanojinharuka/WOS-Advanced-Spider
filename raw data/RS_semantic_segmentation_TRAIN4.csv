,author1,author2,author3,author4,author5,title,conference,publish time,citation,abstract,keyword1,keyword2,keyword3,keyword4,keyword5,keyword6,keyword7,journal/book,author6,keyword8,keyword9,keyword10,keyword11,author7,author8,keyword12,keyword13,keyword14,keyword15,author9
Row_1251,"Nogueira, Keiller","Dalla Mura, Mauro","Chanussot, Jocelyn","Schwartz, William Robson","dos Santos, Jefersson A.",Learning to Semantically Segment High-Resolution Remote Sensing Images,2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR),2016,21,"Land cover classification is a task that requires methods capable of learning high-level features while dealing with high volume of data. Overcoming these challenges, Convolutional Networks (ConvNets) can learn specific and adaptable features depending on the data while, at the same time, learn classifiers. In this work, we propose a novel technique to automatically perform pixel-wise land cover classification. To the best of our knowledge, there is no other work in the literature that perform pixel-wise semantic segmentation based on data-driven feature descriptors for high-resolution remote sensing images. The main idea is to exploit the power of ConvNet feature representations to learn how to semantically segment remote sensing images. First, our method learns each label in a pixel-wise manner by taking into account the spatial context of each pixel. In a predicting phase, the probability of a pixel belonging to a class is also estimated according to its spatial context and the learned patterns. We conducted a systematic evaluation of the proposed algorithm using two remote sensing datasets with very distinct properties. Our results show that the proposed algorithm provides improvements when compared to traditional and state-of-the-art methods that ranges from 5 to 15% in terms of accuracy.",Land-cover Mapping,Pixel-wise Classification,Semantic Segmentation,Deep Learning,Remote Sensing,Feature Learning,High-resolution Images,,,,,,,,,,,,,
Row_1252,"Duan, Zexian","Liu, Jiahang","Ling, Xinpeng","Zhang, Jinlong","Liu, Zhiheng",ERNet: A Rapid Road Crack Detection Method Using Low-Altitude UAV Remote Sensing Images,,MAY 2024,2,"The rapid and accurate detection of road cracks is of great significance for road health monitoring, but currently, this work is mainly completed through manual site surveys. Low-altitude UAV remote sensing can provide images with a centimeter-level or even subcentimeter-level ground resolution, which provides a new, efficient, and economical approach for rapid crack detection. Nevertheless, crack detection networks face challenges such as edge blurring and misidentification due to the heterogeneity of road cracks and the complexity of the background. To address these issues, we proposed a real-time edge reconstruction crack detection network (ERNet) that adopted multi-level information aggregation to reconstruct crack edges and improve the accuracy of segmentation between the target and the background. To capture global dependencies across spatial and channel levels, we proposed an efficient bilateral decomposed convolutional attention module (BDAM) that combined depth-separable convolution and dilated convolution to capture global dependencies across the spatial and channel levels. To enhance the accuracy of crack detection, we used a coordinate-based fusion module that integrated spatial, semantic, and edge reconstruction information. In addition, we proposed an automatic measurement of crack information for extracting the crack trunk and its corresponding length and width. The experimental results demonstrated that our network achieved the best balance between accuracy and inference speed compared to six established models.",UAV remote sensing,road cracks,semantic segmentation,crack quantification,edge detection,,,REMOTE SENSING,,,,,,,,,,,,
Row_1253,"Wang, Zihao","Kang, Xudong","Duan, Puhong","Deng, Bin",,SegIceNet: Activation Information Guided PointFlow for Sea Ice Segmentation,,2024,0,"Optical remote sensing is the major means of monitoring sea ice, which is beneficial for waterway planning, disaster prevention, and environmental research. Currently, a large number of sea ice segmentation methods have been developed. However, they often ignore the boundaries between sea ice and seawater, leading to unavoidable mis-segmentation. To alleviate this issue, this letter proposes a novel semantic segmentation network applied to sea ice remote sensing images, referred to as SegIceNet. First, we design a partial class activation map (PCAM) to select the feature points whose classes are easily confused in feature layers. By constructing the affinity among these points, the network explores potential semantic information and propagates these feature details among adjacent decoder layers. Second, this letter introduces a point-set optimization strategy, which aims to focus on samples of the aforementioned special points during the training process. Experimental results on a real sea ice dataset show that our method is better than other state-of-the-art semantic segmentation methods. Specifically, even in the imbalanced sea ice dataset, the proposed method achieved 94.88% mean intersection over union (mIoU).",Deep learning,partial class activation map (PCAM),remote sensing image,sea ice,semantic segmentation,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,
Row_1254,"Chen, Ziyi","Wang, Cheng","Li, Jonathan","Xie, Nianci","Han, Yan",Reconstruction Bias U-Net for Road Extraction From Optical Remote Sensing Images,,2021,69,"Automatic road extraction from remote sensing images plays an important role for navigation, intelligent transportation, and road network update, etc. Convolutional neural network (CNN)-based methods have presented many achievements for road extraction from remote sensing images. CNN-based methods require a large dataset with high quality labels for model training. However, there is still few standard and large dataset, which is specially designed for road extraction from optical remote sensing images. Besides, the existing end-to-end CNN models for road extraction from remote sensing images are usually with symmetric structure, studying on asymmetric structure between encoding and decoding is rare. To address the above problems, this article first provides a publicly available dataset LRSNY for road extraction from optical remote sensing images with manually labelled labels. Second, we propose a reconstruction bias U-Net for road extraction from remote sensing images. In our model, we increase the decoding branches to obtain multiple semantic information from different upsamplings. Experimental results show that our method achieves better performance compared with other six state-of-the-art segmentation models when testing on our LRSNY dataset. We also test on Massachusetts and Shaoshan datasets. The good performances on the two datasets further prove the effectiveness of our method.",Roads,Remote sensing,Feature extraction,Image segmentation,Semantics,Image reconstruction,Optical sensors,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Du, Jixiang",CNN model,dataset,optical remote sensing image,road extraction,,,,,,,
Row_1255,"van der Plas, Thijs L.","Geikie, Simon T.","Alexander, David G.","Simms, Daniel M.",,Multi-Stage Semantic Segmentation Quantifies Fragmentation of Small Habitats at a Landscape Scale,,NOV 2023,0,"Land cover (LC) maps are used extensively for nature conservation and landscape planning, but low spatial resolution and coarse LC schemas typically limit their applicability to large, broadly defined habitats. In order to target smaller and more-specific habitats, LC maps must be developed at high resolution and fine class detail using automated methods that can efficiently scale to large areas of interest. In this work, we present a Machine Learning approach that addresses this challenge. First, we developed a multi-stage semantic segmentation approach that uses Convolutional Neural Networks (CNNs) to classify LC across the Peak District National Park (PDNP, 1439 km2) in the UK using a detailed, hierarchical LC schema. High-level classes were predicted with 95% accuracy and were subsequently used as masks to predict low-level classes with 72% to 92% accuracy. Next, we used these predictions to analyse the degree and distribution of fragmentation of one specific habitat-wet grassland and rush pasture-at the landscape scale in the PDNP. We found that fragmentation varied across areas designated as primary habitat, highlighting the importance of high-resolution LC maps provided by CNN-powered analysis for nature conservation.",remote sensing,semantic segmentation,convolutional neural network,land cover prediction,habitat fragmentation,,,REMOTE SENSING,,,,,,,,,,,,
Row_1256,"Dang, Bo","Li, Yansheng",,,,MSResNet: Multiscale Residual Network via Self-Supervised Learning for Water-Body Detection in Remote Sensing Imagery,,AUG 2021,35,"Driven by the urgent demand for flood monitoring, water resource management and environmental protection, water-body detection in remote sensing imagery has attracted increasing research attention. Deep semantic segmentation networks (DSSNs) have gradually become the mainstream technology used for remote sensing image water-body detection, but two vital problems remain. One problem is that the traditional structure of DSSNs does not consider multiscale and multishape characteristics of water bodies. Another problem is that a large amount of unlabeled data is not fully utilized during the training process, but the unlabeled data often contain meaningful supervision information. In this paper, we propose a novel multiscale residual network (MSResNet) that uses self-supervised learning (SSL) for water-body detection. More specifically, our well-designed MSResNet distinguishes water bodies with different scales and shapes and helps retain the detailed boundaries of water bodies. In addition, the optimization of MSResNet with our SSL strategy can improve the stability and universality of the method, and the presented SSL approach can be flexibly extended to practical applications. Extensive experiments on two publicly open datasets, including the 2020 Gaofen Challenge water-body segmentation dataset and the GID dataset, demonstrate that our MSResNet can obviously outperform state-of-the-art deep learning backbones and that our SSL strategy can further improve the water-body detection performance.",water-body detection,multiscale residual network (MSResNet),self-supervised learning (SSL),high-resolution remote sensing imagery,,,,REMOTE SENSING,,,,,,,,,,,,
Row_1257,"Xing, Siyuan","Dong, Qiulei","Hu, Zhanyi",,,SCE-Net: Self- and Cross-Enhancement Network for Single-View Height Estimation and Semantic Segmentation,,MAY 2022,10,"Single-view height estimation and semantic segmentation have received increasing attention in recent years and play an important role in the photogrammetry and remote sensing communities. The height information and semantic information of images are correlated, and some recent works have shown that multi-task learning methods can achieve complementation of task-related features and improve the prediction results of the multiple tasks. Although much progress has been made in recent works, how to effectively extract and fuse height features and semantic features is still an open issue. In this paper, a self- and cross-enhancement network (SCE-Net) is proposed to jointly perform height estimation and semantic segmentation on single aerial images. A feature separation-fusion module is constructed to effectively separate and fuse height features and semantic features based on an attention mechanism for feature representation enhancement across tasks. In addition, a height-guided feature distance loss and a semantic-guided feature distance loss are designed based on deep metric learning to achieve task-aware feature representation enhancement. Extensive experiments are conducted on the Vaihingen dataset and the Potsdam dataset to verify the effectiveness of the proposed method. The experimental results demonstrate that the proposed SCE-Net could outperform the state-of-the-art methods and achieve better performance in both height estimation and semantic segmentation.",height estimation,semantic segmentation,single aerial image,convolutional neural networks,multi-task learning,deep metric learning,,REMOTE SENSING,,,,,,,,,,,,
Row_1258,"Gao, Hao","Xiong, Xuejun","Cao, Lin","Yu, Dingfeng","Yang, Guangbing",Pixel-Level Prediction for Ocean Remote Sensing Image Features Fusion Based on Global and Local Semantic Relations,,2021,3,"With the rapid development of remote-sensing imaging technology, remote-sensing images have become increasingly diverse, and people are paying more attention to ocean remote-sensing research. Because ocean remote-sensing data are complex, and the ocean environment is diverse, results will differ, even if the same target is detected at different times in the same scene. To obtain more semantic features and better pixel-level prediction capabilities, this paper proposes a pixel-level ocean remote-sensing image algorithm (GLPO-Net) that combines local and global features. First, texture features, color features, and spatial relationship features are extracted. Second, the algorithm constructs a multiscale local cross-attention mechanism strategy to obtain feature weight information in different directions to fully mine the local features of ocean remote-sensing images. Concurrently, an algorithm constructs a multiscale global cross-attention mechanism strategy to obtain global features. Then, the fusion of global features and local features is described in each submodule to obtain more representative deep features. Finally, small-sample ocean remote-sensing is described via image pixel-level prediction. The algorithm proposed in this paper has been tested with three public ocean remote-sensing datasets. The experimental results show that the proposed GLPO-Net algorithm can learn features from small samples of ocean remote-sensing images. Compared to the prediction results of other remote-sensing image algorithms, GLPO-Net exhibits better prediction capabilities.",Semantics,Remote sensing,Feature extraction,Oceans,Prediction algorithms,Convolution,Sensors,IEEE ACCESS,"Yang, Lei",Ocean remote sensing,deep learning,features fusion,multi-scale convolutional,,,,,,,
Row_1259,"He, Wenjing","Song, Hongjun","Yao, Yuanyuan","Jia, Hongying",,MAPPING OF URBAN AREAS FROM SAR IMAGES VIA SEMANTIC SEGMENTATION,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,1,This letter proposes a new semantic segmentation network called IGFUnet to map the urban areas from large scale SAR images. The network is based on U-net structure which is especially suitable for semantic segmentation of small data set. Inception module is adopted as the encoder to extract rich and dense features. We develop a global feature attention module (GFA) as a channel attention module to perform feature correction on each decoding layer. Combing the inception module and GFA module can take full use of the feature map and efficiently integrate features from different scales. Qualitative and quantitative experiments based on Sentinel-1A have proved the effectiveness of our IGFUnet model.,Synthetic aperture radar (SAR),mapping of urban areas,deep learning,semantic segmentation,,,,,,,,,,,,,,,,
Row_1260,"Laupheimer, Dominik","Haala, Norbert",,,,MULTI-MODAL SEMANTIC MESH SEGMENTATION IN URBAN SCENES,"XXIV ISPRS CONGRESS IMAGING TODAY, FORESEEING TOMORROW, COMMISSION II",2022,3,"The semantic segmentation of the huge amount of acquired 3D data has become an important task in recent years. Meshes have evolved into a standard representation next to Point Clouds (PCs) - not least because of their great visualization possibilities. Compared to PCs, meshes have commonly smaller memory footprints while jointly providing geometrical and high-resolution textural information. For this reason, we opt for semantic mesh segmentation, which is a widely overlooked topic in photogrammetry and remote sensing yet. In this work, we perform an extensive ablation study on multi-modal handcrafted features adapting the Point Cloud Mesh Association (PCMA) (Laupheimer et al., 2020) which establishes explicit connections between faces and points. The multi-modal connections are used in a two-fold manner: (i) to extend per-face descriptors with features engineered on the PC and (ii) to annotate meshes semi-automatically by propagating the manually assigned labels from the PCs. In this way, we derive annotated meshes from the ISPRS benchmark data sets Vaihingen 3D (V3D) and Hessigheim 3D (H3D). To demonstrate the effectiveness of the multi-modal approach, we use well-established and fast Random Forest (RF) models deploying various feature vector compositions and analyze their performances for semantic mesh segmentation. The feature vector compositions consider features derived from the mesh, the PC or both. The results indicate that the combination of radiometric and geometric features outperforms feature sets of a single feature type only. Besides, we observe that relative height is the most crucial feature. The main finding is that the multi-modal feature vector integrates the complementary strengths of the underlying modalities. Whereas the mesh provides outstanding textural information, the dense PCs are superior in geometry. The multi-modal feature descriptor achieves the best performance on both data sets. It significantly outperforms feature sets that incorporate only features derived from the mesh by +7.37 pp and +2.38 pp for mF1 and Overall Accuracy (OA) on V3D. The registered improvement is +9.23 pp and +4.33 pp for mF1 and OA on H3D.",Urban Scene Understanding,Semantic Segmentation,Multi-Modality,Textured Mesh,Point Cloud,,,,,,,,,,,,,,,
Row_1261,"Zheng, Yunping","Xu, Yuan","Shu, Shiqiang","Sarem, Mudar",,Indoor semantic segmentation based on Swin-Transformer,,FEB 2024,1,"In recent years, with the rapid development of Transformer in the field of natural language processing, many researchers have realized its potential and gradually applied it to the field of computer vision, with a proliferation of theoretical approaches represented by Vision Transformer (ViT) and Data-efficient image Transformer (DeiT). On the basis of ViT, the famous Swin-Transformer was proposed as one of the best computer vision neural network backbones, which can be widely used in tasks such as image classification, target detection and video recognition. However, in the field of image segmentation, the semantic segmentation of indoor scenes is still very challenging due to the wide variety of objects, large differences in object sizes, and a large number of overlapping objects with occlusion. Aiming at the problem that the existing semantic segmentation of RGB-D indoor scenes cannot effectively fuse multimodal features, in this paper, we propose a novel indoor semantic segmentation algorithm based on Swin-Transformer. It attempts to apply Swin-Transformer to the field of indoor RGBD semantic segmentation, and tests the performance of the model by conducting extensive experiments on the mainstream indoor semantic segmentation datasets NYU-Depth V2 and SUN RGB-D. The experimental results show that the Swin-L RGB+Depth setting achieves 52.44% MIoU on the NYU-Depth V2 data and 51.15% MIoU on the SUN RGB-D data set, which reflects an excellent performance in the field of indoor semantic segmentation. The improved performance of the Depth features on the indoor semantic segmentation model has also been demonstrated in the experiments by controlling the type of input features. Our source code is publicly available at https://github.com/YunpingZheng/ISSSW.",Deep learning,Transformer,RGB-D semantic segmentation,,,,,JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION,,,,,,,,,,,,
Row_1262,"Chouhan, Avinash","Chutia, Dibyajyoti","Aggarwal, Shiv Prasad",,,Deep Learning Approach for Multi-class Semantic Segmentation of UAV Images,,NOV 2023,0,"Image understanding plays a very crucial role in remote sensing applications. For this, image semantic segmentation is one of the approaches where each pixel of an image is assigned to particular classes based on various features. Aerial semantic segmentation suffers from the class imbalance problem. Proper differentiation of least represented categories is challenging and a goal for the state-of-art approach. In this work, we present a novel deep learning method to perform this task. We proposed a lightweight encoder-decoder network residual depth separable UNet (RDS-UNet) and conditional random field for effective segmentation on very high-resolution aerial images. We proposed patch-with-multi-class sampling to handle the class imbalance problem without increasing the computational overhead during the training process. We created a semi-precise annotated UAV dataset named NESAC UAV Seg for the aerial semantic segmentation task. We demonstrated the efficacy of our model using the publicly available benchmark Drone Deploy dataset and our NESAC UAV Seg dataset. Our model required approximately half the number of trainable parameters and floating point operations compared to other methods. A detailed ablation study is presented to showcase the effectiveness of various modules utilized in our network.",Semantic segmentation,deep learning,UAV,,,,,INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS,,,,,,,,,,,,
Row_1263,"Liu, Yongchang","Liu, Yawen","Duan, Yansong",,,MVG-Net: LiDAR Point Cloud Semantic Segmentation Network Integrating Multi-View Images,,AUG 2024,0,"Deep learning techniques are increasingly applied to point cloud semantic segmentation, where single-modal point cloud often suffers from accuracy-limiting confusion phenomena. Moreover, some networks with image and LiDAR data lack an efficient fusion mechanism, and the occlusion of images may do harm to the segmentation accuracy of a point cloud. To overcome the above issues, we propose the integration of multi-modal data to enhance network performance, addressing the shortcomings of existing feature-fusion strategies that neglect crucial information and struggle with matching modal features effectively. This paper introduces the Multi-View Guided Point Cloud Semantic Segmentation Model (MVG-Net), which extracts multi-scale and multi-level features and contextual data from urban aerial images and LiDAR, and then employs a multi-view image feature-aggregation module to capture highly correlated texture information with the spatial and channel attentions of point-wise image features. Additionally, it incorporates a fusion module that uses image features to instruct point cloud features for stressing key information. We present a new dataset, WK2020, which combines multi-view oblique aerial images with LiDAR point cloud to validate segmentation efficacy. Our method demonstrates superior performance, especially in building segmentation, achieving an F1 score of 94.6% on the Vaihingen Dataset-the highest among the methods evaluated. Furthermore, MVG-Net surpasses other networks tested on the WK2020 Dataset. Compared to backbone network for single point modality, our model achieves overall accuracy improvement of 5.08%, average F1 score advancement of 6.87%, and mean Intersection over Union (mIoU) betterment of 7.9%.",multi-modal semantic segmentation,LiDAR point semantic segmentation,multi-view oblique aerial images,,,,,REMOTE SENSING,,,,,,,,,,,,
Row_1264,"Zhu, Zicong","Kang, Jian","Diao, Wenhui","Feng, Yingchao","Li, Junxi",SIRS: Multitask Joint Learning for Remote Sensing Foreground-Entity Image-Text Retrieval,,2024,1,"The essence of improving the effect of cross-modal image-text retrieval (CIR) lies in the finer-grained modeling of homogeneous features between modalities. However, in remote sensing (RS) scenarios, existing methods usually apply the image-sentence granular feature alignment paradigm, bringing significant difficulties to the fine-grained representation of homogeneous features between modalities. Besides, more complex background noise and extreme scale ranges of foreground targets are hard to distinguish, causing the feature mottle problem. To address the above issues, we propose a novel Semantic-guided Image-text Retrieval framework with Segmentation (SIRS). It is a multitask joint learning framework for plug-and-play and end-to-end training RS CIR models efficiently, including semantic-guided spatial attention (SSA) and adaptive multi-scale weighting (AMW) modules. First, SSA introduces a background reconstruction (BR) branch based on noise perception and a semantic segmentation (SS) branch based on pixel-level prediction. It explores a joint learning strategy that concisely filters background noise and refines foreground features considerably. Second, AMW performs multiscale weighting on various layers of feature map output by the encoder, effectively improving the learning efficiency of foreground targets at different scales. It is worth mentioning that SIRS outputs combination results with image and segmentation mask, which is not available in other methods. Based on the RSITMD dataset, we complete the SS annotation RSITMD-SS to verify the performance of the proposed method. Sufficient and complete experiments verify the effectiveness of the proposed method. With SIRS, the main-stream SVP and CLIP-based methods improve about 7 mR and derive segmentation prediction with acceptable computational cost optionally. The code and associated dataset will be available at https://github.com/StarBurstStream0/SIRS.",Cross-modal image-text retrieval (CIR),feature misalignment,foreground-entity granularity,multitask learning,remote sensing (RS),semantic segmentation (SS),,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Ni, Jingen",,,,,,,,,,,
Row_1265,"Liu, Zirong","Liu, Xinlong","Yu, Menghui","Yang, Xiaohong",,DMAM-UNET:AN IMPROVED UNET SEMANTIC SEGMENTATION FOR WATER BODY EXTRACTION FROM REMOTELY SENSED IMAGE,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"Water body extraction and mapping is an effective way for water resource management. With the development of remote sensing technology and the acquisition of remote sensing data, lots of methods were proposed for water body extraction. However, these existing methods were mainly used for large and continuous water and cannot get good accuracy when applied on broken and discontinuous water bodies. To address this problem, a double multiplication attention module UNet (DMAM-UNet) modified from the UNet network is proposed in this paper. DMAM-UNet mainly improves from UNet on network structure, loss function and data processing. In this study, four different types of broken and discontinuous water body extractions from Landsat were employed to test the performance of the DMAM- UNet method. The results of the four experiments showed that the proposed DMAM-UNet obtains better extraction accuracy than the two other related algorithms.",water extraction,remote sensing,deep learning,UNet,,,,,,,,,,,,,,,,
Row_1266,"Liu, Yang","Li, Qingyong","Li, Xiaobao","He, Shuyi","Liang, Fengjiao",Leveraging Physical Rules for Weakly Supervised Cloud Detection in Remote Sensing Images,,2023,3,"Cloud detection plays a significant role in remote sensing (RS) image applications. Existing deep learning-based cloud detection methods rely on massive precise pixelwise annotations, which are time-consuming and expensive. To alleviate this problem, we propose a weakly supervised cloud detection framework that leverages physical rules to generate weak supervision for cloud detection in RS images. Specifically, a rule-based adaptive pseudo labeling (RAPL) algorithm is devised to adaptively annotate potential cloud pixels based on cloud spectral properties without manual intervention. Unlike existing physical annotations using fixed thresholds, RAPL employs the bidirectional threshold segmentation and adaptive gating mechanism to annotate cloud and boundary masks with more explicit semantic categories and spatial structures separately. Subsequently, these pseudo masks are treated as weak supervision to optimize the heuristic cloud detection network for pixelwise segmentation. Considering that clouds appear as complex geometric structures and nonuniform spectral reflectance, a deformable boundary refining module is designed to enhance the modeling ability of spatial transformation and activate sharp boundaries from translucent cloud regions. Moreover, a harmonic loss is employed to recognize clouds with nonuniform spectral reflectance and suppress the interference of bright backgrounds. Extensive experiments on the GF-1, L8 Biome, and weakly supervised cloud detection (WDCD) datasets demonstrate that the proposed method achieves state-of-the-art results. A public reference implementation of this work in PyTorch is available at https://github.com/NiAn-creator/HeuristicCloudDetection.",Cloud detection,convolutional neural network (CNN),remote sensing (RS) image,weakly supervised semantic segmentation,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Yao, Zhigang",,,,,"Jiang, Jun","Wang, Wen",,,,,
Row_1267,"Zhu, Qing","Liao, Cheng","Hu, Han","Mei, Xiaoming","Li, Haifeng",MAP-Net: Multiple Attending Path Neural Network for Building Footprint Extraction From Remote Sensed Imagery,,JUL 2021,185,"Building footprint extraction is a basic task in the fields of mapping, image understanding, computer vision, and so on. Accurately and efficiently extracting building footprints from a wide range of remote sensed imagery remains a challenge due to the complex structures, variety of scales, and diverse appearances of buildings. Existing convolutional neural network (CNN)-based building extraction methods are criticized for their inability to detect tiny buildings because the spatial information of CNN feature maps is lost during repeated pooling operations of the CNN. In addition, large buildings still have inaccurate segmentation edges. Moreover, features extracted by a CNN are always partially restricted by the size of the receptive field, and large-scale buildings with low texture are always discontinuous and holey when extracted. To alleviate these problems, multiscale strategies are introduced in the latest research works to extract buildings with different scales. The features with higher resolution generally extracted from shallow layers, which extracted insufficient semantic information for tiny buildings. This article proposes a novel multiple attending path neural network (MAP-Net) for accurately extracting multiscale building footprints and precise boundaries. Unlike existing multiscale feature extraction strategies, MAP-Net learns spatial localization-preserved multiscale features through a multiparallel path in which each stage is gradually generated to extract high-level semantic features with fixed resolution. Then, an attention module adaptively squeezes the channel-wise features extracted from each path for optimized multiscale fusion, and a pyramid spatial pooling module captures global dependence for refining discontinuous building footprints. Experimental results show that our method achieved 0.88%, 0.93%, and 0.45% F1-score and 1.53%, 1.50%, and 0.82% intersection over union (IoU) score improvements without increasing computational complexity compared with the latest HRNetv2 on the Urban 3-D, Deep Globe, and WHU data sets, respectively. Specifically, MAP-Net outperforms multiscale aggregation fully convolutional network (MA-FCN), which is the state-of-the-art (SOTA) algorithms with postprocessing and model voting strategies, on the WHU data set without pretraining and postprocessing. The TensorFlow implementation is available at https://github.com/lehaifeng/MAPNet.",Feature extraction,Buildings,Semantics,Data mining,Spatial resolution,Remote sensing,Convolution,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Attention mechanism,building footprint extraction,deep learning,remote sensing imagery,,,semantic segmentation,,,,
Row_1268,"Zhang, Xiaokang","Zhang, Boning","Yu, Weikang","Kang, Xudong",,Federated Deep Learning With Prototype Matching for Object Extraction From Very-High-Resolution Remote Sensing Images,,2023,15,"Deep convolutional neural networks (DCNNs) have become the leading tools for object extraction from very-high-resolution (VHR) remote sensing images. However, the label scarcity problem of local datasets hinders the prediction performances of DCNNs, and privacy concerns regarding remote sensing data often arise in the traditional deep learning schemes. To cope with these problems, we propose a novel federated learning scheme with prototype matching (FedPM) to collaboratively learn a richer DCNN model by leveraging remote sensing data distributed among multiple clients. This scheme conducts the federated optimization of DCNNs by aggregating clients' knowledge in the gradient space without compromising data privacy. Specifically, the prototype matching method is developed to regularize the local training using prototypical representations while reducing the distribution divergence across heterogeneous image data. Furthermore, the derived deviations across local and global prototypes are applied to quantify the effects of local models on the decision boundary and optimize the global model updating via the attention-weighted aggregation scheme. Finally, the sparse ternary compression (STC) method is used to alleviate communication costs. Extensive experimental results derived from VHR aerial and satellite image datasets verify that the FedPM can dramatically improve the prediction performance of DCNNs on object extraction with lower communication costs. To the best of our knowledge, this is the first time that federated learning has been applied for remote sensing visual tasks.",Training,Federated learning,Data models,Remote sensing,Feature extraction,Prototypes,Deep learning,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,federated learning,object extraction,remote sensing images,semantic segmentation,,,,,,,
Row_1269,"Li, Mengmeng","Long, Jiang","Stein, Alfred","Wang, Xiaoqin",,Using a semantic edge-aware multi-task neural network to delineate agricultural parcels from remote sensing images,,JUN 2023,27,"This paper presents a semantic edge-aware multi-task neural network (SEANet) to obtain closed boundaries when delineating agricultural parcels from remote sensing images. It derives closed boundaries from remote sensing images and improves conventional semantic segmentation methods for the extraction of small and irregular agricultural parcels. SEANet integrates three correlated tasks: mask prediction, edge prediction, and distance map estimation. Related features learned from these tasks improve the generalizability of the network. We regard boundary extraction as an edge detection task and extract rich semantic edge features at multiple levels to improve the geometric accuracy of parcel delineation. Moreover, we develop a new multi-task loss that considers the uncertainty of different tasks. We conducted experiments on three high-resolution Gaofen-2 images in Shandong, Xinjiang, and Sichuan provinces, China, and on two medium-resolution Sentinel-2 images from Denmark and the Netherlands. Results showed that our method produced a better layout of agricultural parcels, with higher attribute and geometric accuracy than the existing ResUNet, ResUNet-a, R2UNet, and BsiNet methods on the Shandong and Denmark datasets. The total extraction errors of the parcels produced by our method were 0.214, 0.127, 0.176, 0.211, and 0.184 for the five datasets, respectively. Our method also obtains closed boundaries by one single segmentation, leading to superiority as compared with existing multi-task networks. We showed that it could be applied to images with different spatial resolutions for parcel delineation. Finally, our method trained on the Xinjiang dataset could be successfully transferred to the Shandong dataset with different dates and landscapes. Similarly, we obtained satisfactory results when transferring from the Denmark dataset to the Netherlands dataset. We conclude that SEANet is an accurate, robust, and transferable method for various areas and different remote sensing images. The codes of our model are available at htt ps://github.com/long123524/SEANet_torch.",Agricultural parcel delineation,SEANet,Multi-task neural networks,Semantic edge-aware detection,Uncertainty weighted loss,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,
Row_1270,"Sun, Ying","Zhang, Xinchang","Xin, Qinchuan","Huang, Jianfeng",,Developing a multi-filter convolutional neural network for semantic segmentation using high-resolution aerial imagery and LiDAR data,,SEP 2018,81,"Semantic segmentation of LiDAR and high-resolution aerial imagery is one of the most challenging topics in the remote sensing domain. Deep convolutional neural network (CNN) and its derivatives have recently shown the abilities in pixel-wise prediction of remote sensing data. Many existing deep learning methods fuse LiDAR and high-resolution aerial imagery towards an inter-modal mode and thus overlook the intra-modal statistical characteristics. Additionally, the patch-based CNNs could generate the salt-and-pepper artifacts as characterized by isolated and spurious pixels on the object boundaries and patch edges leading to unsatisfied labelling results. This paper presents a semantic segmentation scheme that combines multi-filter CNN and multi-resolution segmentation (MRS). The multi-filter CNN aggregates LiDAR data and high-resolution optical imagery by multi modal data fusion for semantic labelling, and the MRS is further used to delineate object boundaries for reducing the salt-and-pepper artifacts. The proposed method is validated against two datasets: the ISPRS 2D semantic labelling contest of Potsdam and an area of Guangzhou in China labelled based on existing geodatabases. Various designs of data fusion strategy, CNN architecture and MRS scale are analyzed and discussed. Compared with other classification methods, our method improves the overall accuracies. Experiment results show that our combined method is an efficient solution for the semantic segmentation of LiDAR and high-resolution imagery.",LiDAR,High-resolution imagery,Multi-modal fusion,Multi-resolution segmentation,Semantic segmentation,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,
Row_1271,"Zhang, Ji Yong","Li, De Guang","Wu, Lin Li","Shi, Xin Yao","Wang, Bo",A lightweight self-supervised learning segmentation model for variable and complex high-resolution remote sensing images,,NOV 2024,0,"The complexity and variability of high-resolution remote sensing data, such as high intra-class variability and inter-class similarity, pose significant challenges to model segmentation. To address the problem, this paper constructs a lightweight self-supervised learning model for multi-label segmentation in the form of phased learning of multi-scale features. The model adopts axial depthwise separable convolutions to reduce computational complexity and enhance feature representation, utilizes dilated rates to capture large-scale and multi-scale contextual information for long-distance feature extraction, and incorporates convolution kernels of varying sizes to acquire both local and global feature information for the improved ability of learning feature representation. The experimental results show that our model achieves competitive performance and has smaller weight parameters, memory usage, and lower computational complexity compared with existing classical models that rely on large-scale weight parameters. Additionally, our ablation study delves into the encountered design issues to elucidate the rationality of our approach. The source code is avaiable: https://github.com/zhangjy2008327/remote-sensing-images.",Remote sensing,Self-supervised learning,Multi-scale contextual information,Computational complexity,,,,APPLIED SOFT COMPUTING,,,,,,,,,,,,
Row_1272,"Chen, Kaiqiang","Fu, Kun","Sun, Xian","Weinmann, Michael","Hinz, Stefan",DEEP SEMANTIC SEGMENTATION OF AERIAL IMAGERY BASED ON MULTI-MODAL DATA,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2018,4,"In this paper, we focus on the use of multi-modal data to achieve a semantic segmentation of aerial imagery. Thereby, the multi-modal data is composed of a true orthophoto, the Digital Surface Model (DSM) and further representations derived from these. Taking data of different modalities separately and in combination as input to a Residual Shuffling Convolutional Neural Network (RSCNN), we analyze their value for the classification task given with a benchmark dataset. The derived results reveal an improvement if different types of geometric features extracted from the DSM are used in addition to the true orthophoto.",Semantic segmentation,aerial imagery,multi-modal data,deep learning,Shuffling-CNN,,,,"Jutzi, Boris",,,,,"Weinmann, Martin",,,,,,
Row_1273,"Du, Xianjun","Wu, Hailei",,,,Gated aggregation network for cloud detection in remote sensing image,,APR 2024,5,"Cloud detection is one of the important tasks in remote sensing image preprocessing, and this paper uses RGB preview images of remote sensing images to extract cloud regions efficiently. The preview images make the detection of cloud regions more challenging due to the lack of resolution and spectral information. The existing remote sensing image cloud detection methods, and feature fusion process, due to unreasonable feature fusion strategy, so that the encoded features cannot be fully utilized, which may introduce noise information, and ultimately lead to the problems of false detection and missing detection. To address these problems, this work designs a gated aggregation network (GANet) for remote sensing image cloud detection. GANet has a novel encoder-decoder architecture, a gated feature aggregation module (GFAM), and a pyramidal attention pooling module (PAPM). GFAM bridges the gap between high resolution with spatial details and low-resolution features with high-level semantics, fully selectively fusing semantic and spatial features to alleviate the semantic divide problem when fusing multi-level features. PAPM extracts multi-scale global contextual features without loss of resolution. The method is validated on three datasets: the publicly available 38-Cloud and SPARCS datasets and the self-built Landsat-8 cloud detection dataset with higher spatial resolution. The experimental results show that the proposed method achieves competitive performance under different evaluation metrics. Codes and datasets can be found at https://github.com/HaiLei-Fly/GANet/.",Remote sensing image,Image segmentation,Gated aggregation,Interpretability,,,,VISUAL COMPUTER,,,,,,,,,,,,
Row_1274,"Tian, Yu","Luo, Muying","Wang, Shaoyi","Ji, Shunping",,An interactive method for bridging the gap between deep learning based building contour segmentation and manual annotation,,APR 17 2024,1,"Although the emergence of deep learning has improved the performance of automatic building extraction, there is still a long way to go before it can completely replace the labour-intensive manual delineation of building contours. To narrow the gap between building extraction results of deep learning methods and manual-level annotations, this paper introduces an interactive semantic segmentation framework that uses manual clicks as interactive information to guide the process of semantic segmentation towards the manual annotation level. In our framework, we first use an interactive semantic segmentation network for coarse building extraction from high resolution remote sensing images. Then, we use an optimization network to further refine the extraction results. We comprehensively compare the automatic deep learning methods, the proposed interactive building extraction framework, and the full manual delineation in practical experimental settings. First, our interactive method can significantly improve the performance of automatic building extraction. Second, by comparing the efficiency of manual annotation using the ArcGIS software and our interactive method, it is found that the proposed method can save more than half of the time. The above results show the potential of the interactive method in improving the efficiency of contour annotation and reducing the cost of manual annotation.",Building extraction,interactive learning,semantic segmentation,ArcGIS,high resolution remote sensing images,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1275,"Tian, Shiqi","Zhong, Yanfei","Zheng, Zhuo","Ma, Ailong","Tan, Xicheng",Large-scale deep learning based binary and semantic change detection in ultra high resolution remote sensing imagery: From benchmark datasets to urban application,,NOV 2022,50,"With the acceleration of urban expansion, urban change detection (UCD), as a significant and effective approach, can provide the change information with respect to geospatial objects for dynamic urban analysis. In recent years, through the use of machine learning and artificial intelligence, change detection methods have gradually developed from the traditional pixel-based comparison methods in the 1980s to data-driven deep learning methods. Deep learning methods have huge advantages in the application of remote sensing big data, by virtue of their huge feature extraction and expression capabilities. Many change detection datasets have been released to meet the requirements of deep learning. However, the existing datasets suffer from three bottlenecks: (1) the volume of the datasets is small, which can easily cause overfitting; (2) most datasets have a spatial resolution of meters, making it difficult to detect changes in small objects because there are multi-scale objects in urban areas; and (3) most of the datasets have been designed for binary change detection (BCD), and lack semantic annotation, so that they cannot be used to obtain the direction of the change or to analyze the type of change, for further application in urban areas. Therefore, it is difficult to apply these datasets to detect large-scale urban semantic changes in complex environments. To address these issues, a large-scale ultra high resolution (0.1 m) UCD dataset for deep learning based BCD and semantic change detection (SCD) is introduced in this article, which is named the Hi-UCD dataset. We selected an area of 102 m2 in Tallinn, the capital of Estonia, as the study area. There are a total of 40800 pairs of 512 x 512 patches, nine types of land cover and 48 types of semantic change in the Hi-UCD dataset. We developed three metrics-binary consistency, change area consistency and no-change area consistency-to evaluate the semantic consistency of the SCD methods from different aspects. A comprehensive analysis and investigation is provided in this article after we benchmarked this dataset using deep learning methods for BCD and SCD. We also found that the unchanged samples can help distinguish the changed area, and HRNet used as the backbone to construct a multi-task model can perform well in the Hi-UCD dataset. Meanwhile, the visualization results obtained with the Hi-UCD test set, which is a large geographic area covering 54 km2, are shown to reflect the real-world urban application scenarios. The experimental results show that the Hi-UCD dataset is a challenging yet useful benchmark dataset, which can be used for analyzing large-scale refined urban changes.",Ultra high resolution,Semantic change detection,Deep learning,Remote sensing,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,"Zhang, Liangpei",,,,,,,,,,,
Row_1276,"Yu, Mouzhe","He, Liheng","Shen, Zhehui","Lv, Meng",,STRD-Net: A Dual-Encoder Semantic Segmentation Network for Urban Green Space Extraction,,2024,0,"Urban green spaces significantly influence the production and lifestyle of individuals. Deep learning methods using convolutional neural network (CNN) as the encoder have weak global feature extraction capabilities, often missing individual trees or small areas of low vegetation. Transformer series models have weak local feature extraction capabilities and perform poorly in distinguishing between small categories such as trees and low vegetation. Therefore, we propose a novel dual-encoder semantic segmentation model, swin transformer and resnet50 dual-encoder net (STRD-Net), which integrates a parallel swin transformer (ST) framework and a CNN framework, capable of accepting two different channel ratio images as input, enabling the model to capture both global and local features. In the ST encoder, a convolutional block attention module (CBAM) is added to the head to overcome the ""salt-and-pepper"" noise effect in extraction results. A new patch merging (NPM) module is added after each ST module to further enhance the local feature extraction capabilities of the ST encoder for urban green spaces. In the CNN encoder, an enhanced atrous spatial pyramid pooling (EASPP) module is added after the Resnet50 backbone extraction network to expand the receptive field of the CNN encoder and enhance the global feature extraction capabilities for urban green spaces. The model includes a single skip connection to ensure extraction accuracy while saving computational resources. Results on the Vaihingen and Potsdam datasets indicate that STRD-Net improves both local and global feature extraction capabilities in the extraction of urban green spaces. The code will be available at https://github.com/learn-zhezhe/STRD-Net.",Feature extraction,Remote sensing,Transformers,Semantic segmentation,Vegetation mapping,Normalized difference vegetation index,Convolutional neural networks,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Dual-encoder,land object extraction,remote sensing images,semantic segmentation,,,urban green spaces,,,,
Row_1277,"Yang, Jingyu","Gu, Zongliang","Wu, Ting","Ahmed, Yousef Ameen Esmail",,RUW-Net: A Dual Codec Network for Road Extraction From Remote Sensing Images,,2024,3,"Road information plays an increasingly important role in applications, such as map updating, urban planning, and intelligent supervision. However, roads in remote sensing images may be shaded by trees and buildings or interfered with by farmland. These intrinsic image features can cause road extraction results to suffer from breakage and misidentification problems. To address these problems, this article improves on D-LinkNet and proposes a dual codec structure network, namely RUW-Net. Specifically, we use ReSidual U-blocks instead of ordinary residual blocks to extract more global contextual information during the encoding stage. Moreover, we propose a decoder-encoder combination (DEC) module to build a dual codec structure. The DEC module links the decoder of the first U-block and the encoder of the following U-block to narrow the semantic gap in the encoding and decoding process. The RUW-Net model can extract more multiscale contextual features and effectively use them to enhance the semantic information of road entities. Therefore, the RUW-Net model can obtain more accurate extraction results. We conducted a series of experiments on public datasets, such as DeepGlobe, including comparative, robustness, and ablation experiments. The results show that the proposed model alleviates the road extraction breakage and misidentification problems. Compared with other representative methods, the RUW-Net performs better in terms of completeness and accuracy of road extraction results; overall, its extraction results are also the best. The RUW-Net model provides a new idea for road extraction from remote sensing images.",Multiscale feature,remote sensing (RS) image,road extraction,semantic segmentation,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,
Row_1278,"Lu, Wen","Zhang, Zhiqi","Nguyen, Minh",,,A Lightweight CNN-Transformer Network With Laplacian Loss for Low-Altitude UAV Imagery Semantic Segmentation,,2024,4,"Semantic segmentation is crucial for enabling autonomous flight and landing of low-altitude unmanned aerial vehicles (UAVs) and is indispensable for various intelligent applications. However, real-time semantic segmentation is a computationally intensive task because it involves pixel-wise classification, which renders conventional semantic segmentation networks impractical for deployment on embedded systems of limited hardware resources. Moreover, variations in flight height and object appearance increase the likelihood of misjudgment in segmentation results. To address these challenges, we propose an efficient approach consisting of a convolutional neural network (CNN)-Transformer network and an auxiliary loss. The encoder of the network integrates a newly designed module, which equally handles objects with varying scales. The decoder is composed of the innovative query-value squeeze axial transformer attention (QVSATA), which reduces computational complexity from quadratic in terms of image size to O(2C(H-2+W-2)), linear in terms of image size. By incorporating Laplacian operator convolution, the novel network-agnostic loss effectively captures intricate patterns, boundaries, and small objects. This enables extra penalization of misjudgments in these areas and compels the network to focus on objects that are challenging to distinguish. Our approach attains impressive accuracy when processing 4K resolution images in real time (15 FPS) on a mobile GPU. It demonstrates over 2x faster speed compared to representative lightweight networks, underscoring its suitability for onboard deployment.",Lightweight neural network,remote sensing,semantic segmentation,unmanned aerial vehicle (UAV),,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1279,"Rim, Beanbonyka","Lee, Ahyoung","Hong, Min",,,Semantic Segmentation of Large-Scale Outdoor Point Clouds by Encoder-Decoder Shared MLPs with Multiple Losses,,AUG 2021,14,"Semantic segmentation of large-scale outdoor 3D LiDAR point clouds becomes essential to understand the scene environment in various applications, such as geometry mapping, autonomous driving, and more. With an advantage of being a 3D metric space, 3D LiDAR point clouds, on the other hand, pose a challenge for a deep learning approach, due to their unstructured, unorder, irregular, and large-scale characteristics. Therefore, this paper presents an encoder-decoder shared multi-layer perceptron (MLP) with multiple losses, to address an issue of this semantic segmentation. The challenge rises a trade-off between efficiency and effectiveness in performance. To balance this trade-off, we proposed common mechanisms, which is simple and yet effective, by defining a random point sampling layer, an attention-based pooling layer, and a summation of multiple losses integrated with the encoder-decoder shared MLPs method for the large-scale outdoor point clouds semantic segmentation. We conducted our experiments on the following two large-scale benchmark datasets: Toronto-3D and DALES dataset. Our experimental results achieved an overall accuracy (OA) and a mean intersection over union (mIoU) of both the Toronto-3D dataset, with 83.60% and 71.03%, and the DALES dataset, with 76.43% and 59.52%, respectively. Additionally, our proposed method performed a few numbers of parameters of the model, and faster than PointNet++ by about three times during inferencing.",semantic segmentation,3D LiDAR point clouds,deep learning,remote sensing,,,,REMOTE SENSING,,,,,,,,,,,,
Row_1280,"Ramos, Leo Thomas","Sappa, Angel D.",,,,Multispectral Semantic Segmentation for Land Cover Classification: An Overview,,2024,1,"Land cover classification (LCC) is a process used to categorize the earth's surface into distinct land types. This classification is vital for environmental conservation, urban planning, agricultural management, and climate change research, providing essential data for sustainable decision making. The use of multispectral imaging (MSI), which captures data beyond the visible spectrum, has emerged as one of the most utilized image modalities for addressing this task. In addition, semantic segmentation techniques play a vital role in this domain, enabling the precise delineation and labeling of land cover classes within imagery. The integration of these three concepts has given rise to an intriguing and ever-evolving research field, witnessing continuous advancements aimed at enhancing multispectral semantic segmentation (MSSS) methods for LCC. Given the dynamic nature of this field, there is a need for a thorough examination of the latest trends and advancements to understand its evolving landscape. Therefore, this article presents a review of current aspects in the field of MSSS for LCC, addressing the following key points: 1) prevalent datasets and data acquisition methods; 2) preprocessing methods for managing MSI data; 3) typical metrics and evaluation criteria used for assessing performance of methods; 4) current techniques and methodologies employed; and 5) spectral bands beyond the visible spectrum commonly utilized. Through this analysis, our objective is to provide valuable insights into the current state of MSSS for LCC, contributing to the ongoing development and understanding of this dynamic field while also providing perspectives for future research directions.",Computer vision (CV),deep learning (DL),image segmentation,land cover classification (LCC),multispectral imaging (MSI),semantic segmentation,remote sensing,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,satellite imagery,deep learning (DL),image segmentation,land cover classification (LCC),,,multispectral imaging (MSI),semantic segmentation,remote sensing,satellite imagery,
Row_1281,"Wang, Zhewei","Pan, Zongxu","Long, Hui","Hu, Yuxin",,Enhanced Multitask Semantic Change Detection via Semi-Supervised Learning in LULC Segmentation Subtask,,2024,0,"Change detection (CD) plays a crucial role in remote sensing analysis. Semantic change detection (SCD) further expands CD by incorporating land use and land cover (LULC) segmentation before and after the changes, identifying specific change categories alongside the change area detection. While recent studies combining change area detection and bitemporal LULC segmentation within a multitask framework demonstrate promising performance, they often only utilize pixels in changed areas with change classification labels in LULC segmentation training, overlooking substantial unlabeled LULC data in unchanged areas, which restricts the model's effectiveness. Accordingly, we propose an enhanced multitask SCD method with semi-supervised learning in LULC segmentation, effectively leveraging the extensive unlabeled LULC data and improving the overall performance of the multitask framework. Besides, we introduce a novel loss tailored for this semi-supervised method based on the unique relationship between bitemporal pixel labels in change areas and change classification. Optimization of the semi-supervised loss weighting further refines the training. Experiments on the public dataset validate the effectiveness of these improvements, especially in enhancing change classification performance. Applying our method to the naive model yields improvements in SeK and Fscd, with the increases of up to 2%. The code will be available after the acceptance at https://github.com/ijnokml/scd-enhanced.",Multitask learning,remote sensing images,semantic change detection (SCD),semi-supervised learning,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,
Row_1282,"Guo, Bo","Zhang, Jian","Li, Xu",,,River Extraction Method of Remote Sensing Image Based on Edge Feature Fusion,,2023,5,"The extraction of rivers from remote sensing images is crucial for urban planning and water resource utilization. To address the low accuracy of traditional methods for extracting rivers from remote sensing images in complex scenes, a novel method based on edge feature fusion is proposed. As the input item of Learning Vector Quantization (LVQ), the preprocessed the remote sensing image is fed into four traditional edge detection algorithms for processing, and a finer edge image is obtained. Furthermore, VGG16 and ResNet50 models are utilized as feature extractors for transfer learning. The scale feature output layer and the semantic feature output layer are introduced to construct a Two-Branch Fusion (TBF) model. The scale feature output layer of the model makes full use of the rich convolution features of VGG16, and optimizes the training steps via collaborative loss and deep supervision. The semantic feature output layer extracts the residual maps matched by the ResNet50's different scale convolution layers, and employs edge features to refine the segmentation boundary, thereby sharpening predictions at the edges. Moreover, the TBF model effectively integrates multi-scale edge information, thus improving the prediction accuracy of the model. The proposed model is tested with SVM, FCN, and UNet on the remote sensing river image dataset. The results show that the proposed model has greater PA (Pixel Accuracy), Pre (Precision), and mIoU (mean Intersection over Union) than other models. The test results indicate that the proposed model's river segmentation effect is more refined. When applying this method to the Inria Aerial Image Labeling Dataset and Pascal VOC2012 Dataset, the model's generalization ability is validated, and its evaluation index and segmentation effect are superior to those of other models.",Edge feature fusion,river extraction,remote sensing image,,,,,IEEE ACCESS,,,,,,,,,,,,
Row_1283,"Lu, Wei","Chen, Si-Bao","Tang, Jin","Ding, Chris H. Q.","Luo, Bin",A Robust Feature Downsampling Module for Remote-Sensing Visual Tasks,,2023,6,"Remote-sensing (RS) images present unique challenges for computer vision (CV) due to lower resolution, smaller objects, and fewer features. Mainstream backbone networks show promising results for traditional visual tasks. However, they use convolution to reduce feature map dimensionality, which can result in information loss for small objects in RS images and decreased performance. To address this problem, we propose a new and universal downsampling module named robust feature downsampling (RFD). RFD fuses multiple feature maps extracted by different downsampling techniques, creating a more robust feature map with a complementary set of features. Leveraging this, we overcome the limitations of conventional convolutional downsampling, resulting in a more accurate and robust analysis of RS images. We develop two versions of the RFD module, shallow RFD (SRFD) and deep RFD (DRFD), tailored to adapt to different stages of feature capture and improve feature robustness. We replace the downsampling layers (DSL) of existing mainstream backbones with the RFD module and conduct comparative experiments on several public RS image datasets. The results show significant improvements compared to baseline approaches in RS image classification, object detection, and semantic segmentation. Specifically, our RFD module achieved an average performance gain of 1.5% on the NWPU-RESISC45 classification dataset without utilizing any additional pretraining data, resulting in state-of-the-art performance on this dataset. Moreover, in detection and segmentation tasks on dataset for object detection in aerial images (DOTA) and instance segmentation in aerial images dataset (iSAID), our RFD module outperforms the baseline approaches by 2%-7% when utilizing pretraining data from NWPU-RESISC45. These results highlight the value of the RFD module in enhancing the performance of RS visual tasks. The code is available at https://github.com/lwCVer/RFD.",Classification,detection,feature downsample,remote sensing (RS),segmentation,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1284,"Feng, Yingchao","Sun, Xian","Diao, Wenhui","Li, Jihao","Gao, Xin",Continual Learning With Structured Inheritance for Semantic Segmentation in Aerial Imagery,,2022,29,"With the rapid update and iteration of current aerial image data, the continual learning scenarios and catastrophic forgetting problem attracted increased attention, especially in the semantic segmentation task. However, the existing methods mainly focus on the class continual learning in a single task and are not satisfactory when extended to multiple tasks. In this article, we consider more realistic and complicated settings, namely task continual learning. We revisit the characteristics of semantic segmentation and knowledge distillation (KD) strategy, then propose a general and effective framework, named structured inheritance, to learn new tasks while retaining high performance on old tasks. Specifically, we present two structure-preserving penalties: pixel affinity structure loss and representation consistency structure loss. The former breaks the isolation of pixels and retains the pixel interactive information learned by the old tasks. At the same time, the latter protects high-frequency stationary information between sequence semantic segmentation tasks. Our approach does not need to add extra parameters nor does it need to access the data stream of the old tasks. Therefore, it can be applied in practical applications with strict computational burden, memory cost, and storage budget. Extensive continual learning experiments on four semantic segmentation datasets of Vaihingen, Potsdam, DeepGlobe, and Gaofen challenge semantic segmentation dataset (GCSS) prove the effectiveness of our proposed framework, which outperforms the current state-of-the-art methods and even exceeds the theoretical upper-bound performance of multitask learning. The code and models will be made publicly available.",Task analysis,Semantics,Image segmentation,Remote sensing,Training,Neural networks,Image color analysis,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Fu, Kun",Continual learning,deep convolution neural network,remote sensing,semantic segmentation,,,,,,,
Row_1285,"Huang, Jianjun","Xu, Jindong","Chong, Qianpeng","Li, Ziyi",,BLACK AND ODOROUS WATER DETECTION OF GAOFEN-2 REMOTE SENSING IMAGES BASED ON DEEP LEARNING,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"Black and odorous water seriously affects the ecological balance of rivers and the health of people. Satellite remote sensing technology with its advantages of large range, long time series, low cost, and high efficiency, has provided a new area for water quality detection. In this paper, Gaofen-2 remote sensing data with a spatial resolution of 1 m is leveraged as the data source. We build a high-quality remote sensing image dataset to enrich the data source in the northern coastal zone of China. In addition, we propose a network with an encoder-decoder discriminant structure for black and odorous water detection. In the network, an augmented attention module is designed to capture more comprehensive black and odorous water semantic information. Further, the new loss function is adopted to solve the class imbalance. Experimental results demonstrate that the network is superior to other state-of-theart semantic segmentation methods on our dataset.",Black and odorous water,remote sensing image,convolutional neural network,deep learning,,,,,,,,,,,,,,,,
Row_1286,"Hu, Qiongqiong","Wang, Feiting","Wu, Yuechao","Li, Ying",,U-ONet: Remote sensing image semantic labelling based on octave convolution and coordination attention in U-shape deep neural network,,OCT 2024,0,"Semantic labelling of remote sensing images is crucial for various remote sensing applications. However, the dense distribution of man-made and natural objects with similar colours and geographic proximity poses challenges for achieving consistent and accurate labelling results. To address this issue, a novel deep learning model incorporating an octave convolutional neural networks (CNNs) within an end-to-end U-shaped architecture is presented. The approach differs from conventional CNNs in that it employs octave convolutions instead of standard convolutions. This strategy serves to minimize low-frequency information redundancy while maintaining segmentation accuracy. Furthermore, coordination attention is introduced in the encoder module to enhance the network's ability to extract useful features, focusing on spatial and channel dependencies within the feature maps. This attention mechanism enables the network to better capture channel, direction, and location information. In conclusion, the U-shaped network is engineered with a completely symmetric structure that employs skip connections to merge low-resolution information, used for object class recognition, with high-resolution information to enable precise localization. This configuration ultimately improves segmentation accuracy. Experimental results on two public datasets demonstrate that our U-ONet achieves state-of-the-art performance, making it a compelling choice for remote sensing image semantic labelling applications.Semantic labelling of remote sensing images is crucial but challenging due to complex object distributions. Our U-ONet model, with octave convolutions and coordination attention, achieves state-of-the-art performance by enhancing feature extraction and precise localization, making it an ideal choice for accurate segmentation tasks. image",feature extraction,geophysical image processing,image segmentation,,,,,ELECTRONICS LETTERS,,,,,,,,,,,,
Row_1287,"Mou, Lichao","Hua, Yuansheng","Zhu, Xiao Xiang",,,SPATIAL RELATIONAL REASONING IN NETWORKS FOR IMPROVING SEMANTIC SEGMENTATION OF AERIAL IMAGES,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),2019,7,"Most current semantic segmentation approaches rely on deep convolutional neural networks (CNNs). However, their use of convolution operations with local receptive fields causes failures in modeling contextual spatial relations. Prior works have tried to address this issue by using graphical models or spatial propagation modules in networks. But such models often fail to capture long-range spatial relationships between entities, which leads to spatially fragmented predictions. In this work, we introduce a simple yet effective network unit, the spatial relation module, to learn and reason about global relationships between any two spatial positions, and then produce relation-enhanced feature representations. The spatial relation module is general and extensible, and can be used in a plug-and-play fashion with the existing fully convolutional network (FCN) framework. We evaluate spatial relation module-equipped networks on semantic segmentation tasks using two aerial image datasets. The networks achieve very competitive results, bringing significant improvements over baselines.",Relation network,fully convolutional network,semantic segmentation,aerial imagery,,,,,,,,,,,,,,,,
Row_1288,"Wang, Rui","Cai, Mingxiang","Xia, Zixuan","Zhou, Zhicui",,Remote Sensing Image Road Segmentation Method Integrating CNN-Transformer and UNet,,2023,1,"Real-time and accurate road information is crucial for updating electronic navigation maps. To address the problem of low precision and poor robustness in current semantic segmentation methods for road extraction from remote sensing imagery, we proposed a UNet road semantic segmentation model based on attention mechanism improvement. First, we introduce a CNN-Transformer hybrid structure to the encoder to enhance the feature extraction capabilities of global and local details. Second, the traditional upsampling module in the decoder is replaced with a dual upsampling module to improve feature extraction capabilities and segmentation accuracy. Furthermore, the hard-swish activation function is used instead of ReLU activation function to smooth the curve, which helps to improve the generalization and non-linear feature extraction abilities and avoid gradient vanishing. Finally, a comprehensive loss function combining cross entropy and dice is used to strengthen the segmentation result constraints and further improve segmentation accuracy. Experimental validation is performed on the Ottawa Road Dataset and the Massachusetts Road Dataset. Experimental results show that compared with U-Net, PSPNet, DeepLab V3 and TransUNet networks, this algorithm is the best in terms of MIoU, MPA and F1 score. Among them, on the Ottawa road data set, the MPA of this algorithm reached 95.48%. On the Massachusetts road data set, MPA is 92.56%. This method shows good performance in road extraction.",Road segmentation,deep learning,CNN-transformer,attention,UNet,,,IEEE ACCESS,,,,,,,,,,,,
Row_1289,"Afsar, Rayan","Sultana, Aqsa","Abouzahra, Shaik N.","Aspiras, Theus","Asari, Vijayan K.",Using ResWnet for semantic segmentation of active wildfires from Landsat-8 imagery,PATTERN RECOGNITION AND PREDICTION XXXV,2024,0,"Wildfires are a key aspect of many ecosystems, but climate change has created conditions more conducive for devastating wildfires. Thus, it is imperative that relevant agencies know where small fires occur expeditiously. Remote sensing is a key tool for active fire detection (AFD), and satellite imagery in particular is useful due to covering wide areas. Semantic segmentation architectures like U-Net have been used for AFD and have proven very effective. In this paper, we apply a unique variant of U-Net called ResWnet towards AFD, using a large global dataset. ResWnet achieved a precision of 95% and an F-Score of 94.2%, which is better than a U-Net trained on the same dataset.",Semantic segmentation,U-Net,ResWnet,deep learning,convolutional neural networks,fire,wildfire detection,,,Landsat-8,remote sensing,,,,,,,,,
Row_1290,"Wang, Shuyang","Mu, Xiaodong","Yang, Dongfang","He, Hao","Zhao, Peng",Attention Guided Encoder-Decoder Network With Multi-Scale Context Aggregation for Land Cover Segmentation,,2020,7,"Land cover segmentation is an important and challenging task in the field of remote sensing. Even though convolutional neural networks (CNNs) provide great support for semantic segmentation, standard models are still difficult to capture global information and long-range dependencies in remote sensing images. To overcome these limitations, we proposed an attention guided encoder-decoder network with multi-scale context aggregation to achieve more accurate segmentation of land cover. Based on the structure of the encoder-decoder network, we introduce a multi-scale feature fusion module with two attention modules to the top of the encoder. The multi-scale feature fusion module is employed to aggregate multi-scale features and capture global correlations. The attention modules are used to exploit the long-range dependencies and the interdependence between channels from the perspective of space and channel respectively. The experimental results on the GF-2 images show that our proposed method achieves state-of-the-art performance, with an OA of 84.1% and the mIoU of 62.3%. Compared with the baseline network, our method improves the OA by 3.3% and the mIoU by 4.4%. The comparative experiments also demonstrate that the proposed approach can significantly improve the accuracy of land cover segmentation than other compared methods.",Image segmentation,Semantics,Remote sensing,Feature extraction,Decoding,Licenses,Kernel,IEEE ACCESS,,Remote sensing,semantic segmentation,encoder-decoder network,attention mechanism,,,multi-scale feature,,,,
Row_1291,"Al-Dabbagh, Ali Mahdi","Ilyas, Muhammad",,,,Uni-temporal Sentinel-2 imagery for wildfire detection using deep learning semantic segmentation models,,DEC 31 2023,9,"Wildfires are common disasters that have long-lasting climate effects and serious ecological, social, and economic effects due to climate change. Since Earth observation (EO) satellites were launched into space, remote sensing (RS) has become a more efficient technique that can be used in agriculture, environmental protection, geological exploration, and wildfires. The increasing number of EO satellites orbiting the earth provides huge amounts of data, such as Sentinel-2 with its Multi Spectral Instrument (MSI) sensor. Using uni-temporal Sentinel-2 imagery, we proposed a workflow based on deep learning (DL) semantic segmentation models to detect wildfires. In particular, we created a new big wildfire dataset suitable for semantic segmentation models. We tested our dataset using DL models such as U-Net, LinkNet, DeepLabV3+, U-Net++, and Attention ResU-Net. The results are analysed and compared in terms of the F1 score, the intersection over union (IoU) score, the precision and recall metrics, and the amount of training time each model possesses. The best results were achieved using U-Net with the ResNet50 encoder, with F1-score of 98.78% and IoU of 97.38%, and we developed it into a pre-trained DL Package (DLPK) model that is able to detect and monitor the wildfire from Sentinel-2 images automatically.",Convolutional neural network,wildfire dataset,deep learning,remote sensing,semantic segmentation models,Sentinel-2,,GEOMATICS NATURAL HAZARDS & RISK,,,,,,,,,,,,
Row_1292,"Cao, Zhiying","Fu, Kun","Lu, Xiaode","Diao, Wenhui","Sun, Hao",End-to-End DSM Fusion Networks for Semantic Segmentation in High-Resolution Aerial Images,,NOV 2019,54,"Semantic segmentation in high-resolution aerial images is a fundamental research problem in remote sensing field for its wide range of applications. However, it is difficult to distinguish regions with similar spectral features using only multispectral data. Recent research studies have indicated that the introduction of multisource information can effectively improve the robustness of segmentation method. In this letter, we use digital surface models (DSMs) information as a complementary feature to further improve the semantic segmentation results. To this end, we propose a lightweight and simple DSM fusion (DSMF) branch structure module. Compared with the existing feature extraction structures, proposed DSMF module is simple and can be easily applied to other networks. In addition, we investigate four fusion strategies based on DSMF module to explore the optimal feature fusion strategy and four end-to-end DSMFNets are designed according to the corresponding strategies. We evaluate our models on International Society for Photogrammetry and Remote Sensing Vaihingen data set and all DSMFNets achieve promising results. In particular, DSMFNet-1 achieves an overall accuracy of 91.5% on the test data set.",Convolutional neural networks (CNNs),deep learning,high-resolution aerial images,semantic segmentation,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,"Yan, Menglong",,,,,"Yu, Hongfeng","Sun, Xian",,,,,
Row_1293,"Fang, Fang","Zheng, Daoyuan","Li, Shengwen","Liu, Yuanyuan","Zeng, Linyun",Improved Pseudomasks Generation for Weakly Supervised Building Extraction From High-Resolution Remote Sensing Imagery,,2022,18,"Benefiting from free labeling pixel-level samples, weakly supervised semantic segmentation (WSSS) is making progress in automatically extracting building from high-resolution (HR) remote sensing (RS) imagery. For WSSS methods, generating high-quality pseudomasks is crucial for accurate building extraction.To improve the performance of generating pseudomasks by using image-level labels, this article proposes a weakly supervised building extraction method by combining adversarial climbing and gated convolution. The proposed method optimizes class activation maps (CAMs) by using adversarial climbing strategy, generates accurate class boundary maps by introducing a gated convolution module, and further refines building pseudomasks by fusing pairing semantic affinities and CAMs with a random walk strategy. Experimental results on three datasets-two ISPRS datasets and a self-annotated dataset-demonstrate that the proposed approach outperformed SOTA WSSS methods, leading to improvement of building extraction from HR RS imager. This article provides a new approach for optimizing pseudomasks generation, and a methodological reference for the applications of weakly supervised on RS images.",Buildings,Feature extraction,Image segmentation,Semantics,Cams,Task analysis,Logic gates,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Zhang, Jiahui",Adversarial climbing (AC),building extraction,gated convolution,high-resolution (HR) remote sensing (RS) imagery,"Wan, Bo",,weakly supervised semantic segmentation (WSSS),,,,
Row_1294,"Wang, Lihui","Chen, Yu",,,,Dual-path decoder architecture for semantic segmentation of wheat ears,,JAN 2025,0,"In this study, a dual-path decoder segmentation network (DPDS) is presented, which innovatively introduces a dual-path structure into a semantic segmentation network incorporating atrous spatial pyramid pooling (ASPP). A novel loss function, boundary focal loss (BFLoss), is designed specifically for wheat ears segmentation scenarios, which adaptively adjusts weights for different pixel points through the binarization of boundary information, focusing the training on the edges of wheat ears. It is suggested to apply the DPDS network in conjunction with BFLoss to the semantic segmentation of wheat ears. The experimental results demonstrated that BFLoss possesses advantages over commonly used binary cross entropy loss (BCELoss) and focal loss in semantic segmentation. Additionally, the dual-path decoder architecture was proved to reach higher precision than activating only one of the pathways. In comparative experiments with established semantic segmentation networks, the DPDS model achieved the best performance on several evaluation metrics, and attained a balance between precision and recall. Notably, the combination of DPDS and BFLoss achieved a 91.86% F1 score on the wheat ears semantic segmentation test dataset. Therefore, the DPDS model can be effectively applied to semantic segmentation scenarios of crops like wheat, and also provides new insights for the improvement of existing networks. Code is available at https://github.com/awesome-pythoner/dual-path-decoder-segment.",Semantic segmentation,Dual-path network,Encoder-decoder,Loss function,Wheat ears,UAV remote sensing,,APPLIED INTELLIGENCE,,,,,,,,,,,,
Row_1295,"Chen, Guangsheng","Xu, Weiye","Li, Chao","Jing, Weipeng",,GS-CDNet: a remote sensing image cloud detection method with geographic spatial data integration,,OCT 2024,0,"In optical remote sensing images, clouds exhibit irregular scales and boundaries that vary with elevation across diverse geographical locations. To accurately capture the diverse visual patterns of clouds, we propose a cloud image segmentation approach named GS-CDNet (Geographic Spatial Data-Cloud Detection Network), which is based on the integration of geospatial data with multifaceted self-attention feature extraction, multi-scale feature aggregation, and boundary clarification techniques.Firstly, we utilize geographical coordinates from optical remote sensing images to extract a raster DEM (Digital Elevation Model) from SRTM3. This process creates a dataset consisting of elevation images, longitude, and latitude maps as geospatial data, enhancing the model's capability in spatial positioning for cloud detection. Secondly, the proposed method consists of three interconnected modules within the cloud detection network: the Interleaved Self-Attention module(ISAM) utilizes a variety of self-attention mechanisms in an interleaved manner to extract multi-scale feature information.The Bidirectional Multi-Scale Feature Fusion Module(BIMFM) is responsible for integrating features, enabling a more comprehensive contextual understanding. The Boundary Extraction Module(BEM) utilizes a residual structure to generate a boundary cloud mask, effectively addressing the common issue of boundary blurring in multi-scale cloud masks. Finally, we compared and evaluated GS-CDNet with other cloud detection methods and conducted an ablation study on the key components of the method. The validation of generalization performance demonstrates the exceptional performance of the proposed model in cloud mask generation. Geospatial data and the different modules of the method play a significant role in the model.",Cloud detection,digital elevation model,remote sensing,semantic segmentation,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1296,"Lin, Daoyu","Xu, Guangluan","Wang, Yang","Sun, Xian","Fu, Kun",DENSE-ADD NET: AN NOVEL CONVOLUTIONAL NEURAL NETWORK FOR REMOTE SENSING IMAGE INPAINTING,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2018,6,"Through the recent performance of convolutional neural networks in image processing tasks, we propose a deep fully convolutional network for remote sensing image inpainting. The proposed Dense-Add Net (Dense-Add Network) can alleviate the vanishing-gradient problem, strengthen feature reuse, and substantially reduce the memory usage. We apply residual learning to learn the mappings from corrupted image to recovered image directly; it will back-propagate gradient to the bottom layers and accelerate the training process. We train the proposed Dense-Add Net with a robust Charbonnier loss function which can achieve high-quality reconstruction. The experimental verify the efficacy of our proposed Dense-Add Net.",Generative adversarial nets,synthesize remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,
Row_1297,"Ao, Wei","Zheng, Shunyi","Meng, Yan","Yang, Yang",,Few-Shot Semantic Segmentation via Mask Aggregation,,FEB 17 2024,3,"Few-shot semantic segmentation aims to recognize novel classes with only very few labelled data. This challenging task requires mining of the correlation between the query image and the support images. Previous works have typically regarded it as a pixel-wise classification problem. Therefore, various models have been designed to explore the correlation of pixels between the query image and the support images. However, they focus only on pixel-wise correspondence and ignore the overall correlation of objects. In this paper, we introduce a mask-based classification method for addressing this problem. The mask aggregation network, which is a simple mask classification model, is proposed to simultaneously generate a fixed number of masks and their probabilities of being targets. Then, the final segmentation result is obtained by aggregating all the masks according to their locations. Experiments on both the PASCAL-5i\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$5<^>i$$\end{document} and COCO-20i\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$20<^>i$$\end{document} datasets show that our method performs comparably to the state-of-the-art pixel-based methods. This competitive performance demonstrates the potential of mask classification as an alternative baseline method for few-shot semantic segmentation.",Few-shot segmentation,Mask classification,Semantic segmentation,,,,,NEURAL PROCESSING LETTERS,,,,,,,,,,,,
Row_1298,"Slyusar, Vadym","Sliusar, Ihor","Anatolii, Pavlenko",,,Improved PSP and U-Net Architectures for Forest Segmentation in Remote Sensing Pictures,"2022 IEEE 2ND UKRAINIAN MICROWAVE WEEK, UKRMW",2022,0,"Various PSP and U-Net architectures for forest segmentation in remote sensing pictures have been proposed and investigated. The main improvements of the proposed architectures are based on using the BathNormalization layers, replacing MaxPool2D layers with AveragePooling2D, changing Conv2DTranspose to UpSampling2D blocks, etc. For the training of neural networks was used modified dataset of 128x128 pictures based on the dataset from Kaggle. As a result of improving architecture was given the maximum segmentation accuracy of 80.8 % on the validation set of pictures.",Semantic segmentation,Convolutional Neural Networks,Fully Convolutional Network,U-Net,Pyramid Scene Parsing (PSP),,,,,,,,,,,,,,,
Row_1299,"Axelsson, Maria","Holmberg, Max","Tulldahl, Michael",,,Semantic segmentation of point clouds from scanning lidars,ELECTRO-OPTICAL REMOTE SENSING XVI,2022,0,"A point cloud can provide a detailed three dimensional (3D) description of a scene. Partitioning of a point cloud into semantic classes is important for scene understanding, which can be used in autonomous navigation for unmanned vehicles and in applications including surveillance, mapping, and reconnaissance. In this paper, we give a review of recent machine learning techniques for semantic segmentation of point clouds from scanning lidars and an overview of model compression techniques. We focus especially on scan-based learning approaches, which operate on single sensor sweeps. These methods do not require data registration and are suitable for real-time applications. We demonstrate how these semantic segmentation techniques can be used in defence applications in surveillance or mapping scenarios with a scanning lidar mounted on a small UAV.",Machine learning,point cloud,3D,semantic segmentation,scene understanding,,,,,,,,,,,,,,,
Row_1300,"Merkle, Nina","Azimi, Seyed Majid","Pless, Sebastian","Kurz, Franz",,SEMANTIC VEHICLE SEGMENTATION IN VERY HIGH RESOLUTION MULTISPECTRAL AERIAL IMAGES USING DEEP NEURAL NETWORKS,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),2019,1,"The fusion of complementary information from co-registered multi-modal image data enables a more detailed and more robust understanding of an image scene or specific objects, and is important for several applications in the field of remote sensing. In this paper, the benefits of combining RGB, near infrared (NIR) and thermal infrared (TIR) aerial images for the task of semantic vehicle segmentation through deep neural networks are investigated. Therefore, RGB, NIR and TIR image triplets acquired by the Modular Aerial Camera System (MACS) are precisely co-registered through the application of a virtual camera system and subsequently used for the training of different neural network architectures. Various experiments were conducted to investigate the influence of the different sensor characteristics and an early or late fusion within the network on the quality of the segmentation results.",Aerial Imagery,Data Fusion,Deep Learning,Multispectral Imagery,Vehicle Segmentation,,,,,,,,,,,,,,,
Row_1301,"Huang, Yixiang","Wu, Ming","Jiang, Xin","Li, Jiaao","Xu, Mengqiu",Weakly Supervised Sea Fog Detection in Remote Sensing Images via Prototype Learning,,2023,0,"Sea fog detection is a challenging and significant task in the field of remote sensing. Deep learning-based methods have shown promising potential, but require a large amount of pixel-level labeled data that are time-consuming and labor-intensive to acquire. To scale up the dataset and overcome the limitations of pixel-level annotation, we attempt to explore the existing knowledge from historical statistics for label-efficient sea fog detection. In this article, we propose an image-level weakly supervised sea fog detection dataset (WS-SFDD) and a novel weakly supervised sea fog detection framework via prototype learning, named ProCAM. According to the sea fog events recorded by the Marine Weather Review published quarterly by the National Meteorological Center of China, we collect the sea fog images from Himawari-8 satellite data and obtain free image-level labels to construct the dataset. However, with image-level annotations, the existing weakly supervised semantic segmentation (WSSS) methods mainly rely on class activation maps (CAMs) and have limitations when applied to such a specific scenario: 1) the pseudo-labels (PLs) mainly cover the most discriminative part of object regions that are incomplete; 2) the background is complex with varying atmospheric conditions, and it is difficult to distinguish sea fog from low clouds due to their high similarity in spectral characteristics; and 3) the co-occurring context, such as ""sea,"" distracts the model and thus degrades the performance. To address the above issues, in our proposed ProCAM, we first design a prototype reactivation (PRA) module that reactivates self-similar sea fog regions by pixel-to-prototype feature matching to improve the robustness and completeness of CAMs. Then, we develop a pixel-to-prototype contrastive (PPC) learning method to increase the distance between sea fog and background in the embedding space for learning more discriminative dense features. Finally, a self-augmented regularization (SAR) strategy is presented to decouple sea fog from its co-ocurring context, and thus avoid background interference. Extensive experiments on the WS-SFDD dataset demonstrate that our proposed method ProCAM achieves superior performance with an $F1$ score of 77.59% and a critical success index (CSI) of 63.39%. To the best of our knowledge, this is the first work to perform image-level weakly supervised sea fog detection in remote sensing images.",Prototype learning,remote sensing image segmentation,sea fog detection,weakly supervised learning,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Zhang, Chuang",,,,,"Guo, Jun",,,,,,
Row_1302,"Riyazi, Yassin","Sadjadi, Seyyed Mostafa","Zohrevand, Abbas","Hosseini, Reshad",,High-Resolution Remote Sensing Image Captioning Based on Structured Attention and SAM Network,"2024 32ND INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING, ICEE 2024",2024,0,"Due to its broad applications, remote sensing image captioning (RSIC) has gained popularity in recent years. However, it poses extra challenges for containing low-resolution images with highly structured semantic content. By incorporating image labeling and segmentation, this work develops an RSIC framework using a structured attention module that highlights important semantic components to maintain a geometric and structured shape. The quality and edge emphasis of UCM-captioned photographs are improved by upsampling them to 512x512 pixels. Using the Segment Anything Model (SAM) produces better image proposals, leading to higher accuracy than traditional techniques. A balanced output of large- and small-object masks is facilitated by SAM's promptability. The decoder can more easily learn a suitable statistical model using the model's spatial structure to provide an all-encompassing attention map. This work investigates the effects of multiple hyperparameters, including teacher forcing, the number of region proposals, and the impact of DSR and AVR loss factors. Overall, by combining image labeling and segmentation, this research improves remote sensing capabilities. It also shows how well the structured attention module and SAM work together to improve accuracy and consider different hyperparameter issues.",Image captioning,image segmentation,remote sensing image,structured attention,,,,,,,,,,,,,,,,
Row_1303,"Liu, Yifan","Zhu, Qigang","Cao, Feng","Chen, Junke","Lu, Gang",High-Resolution Remote Sensing Image Segmentation Framework Based on Attention Mechanism and Adaptive Weighting,,APR 2021,27,"Semantic segmentation has been widely used in the basic task of extracting information from images. Despite this progress, there are still two challenges: (1) it is difficult for a single-size receptive field to acquire sufficiently strong representational features, and (2) the traditional encoder-decoder structure directly integrates the shallow features with the deep features. However, due to the small number of network layers that shallow features pass through, the feature representation ability is weak, and noise information will be introduced to affect the segmentation performance. In this paper, an Adaptive Multi-Scale Module (AMSM) and Adaptive Fuse Module (AFM) are proposed to solve these two problems. AMSM adopts the idea of channel and spatial attention and adaptively fuses three-channel branches by setting branching structures with different void rates, and flexibly generates weights according to the content of the image. AFM uses deep feature maps to filter shallow feature maps and obtains the weight of deep and shallow feature maps to filter noise information in shallow feature maps effectively. Based on these two symmetrical modules, we have carried out extensive experiments. On the ISPRS Vaihingen dataset, the F1-score and Overall Accuracy (OA) reached 86.79% and 88.35%, respectively.",multi-scale convolutional,computer vision,semantic segmentation,remote sensing,neural network,ISPRS Vaihingen,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,,,,,,,,,,,
Row_1304,"Li, Huadong","Wei, Ying","Peng, Han","Zhang, Wei",,DiffuPrompter: Pixel-Level Automatic Annotation for High-Resolution Remote Sensing Images with Foundation Models,,JUN 2024,0,"Instance segmentation is pivotal in remote sensing image (RSI) analysis, aiding in many downstream tasks. However, annotating images with pixel-wise annotations is time-consuming and laborious. Despite some progress in automatic annotation, the performance of existing methods still needs improvement due to the high precision requirements for pixel-level annotation and the complexity of RSIs. With the support of large-scale data, some foundational models have made significant progress in semantic understanding and generalization capabilities. In this paper, we delve deep into the potential of the foundational models in automatic annotation and propose a training-free automatic annotation method called DiffuPrompter, achieving pixel-level automatic annotation of RSIs. Extensive experimental results indicate that the proposed method can provide reliable pseudo-labels, significantly reducing the annotation costs of the segmentation task. Additionally, the cross-domain validation experiments confirm the powerful effectiveness of large-scale pseudo-data in improving model generalization performance.",automatic labeling,instance segmentation,remote sensing,prompt generation,training-free,,,REMOTE SENSING,,,,,,,,,,,,
Row_1305,"Pang, Shiyan","Hu, Hanchun","Zuo, Zhiqi","Chen, Jia","Hu, Xiangyun",Masked Feature Modeling for Generative Self-Supervised Representation Learning of High-Resolution Remote Sensing Images,,2024,2,"Intelligent interpretation of remote sensing images using deep learning is heavily reliant on large datasets, and models trained in one domain often struggle with crossdomain application. Pretraining the backbone network via masked image modeling can effectively diminish this reliance on extensive sample data, thereby reducing crossdomain transfer obstacles. However, current masked image models typically employ a pure Transformer architecture, which may not fully capitalize on low-level features. To address these issues, this article proposes masked feature modeling (MFM), a methodology for the generative self-supervised learning of high-resolution remote sensing images that combines convolutional neural network (CNN) and Transformer architectures. This methodology has several advantages: 1) The hybrid CNN + Transformer architecture not only retains the advantages of the local feature representation of the CNN architecture but also has the full-text information modeling capabilities of the Transformer architecture; 2) the feature extraction network outputs multiscale features, and it is easier to add upsampling and a skip connection to improve the accuracy of the downstream dense prediction task; and 3) the pretrained MFM can be applied to various downstream tasks through fine-tuning with limited samples. The publicly available WHU and Massachusetts Building Datasets are used to verify the effectiveness of the proposed method. Extensive experiments involving main properties of the MFM for generative self-supervised learning, fine-tuning the MFM on the downstream semantic segmentation task, and comparisons with the other state-of-the-art generative self-supervised learning algorithms show that, through the combined advantages of the CNN and Transformer architectures, the proposed method has better feature extraction capability and higher accuracy on downstream tasks such as semantic segmentation.",Feature extraction,Remote sensing,Task analysis,Transformers,Semantic segmentation,Self-supervised learning,Computer architecture,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Generative self-supervised learning (SSL),masked feature modeling (MFM),remote sensing,semantic segmentation,,,transformer,,,,
Row_1306,"Liu, Min","Liu, Jiangping","Hu, Hua",,,A Novel Deep Learning Network Model for Extracting Lake Water Bodies from Remote Sensing Images,,FEB 2024,5,"Extraction of lake water bodies from remote sensing images provides reliable data support for water resource management, environmental protection, natural disaster early warning, and scientific research, and helps to promote sustainable development, protect the ecological environment and human health. With reference to the classical encoding-decoding semantic segmentation network, we propose the network model R50A3-LWBENet for lake water body extraction from remote sensing images based on ResNet50 and three attention mechanisms. R50A3-LWBENet model uses ResNet50 for feature extraction, also known as encoding, and squeeze and excitation (SE) block is added to the residual module, which highlights the deeper features of the water body part of the feature map during the down-sampling process, and also takes into account the importance of the feature map channels, which can better capture the multiscale relationship between pixels. After the feature extraction is completed, the convolutional block attention module (CBAM) is added to give the model a global adaptive perception capability and pay more attention to the water body part of the image. The feature map is up-sampled using bilinear interpolation, and the features at different levels are fused, a process also known as decoding, to finalize the extraction of the lake water body. Compared with U-Net, AU-Net, RU-Net, ARU-Net, SER34AUNet, and MU-Net, the R50A3-LWBENet model has the fastest convergence speed and the highest MIoU accuracy with a value of 97.6%, which is able to better combine global and local information, refine the edge contours of the lake's water body, and have stronger feature extraction capability and segmentation performance.",remote sensing,water extraction,lake,semantic segmentation,attention mechanism,ResNet,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,
Row_1307,"Zhang, Bowen","Kong, Yingying","Leung, Henry","Xing, Shiyu",,Urban UAV Images Semantic Segmentation Based on Fully Convolutional Networks with Digital Surface Models,2019 TENTH INTERNATIONAL CONFERENCE ON INTELLIGENT CONTROL AND INFORMATION PROCESSING (ICICIP),2019,4,"Unmanned aerial vehicles (UAV) have had significant progress in the last decade, applying to many fields for its convenience to explore areas that men cannot reach and the progress of image processing. Still, as basis to further application, semantic image segmentation is one of the most difficult challenges. In this paper, we propose a method for urban UAV images semantic segmentation, utilizing the geographical information, digital surface models (DSM). We introduce an end-to-end, dual stream fully convolutional networks (FCN) based classifier with DSMs to get the segmentation results, which utilizes the proposed fusion decision strategy instead of the pixel-level classification strategy, along with a short-cut scheme. The experiments show that the proposed structure performs better than state-of-the-art networks in multiple metrics.",semantic image segmentation,FCN,DSM,UAV,remote sensing,,,,,,,,,,,,,,,
Row_1308,"Li, Ming","Zhang, Hanqi","Gruen, Armin","Li, Deren",,A survey on underwater coral image segmentation based on deep learning,,APR 2024,3,"Image-based coral reef survey technologies have revolutionized the monitoring of coral reefs by offering a cost-effective and noninvasive method for collecting data across large spatial scales and extended periods. Among these technologies, underwater videography has emerged as a well-established and reliable tool for remote sensing in coral research. Automatic segmentation of coral images represents a forward-looking and fundamental research area in underwater remote sensing. It aims to address a major challenge that limits traditional in situ underwater coral survey research: the difficulty of automatically generating accurate and reproducible high-resolution maps of the underlying coral reef ecosystems. Understanding recent achievements and their relevance to coral ecology monitoring needs is crucial for future planning. This paper presents a literature review on underwater coral image segmentation, focusing on the deep learning implementation pipeline. Furthermore, we introduce a new densely annotated dataset specifically designed for the semantic segmentation of underwater coral images. We systematically evaluate State-of-the-Art (SOTA) methodologies and novel techniques not previously applied to coral image semantic segmentation using the proposed dataset. We then discuss their feasibility in this context. Our goal for this review is to spark innovative ideas and directions for future research in underwater coral image segmentation and to provide readers with an accessible overview of some of the most significant advancements in this field over the past decade. By accomplishing these objectives, we hope to advance research in underwater coral image segmentation and support the development of effective monitoring and conservation strategies for coral reef ecosystems.",Deep learning,underwater videography,remote sensing,semantic segmentation,coral reef monitoring,,,GEO-SPATIAL INFORMATION SCIENCE,,,,,,,,,,,,
Row_1309,"Wang, Xiaolei","Hu, Zirong","Shi, Shouhai","Hou, Mei","Xu, Lei",A deep learning method for optimizing semantic segmentation accuracy of remote sensing images based on improved UNet,,MAY 10 2023,25,"Semantic segmentation of remote sensing imagery (RSI) is critical in many domains due to the diverse landscapes and different sizes of geo-objects that RSI contains, making semantic segmentation challenging. In this paper, a convolutional network, named Adaptive Feature Fusion UNet (AFF-UNet), is proposed to optimize the semantic segmentation performance. The model has three key aspects: (1) dense skip connections architecture and an adaptive feature fusion module that adaptively weighs different levels of feature maps to achieve adaptive feature fusion, (2) a channel attention convolution block that obtains the relationship between different channels using a tailored configuration, and (3) a spatial attention module that obtains the relationship between different positions. AFF-UNet was evaluated on two public RSI datasets and was quantitatively and qualitatively compared with other models. Results from the Potsdam dataset showed that the proposed model achieved an increase of 1.09% over DeepLabv3 + in terms of the average F1 score and a 0.99% improvement in overall accuracy. The visual qualitative results also demonstrated a reduction in confusion of object classes, better performance in segmenting different sizes of object classes, and better object integrity. Therefore, the proposed AFF-UNet model optimizes the accuracy of RSI semantic segmentation.",,,,,,,,SCIENTIFIC REPORTS,"Zhang, Xiang",,,,,,,,,,,
Row_1310,"Zhang, Wenkai","Huang, Hai","Schmitz, Matthias","Sun, Xian","Wang, Hongqi",A MULTI-RESOLUTION FUSION MODEL INCORPORATING COLOR AND ELEVATION FOR SEMANTIC SEGMENTATION,ISPRS HANNOVER WORKSHOP: HRIGI 17 - CMRT 17 - ISA 17 - EUROCOW 17,2017,3,"In recent years, the developments for Fully Convolutional Networks (FCN) have led to great improvements for semantic segmentation in various applications including fused remote sensing data. There is, however, a lack of an in-depth study inside FCN models which would lead to an understanding of the contribution of individual layers to specific classes and their sensitivity to different types of input data. In this paper, we address this problem and propose a fusion model incorporating infrared imagery and Digital Surface Models (DSM) for semantic segmentation. The goal is to utilize heterogeneous data more accurately and effectively in a single model instead of to assemble multiple models. First, the contribution and sensitivity of layers concerning the given classes are quantified by means of their recall in FCN. The contribution of different modalities on the pixel-wise prediction is then analyzed based on visualization. Finally, an optimized scheme for the fusion of layers with color and elevation information into a single FCN model is derived based on the analysis. Experiments are performed on the ISPRS Vaihingen 2D Semantic Labeling dataset. Comprehensive evaluations demonstrate the potential of the proposed approach.",Semantic Segmentation,Convolutional Networks,Multi-modal Dataset,Fusion Nets,,,,,"Mayer, Helmut",,,,,,,,,,,
Row_1311,"Abhishek, R.","Chakravorty, Anisha","Chakraborty, Shounak",,,Active learning based semantic segmentation for extraction of minute objects from multispectral satellite images,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,0,"In this manuscript, an attention based Unet (A-Unet) has been proposed for semantic segmentation of enormous remotely sensed images for the extraction of minute objects like electrical substations. For this purpose, a pre-trained deep convolutional neural network (DCNN) has been used in every encoder-decoder level over the A-UNET. It is infused with an intelligent active learning approach for retraining and thereby leveraging the segmentation model for detecting smaller objects from large-scale images. The experimentation has been carried out in three phases: pre-processing the images, training of an A-Unet framework, and active learning based retraining methodology to achieve improvement over the overall learning curve. The results validated over two state-of-the-art remote sensing datasets (of electrical substation and ships) show efficient performance in segmenting the necessary small object boundaries. A mean intersection over union (mioU) calculated for this purpose show promising results in favour of the proposed approach in comparison to the state-of-the-art methodologies.",Semantic Segmentation,U-net,Attention based U-net,Deep Convolutional Neural network,Active learning,,,,,,,,,,,,,,,
Row_1312,"Sui, Baikai","Jiang, Tao","Zhang, Zhen","Pan, Xinliang","Liu, Chenxi",A Modeling Method for Automatic Extraction of Offshore Aquaculture Zones Based on Semantic Segmentation,,MAR 2020,26,"Monitoring of offshore aquaculture zones is important to marine ecological environment protection and maritime safety and security. Remote sensing technology has the advantages of large-area simultaneous observation and strong timeliness, which provide normalized monitoring of marine aquaculture zones. Aiming at the problems of weak generalization ability and low recognition rate in weak signal environments of traditional target recognition algorithm, this paper proposes a method for automatic extraction of offshore fish cage and floating raft aquaculture zones based on semantic segmentation. This method uses Generative Adversarial Networks to expand the data to compensate for the lack of training samples, and uses ratio of green band to red band (G/R) instead of red band to enhance the characteristics of aquaculture spectral information, combined with atrous convolution and atrous space pyramid pooling to enhance the context semantic information, to extract and identify two types of offshore fish cage zones and floating raft aquaculture zones. The experiment is carried out in the eastern coastal waters of Shandong Province, China, and the overall identification accuracy of the two types of aquaculture zones can reach 94.8%. The results show that the method proposed in this paper can realize high-precision extraction both of offshore fish cage and floating raft aquaculture zones.",offshore aquaculture,semantic segmentation,generative adversarial networks,high-resolution remote sensing image,,,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,,,,,,,,,,,
Row_1313,"Ni, Jun","Zhang, Fan","Ma, Fei","Yin, Qiang","Xiang, Deliang",Random Region Matting for the High-Resolution PolSAR Image Semantic Segmentation,,2021,8,"Polarimetric synthetic aperture radar (PolSAR) imagery can provide more intuitive and detailed SAR polarization information, and it is widely used in the classification and semantic segmentation of remote sensing. To bridge the PolSAR data and application, the 2020 Gaofen Challenge on Automated High-Resolution Earth Observation Image Interpretation provides a set of high-quality PolSAR semantic segmentation dataset. A series of preprocessing methods is first used to analyze the PolSAR images to improve the semantic segmentation performance of the PolSAR imagery. A special polarimetric decomposition method is used to extract the features, and the filter and the data truncation are implemented to enhance local and global information of images. And the random region matting method is proposed to expand the training samples. Finally, the DeepLabV3+ method with the ResNet101-V2 is employed to achieve the semantic segmentation. A variety of comparison experiments verifies the effectiveness of our methods. Simultaneously, compared with the classification methods of other groups in the competition, our methods have obvious advantages in the inference time and semantic segmentation accuracy. The proposed method achieved a frequency weighted intersection over union of 75.29% in the contest.",Data augmentation,DeepLabV3+,Gaofen-3,image classification,polarimetric synthetic aperture radar (PolSAR),semantic segmentation,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,
Row_1314,"Ma, Jiabao","Zhou, Wujie","Lei, Jingsheng","Yu, Lu",,Adjacent Bi-Hierarchical Network for Scene Parsing of Remote Sensing Images,,2023,14,"Driven by the rapid development and application of earth observation sensors, the scene parsing of remote sensing images (RSIs) has attracted extensive research attention in recent years. Restricted by the limited local receptive field of successive convolution layers, traditional models of scene parsing cannot effectively and interactively utilize the local-global information and digital surface model (DSM) of RSIs. Comparatively, accurate scene parsing faces more challenges because of unbalanced categories, small targets, and more complex scenes. To address these challenges, herein, we propose a novel adjacent bi-hierarchical network (ABHNet). Specifically, we introduce a DSM-enhanced (DSE) module to excavate characteristic DSM information from DSM images and enhance the red, green, and blue (RGB) features by exploiting informative cues between RGB and DSM modalities. In addition, an adjacent context exploration (ACE) module is proposed, which contains current and adjacent branches. The branches first exploit multiscale complementary characteristics of multilevel features and then integrate these features by applying adjacent exploration. Our model includes five ACE modules-three are deployed to activate detailed features and two obtain deep-guided features. The mutual collaboration of deep and detailed features is more beneficial to the segmentation of small objects. Extensive experiments on two remote sensing benchmark datasets (ISPRS Potsdam and Vaihingen) showed that the proposed ABHNet qualitatively and quantitatively outperformed other methods.",Feature extraction,Decoding,Remote sensing,Image segmentation,Data mining,Transformers,Task analysis,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Adjacent integrate,multimodal,multiscale,remote sensing images (RSIs),,,scene parsing,,,,
Row_1315,"Jiang, Yinan","Zhong, Chaoliang","Zhang, Botao",,,"AGD-Linknet: A Road Semantic Segmentation Model for High Resolution Remote Sensing Images Integrating Attention Mechanism, Gated Decoding Block and Dilated Convolution",,2023,8,"Road information is an important geographic information. Road information extracted from remote sensing images has been widely used in map, traffic, navigation and many other fields. However, the autonomous extraction of road information from high resolution remote sensing images has some problems such as incoherence, incompleteness and poor connectivity, therefore, a semantic segmentation model for roads in high resolution remote sensing images, called AGD-Linknet, is proposed, which integrates attention mechanisms, gated decoder block, and dilated convolution. This model mainly consists of three parts. Firstly, the stem block is used as the initial convolution layer of the model to reduce the information loss in the convolution stage; Secondly, the series-parallel combined dilated convolution and coordinate attention block into the center of the network, which enlarges the receptive field of the network and improves the feature extraction ability of spatial domain and channel domain information; Finally, in the decoder part, gated convolution is introduced to improve the extraction of road edge. Compared with U-Net, Linknet and D-Linknet on the DeepGlobe dataset, the proposed AGD-Linknet has improved the pixel accuracy, mean intersection over union and F1-Score index of road recognition by 1.41%-11.52%, 0.0077-0.1473, 0.0057-0.1292, and has certain effectiveness and feasibility in many scenarios in rural areas, urban, and suburbs. And can be apply to the tasks of road recognition and extraction in high-resolution remote sensing.",Road traffic,Feature extraction,Convolution,Data mining,Decoding,Remote sensing,Logic gates,IEEE ACCESS,,Geographic information systems,remote sensing,road extraction,convolutional neural networks,,,attention mechanisms,dilated convolution,gated convolution,,
Row_1316,"Dowden, Benjamin","De Silva, Oscar","Huang, Weimin",,,Sea Ice Image Semantic Segmentation Using Deep Neural Networks,GLOBAL OCEANS 2020: SINGAPORE - U.S. GULF COAST,2020,1,"Semantic segmentation is the process of classifying pixels in an image into different classes. This method is quite established in autonomous vehicles, biomedical image processing, and remote sensing applications. In this paper, we evaluate the applicability of semantic segmentation for sea ice classification using image feeds from on-board an icebreaker. For this purpose, we evaluate SegNet and PSPNet101 neural network architectures to segment images into four classes: ice, ocean, vessel, and sky. The Nathaniel B. Palmer dataset, which captures 2-month footage of the icebreaker completing an Antarctic expedition was used. A subset of the dataset was labeled to generate a 240-image dataset achieving an accuracy of 97.8% classification for the 26 image test dataset. These results validate the applicability of deep learning methods for sea ice detection using images, which can be further improved by classifying the ice type to support marine navigation and mapping applications.",Semantic segmentation,neural network,intelligent systems,machine learning,,,,,,,,,,,,,,,,
Row_1317,"de Carvalho, Osmar L. F.","de Carvalho Junior, Osmar A.","de Albuquerque, Anesmar O.","Santana, Nickolas C.","Borges, Dibio L.",Rethinking Panoptic Segmentation in Remote Sensing: A Hybrid Approach Using Semantic Segmentation and Non-Learning Methods,,2022,10,"This letter proposes a novel method to obtain panoptic predictions by extending the semantic segmentation task with a few non-learning image processing steps, presenting the following benefits: 1) annotations do not require a specific format [e.g., common objects in context (COCO)]; 2) fewer parameters (e.g., single loss function and no need for object detection parameters); and 3) a more straightforward sliding windows implementation for large image classification (still unexplored for panoptic segmentation). Semantic segmentation models do not individualize touching objects, as their predictions can merge; i.e., a single polygon represents many targets. Our method overcomes this problem by isolating the objects using borders on the polygons that may merge. The data preparation requires generating a one-pixel border, and for unique object identification, we create a list with the isolated polygons, attribute a different value to each one, and use the expanding border (EB) algorithm for those with borders. Although any semantic segmentation model applies, we used the U-Net with three backbones (EfficientNet-B5, EfficientNet-B3, and EfficientNet-B0). The results show that the following hold: 1) the EfficientNet-B5 had the best results with 70% mean intersection over union (mloU); 2) the EB algorithm presented better results for better models; 3) the panoptic metrics show a high capability of identifying things and stuff with 65 panoptic quality (PQ); and 4) the sliding windows on a 2560 x 2560-pixel area has shown promising results, in which the ratio of merged objects by correct predictions was lower than 1% for an classes.",Aerial image,anchor-free,deep learning,sliding windows,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,
Row_1318,"Mai, Chaoyun","Wu, Yibo","Zhai, Yikui","Quan, Hao","Zhou, Jianhong",DBCG-Net: Dual Branch Calibration Guided Deep Network for UAV Images Semantic Segmentation,,2024,0,"Unmanned aerial vehicle (UAV) remote sensing images used for semantic segmentation possess distinct features compared to urban street scene images, including high resolution and a complex background. Spatial information plays a pivotal role in enhancing the performance of semantic segmentation for high-resolution images. The dual-branch architecture for semantic segmentation incorporates supplementary branches to capture spatial information. However, prior research on dual-branch semantic segmentation neglected the interaction between the contextual and spatial branches, leading to suboptimal model performance. In this discourse, the article introduces a dual-branch semantic segmentation framework. This design advances the system's understanding of spatial information while facilitating inter-branch learning through two key modules. Initially, the spatial calibration feature extraction module employs frequency domain processing and learning tactics distinct from the contextual approach to generate image features under varied noise conditions. Calibration is achieved by generating features from diverse angles. Subsequently, the spatially-guided loss function directs the acquisition of spatial information for the spatial branch by condensing the deep image characteristics for the context branch. To assess the generalization capacity of the proposed method, experiments will be conducted on three different datasets. The proposed method's modules will be integrated into three representative dual-branch networks, allowing assessment of the generalization capacity of the key DBCG components. Empirical evidence demonstrates that this approach is highly effective, significantly surpassing the performance of the baseline network.",Semantic segmentation,Calibration,Autonomous aerial vehicles,Feature extraction,Spatial resolution,Semantics,Remote sensing,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Genovese, Angelo",Convolutional neural network (CNN),deep learning,dual-branch calibration guided network,semantic segmentation,"Piuri, Vincenzo","Scotti, Fabio",unmanned aerial vehicles (UAVs),,,,
Row_1319,"Jiang, Mingyu","Shao, Hua","Zhu, Xingyu","Li, Yang",,Green Space Reverse Pixel Shuffle Network: Urban Green Space Segmentation Using Reverse Pixel Shuffle for Down-Sampling from High-Resolution Remote Sensing Images,,JAN 2024,0,"Urban green spaces (UGS) play a crucial role in the urban environmental system by aiding in mitigating the urban heat island effect, promoting sustainable urban development, and ensuring the physical and mental well-being of residents. The utilization of remote sensing imagery enables the real-time surveying and mapping of UGS. By analyzing the spatial distribution and spectral information of a UGS, it can be found that the UGS constitutes a kind of low-rank feature. Thus, the accuracy of the UGS segmentation model is not heavily dependent on the depth of neural networks. On the contrary, emphasizing the preservation of more surface texture features and color information contributes significantly to enhancing the model's segmentation accuracy. In this paper, we proposed a UGS segmentation model, which was specifically designed according to the unique characteristics of a UGS, named the Green Space Reverse Pixel Shuffle Network (GSRPnet). GSRPnet is a straightforward but effective model, which uses an improved RPS-ResNet as the feature extraction backbone network to enhance its ability to extract UGS features. Experiments conducted on GaoFen-2 remote sensing imagery and the Wuhan Dense Labeling Dataset (WHDLD) demonstrate that, in comparison with other methods, GSRPnet achieves superior results in terms of precision, F1-score, intersection over union, and overall accuracy. It demonstrates smoother edge performance in UGS border regions and excels at identifying discrete small-scale UGS. Meanwhile, the ablation experiments validated the correctness of the hypotheses and methods we proposed in this paper. Additionally, GSRPnet's parameters are merely 17.999 M, and this effectively demonstrates that the improvement in accuracy of GSRPnet is not only determined by an increase in model parameters.",urban green space,high-resolution remote sensing imagery,deep learning,semantic segmentation,,,,FORESTS,,,,,,,,,,,,
Row_1320,"Wang, Deyi","Han, Min",,,,SA-U-Net plus plus : SAR marine floating raft aquaculture identification based on semantic segmentation and ISAR augmentation,,JAN 18 2021,6,"Marine floating raft aquaculture (FRA) monitoring is vital for environment protection and mariculture management. Synthetic aperture radar (SAR) could provide high-quality remote sensing images under all weather conditions compared with the existing optical remote-sensing-based methods. Traditional SAR monitoring methods extract the pixel feature of marine FRA in single patches, which commonly leads to poor generalization. We propose a self-attention semantic segmentation method based on modified U-Net++ (SA-U-Net++) for FRA segmentation, which could automatically extract semantic feature information, and provide superior performance under complicated scenes. The proposed self-attention backbone could help to extract more precise features and enhance the overall accuracy. Furthermore, we propose a FRA-ISAR data generation method based on inverse SAR (ISAR) imaging to alleviate the sample shortage problem. We introduce the semantic segmentation method into FRA-SAR segmentation for the first time. The experiments verify the effectiveness and superiority of SAR FRA segmentation based on the proposed SA-U-Net++ model compared with the existed semantic segmentation approaches. (C) 2021 Society of Photo-Optical Instrumentation Engineers (SPIE)",floating raft aquaculture,synthetic aperture radar,deep learning,semantic segmentation,inverse synthetic aperture radar,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,
Row_1321,"Wang, Di","Zhang, Jing","Du, Bo","Xu, Minqiang","Liu, Lin",SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model,ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 36 (NEURIPS 2023),2023,0,"The success of the Segment Anything Model (SAM) demonstrates the significance of data-centric machine learning. However, due to the difficulties and high costs associated with annotating Remote Sensing (RS) images, a large amount of valuable RS data remains unlabeled, particularly at the pixel level. In this study, we leverage SAM and existing RS object detection datasets to develop an efficient pipeline for generating a large-scale RS segmentation dataset, dubbed SAMRS. SAMRS totally possesses 105,090 images and 1,668,241 instances, surpassing existing high-resolution RS segmentation datasets in size by several orders of magnitude. It provides object category, location, and instance information that can be used for semantic segmentation, instance segmentation, and object detection, either individually or in combination. We also provide a comprehensive analysis of SAMRS from various aspects. Moreover, preliminary experiments highlight the importance of conducting segmentation pre-training with SAMRS to address task discrepancies and alleviate the limitations posed by limited training data during fine-tuning. The code and dataset will be available at SAMRS.",,,,,,,,,"Tao, Dacheng",,,,,"Zhang, Liangpei",,,,,,
Row_1322,"Cao, Yungang","Zhang, Shuang","Sui, Baikai","Xie, Yakun","Zhu, Jun",IBCO-Net: Integrity-Boundary-Corner Optimization in a General Multistage Network for Building Fine Segmentation From Remote Sensing Images,,2023,3,"Building extraction is a significant topic in high-resolution remote sensing. Insufficient integrity, irregular boundaries, and inaccurate corners remain a problem for existing methods. However, individually optimizing one of these aspects may leave problems in others. Unfortunately, few methods consider integrity, boundary, and corner simultaneously. In this study, we propose a three-stage network [integrity-boundary-corner optimization in a general multistage network (IBCO-Net)] incorporating integrity-boundary-corner optimization for fine segmentation of buildings. First, long-range dependent and spatial-continuous (LDSC) blocks are plugged into the decoder to enhance building integrity. Second, the direction field correction module (DFCM) controls the overall shape of the building by learning the direction field and executing an iterative correction algorithm. Finally, the multistrategy point refinement module (MSPRM) selects boundary and corner points for reclassification to further refine the boundary and relocate corners, and a hybrid loss function supervises IBCO-Net to optimize each stage. Comparative experiments were conducted on three datasets: the Massachusetts building dataset, the ISPRS Potsdam dataset, and the dataset of building instances of typical cities in China. We evaluated common pixel-level metrics and object-level boundary and corner metrics, with experimental results showing that IBCO-Net outperforms eight state-of-the-art convolution neural network (CNN) and transformer-based methods. In addition, the generality of the proposed method is demonstrated via its performance by applying nine existing backbone networks.",& nbsp;Boundary and corner optimization,building extraction,high-resolution remote sensing,multistage network,semantic segmentation,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1323,"Zhang, Ke","Bello, Inuwa Mamuda","Su, Yu","Wang, Jingyu","Maryam, Ibrahim",Multiscale depthwise separable convolution based network for high-resolution image segmentation,,SEP 17 2022,4,"Deep learning-based segmentation methods have demonstrated significant performance over their traditional counterparts. However, striving for better accuracy with such networks usually leads to the deterioration of the network's computational efficiency, thereby rendering them inefficient for deployment on resource constraint devices. Establishing the required tradeoff between the accuracy of pixel prediction and computational efficiency remains challenging. In this article, a lightweight multiscale segmentation framework is proposed. We leverage the representation power of different receptive fields to attain optimal accuracy while maintaining computational efficiency by embedding the sparse network architecture with the depthwise separable convolution at the multiscale level. Experimental results from two challenging remote sensing segmentation datasets show that the proposed network can achieve substantial pixel prediction accuracy at relatively low computational overhead compared to state-of-the-art networks.",Multiscale,segmentation,remote sensing,depthwise separable convolution,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1324,He Xiaoying,Xu Weiming,Pan Kaixiang,Wang Juan,Li Ziwei,Classification of High-Resolution Remote Sensing Image Based on Swin Transformer and Convolutional Neural Network,,JUL 2024,3,"It is challenging to directly obtain global information of existing deep learning-based remote sensing intelligent interpretation methods, resulting in blurred object edges and low classification accuracy between similar classes. This study proposes a semantic segmentation model called SRAU-Net based on Swin Transformer and convolutional neural network. SRAU-Net adopts a Swin Transformer encoder-decoder framework with a U-Net shape and introduces several improvements to address the limitations of previous methods. First, Swin Transformer and convolutional neural network are used to construct a dual-branch encoder, which effectively captures spatial details with different scales and complements the context features, resulting in higher classification accuracy and sharper object edges. Second, a feature fusion module is designed as a bridge for the dual-branch encoder. This module efficiently fuses global and local features in channel and spatial dimensions, improving the segmentation accuracy for small target objects. Moreover, the proposed SRAU-Net model incorporates a feature enhancement module that utilizes attention mechanisms to adaptively fuse features from the encoder and decoder and enhances the aggregation of spatial and semantic features, further improving the ability of the model to extract features from remote sensing images. The effectiveness of the proposed SRAU-Net model is demonstrated using the ISPRS Vaihingen dataset for land cover classification. The results show that SRAU-Net outperforms other models in terms of overall accuracy and F1 score, achieving 92.06% and 86.90%, respectively. Notably, the SRAU-Net model excels in extracting object edge information and accurately classifying small-scale regions, with an improvement of 2.57 percentage points in the overall classification accuracy compared with the original model. Furthermore, it effectively distinguishes remote sensing objects with similar characteristics, such as trees and low vegetation.",high-resolution remote sensing image,convolutional neural network,Swin Transformer,feature fusion,semantic segmentation,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,
Row_1325,"Wang, Zhongchen","Xia, Min","Weng, Liguo","Hu, Kai","Lin, Haifeng",Dual Encoder-Decoder Network for Land Cover Segmentation of Remote Sensing Image,,2024,20,"Although the vision transformer-based methods (ViTs) exhibit an excellent performance than convolutional neural networks (CNNs) for image recognition tasks, their pixel-level semantic segmentation ability is limited due to the lack of explicit utilization of local biases. Recently, a variety of hybrid structures of ViT and CNN have been proposed, but these methods have poor multiscale fusion ability and cannot accurately segment high-resolution and high-content complex land cover remote sensing images. Therefore, a dual encoder-decoder network named DEDNet is proposed in this work. In the encoding stage, the local and global information of the image is extracted by parallel CNN encoder and transformer encoder. In the decoding stage, the cross-stage fusion module is constructed to achieve neighborhood attention guidance to enhance the positioning of small targets, effectively avoiding intraclass inconsistency. At the same time, the multihead feature extraction module is proposed to strengthen the recognition ability of the target boundary and effectively avoid interclass ambiguity. Before outputting, the fusion spatial pyramid pooling classifier is proposed to merge the outputs of the two decoding strategies. The experiments demonstrate that the proposed model has superior generalization performance and can handle various semantic segmentation tasks of land cover.",Dual encoder-decoder,image segmentation,land cover,vision transformer,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,
Row_1326,"Sun, Wenbo","Gao, Zhi","Cui, Jinqiang","Ramesh, Bharath","Zhang, Bin",Semantic Segmentation Leveraging Simultaneous Depth Estimation,,FEB 2021,4,"Semantic segmentation is one of the most widely studied problems in computer vision communities, which makes a great contribution to a variety of applications. A lot of learning-based approaches, such as Convolutional Neural Network (CNN), have made a vast contribution to this problem. While rich context information of the input images can be learned from multi-scale receptive fields by convolutions with deep layers, traditional CNNs have great difficulty in learning the geometrical relationship and distribution of objects in the RGB image due to the lack of depth information, which may lead to an inferior segmentation quality. To solve this problem, we propose a method that improves segmentation quality with depth estimation on RGB images. Specifically, we estimate depth information on RGB images via a depth estimation network, and then feed the depth map into the CNN which is able to guide the semantic segmentation. Furthermore, in order to parse the depth map and RGB images simultaneously, we construct a multi-branch encoder-decoder network and fuse the RGB and depth features step by step. Extensive experimental evaluation on four baseline networks demonstrates that our proposed method can enhance the segmentation quality considerably and obtain better performance compared to other segmentation networks.",CNN,semantic segmentation,depth estimation,multi-source feature fusion,,,,SENSORS,"Li, Ziyao",,,,,,,,,,,
Row_1327,"Li, Yongjian","Li, He","Fan, Dazhao","Li, Zhixin","Ji, Song",Improved Sea Ice Image Segmentation Using U2-Net and Dataset Augmentation,,AUG 2023,5,"Sea ice extraction and segmentation of remote sensing images is the basis for sea ice monitoring. Traditional image segmentation methods rely on manual sampling and require complex feature extraction. Deep-learning-based semantic segmentation methods have the advantages of high efficiency, intelligence, and automation. Sea ice segmentation using deep learning methods faces the following problems: in terms of datasets, the high cost of sea ice image label production leads to fewer datasets for sea ice segmentation; in terms of image quality, remote sensing image noise and severe weather conditions affect image quality, which affects the accuracy of sea ice extraction. To address the quantity and quality of the dataset, this study used multiple data augmentation methods for data expansion. To improve the semantic segmentation accuracy, the SC-U2-Net network was constructed using multiscale inflation convolution and a multilayer convolutional block attention module (CBAM) attention mechanism for the U2-Net network. The experiments showed that (1) data augmentation solved the problem of an insufficient number of training samples to a certain extent and improved the accuracy of image segmentation; (2) this study designed a multilevel Gaussian noise data augmentation scheme to improve the network's ability to resist noise interference and achieve a more accurate segmentation of images with different degrees of noise pollution; (3) the inclusion of a multiscale inflation perceptron and multilayer CBAM attention mechanism improved the ability of U2-Net network feature extraction and enhanced the model accuracy and generalization ability.",sea ice segmentation,U-2-Net,remote sensing images,,,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,
Row_1328,"Wang, Lei","Chen, Cheng","Chen, Fang","Wang, Ning","Li, Congrong",UGTransformer: A Sheep Extraction Model From Remote Sensing Images for Animal Husbandry Management,,2024,5,"The extraction of sheep from satellite images plays an extremely important role in the precise automation of animal husbandry management. Current methods of extracting sheep mainly use hardware, such as radio frequency equipment and visual ear tags, which are prone to loss or damage. In this study, a new network, UGTransformer, was developed to extract sheep from high spatial resolution remote sensing (RS) images. In UGTransformer, a merge block was designed to fuse two scales of features in the encoder to improve the multiscale feature fusion capability. It enhanced the integration of global context features and spatial detailed features by combining the features in the decoder. A global connectivity module containing two sliding sub-modules, horizontal and vertical, was developed to correlate the horizontal and vertical features and correlate the arbitrary positions of the feature maps through the integration of the two modules, which realized the extraction of global contextual information. Our experimental results showed that the proposed UGTransformer performed well in comparison with UNet, Deeplab v3+, DCSwin, BANet, and UNetFormer, four recently proposed network structures for semantic segmentation. UGTransformer achieved at least a 1.8% increase in mean intersection over the union. This study not only provided potential solutions for the problems inherent in large-scale sheep extraction but also developed mechanisms for small-object extraction. The implementation code is available at https://github.com/chenchengStore/GlobalLocalAttention, and the RS images used in this study are available at https://github.com/chencheng-2023/UGTransformer-remote-sensing-images.",Animal husbandry,extraction of sheep,high spatial resolution remote sensing (RS) images,semantic segmentation,small-object extraction,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Zhang, Haiying",,,,,"Wang, Yu","Yu, Bo",,,,,
Row_1329,"Zermatten, Valerie","Navarro, Javiera Castillo","Hughes, Lloyd","Kellenberger, Tobias","Tuia, Devis",TEXT AS A RICHER SOURCE OF SUPERVISION IN SEMANTIC SEGMENTATION TASKS,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"This paper introduces TACOSS a text-image alignment approach that allows explainable land cover semantic segmentation by directly integrating semantic concepts encoded from texts. TACOSS combines convolutional neural networks for visual feature extraction with semantic embeddings provided by a language model. By leveraging contrastive learning approaches, we learn an alignment between the visual and the (fixed) textual representations. In addition to producing standard semantic segmentation outputs, our model enables interactive queries with RS images using natural language prompts. The experimental results obtained on 50cm resolution aerial data from Switzerland show that TACOSS performs similarly to a standard semantic segmentation model while allowing the flexible usage of in- and out-of-vocabulary terms for the interactions with the image.",land cover semantic segmentation,contrastive learning,vision-language models,,,,,,,,,,,,,,,,,
Row_1330,"Dai, Xin","Xia, Min","Weng, Liguo","Hu, Kai","Lin, Haifeng",Multiscale Location Attention Network for Building and Water Segmentation of Remote Sensing Image,,2023,38,"Traditional building and water segmentation methods are vulnerable to noise interference, and hence, they could not avoid missed and false detections in the detection process. Excessive deep learning downsampling would lead to significant loss of feature map information, image location information offset, and the overall effect of falling apart. To address these issues, a multiscale location attention network (MSLANet) is proposed. Location-spatial information and channel information are particularly important for edge detail segmentation in building and water cover. The network includes a location channel attention (LCA) unit to focus on tributary details of rivers and segmentation of building edge eaves. Moreover, this article builds a dual-branch multiscale aggregation (DBMSA) unit to obtain deeper multiscale semantic information. Finally, the multiscale fusion unit (MSF) is used to guide the information merging of multiple stages, and the boundary information is improved by splicing the acquired deep multiscale information with the information of the relevant feature extraction layer in the downsampling. The experimental results on several datasets show that the proposed approach outperforms other methodologies in segmentation accuracy.",Image segmentation,Buildings,Feature extraction,Convolution,Semantics,Remote sensing,Water resources,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Qian, Ming",Attention,building and water,deep learning,image segmentation,,,,,,,
Row_1331,"Panboonyuen, Teerapong","Jitkajornwanich, Kulsawasd","Lawawirojwong, Siam","Srestasathiern, Panu","Vateekul, Peerapon",Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning,,JAN 1 2019,83,"In the remote sensing domain, it is crucial to complete semantic segmentation on the raster images, e.g., river, building, forest, etc., on raster images. A deep convolutional encoder-decoder (DCED) network is the state-of-the-art semantic segmentation method for remotely sensed images. However, the accuracy is still limited, since the network is not designed for remotely sensed images and the training data in this domain is deficient. In this paper, we aim to propose a novel CNN for semantic segmentation particularly for remote sensing corpora with three main contributions. First, we propose applying a recent CNN called a global convolutional network (GCN), since it can capture different resolutions by extracting multi-scale features from different stages of the network. Additionally, we further enhance the network by improving its backbone using larger numbers of layers, which is suitable for medium resolution remotely sensed images. Second, channel attention is presented in our network in order to select the most discriminative filters (features). Third, domain-specific transfer learning is introduced to alleviate the scarcity issue by utilizing other remotely sensed corpora with different resolutions as pre-trained data. The experiment was then conducted on two given datasets: (i) medium resolution data collected from Landsat-8 satellite and (ii) very high resolution data called the ISPRS Vaihingen Challenge Dataset. The results show that our networks outperformed DCED in terms of F1 for 17.48% and 2.49% on medium and very high resolution corpora, respectively.",deep convolutional neural networks,multi-class segmentation,global convolutional network,channel attention,transfer learning,ISPRS Vaihingen,Landsat-8,REMOTE SENSING,,,,,,,,,,,,
Row_1332,"Irwansyah, Edy","Heryadi, Yaya","Gunawan, Alexander Agung Santoso",,,Semantic Image Segmentation for Building Detection in Urban Area with Aerial Photograph Image using U-Net Models,"2020 IEEE ASIA-PACIFIC CONFERENCE ON GEOSCIENCE, ELECTRONICS AND REMOTE SENSING TECHNOLOGY (AGERS 2020): UNDERSTANDING THE INTERCTION OF LAND, OCEAN AND ATMOSPHERE: DISASTER MITIGATION AND REGIONAL RESILLIENCE",2020,5,"Detecting building location distribution in an urban area has been a concern of city government in many developing countries as a basis for city planning and development. In recent years, deep learning has gained research attention as the most attractive approach to address classification in the remote sensing field. One application of deep learning is a semantic image segmentation method whose aim is to classify each pixel in the image into a predetermined set of labels. In this experiment, the objective of semantic image segmentation is building detection in urban areas using a deep learning model in which each image pixel is categorized into either building or non-building label. Based on experimentation using aerial photograph imagery of Pasar Minggu Sub-District, South Jakarta City District, DKI. Jakarta Province and UNet model achieved 0.83 average training accuracy and 0,87 testing accuracy",building detection,semantic segmentation,unet,aerial photograph,,,,,,,,,,,,,,,,
Row_1333,"Fan, Jianchao","Zhou, Jianlin","Wang, Xinzhe","Wang, Jun",,A Self-Supervised Transformer With Feature Fusion for SAR Image Semantic Segmentation in Marine Aquaculture Monitoring,,2023,5,"The rapid development of the marine aquaculture industry has brought about a series of environmental problems that need to be monitored and planned. There is abundant marine aquaculture data obtained through synthetic aperture radar (SAR) remote sensing over a long period. With a large amount of unlabeled data, self-supervised learning can describe the feature representation of targets. However, when self-supervised learning meets big data, it often leads to semantic information loss, such as interclass misjudgment and intraclass discontinuity. To address this issue, this article proposes a self-supervised transformer with feature fusion (STFF) for the semantic segmentation of SAR images in marine aquaculture monitoring. STFF consists mainly of a self-attention encoding module with a hybrid loss function and a semantic segmentation decoding module with feature fusion. For encoding, the transformer is pretrained via self-supervised learning based on a hybrid loss function to enrich local, global, and edge information for dealing with semantic information loss and data imbalance in whole-scene SAR images. For decoding, the features extracted from transformer blocks are fused to enhance semantic characteristics, improve the intraclass continuity of segmentation, and reduce the occurrence of interclass misjudgment. The superiority of the proposed method to state-of-the-art algorithms is demonstrated via experimentation on GaoFen-3 and Radarsat-2 SAR datasets. The code has been available at https://github.com/fjc1575/MarineAquaculture/tree/main/STFF-code for the sake of reproducibility.",Transformers,Aquaculture,Feature extraction,Radar polarimetry,Semantic segmentation,Semantics,Remote sensing,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Marine aquaculture,self-supervised learning,semantic segmentation,synthetic aperture radar (SAR) images,,,transformer-based model,,,,
Row_1334,"Graf, Lukas","Bach, Heike","Tiede, Dirk",,,Semantic Segmentation of Sentinel-2 Imagery for Mapping Irrigation Center Pivots,,DEC 2020,10,"Estimating the number and size of irrigation center pivot systems (CPS) from remotely sensed data, using artificial intelligence (AI), is a potential information source for assessing agricultural water use. In this study, we identified two technical challenges in the neural-network-based classification: Firstly, an effective reduction of the feature space of the remote sensing data to shorten training times and increase classification accuracy is required. Secondly, the geographical transferability of the AI algorithms is a pressing issue if AI is to replace human mapping efforts one day. Therefore, we trained the semantic image segmentation algorithm U-NET on four spectral channels (U-NET SPECS) and the first three principal components (U-NET principal component analysis (PCA)) of ESA/Copernicus Sentinel-2 images on a study area in Texas, USA, and assessed the geographic transferability of the trained models to two other sites: the Duero basin, in Spain, and South Africa. U-NET SPECS outperformed U-NET PCA at all three study areas, with the highest f1-score at Texas (0.87, U-NET PCA: 0.83), and a value of 0.68 (U-NET PCA: 0.43) in South Africa. At the Duero, both models showed poor classification accuracy (f1-score U-NET PCA: 0.08; U-NET SPECS: 0.16) and segmentation quality, which was particularly evident in the incomplete representation of the center pivot geometries. In South Africa and at the Duero site, a high rate of false positive and false negative was observed, which made the model less useful, especially at the Duero test site. Thus, geographical invariance is not an inherent model property and seems to be mainly driven by the complexity of land-use pattern. We do not consider PCA a suited spectral dimensionality reduction measure in this. However, shorter training times and a more stable training process indicate promising prospects for reducing computational burdens. We therefore conclude that effective dimensionality reduction and geographic transferability are important prospects for further research towards the operational usage of deep learning algorithms, not only regarding the mapping of CPS.",center pivot systems,irrigation,semantic segmentation,U-NET,neural network,AI,Sentinel-2,REMOTE SENSING,,,,,,,,,,,,
Row_1335,"Wang, Jing","Zhang, Chen","Lin, Tianwen",,,ConvNeXt-UperNet-Based Deep Learning Model for Road Extraction from High-Resolution Remote Sensing Images,,2024,1,"When existing deep learning models are used for road extraction tasks from high-resolution images, they are easily affected by noise factors such as tree and building occlusion and complex backgrounds, resulting in incomplete road extraction and low accuracy. We propose the introduction of spatial and channel attention modules to the convolutional neural network ConvNeXt. Then, ConvNeXt is used as the backbone network, which cooperates with the perceptual analysis network UPerNet, retains the detection head of the semantic segmentation, and builds a new model ConvNeXt-UPerNet to suppress noise interference. Training on the open-source DeepGlobe and CHN6-CUG datasets and introducing the DiceLoss on the basis of CrossEntropyLoss solves the problem of positive and negative sample imbalance. Experimental results show that the new network model can achieve the following performance on the DeepGlobe dataset: 79.40% for precision (Pre), 97.93% for accuracy (Acc), 69.28% for intersection over union (IoU), and 83.56% for mean intersection over union (MIoU). On the CHN6-CUG dataset, the model achieves the respective values of 78.17% for Pre, 97.63% for Acc, 65.4% for IoU, and 81.46% for MIoU. Compared with other network models, the fused ConvNeXt-UPerNet model can extract road information better when faced with the influence of noise contained in high-resolution remote sensing images. It also achieves multiscale image feature information with unified perception, ultimately improving the generalization ability of deep learning technology in extracting complex roads from high-resolution remote sensing images.",Deep learning,semantic segmentation,remote sensing imagery,road extraction,,,,CMC-COMPUTERS MATERIALS & CONTINUA,,,,,,,,,,,,
Row_1336,"Dahle, Felix","Tanke, Julian","Wouters, Bert","Lindenbergh, Roderik",,SEMANTIC SEGMENTATION OF HISTORICAL PHOTOGRAPHS OF THE ANTARCTICA PENINSULA,"XXIV ISPRS CONGRESS IMAGING TODAY, FORESEEING TOMORROW, COMMISSION II",2022,1,"A huge archive of historical images of the Antarctica taken by the US Navy between 1940 and 2000 is publicly available. These images have not yet been used for large-scale computer-driven analysis as they were captured with analog cameras. They were only later digitized and contain no semantic information. Most modern deep-learning based semantic segmentation algorithms are trained on modern images and fail on these scanned historical images, due to varying image quality, lack of color information, and most crucially, due to artifacts in both imaging as well as scanning (e.g. Newton's rings). The analysis of such historic data can give a view on Antarctica's glaciers predating modern satellite imagery and provide a unique insight into the long-term impact of changing climate conditions with essential validation data for climate modelling. An important first step for analysis of such data is the extraction and localization of semantic information, e.g. where in the image is water, rocks, or snow. In this work we present the first deep-learning based method to perform semantic segmentation on historical imagery archives of the Antarctic Peninsula. Our results show that our method can handle very challenging images even after being trained with only a low number of training data and catch the general semantic meaning of a scene. For eight test images we achieve an accuracy of 74%, where the majority of errors can be explained by the classification of ice as snow.",Semantic Segmentation,Historic images,Cryospheric Images,U-Net,Machine Learning,Antarctica,,,,,,,,,,,,,,
Row_1337,"Cha, Keumgang","Seo, Junghoon","Choi, Yeji",,,Contrastive Multiview Coding With Electro-Optics for SAR Semantic Segmentation,,2022,12,"In the training of deep learning models, how the model parameters are initialized greatly affects the model performance, sample efficiency, and convergence speed. Recently, representation learning for model initialization has been actively studied in the remote sensing field. In particular, the appearance characteristics of the imagery obtained using the synthetic aperture radar (SAR) sensor are quite different from those of general electro-optical (EO) images, and thus, representation learning is even more important in remote sensing domain. Motivated from contrastive multiview coding, we propose multimodal representation learning for SAR semantic segmentation. Unlike previous studies, our method jointly uses EO imagery, SAR imagery, and a label mask. Several experiments show that our approach is superior to the existing methods in model performance, sample efficiency, and convergence speed.",Image segmentation,Synthetic aperture radar,Training,Radar polarimetry,Semantics,Encoding,Sensors,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Contrastive multiview coding,data fusion,multimodal representation learning,synthetic aperture radar (SAR) semantic segmentation,,,,,,,
Row_1338,"Chen, Jie","He, Peien","Zhu, Jingru","Guo, Ya","Sun, Geng",Memory-Contrastive Unsupervised Domain Adaptation for Building Extraction of High-Resolution Remote Sensing Imagery,,2023,12,"Deep learning-based semantic segmentation has been widely applied for building extraction. However, due to the domain gap, the extraction of building in high-resolution remote sensing imagery is difficult when the model trained on a source dataset is directly used to test on a target data. Considering that humans can retrieve memory to deal with correlative tasks in different domains, memory mechanisms have been developed effectively to assist cross-domain feature extraction. Besides, the effectiveness of memory mechanisms highly depends on the correlation of memory to the task. Therefore, the domain-invariant memory is crucial in cross-domain building extraction task. To this end, a memory-contrastive unsupervised domain adaptation (DA) method is proposed on the basis of a novel memory mechanism. Specifically, to facilitate the model to memorize domain-invariant features, we first conduct a normalization-based image style transfer strategy and a discriminator-based adversarial method at the image level and feature level, respectively. Subsequently, we carry out a memory-contrastive module to obtain domain-invariant features. Especially, a teacher-student network is exploited to help knowledge transferring by knowledge distillation to enhance the performance of the memory-contracted (MCD) module. To narrow the distance between the two domains, a memory bank is designed to store and update category features obtained from the source domain, and then, the similarity between category features in the target domain and memory bank is calculated. Results of the cross-domain experiments show that the proposed method can achieve optimal building extraction (github: https://github.com/RS-CSU/MDANet).",Feature extraction,Task analysis,Data mining,Buildings,Remote sensing,Training,Semantics,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Deng, Min",Building extraction,contrastive learning,domain adaption,memory,"Li, Haifeng",,semantic segmentation,,,,
Row_1339,"Zhang, Mingmei","Xue, Yongan","Zhan, Yuanyuan","Zhao, Jinling",,Semi-Supervised Semantic Segmentation-Based Remote Sensing Identification Method for Winter Wheat Planting Area Extraction,,DEC 2023,1,"To address the cost issue associated with pixel-level image annotation in fully supervised semantic segmentation, a method based on semi-supervised semantic segmentation is proposed for extracting winter wheat planting areas. This approach utilizes self-training with pseudo-labels to learn from a small set of images with pixel-level annotations and a large set of unlabeled images, thereby achieving the extraction. In the constructed initial dataset, a random sampling strategy is employed to select 1/16, 1/8, 1/4, and 1/2 proportions of labeled data. Furthermore, in conjunction with the concept of consistency regularization, strong data augmentation techniques are applied to the unlabeled images, surpassing classical methods such as cropping and rotation to construct a semi-supervised model. This effectively alleviates overfitting caused by noisy labels. By comparing the prediction results of different proportions of labeled data using SegNet, DeepLabv3+, and U-Net, it is determined that the U-Net network model yields the best extraction performance. Moreover, the evaluation metrics MPA and MIoU demonstrate varying degrees of improvement for semi-supervised semantic segmentation compared to fully supervised semantic segmentation. Notably, the U-Net model trained with 1/16 labeled data outperforms the models trained with 1/8, 1/4, and 1/2 labeled data, achieving MPA and MIoU scores of 81.63%, 73.31%, 82.50%, and 76.01%, respectively. This method provides valuable insights for extracting winter wheat planting areas in scenarios with limited labeled data.",semi-supervised classification,sematic segmentation,winter wheat,self-training,data augmentation,,,AGRONOMY-BASEL,,,,,,,,,,,,
Row_1340,"Cui, Wei","Hao, Yuanjie","Xu, Xing","Feng, Zhanyun","Zhao, Huilin",Remote Sensing Scene Graph and Knowledge Graph Matching with Parallel Walking Algorithm,,OCT 2022,7,"In deep neural network model training and prediction, due to the limitation of GPU memory and computing resources, massive image data must be cropped into limited-sized samples. Moreover, in order to improve the generalization ability of the model, the samples need to be randomly distributed in the experimental area. Thus, the background information is often incomplete or even missing. On this condition, a knowledge graph must be applied to the semantic segmentation of remote sensing. However, although a single sample contains only a limited number of geographic categories, the combinations of geographic objects are diverse and complex in different samples. Additionally, the involved categories of geographic objects often span different classification system branches. Therefore, existing studies often directly regard all the categories involved in the knowledge graph as candidates for specific sample segmentation, which leads to high computation cost and low efficiency. To address the above problems, a parallel walking algorithm based on cross modality information is proposed for the scene graph-knowledge graph matching (PWGM). The algorithm uses a graph neural network to map the visual features of the scene graph into the semantic space of the knowledge graph through anchors and designs a parallel walking algorithm of the knowledge graph that takes into account the visual features of complex scenes. Based on the algorithm, we propose a semantic segmentation model for remote sensing. The experiments demonstrate that our model improves the overall accuracy by 3.7% compared with KGGAT (which is a semantic segmentation model using a knowledge graph and graph attention network (GAT)), by 5.1% compared with GAT and by 13.3% compared with U-Net. Our study not only effectively improves the recognition accuracy and efficiency of remote sensing objects, but also offers useful exploration for the development of deep learning from a data-driven to a data-knowledge dual drive.",graph neural network,knowledge subgraph,graph matching,graph pooling,parallel walking,remote sensing,,REMOTE SENSING,"Xia, Cong",,,,,"Wang, Jin",,,,,,
Row_1341,"Chen, Hui","Cheng, Liang","Li, Ning","Yao, Yunchang","Cheng, Jian",S&GDA: An Unsupervised Domain Adaptive Semantic Segmentation Framework Considering Both Imaging Scene and Geometric Domain Shifts,,2023,1,"Unsupervised domain adaptation uses labeled data from a source domain (SD) to help learn a target domain (TD) without any labeled data. Previous studies have not systematically analyzed the causes of remote sensing (RS) domain shifts, making it difficult to effectively model domain shifts caused by differences in a geographic scene and platform imaging positions and attitudes. Therefore, this study conducts a detailed analysis of the causes of domain shifts in RS images, and an unsupervised domain adaptive semantic segmentation (UDASS) framework, called ""S & GDA"" that considers both imaging scene and geometric domain shifts is proposed. S & GDA comprised two modules: imaging scene simulation and imaging geometric simulation modules. The imaging scene simulation module is instrumental in mitigating domain shifts in geographical scenes due to variations in natural and human factors, thereby achieving cross-domain imaging scene consistency. Meanwhile, the imaging geometric simulation module allows for accurate simulation of domain shifts caused by changes in the position and attitude of a platform, ensuring cross-domain imaging geometry consistency. Note that none of these modules add additional parameters or computational complexity to the model as they only work on the input side of the data. Comprehensive experiments are conducted on the LoveDA and ISPRS datasets to evaluate S & GDA. Results indicate that S & GDA outperforms the state-of-the-art (SOTA) UDASS method by 3.12% of mIoU and can achieve 85% of the performance of the fully supervised method.",Deep learning,high-resolution remote-sensing (RS) images,land cover classification,semantic segmentation,unsupervised domain adaptation,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Zhang, Ka",,,,,,,,,,,
Row_1342,"Lee, Moung-Jin","Lee, Won-Jin","Lee, Seung-Kuk","Jung, Hyung-Sup",,Deep Learning for Remote Sensing Applications,,DEC 2022,1,"Recently, deep learning has become more important in remote sensing data processing. Huge amounts of data for artificial intelligence (AI) has been designed and built to develop new technologies for remote sensing, and AI models have been learned by the AI training dataset. Artificial intelligence models have developed rapidly, and model accuracy is increasing accordingly. However, there are variations in the model accuracy depending on the person who trains the AI model. Eventually, experts who can train AI models well are required more and more. Moreover, the deep learning technique enables us to automate methods for remote sensing applications. Methods having the performance of less than about 60% in the past are now over 90% and entering about 100%. In this special issue, thirteen papers on how deep learning techniques are used for remote sensing applications will be introduced.",Key Words: Remote sensing,Deep learning,Artificial intelligence,,,,,KOREAN JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1343,"Li, Qingyu","Mou, Lichao","Hua, Yuansheng","Sun, Yao","Jin, Pu",INSTANCE SEGMENTATION OF BUILDINGS USING KEYPOINTS,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,18,"Building segmentation is of great importance in the task of remote sensing imagery interpretation. However, the existing semantic segmentation and instance segmentation methods often lead to segmentation masks with blurred boundaries. In this paper, we propose a novel instance segmentation network for building segmentation in high-resolution remote sensing images. More specifically, we consider segmenting an individual building as detecting several keypoints. The detected keypoints are subsequently reformulated as a closed polygon, which is the semantic boundary of the building. By doing so, the sharp boundary of the building could be preserved. Experiments are conducted on selected Aerial Imagery for Roof Segmentation (AIRS) dataset, and our method achieves better performance in both quantitative and qualitative results with comparison to the state-of-the-art methods. Our network is a bottom-up instance segmentation method that could well preserve geometric details.",deep network,instance segmentation,keypoint detection,building,aerial imagery,,,,"Shi, Yilei",,,,,"Zhu, Xiao Xiang",,,,,,
Row_1344,"Zou, Yanling","Weinacker, Holger","Koch, Barbara",,,"Towards Urban Scene Semantic Segmentation with Deep Learning from LiDAR Point Clouds: A Case Study in Baden-Wurttemberg, Germany",,AUG 2021,10,"An accurate understanding of urban objects is critical for urban modeling, intelligent infrastructure planning and city management. The semantic segmentation of light detection and ranging (LiDAR) point clouds is a fundamental approach for urban scene analysis. Over the last years, several methods have been developed to segment urban furniture with point clouds. However, the traditional processing of large amounts of spatial data has become increasingly costly, both time-wise and financially. Recently, deep learning (DL) techniques have been increasingly used for 3D segmentation tasks. Yet, most of these deep neural networks (DNNs) were conducted on benchmarks. It is, therefore, arguable whether DL approaches can achieve the state-of-the-art performance of 3D point clouds segmentation in real-life scenarios. In this research, we apply an adapted DNN (ARandLA-Net) to directly process large-scale point clouds. In particular, we develop a new paradigm for training and validation, which presents a typical urban scene in central Europe (Munzingen, Freiburg, Baden-Wurttemberg, Germany). Our dataset consists of nearly 390 million dense points acquired by Mobile Laser Scanning (MLS), which has a rather larger quantity of sample points in comparison to existing datasets and includes meaningful object categories that are particular to applications for smart cities and urban planning. We further assess the DNN on our dataset and investigate a number of key challenges from varying aspects, such as data preparation strategies, the advantage of color information and the unbalanced class distribution in the real world. The final segmentation model achieved a mean Intersection-over-Union (mIoU) score of 54.4% and an overall accuracy score of 83.9%. Our experiments indicated that different data preparation strategies influenced the model performance. Additional RGB information yielded an approximately 4% higher mIoU score. Our results also demonstrate that the use of weighted cross-entropy with inverse square root frequency loss led to better segmentation performance than when other losses were considered.",urban scene,mobile mapping,deep learning,remote sensing,point clouds,semantic segmentation,unbalanced classes,REMOTE SENSING,,,,,,,,,,,,
Row_1345,"Bai, Xiangtian","Guo, Li","Huo, Hongyuan","Zhang, Jiangshui","Zhang, Yi",Rse-net: Road-shape enhanced neural network for Road extraction in high resolution remote sensing image,,OCT 17 2024,4,"Automatically extracting roads from remote sensing images is increasingly important for autonomous driving and geographic information systems. However, due to factors such as ground objects blocking road boundaries and some roads being narrow and long in high-resolution remote sensing images, it is difficult for the current mainstream pixel-level extraction model to guarantee the continuity of roads and the smoothness of boundaries. To solve this problem, this paper designs a neural network Rse-net based on semantic segmentation, using a two-stream semantic segmentation network algorithm to make the network pay more attention to the boundary information of roads and narrow roads. This structure uses shape stream to process road boundary information (roads' shape stream), and processes it in parallel with classic stream. Use the Gated Shape CNN to connect the two streams of roads' shape stream and classic stream, and use the classical stream to optimize the boundary information in the shape stream. At the same time, a multi-scale convolutional attention mechanism is used between the decoder and the encoder to expand the receptive field through large-core attention, and obtain more semantic information without causing too much calculation. Finally, the effectiveness of the proposed network is verified by comparative experiments.",road extraction,semantic segmentation,deep learning,Convolutional Neural Network,multi-scale,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,"Li, Zhao-Liang",,,,,,,,,,,
Row_1346,"Xiao, Rong","Wang, Yuze","Tao, Chao",,,Fine-Grained Road Scene Understanding From Aerial Images Based on Semisupervised Semantic Segmentation Networks,,2022,8,"High-precision electronic maps are required to provide more detailed and accurate information than traditional maps. With the rapid development of high-resolution remote sensing technology, it has become possible to extract fine-grained road scene information such as vehicles, road lines, zebra crossings, ground signs, and lane widths of roads from unmanned aerial vehicle (UAV) remote sensing images, which opens up opportunities for automatic mapping high-precision maps. The traditional method of deciphering remote sensing images is often obtained through manual visual interpretation. Due to the high cost and long lead time of this method, it leads to inefficiencies in updating large amounts of information. To address this problem, this letter models the fine-grained road scene understanding task as an image semantic segmentation problem and innovatively proposes a semisupervised fully convolutional neural network to extract the information efficiently at a low cost. Compared with the traditional supervised full convolutional neural network, this method can simultaneously optimize the standard supervised classification loss on labeled samples and the unsupervised consistency loss on unlabeled samples by using an integrated prediction technology and then input them to the end-to-end semantic segmentation network for training. This method is designed to effectively improve the classification accuracy of the semantic segmentation network and validly alleviates overfitting problems in the case of small numbers of labeled samples. In order to verify the effectiveness of this method, we constructed a data set for experimental, which is used to verify the effect of a variable number of unlabeled samples on model performance. Experimental results show that our method can efficiently complete the extraction of fine-grained road scene information such as vehicles, road lines, zebra crossings, ground signs, and lane widths of roads with a small number of labeled samples.",Roads,Semantics,Training,Image segmentation,Data models,Task analysis,Feature extraction,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Deep learning,road scene understanding,semisupervised learning (SSL),UAV remote sensing,,,,,,,
Row_1347,"Yu, Zhenyu","Wang, Jinnian","Yang, Xiankun","Ma, Juan",,Superpixel-Based Style Transfer Method for Single-Temporal Remote Sensing Image Identification in Forest Type Groups,,AUG 2023,1,"Forests are the most important carbon reservoirs on land, and forest carbon sinks can effectively reduce atmospheric CO2 concentrations and mitigate climate change. In recent years, various satellites have been launched that provide opportunities for identifying forest types with low cost and high time efficiency. Using multi-temporal remote sensing images and combining them with vegetation indices takes into account the vegetation growth pattern and substantially improves the identification accuracy, but it has high requirements for imaging, such as registration, multiple times, etc. Sometimes, it is difficult to satisfy, the plateau area is severely limited by the influence of clouds and rain, and Gaofen (GF) data require more control points for orthophoto correction. The study area was chosen to be Huize County, situated in Qujing City of Yunnan Province, China. The analysis was using the GF and Landsat images. According to deep learning and remote sensing image feature extraction methods, the semantic segmentation method of F-Pix2Pix was proposed, and the domain adaptation method according to transfer learning effectively solved the class imbalance in needleleaf/broadleaf forest identification. The results showed that (1) this method had the best performance and a higher accuracy than the existing products, 21.48% in non-forest/forest and 29.44% in needleleaf/broadleaf forest for MIoU improvement. (2) Applying transfer learning domain adaptation to semantic segmentation showed significant benefits, and this approach utilized satellite images of different resolutions to solve the class imbalance problem. (3) It can be used for long-term monitoring of multiple images and has strong generalization. The identification of needleleaf and broadleaf forests combined with the actual geographical characteristics of the forest provides a foundation for the accurate estimation of regional carbon sources/sinks.",forest identification,superpixel,semantic segmentation,class imbalance,needleleaf and broadleaf forest,,,REMOTE SENSING,,,,,,,,,,,,
Row_1348,"Kerdegari, Hamideh","Razaak, Manzoor","Argyriou, Vasileios","Remagnino, Paolo",,Urban Scene Segmentation using Semi-supervised GAN,IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXV,2019,7,"Semantic segmentation of remote sensing data such as multispectral imagery has been boosted recently using deep convolutional neural networks (CNN). However, segmentation of multispectral images using supervised machine learning algorithms such as CNN requires a significant number of pixel-level annotated data, often unavailable, making the task extremely challenging. To address this, this paper puts forward a semi-supervised framework, based on generative adversarial networks (GAN). The proposed solution consists of a generator network to provide photo-realistic images as extra training data to a multi-class classifier acting as a discriminator and trained on a small annotated dataset. Performance of the proposed semi-supervised GAN is evaluated on two benchmarks multispectral semantic segmentation datasets collected from urban scenes of Vaihingen and Potsdam. Results indicate that the proposed framework achieves competitive performance compared to state-of-the-art semantic segmentation methods and show the potential of GAN-based methods for the challenging task of multispectral image segmentation.",Semantic segmentation,Generative adversarial networks (GAN),Semi-supervised GAN,Convolutional neural network,Multispectral imagery,,,,,,,,,,,,,,,
Row_1349,"Weinmann, Martin","Hinz, Stefan","Weinmann, Michael",,,A Hybrid Semantic Point Cloud Classification-Segmentation Framework Based on Geometric Features and Semantic Rules,,AUG 2017,9,"In this paper, we focus on semantic point cloud classification taking into account standard failure cases reported in a variety of investigations. We present a hybrid two-step framework integrating classification, segmentation and semantic rules in a common end-to-end processing pipeline from irregularly distributed points to semantically labelled point clouds. The first step of our framework consists of a point-wise semantic point cloud classification based on rather intuitive, handcrafted, low-level geometric features extracted from local neighbourhoods of locally adaptive size. The second step of our framework consists of refining the point-wise classification results by considering semantic rules applied to geometric features extracted on the basis of an over-segmentation of the derived class-wise point clouds. We demonstrate the performance of our framework on a standard benchmark dataset for which we obtain a semantic labelling of high accuracy and high plausibility.",Laser scanning,Point cloud,Feature extraction,Classification,Semantic segmentation,,,PFG-JOURNAL OF PHOTOGRAMMETRY REMOTE SENSING AND GEOINFORMATION SCIENCE,,,,,,,,,,,,
Row_1350,"Ouyang, Shubing","Chen, Weitao","Qin, Xuwen","Yang, Jinzhong",,Geological Background Prototype Learning-Enhanced Network for Remote-Sensing-Based Engineering Geological Lithology Interpretation in Highly Vegetated Areas,,2024,3,"Remote-sensing-based interpretation of engineering geological lithology units provides essential lithological information for engineering planning, design, and construction projects. However, dense vegetation cover can easily interfere with the lithological spectral characteristics of optical remote sensing. Utilizing known geological background prototype-guided knowledge as a guide can mitigate interpretation interference from vegetation. Hence, for the interpretation of engineering geological lithology units, we aimed to develop a novel semantic segmentation network, LSBPnet, which combines a geological background prototype-guided learning model with a U-Net-based model. The geological background prototype-guided learning model employs lithological prototype features learned from the surrounding geological units of the predicted target to identify similarities with the predicted target's features. The model encompasses three steps: inputting known background images and known labels, learning prototype features of the background information, and learning target features based on the background information prototype perception. Using the engineering geological lithology of the Yangtze River's middle reaches as the focus area, which is a region with high vegetation cover, the proposed network achieved a 13.94% increase in the overall accuracy compared with that of commonly prevalent semantic segmentation networks. Incorporating the geological background prototype-guided knowledge helped to strengthen the distinction and relationship between the target and its surrounding lithological features, thereby improving the assessment of prediction outcomes. The diverse ablation experiments further highlighted the importance of incorporating background prototype-guided knowledge.",Geology,Rocks,Vegetation mapping,Feature extraction,Remote sensing,Surveys,Soft sensors,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Deep learning,engineering geological lithology units,prototype learning,remote sensing,,,semantic segmentation,,,,
Row_1351,"Park, Seong Wook","Lee, Yang Won",,,,Detection of forest disaster using satellite images with sematic segmentation,IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXV,2019,2,"In recent 10 years, forest damage caused by forest fires in Korea has increased significantly compared to previous years. Therefore, interest and concern about damage caused by forest fires are very important in terms of environmental and ecosystem. According to various domestic and international research results, forests perform functions such as reporting of life resources, prevention of desertification, and adjustment of micro climate. There are many studies to extract the damage areas based on hyper spectral aerial image, high resolution satellite image, vegetation index and factors affecting the forest environment. However, there are limitations that the indexes have different threshold values depending on the region and season, and the threshold value must be continuously adjusted in order to detect the concentration of the damage areas. In this study, we detected forest disaster damaged areas through satellite image data and deep learning. We collected image data on Landsat satellite and applied to the detection of damaged area using U-net [1] and SegNet [2] models. We tried to verify the applicability of semantic segmentation for remote sensing, compare and evaluate each model, and build an optimal forest disaster detection model.",Forest disaster,Remote sensing,Satellite images,Semantic segmentation,,,,,,,,,,,,,,,,
Row_1352,"Wen, Dawei","Huang, Xin","Yang, Qiquan","Tang, Jianqin",,Adaptive Self-Paced Collaborative and 3-D Adversarial Multitask Network for Semantic Change Detection Using Zhuhai-1 Orbita Hyperspectral Remote Sensing Imagery,,2024,2,"In recent years, numerous change detection methodologies have been proposed, with a predominant focus on binary change detection. Furthermore, there exists a paucity of research addressing semantic change detection in scenarios where solely binary change labels are available. This article introduces a multitask network for semantic change detection. First, 3-D ResUnet model is employed to generate initial multitemporal land cover results through postclassification comparison. Subsequently, the multitask network, encompassing two subtasks-binary change detection and multitemporal semantic segmentation-is proposed. Specifically, the shared branch of the network employs 3-D residual blocks to extract joint spectral-spatial features. In the subsequent task-specific branch, a 3-D GAN is incorporated for the binary change detection task to enhance the discrimination ability of latent high-level features for changes. Novel adaptive self-paced learning and certainty-weighted focal loss are proposed for multitemporal semantic segmentation to mitigate adverse effects from noisy semantic labels by considering sample complexity and reliability in the network optimization process. Experiments conducted on the Orbita Hyperspectral dataset in the Xiong'an New Area demonstrate the superior performance of the proposed method, achieving 99.28% and 76.60% for overall accuracy and kappa, respectively. This outperformance is notable when compared to other methods, such as Str4 and Bi-SRNet, showing an increase of 39.82% and 54.17% for kappa. Moreover, comparative experiments on SECOND further confirm the advantage of the proposed method, achieving 54.62% for kappa and outperforming other comparative methods, such as Bi-SRNet (47.61%).",Change detection,hyperspectral remote sensing,multitask learning,pseudolabel,semantic change detection,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,
Row_1353,"Zhang, Guangyun","Zhang, Rongting",,,,MeshNet-SP: A Semantic Urban 3D Mesh Segmentation Network with Sparse Prior,,NOV 2023,1,"A textured urban 3D mesh is an important part of 3D real scene technology. Semantically segmenting an urban 3D mesh is a key task in the photogrammetry and remote sensing field. However, due to the irregular structure of a 3D mesh and redundant texture information, it is a challenging issue to obtain high and robust semantic segmentation results for an urban 3D mesh. To address this issue, we propose a semantic urban 3D mesh segmentation network (MeshNet) with sparse prior (SP), named MeshNet-SP. MeshNet-SP consists of a differentiable sparse coding (DSC) subnetwork and a semantic feature extraction (SFE) subnetwork. The DSC subnetwork learns low-intrinsic-dimensional features from raw texture information, which increases the effectiveness and robustness of semantic urban 3D mesh segmentation. The SFE subnetwork produces high-level semantic features from the combination of features containing the geometric features of a mesh and the low-intrinsic-dimensional features of texture information. The proposed method is evaluated on the SUM dataset. The results of ablation experiments demonstrate that the low-intrinsic-dimensional feature is the key to achieving high and robust semantic segmentation results. The comparison results show that the proposed method can achieve competitive accuracies, and the maximum increase can reach 34.5%, 35.4%, and 31.8% in mR, mF1, and mIoU, respectively.",3D real scene,urban 3D mesh,semantic segmentation,sparse prior,low intrinsic dimension,convolutional neural network,,REMOTE SENSING,,,,,,,,,,,,
Row_1354,"Chen, Cong","Wang, Yuzhu","Yang, Shuang","Ji, Xiaohui","Wang, Gongwen",A K-Net-based hybrid semantic segmentation method for extracting lake water bodies,,NOV 2023,7,"Lakes have a crucial impact on natural disaster prevention, resource recycling, maintenance of agricultural production and daily life. The traditional way of acquiring lake water body information lacks efficiency, is dangerous, and is not suitable for lake water body information acquisition and real-time monitoring. For this reason, the automated lake water body extraction method based on deep learning semantic segmentation model is gradually becoming a mainstream method. However, most of the semantic segmentation models used for lake extraction today express features through static semantics, while ignoring the extraction relationships of different convolutional kernels for these features. In order to better extract lake water bodies from remote sensing images, this paper proposes a hybrid semantic segmentation method based on K-Net, which achieves high accuracy extraction of lake water bodies by introducing dynamic semantic kernels to iteratively refine the feature infor-mation. The superiority of the K-Net-based hybrid model on a Google remote sensing image dataset of lakes is validated. The experimental results show that (1) the hybrid model is able to achieve accurate extraction of lake water bodies, with the UperNet +K-Net model using Swin-l performing the best among all six evaluation metrics, with mean intersection over union (mIoU) reaching 97.77%; and that (2) after incorporating the K-Net module, all tested models obtain a larger mIoU than before.",K -Net,Semantic segmentation,Water body,Hybrid model,,,,ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,,,,,,,,,,,,
Row_1355,"Zhang, Zhihui","Guo, Li","Wang, Xinzhe","Fan, Jianchao",,BAGAN: Semantic Segmentation of Oil Spills in Optical Remote Sensing Images Based on Bottleneck Attention Module and Generative Adversarial Networks,,2024,0,"To address the serious threat that marine oil spills pose to the ecological environment, satellite remote sensing technology for monitoring these spills is especially important. Optical satellite remote sensing, with its benefits of high spatial and temporal resolution, high spectral resolution, and autonomous operation, can effectively track and monitor marine oil spill incidents. However, the acquisition of optical satellite data is highly susceptible to climate conditions, making it difficult to obtain sufficient data for efficient information extraction. Therefore, this article proposes a bottleneck attention generative adversarial network (BAGAN) that can accurately extract oil spill areas from visible remote sensing data. The addition of data preprocessing and bottleneck attention module (BAM) in the model has improved the accuracy of oil spill segmentation. Through the optical remote sensing data of GF-1 satellite in the Bohai Bay from 2022 to 2023, the average accuracy reached 89.11%, verifying that this model is superior to other models in semantic segmentation.",Marine oil spill monitoring,optical image,generative adversarial networks,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1356,"Vardanjani, Samaneh Molavi","Fathi, Abdolhossein","Moradkhani, Kaveh",,,Grsnet: gated residual supervision network for pixel-wise building segmentation in remote sensing imagery,,JUL 3 2022,6,"The increasing development of imaging technology has made aerial image analysis one of the most widely used fields in image processing. Building extraction is the basic step in analysing urban structures, detecting construction violations, updating urban geographical divisions, and forecasting natural disasters. In this study, the aim is to automatically segment buildings in high-resolution satellite images using a new hybrid deep learning model, named Gated Residual Supervision Network (GRSNet). GRSNet extends the UNet framework by including three important components, i.e. attention gates (AG), residual units, and deep supervision part, with the main focus on transferring and reusing features. At first, the AG mechanism utilizes channel and spatial fine features to merge them effectively and deep supervision transfers feature details from the depths of the network. Then, residual units retrieve the information at different levels to train the model quickly. Finally, a fully connected classifier recovers features from the input image. GRSNet is evaluated on Massachusetts buildings public dataset and Inria aerial image labeling benchmark. The obtained results show the superiority of the proposed method compared to other deep learning-based state-of-the-art building segmentation methods with an intersection over union (IOU) of 89.86% and an F1-score of 94.53%.",building extraction,deep learning,semantic segmentation,high-resolution imagery,remote sensing,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1357,"Cui, Fengzhi","Jiang, Jie",,,,MTSCD-Net: A network based on multi-task learning for semantic change detection of bitemporal remote sensing images,,APR 2023,31,"In recent years, change detection has been one of the hot research topics within the field of remote sensing. Previous studies have concentrated on binary change detection (BCD), but it doesn't meet the current needs. Therefore, semantic change detection (SCD) is also gradually developing, which focuses on determining the specific changed type while obtaining changed areas. In the paper, we propose a multi-task learning method (MTSCD-Net) for SCD task. The SCD task is decoupled into two related subtasks, semantic segmentation (SS) and BCD, then unifies them under the same framework. Multi-scale features are extracted using the Siamese semantic-aware encoder based on Swin Transformer, and the aggregation module is designed to combine features. Then, the change information extraction module is designed to enhance the capacity to express features by fully integrating the two-level difference features that are generated from fused features. Moreover, in the decoder stage, the spatial attention weight map is obtained using the features of the BCD subtask, which provides location prior information for the features of the SS subtask. It helps fully explore the correlation between the two subtasks. The two loss functions of subtasks are weighted to train MTSCD-Net. The comparative experiments results on two typical SCD datasets confirm the advantage of MTSCD-Net for SCD task. For the SeK index, MTSCD-Net achieves 3.96% and 20.57% on HRSCD and SECOND datasets, respectively. This outperforms other comparative methods such as Bi-SRNet (which achieves 4.86% and 1.47% higher on two datasets, respectively). The same is true for the Score metric. Moreover, the ablation experiment results confirm the effectiveness of key modules.",Remote sensing,Multi -task learning,Siamese network,Deep learning,Semantic change detection,,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,
Row_1358,"Wei, Ruizeng","Wang, Lei","Wang, Tong","Zhou, Enze","Liu, Shuqin",Segmentation of high-voltage transmission wires from remote sensing images using U-Net with sample generation,,AUG 3 2022,2,"This paper proposes an approach under the framework of deep learning for the task of identification and extraction of high-voltage transmission wires in power grids based on high-resolution optical remote sensing images. First, the task is defined as a semantic segmentation problem. Based on the recently proposed Swin-Unet framework, an intuitive improvement named 'Swin-Unet-M' with a refined linear embedding is proposed, improving the wire segmentation performance. Second, an effective sample synthesis technique is developed to generate training images containing wires. The wire target is randomly merged in background images to generate a rich pseudo-wire segmentation training dataset. Consequently, the cost of manually pixel-wise wire labelling is largely reduced. Experimental results demonstrate that the proposed Swin-Unet-M model can obtain higher segmentation performance and effectively identify and extract the wires in real images.",Power grids,transmission wire segmentation,U-Net,Swin Transformer,training sample generation,,,REMOTE SENSING LETTERS,"He, Huan",,,,,"Yang, Hemeng","Wang, Sen",,,,,
Row_1359,"Wang, Di","Zhang, Qiming","Xu, Yufei","Zhang, Jing","Du, Bo",Advancing Plain Vision Transformer Toward Remote Sensing Foundation Model,,2023,109,"Large-scale vision foundation models have made significant progress in visual tasks on natural images, with vision transformers (ViTs) being the primary choice due to their good scalability and representation ability. However, large-scale models in remote sensing (RS) have not yet been sufficiently explored. In this article, we resort to plain ViTs with about 100 million parameters and make the first attempt to propose large vision models tailored to RS tasks and investigate how such large models perform. To handle the large sizes and objects of arbitrary orientations in RS images, we propose a new rotated varied-size window attention to replace the original full attention in transformers, which can significantly reduce the computational cost and memory footprint while learning better object representation by extracting rich context from the generated diverse windows. Experiments on detection tasks show the superiority of our model over all state-of-the-art models, achieving 81.24% mean average precision (mAP) on the DOTA-V1.0 dataset. The results of our models on downstream classification and segmentation tasks also show competitive performance compared to existing advanced methods. Further experiments show the advantages of our models in terms of computational complexity and data efficiency in transferring. The code and models will be released at https://github.com/ViTAE-Transformer/Remote-Sensing-RVSA.",Task analysis,Transformers,Computational modeling,Feature extraction,Remote sensing,Adaptation models,Visualization,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Tao, Dacheng",Object detection,remote sensing (RS),scene classification,semantic segmentation,"Zhang, Liangpei",,vision transformer (ViT),,,,
Row_1360,"Stache, Felix","Westheider, Jonas","Magistri, Federico","Stachniss, Cyrill","Popovic, Marija",Adaptive path planning for UAVs for multi-resolution semantic segmentation?,,JAN 2023,14,"Efficient data collection methods play a major role in helping us better understand the Earth and its ecosystems. In many applications, the usage of unmanned aerial vehicles (UAVs) for monitoring and remote sensing is rapidly gaining momentum due to their high mobility, low cost, and flexible deployment. A key challenge is planning missions to maximize the value of acquired data in large environments given flight time limitations. This is, for example, relevant for monitoring agricultural fields. This paper addresses the problem of adaptive path planning for accurate semantic segmentation of using UAVs. We propose an online planning algorithm which adapts the UAV paths to obtain highresolution semantic segmentations necessary in areas with fine details as they are detected in incoming images. This enables us to perform close inspections at low altitudes only where required, without wasting energy on exhaustive mapping at maximum image resolution. A key feature of our approach is a new accuracy model for deep learning-based architectures that captures the relationship between UAV altitude and semantic segmentation accuracy. We evaluate our approach on different domains using real-world data, proving the efficacy and generability of our solution.(c) 2022 Elsevier B.V. All rights reserved.",Unmanned aerial vehicles,Semantic segmentation,Planning,Terrain monitoring,,,,ROBOTICS AND AUTONOMOUS SYSTEMS,,,,,,,,,,,,
Row_1361,"Yuan, Hao","Shen, Yongjian","Lv, Ning","Li, Yuheng","Chen, Chen",Residual Attention Mechanism for Remote Sensing Target Hiding,,OCT 2023,0,"In this paper, we investigate deep-learning-based image inpainting techniques for emergency remote sensing mapping. Image inpainting can generate fabricated targets to conceal real-world private structures and ensure informational privacy. However, casual inpainting outputs may seem incongruous within original contexts. In addition, the residuals of original targets may persist in the hiding results. A Residual Attention Target-Hiding (RATH) model has been proposed to address these limitations for remote sensing target hiding. The RATH model introduces the residual attention mechanism to replace gated convolutions, thereby reducing parameters, mitigating gradient issues, and learning the distribution of targets present in the original images. Furthermore, this paper modifies the fusion module in the contextual attention layer to enlarge the fusion patch size. We extend the edge-guided function to preserve the original target information and confound viewers. Ablation studies on an open dataset proved the efficiency of RATH for image inpainting and target hiding. RATH had the highest similarity, with a 90.44% structural similarity index metric (SSIM), for edge-guided target hiding. The training parameters had 1M fewer values than gated convolution (Gated Conv). Finally, we present two automated target-hiding techniques that integrate semantic segmentation with direct target hiding or edge-guided synthesis for remote sensing mapping applications.",emergency remote sensing mapping,image inpainting,residual attention mechanism,target hiding,,,,REMOTE SENSING,"Zhang, Zhouzhou",,,,,,,,,,,
Row_1362,"Huang, Shuowen","Hu, Qingwu","Zhao, Pengcheng","Li, Jiayuan","Ai, Mingyao",ALS Point Cloud Semantic Segmentation Based on Graph Convolution and Transformer With Elevation Attention,,2024,2,"Semantic segmentation of airborne point clouds is crucial for 3D scene reconstruction and remote sensing in surveying applications. Current deep learning methods for point clouds primarily focus on effectively aggregating local neighborhood information. However, they often overlook the fusion of global context information and elevation features, which are vital for airborne point clouds. In this study, we propose Dense-LGEANet, a novel network with dense connected architecture and multiscale feature supervision based on our designed LGEA module. The key component of our LGEA module is the combination of the graph convolution block and the transformer block with elevation attention. It can effectively fuse local neighborhood information and global context information to improve the accuracy of semantic segmentation of airborne point cloud. Moreover, the designed dense connected network architecture can enhance the feature extraction capability for point cloud objects at different scales by facilitating interactions between multiple up-sampling and down-sampling layers. We have conducted multiple experiments on the public point cloud dataset. Experimental results show that our method can achieve an mIoU of 58.5% and an mF1 of 72.0% on the ISPRS Vaihingen 3D dataset, while an mIoU of 67.2% and an mF1 of 78.3% on the LASDU dataset. It demonstrates the superior performance of our network and the effectiveness of the proposed feature enhancement module and network architecture.",Airborne laser scanning (ALS),graph convolution,point cloud,semantic segmentation,transformer,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Wang, Shaohua",,,,,,,,,,,
Row_1363,"Chen, Zizi","Small, Gary W.",,,,Semantic segmentation of chemical plumes from airborne multispectral infrared images using U-Net,,DEC 2022,1,"The United States Environmental Protection Agency Airborne Spectral Photometric Environmental Collection Technology program provides infrared (IR) remote sensing capabilities from an aircraft platform to assist first responders in managing chemical releases into the atmosphere. One of the instruments used is a downward-looking eight-band multispectral imaging system that receives the upwelling IR radiance from the ground and atmosphere below the aircraft. Volatile organic compounds absorb and emit IR radiation at characteristic wavelengths and produce unique signatures in the imaging data. To automate the detection of chemical plumes, this research applied a deep learning-based semantic segmentation model on multispectral images collected during controlled releases of methanol. A U-Net model was developed for this application, and multiple experiments were conducted to optimize the model. Issues studied included the use of a temperature and emissivity separation algorithm to suppress temperature effects, a custom normalization method to reduce scene composition variance, experiments to shrink the network architecture, and evaluation of the utility of data augmentation. The optimized U-Net model was able to detect the plume area in images collected across different temperatures and locations while achieving false detection rates < 0.02%. The U-Net model exhibited an improved ability to discriminate methanol plumes from other scene elements such as buildings and roads when compared to the performance of shallow neural networks studied in previous work.",Remote sensing,Infrared multispectral imaging,Deep learning,Semantic segmentation,U-Net,,,NEURAL COMPUTING & APPLICATIONS,,,,,,,,,,,,
Row_1364,"Shao, Zhenfeng","Zhou, Weixun","Deng, Xueqing","Zhang, Maoding","Cheng, Qimin",Multilabel Remote Sensing Image Retrieval Based on Fully Convolutional Network,,2020,148,"Conventional remote sensing image retrieval (RSIR) system usually performs single-label retrieval where each image is annotated by a single label representing the most significant semantic content of the image. In this scenario, however, the scene complexity of remote sensing images is ignored, where an image might have multiple classes (i.e., multiple labels), resulting in poor retrieval performance. We therefore propose a novel multilabel RSIR approach based on fully convolutional network (FCN). Specifically, FCN is first trained to predict segmentation map of each image in the considered image archive. We then obtain multilabel vector and extract region convolutional features of each image based on its segmentation map. The extracted region features are finally used to perform region-based multilabel retrieval. The experimental results show that our approach achieves state-of-the-art performance in contrast to handcrafted and convolutional neural network features.",Fully convolutional networks (FCN),multilabel retrieval,multilabel vector,region convolutional features (RCFs),remote sensing image retrieval (RSIR),single-label retrieval,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,
Row_1365,"Dong, Runmin","Li, Weijia","Fu, Haohuan","Xia, Maocai","Zheng, Juepeng",Semantic segmentation based large-scale oil palm plantation detection using high-resolution satellite images,AUTOMATIC TARGET RECOGNITION XXIX,2019,4,"Detecting oil palm plantation from high-resolution satellite images can provide the necessary information for palm oil production estimation and oil palm plantation layout planning, etc. In this paper, we proposed a novel semantic segmentation based approach for large-scale oil palm plantation detection using QuickBird images and Google Earth Images (in 0.6-m spatial resolution) in Malaysia. We manually labeled a dataset for pixel-wise semantic segmentation into four categories: oil palm plantation, other vegetation, impervious/cloud, and the others (e.g. water and uncertain pixels). We presented an end-to-end deep convolutional neural network (DCNN) for semantic segmentation followed by fully connected conditional random fields (CRF) and applied an ensemble learning method to improve the localization of boundaries. The overall accuracy and mean IoU of our proposed approach in test regions are 95.27% and 88.46%, which are greatly better than the results of the other three common semantic segmentation methods and patch-based CNN method.",oil palm,semantic segmentation,deep learning,remote sensing,satellite images,,,,"Yu, Le",,,,,,,,,,,
Row_1366,"Yang, Wen","Zhang, Xun","Chen, Lijun","Sun, Hong",,Semantic Segmentation of Polarimetric SAR Imagery Using Conditional Random Fields,2010 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2010,8,"The paper proposes a fast and accurate semantic segmentation approach for a large Polarimetric SAR (PolSAR) image using Conditional Random Fields (CRFs). It efficiently incorporates the polarimetric signatures, texture and intensity features into a unite CRFs model, and employs a fast max-margin training method for parameters learning. Experiments on RadarSat-2 PolSAR data in Flevoland test site demonstrate that our approach achieves precise segmentation results with a few well-selected training samples.",polarimetric SAR,semantic segmentation,conditional random fields,,,,,,,,,,,,,,,,,
Row_1367,"Bergamasco, Luca","Bovolo, Francesca","Bruzzone, Lorenzo",,,A Dual-Branch Deep Learning Architecture for Multisensor and Multitemporal Remote Sensing Semantic Segmentation,,2023,14,"Multisensor data analysis allows exploiting heterogeneous data regularly acquired by the many available remote sensing (RS) systems. Machine- and deep-learning methods use the information of heterogeneous sources to improve the results obtained by using single-source data. However, the state-of-the-art methods analyze either the multiscale information of multisensor multiresolution images or the time component of image time series. We propose a supervised deep-learning classification method that jointly performs a multiscale and multitemporal analysis of RS multitemporal images acquired by different sensors. The proposed method processes very-high-resolution (VHR) images using a residual network with a wide receptive field that handles geometrical details and multitemporal high-resolution (HR) image using a 3-D convolutional neural network that analyzes both the spatial and temporal information. The multiscale and multitemporal features are processed together in a decoder to retrieve a land-cover map. We tested the proposed method on two multisensor and multitemporal datasets. One is composed of VHR orthophotos and Sentinel-2 multitemporal images for pasture classification, and another is composed of VHR orthophotos and Sentinel-1 multitemporal images. Results proved the effectiveness of the proposed classification method.",Deep learning (DL) classification,multiresolution,multisensor data,multitemporal images,remote sensing (RS),very-high-resolution (VHR) images,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,
Row_1368,"Wang, Yongzhi","Lv, Hua","Deng, Rui","Zhuang, Shengbing",,A Comprehensive Survey of Optical Remote Sensing Image Segmentation Methods,,SEP 2 2020,17,"Many papers have reviewed remote sensing image segmentation (RSIS) algorithms currently. Those existing surveys are insufficiently exhaustive to sort out the various RSIS methods, it is impossible to comprehensively compare characteristics of different RSIS methods. In addition, the segmentation efficiency and accuracy of the RSIS methods cannot always meet the subsequent image analysis requirements. Thus, a clear comparative analysis of various RSIS methods is essential to provide an in-depth understanding of RSIS and theoretical ideas for conducting in-depth research in the future. The goal of this article is to provide readers with the latest information on optical RSIS technology. Comparative measures of these methods are provided in terms of conceptual details, the merits and demerits, and the performance of various RSIS methods. Moreover, various RSIS methods' experiments are carried out on optical images using the NWPU VHR-10 public remote sensing datasets. Through the review of optical RSIS methods, this paper provides data as complete as possible for further related research and development of RSIS methods.",,,,,,,,CANADIAN JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1369,"Wang, Hanxiang","Yu, Fan","Xie, Junwei","Wang, Haonan","Zheng, Haotian",Road Extraction Based On Improved Deeplabv3 Plus In Remote Sensing Image,URBAN GEOINFORMATICS 2022,2022,7,"Urban roads in remote sensing images will be disturbed by surrounding ground features such as building shadows and tree shadows, and the extraction results are prone to problems such as incomplete road structure, poor topological connectivity, and poor accuracy. For mountain roads, there will also be problems such as hill shadow or vegetation occlusion. We propose an improved Deeplabv3+ semantic segmentation network method. This method uses ResNeSt, which introduces channel attention, as the backbone network, and combines the ASPP module to obtain multi-scale information, thereby improving the accuracy of road extraction. Analysis of the experimental results on the Deeplglobe dataset shows that the intersection ratio and accuracy of the method in this paper are 63.15% and 73.16%, respectively, which are better than other methods.",Remote Sensing,Road Extraction,High-Resolution Remote Sensing Image,Deep Learning,Deeplabv3 Plus,ASPP,,,,,,,,,,,,,,
Row_1370,"Wang, Yupei","Shi, Hao","Zhuang, Yin","Sang, Qianbo","Chen, Liang",Bidirectional Grid Fusion Network for Accurate Land Cover Classification of High-Resolution Remote Sensing Images,,2020,4,"Land cover classification has achieved significant advances by employing deep convolutional network (ConvNet) based methods. Following the paradigm of learning deep models, land cover classification is modeled as semantic segmentation of very high resolution remote sensing images. In order to obtain accurate segmentation results, high-level categorical semantics and low-level spatial details should be effectively fused. To this end, we propose a novel bidirectional gird fusion network to aggregate the multilevel features across the ConvNet. Specifically, the proposed model is characterized by a bidirectional fusion architecture, which enriches diversity of feature interaction by encouraging bidirectional information flow. In this way, our model gains mutual benefits between top-down and bottom-up information flows. Moreover, a grid fusion architecture is then followed for further feature refinement in a dense and hierarchical fusion manner. Finally, effective feature upsampling is also critical for the multiple fusion operations. Consequently, a content-aware feature upsampling kernel is incorporated for further improvement. Our whole model consistently achieves significant improvement over state-of-the-art methods on two major datasets, ISPRS and GID.",Semantics,Remote sensing,Image segmentation,Image resolution,Task analysis,Kernel,Earth,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Deep learning,land cover classification,semantic segmentation,,,,,,,,
Row_1371,"Xiong, Zhitong","Chen, Sining","Shi, Yilei","Zhu, Xiao Xiang",,Self-Supervised Pretraining With Monocular Height Estimation for Semantic Segmentation,,2024,1,"Monocular height estimation (MHE) is key for generating 3-D city models, essential for swift disaster response. Moving beyond the traditional focus on performance enhancement, our study breaks new ground by probing the interpretability of MHE networks. We have pioneeringly discovered that neurons within MHE models demonstrate selectivity for both height and semantic classes. This insight sheds light on the complex inner workings of MHE models and inspires innovative strategies for leveraging elevation data more effectively. Informed by this insight, we propose a pioneering framework that employs MHE as a self-supervised pretraining method for remote sensing (RS) imagery. This approach significantly enhances the performance of semantic segmentation tasks. Furthermore, we develop a disentangled latent transformer (DLT) module that leverages explainable deep representations from pretrained MHE networks for unsupervised semantic segmentation. Our method demonstrates the significant potential of MHE tasks in developing foundation models for sophisticated pixel-level semantic analyses. Additionally, we present a new dataset designed to benchmark the performance of both semantic segmentation and height estimation tasks. The dataset and code will be publicly available at https://github.com/zhu-xlab/DLT-MHE.pytorch.",Semantics,Task analysis,Estimation,Neurons,Semantic segmentation,Data models,Buildings,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Foundation models,interpretable deep learning,monocular height estimation (MHE),self-supervised pretraining,,,,,,,
Row_1372,"Zhao, Zhiyu","Chen, Yunhao","Li, Kangning","Ji, Weizhen","Sun, Hao",Extracting Photovoltaic Panels From Heterogeneous Remote Sensing Images With Spatial and Spectral Differences,,2024,3,"The accurate extraction of the installation area of the photovoltaic power station is an important basis for the management of the photovoltaic power generation system. Deep learning has proven to be a powerful tool for rapidly detecting the distribution of photovoltaic panels in remote sensing images. The wealth of information from various remote sensing sensors aids in distinguishing photovoltaic pixels within complex backgrounds. However, the distinct imaging characteristics of different sensors present challenges for deep learning models. In this article, we propose a deep learning extraction method for photovoltaic panels that effectively improves the spatial and spectral differences inherent in remote sensing images. Considering the characteristics of different sensors, two attention modules and a feature fusion module are applied to suppress the inconsistency of spatial resolution and spectral resolution. Based on the Unet model, we implement the photovoltaic power station identification method and compare it with several commonly used semantic segmentation models. Qualitative and quantitative accuracy assessments show that the PV-Unet method can effectively overcome the spatial and spectral differences of remote sensing images. It achieves 98.04% F1 score and 96.15% IoU on the test dataset, verifying the superiority of this method. PV-Unet method has the potential for identifying photovoltaic panels from multisource remote sensing data.",Deep learning,feature extraction,photovoltaic (PV) panels,remote sensing,spatial and spectral differences,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,
Row_1373,"Lu, Junyan","Hu, Qinglei",,,,Semantic Joint Monocular Remote Sensing Image Digital Surface Model Reconstruction Based on Feature Multiplexing and Inpainting,,2022,3,"Digital surface model (DSM) presents height information of the Earth's surface and plays an important role in many remote sensing (RS) applications. Since the conventional acquisition of DSM is laborious and expensive, DSM reconstruction from monocular RS images has attracted extensive research in recent years, which is an ill-posed problem and thus rather challenging. Related works have achieved great accomplishments in this regard; however, they still face some limitations in training robustness, accuracy, and efficiency. To address the issues, a semantic joint monocular RS image DSM regression framework is proposed in this article, whose salient points include that: 1) semantic segmentation is integrated into the DSM regression task so that a shared backbone can extract complementary features from each objective to improve the performance of the individual task. Meanwhile, based on the consistency of the two training objectives, a two-stage joint loss function is introduced to improve the convergence and robustness of model training; 2) an encoding-decoding backbone is designed based on feature multiplexing, which simultaneously achieves multiscale feature fusion and information decoupling, thereby greatly reducing model parameters and improving efficiency while ensuring feature extraction effect; and 3) an iterative upsampling approach is introduced to transform the full-scale spatial features into large receptive-field and locally discriminative dynamic kernels, which are used to inpaint coarse-grained features while decoding, thus enhancing regression accuracy. Finally, experiments demonstrate the effectiveness of the proposal. It is easy to train and achieves superior or comparable accuracy compared with state-of-the-art related works while improving the efficiency by a large margin.",Digital surface model (DSM) reconstruction,feature inpainting upsampling (FIU),feature multiplexing,joint multitask,monocular remote sensing (RS) image,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1374,"Zhang, Zhili","Xu, Jiabo","Hu, Xiangyun","Yang, Bingnan","Zhang, Mi",Faster Interactive Segmentation of Identical-Class Objects With One Mask in High-Resolution Remotely Sensed Imagery,,2025,0,"Interactive segmentation (IS) using minimal prompts like points and bounding boxes facilitates rapid image annotation, which is crucial for enhancing data-driven deep learning methods. Traditional IS methods, however, process only one target per interaction, leading to inefficiency when annotating multiple identical-class objects in remote sensing imagery (RSI). To address this issue, we present a new task-identical-class object detection (ICOD) for rapid IS in RSI. This task aims to only identify and detect all identical-class targets within an image, guided by a specific category target in the image with its mask. For this task, we propose an ICOD network (ICODet) with a two-stage object detection framework, which consists of a backbone, feature similarity analysis module (S3QFM), and an identical-class object detector. In particular, the S3QFM analyzes feature similarities from images and support objects at both feature-space and semantic levels, generating similarity maps. These maps are processed by a region proposal network (RPN) to extract target-level features, which are then refined through a simple feature comparison module and classified to precisely identify identical-class targets. To evaluate the effectiveness of this method, we construct two datasets for the ICOD task: one containing a diverse set of buildings and another containing multicategory RSI objects. Experimental results show that our method outperforms the compared methods on both datasets. This research introduces a new method for rapid IS of RSI and advances the development of fast interaction modes, offering significant practical value for data production and fundamental applications in the remote sensing community.",Image segmentation,Feature extraction,Object detection,Remote sensing,Measurement,Buildings,Semantics,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Proposals,Object recognition,Image retrieval,Identical-class object detection (ICOD),,,image feature similarity analysis,interactive segmentation (IS),remote sensing images,,
Row_1375,"Zhou, Wen","Ming, Dongping","Hong, Zhaoli","Lv, Xianwei",,Scene division based stratified object oriented remote sensing image classification,2018 FIFTH INTERNATIONAL WORKSHOP ON EARTH OBSERVATION AND REMOTE SENSING APPLICATIONS (EORSA),2018,1,"High spatial resolution remote sensing image has a wealth of information about ground objects, and the characteristics of objects such as shape, spectrum, texture etc. are richer than medium and low resolution images. Object oriented image analysis is the main stream method in high spatial remote sensing image analysis. Segmentation is the basis of image classification. The traditional image segmentation method uses the same set of parameters for the entire image. However, due to objects' scale-dependent nature, the optimal segmentation parameters for an overall image may not be suitable for every objects, it just a compromise of all objects. According to idea of spatial dependence, the similar objects which have the similar spatial scale often gathered in same regions and dominated in that area, such as city region contains many kinds of objects, may has a small inherent scale, while countryside region has simple ground distribution pattern can use a small scale to segment. So, putting regional image which was divided by overall image as processing image, the over-segmentation and under-segmentation problems will be released. Based above, this paper proposes a scene division based stratified object oriented image analysis method. Since image's hue information can quantify color information, and make the similar color have similar hue value. So, image's hue information has become the mainly basis of scene division, and in order to use this information effectively, second order matrix was used to reflect the relationship of hue values. Second order matrix can reflect image's visual complexity, so it can used to divide the overall image into simple and complex objects occupied scenes. What's more the complex objects occupied scenes can be re-divided into several scenes of which the composition is simple or single. In order to set segmentation parameters accurately and quickly, spatial estimation method was used to calculate every scene image's scale parameter, thus optimal scale can be selected for every scene. Since the complexity of data is effectively reduced by dividing the overall image into several scenes, and spatial estimation method can help select optimal segmentation parameters, so the final classification result can be improved efficiently, and the experiment result also proved the effectiveness of the proposed method.",stratified segmentation,scene division,spatial estimation,visual complexity,middle semantic,,,,,,,,,,,,,,,
Row_1376,"Maretto, Raian Vargas","Korting, Thales Sehn","Garcia Fonseca, Leila Maria",,,AN EXTENSIBLE AND EASY-TO-USE TOOLBOX FOR DEEP LEARNING BASED ANALYSIS OF REMOTE SENSING IMAGES,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),2019,4,"Deep Learning (DL) methods are currently the state-of-the-art in Machine Learning and Pattern Recognition. In recent years, DL has been successfully applied to Remote Sensing (RS) image processing for several tasks, from pre-processing to classification. This paper presents DeepGeo, a toolbox that provides state-of-the-art DL algorithms for RS image classification and analysis. DeepGeo focuses on providing easy-to-use and extensible methods, making it easier to those RS analysts without strong programming skills.",Deep Learning,Convolutional Neural Networks,Remote Sensing,Semantic Segmentation,,,,,,,,,,,,,,,,
Row_1377,"Kotaridis, Ioannis","Lazaridou, Maria",,,,Cnns in land cover mapping with remote sensing imagery: a review and meta-analysis,,OCT 2 2023,10,"Convolutional neural network (CNN) comprises the most common and extensively used network in the field of deep learning (DL). The design of CNNs was influenced by neurons, like a traditional neural network. CNN has fundamental advantage over earlier works since it can detect in an automatic way critical features without the need for human intervention. CNNs have been widely employed in various applications, including land cover classification. Multiple CNN architectures have been introduced over the previous decade. Applications rely on model architecture to improve their performance. The CNN architecture has undergone several alterations up to this day. Several CNN architectures have been introduced in the literature, depicting strong and weak points. This review article presents an overview of the development of convolutional neural networks as they are described in state-of-the-art literature, including remote sensing books, journals, and conferences. Following a thorough assessment of current CNN case studies in land cover mapping through statistical analysis, informative results pertaining to the implemented CNN architecture are presented including relevant findings such as the framework that was utilized, highlighting the most popular choices among the users. It has to be noted that there is not a miraculous CNN model, and the statistical findings reflect the latest developments. Finally, current issues and innovative aspects are addressed.",CNN,deep learning,image classification,remote sensing,semantic segmentation,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1378,"Fan, Junfu","Shi, Zongwen","Ren, Zhoupeng","Zhou, Yuke","Ji, Min",DDPM-SegFormer: Highly refined feature land use and land cover segmentation with a fused denoising diffusion probabilistic model and transformer,,SEP 2024,2,"The semantic segmentation of land use and land cover (LULC) is a crucial and widely employed remote sensing task. Conventional convolutional neural networks and vision transformers have been extensively utilized for LULC segmentation. However, high-resolution remote sensing images contain a wealth of spatial and color texture information, which is not fully exploited by traditional deep learning approaches. The information bottleneck of CNNs and transformers results in the loss of a significant amount of texture detail information during the feature extraction process, which further limits the performance of LULC segmentation. We present DDPM-SegFormer, a new framework that merges a denoising diffusion probabilistic model (DDPM) and vision transformer for LULC segmentation. The aim is to address the difficulties arising from extraction in complex geographic landscapes and to alleviate information bottlenecks. The framework utilizes the ability of a DDPM to generate refined semantic features and that of vision transformer to model the global image context. Our framework introduces two main innovations. First, we use a DDPM for the first time in LULC segmentation to generate highly refined multiscale semantic features. This approach alleviates the information bottleneck caused by relying solely on a CNN or transformer architecture. Second, we develop an effective feature-level fusion strategy that utilizes multihead cross-attention between the DDPM and Transformer. This approach achieves the harmonious fusion of fine-scale semantic features, generating continuous and highly refined semantic features that enhance the segmentation accuracy. The results indicate that DDPM-SegFormer achieves an MIOU of 83.72% and an F1-score of 90.97% for the large-scale LoveDA dataset and an MIOU of 90.91% and an F1score of 93.30% for the Tarim Basin LULC dataset in a desert scenario. The research demonstrated that the refined and continuous semantic features produced by DDPM-SegFormer can significantly enhance LULC segmentation performance.",Land use and land cover,Semantic segmentation,Remote sensing images,Denoising diffusion probabilistic model,Feature fusion,Information bottlenecks,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,
Row_1379,"Xu, Hanwen","Tang, Xinming","Ai, Bo","Yang, Fanlin","Wen, Zhen",Feature-Selection High-Resolution Network With Hypersphere Embedding for Semantic Segmentation of VHR Remote Sensing Images,,2022,13,"Very-high-resolution (VHR) remote sensing images contain various multiscale objects, such as large-scale buildings and small-scale cars. However, these multiscale objects cannot be considered simultaneously in the widely used backbones with a large downsampling factor (e.g., VGG-like and ResNet-like), resulting in the appearance of various context aggregation approaches, such as fusing low-level features and attention-based modules. To alleviate this problem caused by backbones with a large downsampling factor, we propose a feature-selection high-resolution network (FSHRNet) based on an observation: if the features maintain high resolution throughout the network, a high precision segmentation result can be obtained by only using a 1 x 1 convolution layer with no need for complex context aggregation modules. Specifically, the backbone of FSHRNet is a multi-branch structure similar to HRNet where the high-resolution branch is the principal line. Then, a lightweight dynamic weight module, named the feature-selection convolution (FSConv) layer, is presented to fuse multiresolution features, allowing adaptively feature selection based on the characteristic of objects. Finally, a specially designed 1 x 1 convolution layer derived from hypersphere embedding is used to produce the segmentation result. Experiments with other widely used methods show that the proposed FSHRNet obtains competitive performance on the ISPRS Vaihingen dataset, the ISPRS Potsdam dataset, and the iSAID dataset.",Feature-selection convolution (FSConv) layer,high-resolution backbone,hypersphere embedding,multiscale object,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Yang, Xiaomeng",,,,,,,,,,,
Row_1380,"Xu, Fang","Shi, Yilei","Yang, Wen","Zhu, Xiaoxiang",,MULTI-MODAL MULTI-TASK LEARNING FOR SEMANTIC SEGMENTATION OF LAND COVER UNDER CLOUDY CONDITIONS,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,2,"The majority of the Earth's surface is covered by clouds, causing optical images to suffer serious degradation of ground information. Synthetic Aperture Radar (SAR) images with the cloud-penetration capability could provide supplementary information to optical images. Thus, the fusion of optical and SAR image can remarkably improve the interpretation accuracy under cloudy conditions. In this paper, we propose to exploit related cloud removal task for accurate multi-modal semantic segmentation of land cover. Towards this goal, we develop an end-to-end learnable architecture which solves the tasks of cloud removal and semantic segmentation of land cover jointly. The cloud removal task encourages to learn knowledgeable features to overcome negative effects of semantic ambiguity. Our experiments show that the proposed algorithm can effectively improve the semantic segmentation accuracy under cloudy conditions.",Semantic segmentation of land cover,multi-modal,multi-task,cloud removal,,,,,,,,,,,,,,,,
Row_1381,"Ma, Lei","Liu, Yu","Zhang, Xueliang","Ye, Yuanxin","Yin, Gaofei",Deep learning in remote sensing applications: A meta-analysis and review,,JUN 2019,"1,338","Deep learning (DL) algorithms have seen a massive rise in popularity for remote-sensing image analysis over the past few years. In this study, the major DL concepts pertinent to remote-sensing are introduced, and more than 200 publications in this field, most of which were published during the last two years, are reviewed and analyzed. Initially, a meta-analysis was conducted to analyze the status of remote sensing DL studies in terms of the study targets, DL model(s) used, image spatial resolution(s), type of study area, and level of classification accuracy achieved. Subsequently, a detailed review is conducted to describe/discuss how DL has been applied for remote sensing image analysis tasks including image fusion, image registration, scene classification, object detection, land use and land cover (LULC) classification, segmentation, and object-based image analysis (OBIA). This review covers nearly every application and technology in the field of remote sensing, ranging from pre-processing to mapping. Finally, a conclusion regarding the current state-of-the art methods, a critical conclusion on open challenges, and directions for future research are presented.",Deep learning (DL),Remote sensing,LULC classification,Object detection,Scene classification,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,"Johnson, Brian Alan",,,,,,,,,,,
Row_1382,"Wan, Zhechun","Zhang, Qian","Zhang, Guixu",,,Low-Level Feature Enhancement Network for Semantic Segmentation of Buildings,,2022,2,"In recent years, convolutional neural networks (CNNs) have been widely used in extracting buildings from remote sensing images. Both semantic representation and spatial location details are crucial for this task. We propose the methods to enhance the performance of semantic segmentation by using these low-level features considering that man-made buildings in aerial images have strong textures and edges. Texture Enhancement Attention Module (TEAM) is proposed to strengthen feature in the position with rich texture and improve the semantic representation. Edge Extraction Module (EEM) is applied for directly guiding spatial details learning, which starts with super-resolution maps created by Super-Resolution Module (SRM). Detail Supplement Module (DSM) is designed to further provide the details for decoder. On this basis, we propose a low-level feature enhancement network (LFENet) for semantic segmentation of buildings. Experimental results on two aerial datasets show that our works greatly improve the accuracy over the baseline and other models.",Semantics,Feature extraction,Buildings,Convolution,Image edge detection,Superresolution,Correlation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Building extraction,convolutional neural networks (CNNs),edge,semantic segmentation,,,texture,,,,
Row_1383,"Grzeczkowicz, Gregoire","Vallet, Bruno",,,,SEMANTIC SEGMENTATION OF URBAN TEXTURED MESHES THROUGH POINT SAMPLING,"XXIV ISPRS CONGRESS IMAGING TODAY, FORESEEING TOMORROW, COMMISSION II",2022,4,"Textured meshes are becoming an increasingly popular representation combining the 3D geometry and radiometry of real scenes. However, semantic segmentation algorithms for urban mesh have been little investigated and do not exploit all radiometric information. To address this problem, we adopt an approach consisting in sampling a point cloud from the textured mesh, then using a point cloud semantic segmentation algorithm on this cloud, and finally using the obtained semantic to segment the initial mesh. In this paper, we study the influence of different parameters such as the sampling method, the density of the extracted cloud, the features selected (color, normal, elevation) as well as the number of points used at each training period. Our result outperforms the state-of-the-art on the SUM dataset, earning about 4 points in OA and 18 points in mIoU.",Mesh,Semantic Segmentation,Point Sampling,,,,,,,,,,,,,,,,,
Row_1384,"Dang, Bo","Li, Yansheng","Zhang, Yongjun","Ma, Jiayi",,Progressive Learning With Cross-Window Consistency for Semi-Supervised Semantic Segmentation,,2024,0,"Semi-supervised semantic segmentation focuses on the exploration of a small amount of labeled data and a large amount of unlabeled data, which is more in line with the demands of real-world image understanding applications. However, it is still hindered by the inability to fully and effectively leverage unlabeled images. In this paper, we reveal that cross-window consistency (CWC) is helpful in comprehensively extracting auxiliary supervision from unlabeled data. Additionally, we propose a novel CWC-driven progressive learning framework to optimize the deep network by mining weak-to-strong constraints from massive unlabeled data. More specifically, this paper presents a biased cross-window consistency (BCC) loss with an importance factor, which helps the deep network explicitly constrain confidence maps from overlapping regions in different windows to maintain semantic consistency with larger contexts. In addition, we propose a dynamic pseudo-label memory bank (DPM) to provide high-consistency and high-reliability pseudo-labels to further optimize the network. Extensive experiments on three representative datasets of urban views, medical scenarios, and satellite scenes with consistent performance gain demonstrate the superiority of our framework. Our code is released at https://jack-bo1220.github.io/project/CWC.html.",Semantic segmentation,Semantics,Data models,Pipelines,Visualization,Remote sensing,Medical diagnostic imaging,IEEE TRANSACTIONS ON IMAGE PROCESSING,,Semi-supervised semantic segmentation,consistency loss,pseudo-label supervision,,,,,,,,
Row_1385,"Ren, Shasha","Liu, Qiong",,,,Small Target Augmentation for Urban Remote Sensing Image Real-Time Segmentation,,FEB 2024,3,"Urban remote sensing (URS) image segmentation is very important for many applications from automotive navigation to infrastructure monitoring, and urban management. There are numerous small targets in URS image due to a large shooting field of view. However, the existing learning-based real-time image segmentation methods are not strong enough to handle small target and edge segmentation problems well, resulting in small targets that are easy to be missed, and blurred target edges. To segment the small targets and edges more accurately in real time, we propose a fast URS image segmentation method based on a multi-layer pixel attention mechanism (MPAM). We improve the performance and efficiency of the URS image segmentation from two perspectives: model and data. Specifically, to enhance semantic detailed information such as small targets and edges, we design mask-guided edge and small target feature enhancement modules in the real-time segmentation network. In addition, we propose a small target data enhancement method which uses an interpolation algorithm to amplify small targets in URS images, in order to improve the efficiency of existing URS data. The experimental results on Vaihingen, Potsdam, and DLRSD datasets show that the segmentation accuracy of our method reaches 86.74% mIoU, which is better than the state-of-the-art algorithms STDC, CFNet, and UNetFormer.",URS image segmentation,small target,data augmentation,edge,,,,IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS,,,,,,,,,,,,
Row_1386,"Mou, Lichao","Hua, Yuansheng","Zhu, Xiao Xiang",,,Relation Matters: Relational Context-Aware Fully Convolutional Network for Semantic Segmentation of High-Resolution Aerial Images,,NOV 2020,137,"Most current semantic segmentation approaches fall back on deep convolutional neural networks (CNNs). However, their use of convolution operations with local receptive fields causes failures in modeling contextual spatial relations. Prior works have sought to address this issue by using graphical models or spatial propagation modules in networks. But such models often fail to capture long-range spatial relationships between entities, which leads to spatially fragmented predictions. Moreover, recent works have demonstrated that channel-wise information also acts a pivotal part in CNNs. In this article, we introduce two simple yet effective network units, the spatial relation module, and the channel relation module to learn and reason about global relationships between any two spatial positions or feature maps, and then produce Relation-Augmented (RA) feature representations. The spatial and channel relation modules are general and extensible, and can be used in a plug-andplay fashion with the existing fully convolutional network (FCN) framework. We evaluate relation module-equipped networks on semantic segmentation tasks using two aerial image data sets, namely International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam data sets, which fundamentally depend on long-range spatial relational reasoning. The networks achieve very competitive results, a mean F1 score of 88.54% on the Vaihingen data set and a mean F1 score of 88.01% on the Potsdam data set, bringing significant improvements over baselines.",Fully convolutional network (FCN),high resolution aerial imagery,relation network,semantic segmentation,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1387,"Xin, Jiang","Zhang, Xinchang","Zhang, Zhiqiang","Fang, Wu",,Road Extraction of High-Resolution Remote Sensing Images Derived from DenseUNet,,NOV 2019,77,"Road network extraction is one of the significant assignments for disaster emergency response, intelligent transportation systems, and real-time updating road network. Road extraction base on high-resolution remote sensing images has become a hot topic. Presently, most of the researches are based on traditional machine learning algorithms, which are complex and computational because of impervious surfaces such as roads and buildings that are discernible in the images. Given the above problems, we propose a new method to extract the road network from remote sensing images using a DenseUNet model with few parameters and robust characteristics. DenseUNet consists of dense connection units and skips connections, which strengthens the fusion of different scales by connections at various network layers. The performance of the advanced method is validated on two datasets of high-resolution images by comparison with three classical semantic segmentation methods. The experimental results show that the method can be used for road extraction in complex scenes.",high-resolution remote sensing imagery,multi-scale,road extraction,machine learning,DenseUNet,,,REMOTE SENSING,,,,,,,,,,,,
Row_1388,"Yu, Lingjuan","Shao, Qiqi","Guo, Yuting","Xie, Xiaochun","Liang, Miaomiao",Complex-Valued U-Net with Capsule Embedded for Semantic Segmentation of PolSAR Image,,MAR 2023,2,"In recent years, semantic segmentation with pixel-level classification has become one of the types of research focus in the field of polarimetric synthetic aperture radar (PolSAR) image interpretation. Fully convolutional network (FCN) can achieve end-to-end semantic segmentation, which provides a basic framework for subsequent improved networks. As a classic FCN-based network, U-Net has been applied to semantic segmentation of remote sensing images. Although good segmentation results have been obtained, scalar neurons have made it difficult for the network to obtain multiple properties of entities in the image. The vector neurons used in the capsule network can effectively solve this problem. In this paper, we propose a complex-valued (CV) U-Net with a CV capsule network embedded for semantic segmentation of a PolSAR image. The structure of CV U-Net is lightweight to match the small PolSAR data, and the embedded CV capsule network is designed to extract more abundant features of the PolSAR image than the CV U-Net. Furthermore, CV dynamic routing is proposed to realize the connection between capsules in two adjacent layers. Experiments on two airborne datasets and one Gaofen-3 dataset show that the proposed network is capable of distinguishing different types of land covers with a similar scattering mechanism and extracting complex boundaries between two adjacent land covers. The network achieves better segmentation performance than other state-of-art networks, especially when the training set size is small.",semantic segmentation,complex-valued U-Net,complex-valued capsule network,polarimetric synthetic aperture radar,,,,REMOTE SENSING,"Hong, Wen",,,,,,,,,,,
Row_1389,"Xu, Hongzhang","He, Hongjie","Zhang, Ying","Ma, Lingfei","Li, Jonathan",A comparative study of loss functions for road segmentation in remotely sensed road datasets,,FEB 2023,22,"Road extraction from remote sensing imagery is a fundamental task in the field of image semantic segmentation. For this goal, numerous supervised deep learning techniques have been created, along with the employment of various loss functions that play a crucial role in determining the performances of supervised learning models. However, there is a lack of comprehensive analysis of the performance differences between the loss functions for road segmentation in remote sensing imagery. Therefore, this study conducts a comparative study of 12 wellknown loss functions used widely in the field of image segmentation by training and evaluating the representative D-LinkNet network for road segmentation tasks with two publicly available remote sensing road datasets consisting of very high-resolution aerial and satellite images. The results show that different loss functions could lead to very different outcomes using the D-LinkNet, with varying focuses such as on overall model performances, precision, or recall. By dividing the loss functions into the distribution-based, region-based, and compound ones, we found that the region-based loss function type led to generally better model performances than the distribution-based one in terms of F1, IoU, and the road segmentation maps, with the compound loss function type being comparable to the region-based one. This paper eventually tries to offer suggestions for choosing the loss function that best suits the purposes of road segmentation-related studies.",Road extraction,Image segmentation,Loss function,Cross -entropy,Dice,D-LinkNet,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,
Row_1390,Jin Shu,Guan Mo,Bian Yuchan,Wang Shulei,,Building Extraction from Remote Sensing Images Based on Improved U-Net,,FEB 2023,2,"Building extraction from remote sensing images is of great significance to the construction of smart cities. Aiming to improve the low accuracy of traditional methods in extracting remote sensing images with a complex background, a remote sensing image building extraction method (MA-Unet) based on U-Net is proposed. This method mainly uses an encoder and a decoder. A convolutional block attention module is introduced into the encoder, in which a channel attention module is used to screen more important features and suppress invalid features, and a spatial attention module is used to screen deeper semantic features. An atrous spatial pyramid pooling module is introduced to extract features with different scales. In the decoder, to fuse object features with different scales, feature maps in the decoder are upsampled and connected in series. This information aggregation solves the difficulty of detecting objects with different scales to some extent. The experimental results show that MA-Unet method is superior to the U-Net method in terms of accuracy, precision, and intersection over union (IoU) by 1. 7 percentage points, 2. 1 percentage points, and 1. 6 percentage points on the Massachusetts building dataset and by 1. 1 percentage points, 1. 4 percentage points, and 2. 3 percentage points on the WHU building dataset, respectively. It is a more effective and practical target extraction method.",remote sensing image,semantic segmentation,building extraction,attention mechanism,multi-scale,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,
Row_1391,"Zhao, Shaobo","Dong, Youqiang","Cheng, Xi","Huo, Yu","Zhang, Min",Remote Sensing Image Denoising Based on Feature Interaction Complementary Learning,,OCT 2024,0,"Optical remote sensing images are of considerable significance in a plethora of applications, including feature recognition and scene semantic segmentation. However, the quality of remote sensing images is compromised by the influence of various types of noise, which has a detrimental impact on their practical applications in the aforementioned fields. Furthermore, the intricate texture characteristics inherent to remote sensing images present a significant hurdle in the removal of noise and the restoration of image texture details. In order to address these challenges, we propose a feature interaction complementary learning (FICL) strategy for remote sensing image denoising. In practical terms, the network is comprised of four main components: noise predictor (NP), reconstructed image predictor (RIP), feature interaction module (FIM), and fusion module. The combination of these modules serves to not only complete the fusion of the prediction results of NP and RIP, but also to achieve a deep coupling of the characteristics of the two predictors. Consequently, the advantages of noise prediction and reconstructed image prediction can be combined, thereby enhancing the denoising capability of the model. Furthermore, comprehensive experimentation on both synthetic Gaussian noise datasets and real-world denoising datasets has demonstrated that FICL has achieved favorable outcomes, emphasizing the efficacy and robustness of the proposed framework.",remote sensing image denoising,deep learning,feature interaction,complementary learning,,,,REMOTE SENSING,"Wang, Hai",,,,,,,,,,,
Row_1392,"Xia, Liegang","Luo, Jiancheng","Zhang, Junxia","Zhu, Zhiwen","Gao, Lijing",Semantic edge-guided object segmentation from high-resolution remotely sensed imagery,,DEC 17 2021,2,"Image segmentation is a basic task of Object-Based Image Analysis (OBIA) in remote sensing. Traditionally, the algorithms for this task are mostly based on region merging procedure. But the segmented objects may correlate poorly with the actual object boundary. Inspired by the recent remarkable improvements on edge detection with deep learning, we propose a semantic edge-guided segmentation with deep learning method to extract meaningful geographic objects from High-resolution Remote Sensing (HRS) images. The method consists of three stages: in the first stage, geographic object boundaries are manually labelled and randomly augmented to generate training data; in the second stage, a fully convolutional neural networks with Encoder-Decoder structure and multiscale supervised nets are trained to detect edges at multiple scales. The detected edges with semantic information do not only include local details but also global edge structure, which are more in accordance with human perception and suitable for conversion to actual geographic boundaries; in the third stage, the detected edges are thinned and extended to construct complete object boundaries according to calculated edge strength. The average precision of our method for the two datasets was 0.902 and 0.866, which is higher than the state-of-the-art deep learning models including RCF, BDCN and DexiNed have obtained. And line IoU improvement of at least 8.46% and F1 score improvement of at least 8.13% in the two datasets. The code of DDLNet is publicly available at https://github.com/Pikachu-zzZ/SEGOS.",,,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,"Yang, Haiping",,,,,,,,,,,
Row_1393,"Shi, Weipeng","Qin, Wenhu","Chen, Allshine",,,Towards Robust Semantic Segmentation of Land Covers in Foggy Conditions,,SEP 2022,6,"When conducting land cover classification, it is inevitable to encounter foggy conditions, which degrades the performance by a large margin. Robustness may be reduced by a number of factors, such as aerial images of low quality and ineffective fusion of multimodal representations. Hence, it is crucial to establish a reliable framework that can robustly understand remote sensing image scenes. Based on multimodal fusion and attention mechanisms, we leverage HRNet to extract underlying features, followed by the Spectral and Spatial Representation Learning Module to extract spectral-spatial representations. A Multimodal Representation Fusion Module is proposed to bridge the gap between heterogeneous modalities which can be fused in a complementary manner. A comprehensive evaluation study of the fog-corrupted Potsdam and Vaihingen test sets demonstrates that the proposed method achieves a mean F1score exceeding 73%, indicating a promising performance compared to State-Of-The-Art methods in terms of robustness.",semantic segmentation,attention mechanism,robust deep learning,remote sensing,data fusion,,,REMOTE SENSING,,,,,,,,,,,,
Row_1394,"Lyu, Ye","Vosselman, George","Xia, Gui-Song","Yilmaz, Alper","Yang, Michael Ying",UAVid: A semantic segmentation dataset for UAV imagery,,JUL 2020,169,"Semantic segmentation has been one of the leading research interests in computer vision recently. It serves as a perception foundation for many fields, such as robotics and autonomous driving. The fast development of semantic segmentation attributes enormously to the large scale datasets, especially for the deep learning related methods. There already exist several semantic segmentation datasets for comparison among semantic segmentation methods in complex urban scenes, such as the Cityscapes and CamVid datasets, where the side views of the objects are captured with a camera mounted on the driving car. There also exist semantic labeling datasets for the airborne images and the satellite images, where the nadir views of the objects are captured. However, only a few datasets capture urban scenes from an oblique Unmanned Aerial Vehicle (UAV) perspective, where both of the top view and the side view of the objects can be observed, providing more information for object recognition. In this paper, we introduce our UAVid dataset, a new high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. Our UAV dataset consists of 30 video sequences capturing high-resolution images in oblique views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. We have provided several deep learning baseline methods with pre-training, among which the proposed Multi-Scale-Dilation net performs the best via multi-scale feature extraction, reaching a mean intersection-over-union (IoU) score around 50%. We have also explored the influence of spatial-temporal regularization for sequence data by leveraging on feature space optimization (FSO) and 3D conditional random field (CRF). Our UAVid website and the labeling tool have been published online (https://uavid.nl/).",UAV,Semantic segmentation,Deep learning,Dataset,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,
Row_1395,"Hu, Qiongqiong","Wang, Feiting","Li, Ying",,,EAD-Net: Efficiently Asymmetric Network for Semantic Labeling of High-Resolution Remote Sensing Images with Dynamic Routing Mechanism,,MAY 2024,1,"Semantic labeling of high-resolution remote sensing images (HRRSIs) holds a significant position in the remote sensing domain. Although numerous deep-learning-based segmentation models have enhanced segmentation precision, their complexity leads to a significant increase in parameters and computational requirements. While ensuring segmentation accuracy, it is also crucial to improve segmentation speed. To address this issue, we propose an efficient asymmetric deep learning network for HRRSIs, referred to as EAD-Net. First, EAD-Net employs ResNet50 as the backbone without pooling, instead of the RepVGG block, to extract rich semantic features while reducing model complexity. Second, a dynamic routing module is proposed in EAD-Net to adjust routing based on the pixel occupancy of small-scale objects. Concurrently, a channel attention mechanism is used to preserve their features even with minimal occupancy. Third, a novel asymmetric decoder is introduced, which uses convolutional operations while discarding skip connections. This not only effectively reduces redundant features but also allows using low-level image features to enhance EAD-Net's performance. Extensive experimental results on the ISPRS 2D semantic labeling challenge benchmark demonstrate that EAD-Net achieves state-of-the-art (SOTA) accuracy performance while reducing model complexity and inference time, while the mean Intersection over Union (mIoU) score reaching 87.38% and 93.10% in the Vaihingen and Potsdam datasets, respectively.",deep learning,small-scale object,low-level features,channel attention mechanism,asymmetric decoder,,,REMOTE SENSING,,,,,,,,,,,,
Row_1396,"Zhang, Zhijie","Zhang, Chuanrong","Li, Weidong",,,SEMANTIC SEGMENTATION OF URBAN BUILDINGS FROM VHR REMOTELY SENSED IMAGERY USING ATTENTION-BASED CNN,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,2,"With the emergence of large scale Very High Resolution (VHR) remotely sensed imageries, more accurate urban building segmentation now become possible. Meanwhile, various appearances along with complicated background of the urban VHR imageries make accurate segmentation of urban buildings yet a very challenging task. In this study, we proposed DeepAttentionUnet, which is an end-to-end attention-based model following the basic structure of U-Net and being integrated with attention mechanism so that the model can automatically learn to focus on target building structure of different sizes and shapes. This model adopts attention mechanism to implicitly highlight useful features of buildings while suppressing irrelevant areas of input images and combines with residual learning method to ease training process by alleviating the degradation problem in the training process that often occurs in such applications. Our proposed DeepAttentionUnet was tested with a set of 0.075m resolution aerial images and was compared for performance under the exact same conditions with other two state-of-the-art segmentation networks, i. e. SegNet and U-Net, respectively. The experiment results suggested that in both quantitative and visual evaluation, our proposed model outperformed other two models in urban building semantic segmentation tasks in terms of F1 score, Kappa coefficient and overall accuracy, moreover, it also takes the advantage of having much less parameters compared with other two state-of-art models in comparison.",Attention-Based Model,U-Net,VHR Imagery,Urban Building Segmentation,DeepAttentionUnet,,,,,,,,,,,,,,,
Row_1397,"Ren, Shun","Liu, Xuan","Liu, Hongyan","Wang, Lu",,CULTIVATED LAND SEGMENTATION OF REMOTE SENSING IMAGE BASED ON PSPNET OF ATTENTION MECHANISM,,2022,2,"The monitoring of cultivated land area is an important content to ensure food security, among which the real-time monitoring of cultivated land using remote sensing image is one of the most important means to guarantee the same. Remote sensing image from the Gaofen-2 satellite is often affected by seasonal climate in the process of obtaining the information of topographical features, which makes the colour contrast of the data decrease and leads to classification errors. Therefore, the cultivated land images in spring and winter are taken as the experimental objects so as to address the problems of ""the same spectrum foreign matter"" between spring cultivated land and vegetation, winter cultivated land and bare soil. Paper contents are as follows: (1) The Pyramid Scene Parsing Network with residual attention mechanism is used to extract cultivated land; and the residual attention mechanism is applied to automatically adjust the weight to enhance the useful features and improve the segmentation effect; (2) to reduce the impact of semantic information loss on edge detail recognition with the deepening of network layer, the method of high-resolution representation of detail features in shallow attention network layer is integrated to optimize the network; (3) because of the unbalanced proportion of samples in the data set, the network training is easy to over fit and the dice loss function is used to solve the problem of imbalance between positive and negative samples. It can be seen from the experimental results that compared with the classic algorithm, under the influence of spring and winter, the cultivated land identification effect will be better and the application of farmland extraction can be realized.",Attention mechanism,residual network,PSPNet,cultivated land,remote sensing image,,,INTERNATIONAL JOURNAL OF ROBOTICS & AUTOMATION,,,,,,,,,,,,
Row_1398,"Chouhan, Avinash","Sur, Arijit","Chutia, Dibyajyoti",,,AGGREGATED CONTEXT NETWORK FOR SEMANTIC SEGMENTATION OF AERIAL IMAGES,"2022 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",2022,1,"With the considerable advancement of remote sensing technology and computer vision, automatic scene understanding for very high-resolution aerial (VHR) imagery became a necessary research topic. Semantic segmentation of VHR imagery is an important task where context information plays a crucial role. Adequate feature delineation is difficult due to high-class imbalance in remotely sensed data. In this work, we proposed a variant of encoder-decoder-based architecture where residual attentive skip connections are incorporated. We added a multi-context block in each of the encoder units to capture multi-scale and multi-context features and used dense connections for effective feature extraction. A comprehensive set of experiments reveal that the proposed scheme outperformed recently published work by 3% in overall accuracy and F1 score for ISPRS Vaihingen and ISPRS Potsdam benchmark datasets.",Semantic segmentation,residual attentive connection,multi-context block,,,,,,,,,,,,,,,,,
Row_1399,"Ding, Lei","Zheng, Kai","Lin, Dong","Chen, Yuxing","Liu, Bing",MP-ResNet: Multipath Residual Network for the Semantic Segmentation of High-Resolution PolSAR Images,,2022,25,"There are limited studies on the semantic segmentation of high-resolution polarimetric synthetic aperture radar (PolSAR) images due to the scarcity of training data and the complexity of managing speckle noise. The Gaofen contest has provided open access a high-quality PolSAR semantic segmentation dataset. Taking this opportunity, we propose a multipath residual network (MP-ResNet) architecture for the semantic segmentation of high-resolution PolSAR images. Compared to conventional U-shape encoder-decoder convolutional neural network (CNN) architectures, the MP-ResNet learns semantic context with its parallel multiscale branches, which greatly enlarges its valid receptive fields and improves the embedding of local discriminative features. In addition, MP-ResNet adopts a multilevel feature fusion design in its decoder to effectively exploit the features learned from its different branches. Comparisons with the baseline method of fully connected network (FCN with ResNet34) show that the MP-ResNet has achieved significant accuracy improvements. It also surpasses several state-of-the-art methods in terms of overall accuracy (OA), mF(1) and frequency weighted intersection over union (fwIoU), with only a limited increase of computational costs. This CNN architecture can be used as a baseline method for future studies on the semantic segmentation of PolSAR images.",Convolutional neural network (CNN),polarimetric synthetic aperture radar (PolSAR) image analysis,remote sensing,semantic segmentation,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,"Li, Jiansheng",,,,,"Bruzzone, Lorenzo",,,,,,
Row_1400,"Venugopal, N.",,,,,Automatic Semantic Segmentation with DeepLab Dilated Learning Network for Change Detection in Remote Sensing Images,,JUN 2020,38,"Automatic change detection is an interesting research area in remote sensing (RS) technology aims to detect the changes in synthetic aperture radar (SAR) and multi-temporal hyperspectral images acquired at different time intervals. This method identifies the differences between the images and accomplishes the classification result into changed and unchanged areas. However, the existing algorithms are degraded due to noises present in the RS images. The main aim of the proposed method is the automatic semantic segmentation based change detection that produces a final change between the two input images. This paper proposes a feature learning method named deep lab dilated convolutional neural network (DL-DCNN) for the detection of changes from the images. The proposed approach consists of three stages: (i) pre-processing, (ii) semantic segmentation based change detection and (iii) accuracy assessment. Initially, preprocessing is performed to correct the errors and to obtain detailed information from the scene. Then, map the changes between the two images with the help of a trained network. The DCNN network performs fine-tuning and determines the relationship between two images as changed and unchanged pixel areas. The experimental analysis conducted on various datasets and compared with several existing algorithms. The experimental analysis is performed in terms of F-score, percentage correct classification, kappa coefficient, and overall error rate measures to show a better performance measure than the other state-of-art approaches.",Deep neural network,Change detection,Synthetic aperture radar images,Convolutional neural network,Multi-temporal detection techniques,,,NEURAL PROCESSING LETTERS,,,,,,,,,,,,
Row_1401,"Zhang, Haibo","Hong, Hanyu","Zhu, Ying","Zhang, Yaozong","Wang, Pengtian",SEMI-SUPERVISED SEMANTIC SEGMENTATION OF SAR IMAGES BASED ON CROSS PSEUDO-SUPERVISION,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,3,"Due to the unique imaging mechanism and wide application of synthetic aperture radar (SAR), SAR image interpretation has been researched by more and more scholars. The supervised SAR image semantic segmentation methods that based on deep learning require a large number of accurate pixel-level labels, which are very hard to obtain. The lack of labeled samples limits the practical application of deep learning methods in SAR image semantic segmentation. To reduce the requirement of labeled data, we decided to introduce the cross pseudo-supervision network (CPS-Net) into SAR image semi-supervised semantic segmentation and promote the development of semi-supervised learning in SAR image interpretation. The semi-supervised segmentation based on CPS-Net has the following advantages: (1) CPS-Net encourages high similarity between two networks with the same input data, which helps improve the performance. (2) CPS-Net can make better use of the pseudo-supervision of unlabeled data to guide the network training. Experimental results show that CPS-Net achieves excellent semi-supervised semantic segmentation results on Sentinel-1 dual-polarization data with less labeled data. Compared with well-known semantic segmentation methods U-Net and DeeplabV3+, the performance of SAR image segmentation is significantly improved.",SAR,Semi-supervised,Semantic segmentation,Cross pseudo-supervision network,,,,,"Wang, Lei",,,,,,,,,,,
Row_1402,"Li, Z.","Wu, B.","Chen, Z.","Ma, Y.",,TRANSFORMER-BASED METHOD FOR SEMANTIC SEGMENTATION AND RECONSTRUCTION OF THE MARTIAN SURFACE,"GEOSPATIAL WEEK 2023, VOL. 48-1",2023,0,"The last decade has witnessed a great advance in deep space exploration, such as the rover missions to Mars. Semantic information on the Martian surface is garnering more attention, for its ability to distinguish the surface landforms for rover traverse planning and facilitating 3D reconstruction. The state-of-the-art studies on semantic segmentation exclusively leveraged transformer-based methods, and the results have been verified to outperform the traditional convolutional neural networks. However, few datasets concerning the Martian surface have been generated, and the publicly available network models were all trained on the common Earth dataset. Constructing a pixel-wise semantic segmentation dataset requires lots of human labor, especially for training a large transformer network. Furthermore, the results of semantic segmentation were typically used for intuitive visualization but seldom exploited in the 3D reconstruction pipeline. To address these problems, this paper presents the following three contributions: (1) introducing an approach to generate a large dataset for Mars in a semi-automatic way; (2) development of a novel variant of transformer designed for multi-view semantic segmentation to improve the accuracy; (3) development of a semantic-aware dense image matching method for improved matching performances assisted with the semantic information. Experimental results using the dataset collected at the Zhurong landing site on Mars have shown superior performances of the proposed methods as compared with traditional methods.",Semantic segmentation,reconstruction,deep learning,transformer,,,,,,,,,,,,,,,,
Row_1403,"Sun, Dechao","Gao, Guang","Huang, Lijun","Liu, Yunpeng","Liu, Dongquan",Extraction of water bodies from high-resolution remote sensing imagery based on a deep semantic segmentation network,,JUN 25 2024,3,"The precise delineation of urban aquatic features is of paramount importance in scrutinizing water resources, monitoring floods, and devising water management strategies. Addressing the challenge of indistinct boundaries and the erroneous classification of shadowed regions as water in high-resolution remote sensing imagery, we introduce WaterDeep, which is a novel deep learning framework inspired by the DeepLabV3 + architecture and an innovative fusion mechanism for high- and low-level features. This methodology first creates a comprehensive dataset of high-resolution remote sensing images, then progresses through the Xception baseline network for low-level feature extraction, and harnesses densely connected Atrous Spatial Pyramid Pooling (ASPP) modules to assimilate multi-scale data into sophisticated high-level features. Subsequently, the network decoder amalgamates the elemental and intricate features and applies dual-line interpolation to the amalgamated dataset to extract aqueous formations from the remote images. Experimental evidence substantiates that WaterDeep outperforms its existing deep learning counterparts, achieving a stellar overall accuracy of 99.284%, FWIoU of 95.58%, precision of 97.562%, recall of 95.486%, and F1 score of 96.513%. It also excels in the precise demarcation of edges and the discernment of shadows cast by urban infrastructure. The superior efficacy of the proposed method in differentiating water bodies in complex urban environments has significant practical applications in real-world contexts.",Water bodies,Deep learning,Remote sensing imagery,Atrous spatial pyramid pooling,,,,SCIENTIFIC REPORTS,,,,,,,,,,,,
Row_1404,"Saha, Sudipan","Shahzad, Muhammad","Mou, Lichao","Song, Qian","Zhu, Xiao Xiang",Unsupervised Single-Scene Semantic Segmentation for Earth Observation,,2022,20,"Earth observation data have huge potential to enrich our knowledge about our planet. An important step in many Earth observation tasks is semantic segmentation. Generally, a large number of pixelwise labeled images are required to train deep models for supervised semantic segmentation. On the contrary, strong intersensor and geographic variations impede the availability of annotated training data in Earth observation. In practice, most Earth observation tasks use only the target scene without assuming availability of any additional scene, labeled or unlabeled. Keeping in mind such constraints, we propose a semantic segmentation method that learns to segment from a single scene, without using any annotation. Earth observation scenes are generally larger than those encountered in typical computer vision datasets. Exploiting this, the proposed method samples smaller unlabeled patches from the scene. For each patch, an alternate view is generated by simple transformations, e.g., addition of noise. Both views are then processed through a two-stream network and weights are iteratively refined using deep clustering, spatial consistency, and contrastive learning in the pixel space. The proposed model automatically segregates the major classes present in the scene and produces the segmentation map. Extensive experiments on four Earth observation datasets collected by different sensors show the effectiveness of the proposed method. Implementation is available at https://gitlab.lrz.de/ai4eo/cd/-/tree/main/unsupContrastiveSemanticSeg.",Earth,Image segmentation,Semantics,Training,Deep learning,Task analysis,Supervised learning,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Deep learning,self-supervised learning,semantic segmentation,single-scene training,,,,,,,
Row_1405,"Zhang, Tingting","Hu, Danni","Wu, Chunxiao","Liu, Yundan","Yang, Jianyu",Large-scale apple orchard mapping from multi-source data using the semantic segmentation model with image- to- image translation and transfer learning,,OCT 2023,6,"Large-scale mapping of apple orchards through remote sensing is of great significance for apple production management and the sustainable development of the apple industry. The flexible unmanned aerial vehicle (UAV) data and a wide swath of Sentinel-2 (S2) data provided the opportunity to map apple orchards accurately and in a timely manner over a large area. In order to fully combine the advantages of these data and realize the accurate monitoring of apple orchards, this study proposed a semantic segmentation method based on Cycle-Consistent Generative Adversarial Networks (CycleGAN) and transfer learning model (Trans_GAN). First, semantic segmentation models (Fully Convolutional Networks, U-Net, SegNet, DeepLabv3+) were compared. The model with the best performance on both S2 and UAV was selected as the optimal apple orchard recognition model. Second, to solve the problem of domain differences between S2 and UAV, CycleGAN was introduced to convert UAV images into the style of S2 images (Fake S2, F_S2). Finally, this study introduced the transfer learning method and used the F_S2 to assist S2 images to complete the task of extracting large-scale apple planting. Trans_GAN was tested in Zibo and Yantai. The results showed the ability of SegNet to refine the segmentation results and, as a result, achieve the highest extraction accuracy on both UAV and S2 images. The proposed method produced results as high as 20.93% in recall and 15.93% in F1 when compared to the SegNet method based on S2 without image-to-image translation and transfer learning. Therefore, the Trans_GAN method opens a new window for large-scale remote sensing apple orchard mapping.",Apple orchard mapping,Remote sensing,Semantic segmentation,Transfer learning,Image-to-image translation,,,COMPUTERS AND ELECTRONICS IN AGRICULTURE,"Tang, Kaixuan",,,,,,,,,,,
Row_1406,"Yu, Mengqi","Huang, Hongzhi","Liu, Hong","He, Shuyi","Qiao, Fei",Optimizing FPGA-based Convolutional Encoder-Decoder Architecture for Semantic Segmentation,"2019 9TH IEEE ANNUAL INTERNATIONAL CONFERENCE ON CYBER TECHNOLOGY IN AUTOMATION, CONTROL, AND INTELLIGENT SYSTEMS (IEEE-CYBER 2019)",2019,3,"Convolutional neural networks (CNNs) for visual semantic segmentation have been attracting considerable attention recently because of their superior support for many significant tasks, such as autonomous driving, semantic SLAM (simultaneous localization and mapping) and remote sensing surveying and mapping. These kinds of applications generally need to he implemented on the smart terminals, which means that a kind of hardware platform with high energy efficiency and real-time performance is required. However, CNNs for semantic segmentation usually contain sonic, symmetrical encoders and decoders, corresponding to the down-sampling process (e.g., pooling, convolution) and the up-sampling process (e.g., unpooling, deconvolution). All of these processes are computing and storage intensive, which limits their applicability in the resource constrained embedded systems. In this paper, an FPGA-based accelerator programed by OpenCL is proposed. We evaluate its performance on the CamVid dataset. The global accuracy only drops by 2.04% with 8-bit quantization. Additionally, the system shows 48.89 GOPS and 2.4x real-time performance against CPU when running on an Arria-10 GX1150 device.",FPGA,Convolutional Neural Networks,Encoder-Decoder,Semantic Segmentation,Accelerator,,,,"Luo, Li",,,,,"Xie, Fugui","Liu, Xin-Jun",,,,,"Yang, Huazhong"
Row_1407,"Fan, Zhiyong","Hou, Jianmin","Zang, Qiang","Chen, Yunjie","Yan, Fei",River Segmentation of Remote Sensing Images Based on Composite Attention Network,,JAN 5 2022,8,"River segmentation of remote sensing images is of important research significance and application value for environmental monitoring, disaster warning, and agricultural planning in an area. In this study, we propose a river segmentation model in remote sensing images based on composite attention network to solve the problems of abundant river details in images and the interference of non-river information including bridges, shadows, and roads. To improve the segmentation efficiency, a composite attention mechanism is firstly introduced in the central region of the network to obtain the global feature dependence of river information. Next, in this study, we dynamically combine binary cross-entropy loss that is designed for pixel-wise segmentation and the Dice coefficient loss that measures the similarity of two segmentation objects into a weighted one to optimize the training process of the proposed segmentation network. The experimental results show that compared with other semantic segmentation networks, the evaluation indexes of the proposed method are higher than those of others, and the river segmentation effect of CoANet model is significantly improved. This method can segment rivers in remote sensing images more accurately and coherently, which can meet the needs of subsequent research.",,,,,,,,COMPLEXITY,,,,,,,,,,,,
Row_1408,"Saralioglu, Ekrem","Gungor, Oguz",,,,Semantic segmentation of land cover from high resolution multispectral satellite images by spectral-spatial convolutional neural network,,JAN 7 2022,41,"Research to improve the accuracy of very high-resolution satellite image classification algorithms is still one of the hot topics in the field of remote sensing. Successful results of deep learning methods in areas such as image classification and object detection have led to the application of these methods to remote sensing problems. Recently, Convolutional Neural Networks (CNNs) are among the most common deep learning methods used in image classification, however, the use of CNN's in satellite image classification is relatively new. Due to the high computational complexity of 3D CNNs, which aim to extract both spatial and spectral information, 2D CNNs focussing on the extraction of spatial information are often preferred. High-resolution satellite images, however, contain crucial spectral information as well as spatial information. In this study, a 3D-2D CNN model using both spectral and spatial information was applied to extract more accurate land cover information from very high-resolution satellite images. The model was applied on a Worldview-2 satellite image including agricultural product areas such as tea, hazelnut groves and land use classes such as buildings and roads. The results of the CNN based model were also compared against those of the Support Vector Machine (SVM) and Random Forest (RF) algorithms. The post-classification accuracies were obtained using 800 control points generated by a web interface created for crowdsourcing purposes. The classification accuracy was 95.6% for the 3D-2D CNN model, 89.2% for the RF and 86.4% for the SVM.",Remote sensing,classification,deep learning,semantic segmentation,,,,GEOCARTO INTERNATIONAL,,,,,,,,,,,,
Row_1409,"Liu, Wei","Kang, Ziwen","Liu, Jiawei","Lin, Yiyuan","Yu, Yongtao",A Multitask CNN-Transformer Network for Semantic Change Detection From Bitemporal Remote Sensing Images,,2024,0,"Bitemporal remote sensing (RS) semantic change detection (SCD) involves discerning and categorizing changes in the same geographical area across two RS images taken at different times. High-performance SCD approaches typically address this task using multitask networks that simultaneously handle binary change detection (BCD) and semantic segmentation (SS). Despite significant advancements in SCD research, constructing a multitask network that fully explores the correlation between BCD and SS remains challenging. To address this, we propose a novel approach called the multitask CNN-transformer network (MCTNet), tailored for SCD using bitemporal RS images. Our Siamese network simultaneously tackles SS and BCD via three subnetworks: two for SS and one for BCD. The methodology begins with a multiscale convolutional neural network (CNN) extracting local features from input images, and converting them into tokens. A Transformer module with an encoder-decoder architecture then captures long-range dependencies among these visual tokens. The extracted features are subsequently passed to multitask heads, generating predicted outputs. To ensure that the BCD results remain consistent regardless of the order of images in the input pair, we introduce spatiotemporal feature learning (SFL), enabling the acquisition of temporal-symmetric representations for BCD. Extensive experimental validation on the WHU-CD, SECOND, and HRSCD datasets demonstrates the effectiveness and efficiency of MCTNet for both SS and BCD tasks. The source code for this article will be published on GitHub in the future https://github.com/kangziwen1/MCTNet.",Feature extraction,Semantics,Convolutional neural networks,Head,Context modeling,Computer architecture,Land surface,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Li, Jonathan",Deep learning,Accuracy,Change detection (CD),multitask learning (MTL),,,semantic segmentation (SS),transformer,transformer,,
Row_1410,"Feng, Shuangda","Gao, Mingxing","Jin, Xiaowei","Zhao, Ting","Ang, Feng Y.",Fine-grained damage detection of cement concrete pavement based on UAV remote sensing image segmentation and stitching,,FEB 28 2024,3,"A hybrid model for pavement damage segmentation, stitching, and detection based on unmanned aerial vehicle (UAV) remote-sensing pavement images was proposed as high-resolution panoramic images are required to analyze pavement damage feature parameters during the inspection process. Based on target recognition, active stereo vision was used to determine the autonomy awareness of the UAV system in the 3D structure. Then, incremental search strategy was employed to triangulate scale-invariant feature transform (SIFT) point features based on geometric constraints. Perspective-n-point and random sample consensus (PNP-RANSAC) algorithm were combined to find the best matching surface for mis-matching rejection, and the trigonometric function theory was used to fuse images. Subsequently, the semantic segmentation framework based on deep learning context encoder network (CE-Net) was employed to train suitable models to simultaneously detect and segment pavement damages. Finally, the stitching technique and hue, saturation, value (HSV) segmentation technique were combined to obtain the feature parameters to be detected. This method achieved fast and accurate stitching, feature parameter segmentation, and pavement health condition assessment of long-threaded pavement disease image datasets. The splicing accuracy rate and the global feature segmentation overlap rate were greater than 84% and 80%, respectively. The experimental data show that the method is able to detect full-width pavement breakage parameters better than existing optical sensor-based automotive or drone inspection methods.",Geometric constraints,Panorama stitching,Pavement breakage detection,Scale invariant feature transformation,Semantic segmentation,,,MEASUREMENT,,,,,,,,,,,,
Row_1411,Zhang Kun,Zhu Yawei,Wang Xiaohong,Zhang Liting,Zhong Ruofei,Three-Dimensional Point Cloud Semantic Segmentation Network Based on Spatial Graph Convolution Network,,JAN 2023,6,"With the increasing demand for intelligent construction in science and technology, semantic segmentation technology has attracted extensive attention from scholars in the field of graphics and images. This technology provides effective decision support for target tracking, visual control, and other technologies. However, the operation efficiency and segmentation accuracy of the three-dimensional (3D) point cloud semantic segmentation model are bottlenecks to its development. A semantic segmentation network model of the 3D point cloud, called point cloud+ graph convolution network (PCGCN) is proposed. PCGCN uses the EdgeConv network to extract local features and ResNet to enhance the transmission of features, fuse the local features of different scales, and participate in semantic segmentation of the 3D point cloud. In the process of deep learning, PCGCN solves the problem of the lack of local features and improves the segmentation effect. Furthermore, in the point cloud deep learning network, the introduction of ResNet improves the accuracy of semantic segmentation. Experiments are carried out using ShapeNet and S3DIS datasets. The experimental results show that the PCGCN accuracies are 85. 1% on the ShapeNet dataset and 81. 3% on the S3DIS dataset.",remote sensing,three-dimensional image processing,point cloud,semantic segmentation,graph convolution network,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,
Row_1412,"Benjdira, Bilel","Bazi, Yakoub","Koubaa, Anis","Ouni, Kais",,Unsupervised Domain Adaptation Using Generative Adversarial Networks for Semantic Segmentation of Aerial Images,,JUN 1 2019,162,"Segmenting aerial images is of great potential in surveillance and scene understanding of urban areas. It provides a mean for automatic reporting of the different events that happen in inhabited areas. This remarkably promotes public safety and traffic management applications. After the wide adoption of convolutional neural networks methods, the accuracy of semantic segmentation algorithms could easily surpass 80% if a robust dataset is provided. Despite this success, the deployment of a pretrained segmentation model to survey a new city that is not included in the training set significantly decreases accuracy. This is due to the domain shift between the source dataset on which the model is trained and the new target domain of the new city images. In this paper, we address this issue and consider the challenge of domain adaptation in semantic segmentation of aerial images. We designed an algorithm that reduces the domain shift impact using generative adversarial networks (GANs). In the experiments, we tested the proposed methodology on the International Society for Photogrammetry and Remote Sensing (ISPRS) semantic segmentation dataset and found that our method improves overall accuracy from 35% to 52% when passing from the Potsdam domain (considered as source domain) to the Vaihingen domain (considered as target domain). In addition, the method allows efficiently recovering the inverted classes due to sensor variation. In particular, it improves the average segmentation accuracy of the inverted classes due to sensor variation from 14% to 61%.",convolutional neural networks,semantic segmentation,aerial imagery,domain adaptation,gener ative adversarial networks,,,REMOTE SENSING,,,,,,,,,,,,
Row_1413,"Gao, Weixiao","Nan, Liangliang","Boom, Bas","Ledoux, Hugo",,PSSNet: Planarity-sensible Semantic Segmentation of large-scale urban meshes,,FEB 2023,17,"We introduce a novel deep learning-based framework to interpret 3D urban scenes represented as textured meshes. Based on the observation that object boundaries typically align with the boundaries of planar regions, our framework achieves semantic segmentation in two steps: planarity-sensible over-segmentation followed by semantic classification. The over-segmentation step generates an initial set of mesh segments that capture the planar and non-planar regions of urban scenes. In the subsequent classification step, we construct a graph that encodes the geometric and photometric features of the segments in its nodes and the multi-scale contextual features in its edges. The final semantic segmentation is obtained by classifying the segments using a graph convolutional network. Experiments and comparisons on two semantic urban mesh benchmarks demonstrate that our approach outperforms the state-of-the-art methods in terms of boundary quality, mean IoU (intersection over union), and generalization ability. We also introduce several new metrics for evaluating mesh over-segmentation methods dedicated to semantic segmentation, and our proposed over segmentation approach outperforms state-of-the-art methods on all metrics. Our source code is available at https://github.com/WeixiaoGao/PSSNet.",Texture meshes,Semantic segmentation,Over-segmentation,Urban scene understanding,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,
Row_1414,"Weng, Qian","Chen, Hao","Chen, Hongli","Guo, Wenzhong","Mao, Zhengyuan",A Multisensor Data Fusion Model for Semantic Segmentation in Aerial Images,,2022,5,"Semantic segmentation in high-resolution aerial images is a fundamental and challenging task with a wide range of applications. Although many segmentation methods with convolutional neural networks have achieved inspiring results, it is still difficult to distinguish regions with similar spectral features only using high-resolution data. Besides, the traditional data-independent upsampling methods may lead to suboptimal results. This letter proposes a multisensor data fusion model (MSDFM). Following the classical encoder-decoder structure, MSDFM regards colored digital surface models (colored-DSMs) data as a complementary input for further detailed feature extraction. A data-dependent upsampling (DUpsampling) method is adopted in the decoder stage instead of the common upsampling approaches to improve the classification accuracy of pixels of the small objects. Extensive experiments on Vaihingen and Potsdam datasets demonstrate that our proposed MSDFM outperforms most related models. Significantly, segmentation performance for the car category surpasses state-of-the-art methods over the International Society of Photogrammetry and Remote Sensing (ISPRS) Vaihingen dataset.",Semantics,Automobiles,Image segmentation,Feature extraction,Vegetation,Decoding,Deconvolution,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Digital surface model (DSM),high-resolution aerial images,semantic segmentation,,,,,,,,
Row_1415,"Xu, Leilei","Liu, Yujun","Shi, Shanqiu","Zhang, Hao","Wang, Dan",Land-Cover Classification With High-Resolution Remote Sensing Images Using Interactive Segmentation,,2023,1,"Deep convolutional neural network (CNN) has been increasingly applied in interpretation of remote sensing image such as automatically mapping land cover. Although the automatic CNN method achieves relatively high accuracy, there are still many misclassified areas. Considering that it is still far from practical application, this paper proposes a semi-automatic auxiliary scheme for land cover classification whose core idea is to use an interactive segmentation network. To infer the rough positions and categories of objects, a CNN is relied on to classify images in a patch-wise manner. Then an interactive segmentation method is proposed by accepting user-clicks on the inside and outside of object to guide the model for the segmentation task in the patches. This model also introduces different interactive modules to better integrate features of different scales. In addition, we create a large-scale sample library containing five common land cover categories which covers Jiangsu Province, China, and includes both aerial and satellite imagery. On our sample, we gave a thorough evaluation of most recent deep learning-based methods. The experimental results shown by our interactive segmentation also far outperform the recent semantic segmentation method, which provides a reference for semi-automatic land cover mapping.",Deep learning,CNN,sample,interactive segmentation,,,,IEEE ACCESS,,,,,,,,,,,,
Row_1416,"Wei, Debin","Xie, Hongji","Li, Pinru","Xu, Yongqiang",,A Learning Framework With Multispectral Band-Differentiated Encoding for Remote Sensing Water Body Detection,,2024,1,"Classic deep convolutional neural network (DCNN) models have demonstrated notable efficacy in segmenting remote sensing images. However, their ability to enhance the precision of water body detection, particularly for smaller ones amid intricate backgrounds, remains challenging. This article proposes the negative Laplacian filter (NLF) method as a solution, enhancing regional color contrast during preprocessing to capture more intricate details effectively. Furthermore, a novel approach employs a differential dual-encoding structure that encodes diverse spectra based on their spectral attributes. Lastly, leveraging prior insights from remote sensing, we introduce the weak label weight adjustment operation for refining predicted images in postprocessing stages. The proposed model significantly outperforms the comparison models on our remote sensing water body dataset.",Multispectral remote sensing,negative Laplacian filtering (NLF),semantic segmentation,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,
Row_1417,"Chai, Dengfeng","Newsam, Shawn","Huang, Jingfeng",,,Aerial image semantic segmentation using DCNN predicted distance maps,,MAR 2020,44,"This paper addresses the challenge of learning spatial context for the semantic segmentation of high-resolution aerial images using Deep Convolutional Neural Networks (DCNNs). The proposed solution involves deriving a signed distance map for each semantic class from a ground truth label map and training a DCNN to predict this distance map instead of a score map for each class. Since the distance between a target pixel and its nearest object boundary measures how far the pixel penetrates an object, the distance maps encode spatial context, particularly spatial smoothness. Positive pixel values in the distance maps correspond to the correct class and negative values correspond to the incorrect class. A final label map is derived from the predicted distance maps by selecting the class with the maximum distance. Since neighboring pixels in the distance maps have similar values, the segmentation results are smoother than current approaches. The results are shown to be even better than performing post-processing using fully connected Conditional Random Fields (CRFs), a common approach to smoothing the segmentations produced DCNNs. Experimental results on the semantic labeling challenge dataset show the proposed approach outperforms most state-of-the-art methods. Our main contribution, though, is the novel idea of replacing the pixel-wise class score maps of DCNNs with distance maps. This is therefore orthogonal and complementary to other techniques employed by the state-of-the-art methods and could therefore be used to improve upon them.",Deep learning,Semantic segmentation,DCNNs,Distance maps,Distance transform,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,
Row_1418,"Li, Qiupeng","Kong, Yingying",,,,An Improved SAR Image Semantic Segmentation Deeplabv3+Network Based on the Feature Post-Processing Module,,APR 2023,8,"Synthetic Aperture Radar (SAR) can provide rich feature information under all-weather and day-night conditions because it is not affected by climatic conditions. However, multiplicative speckle noise exists in SAR images, which makes it difficult to accurately identify some fuzzy targets in SAR images, such as roads and rivers, during semantic segmentation. This paper proposes an improved Deeplabv3+ network that can be effectively applied to the semantic segmentation task of SAR images. Firstly, this paper added the attention mechanism and, combined with the idea of an image pyramid, proposed the Feature Post-Processing Module (FPPM) to post-process the network output feature map, obtain better fine image features, and solve the problem of fuzzy texture and spectral features of SAR images. Compared to the original Deeplabv3+ network, the segmentation accuracy has been improved by 3.64% and mIoU improved by 1.09%. Secondly, to solve the problems of limited SAR image data and an unbalanced sample, this paper used the focal loss function to improve the backbone function of the network, which increased the mIoU by 1.01%. Finally, the Atrous Spatial Pyramid Pooling (ASPP) module was improved and the 3 x 3 void convolution in ASPP was decomposed into 2D, which can maintain the void ratio and effectively reduce the calculation amount of the module, shorten the training time by 19 ms and improve the semantic segmentation effect.",SAR image semantic segmentation,Deeplabv3+,attention mechanism,focal loss function,,,,REMOTE SENSING,,,,,,,,,,,,
Row_1419,"Diakogiannis, Foivos, I","Waldner, Francois","Caccetta, Peter","Wu, Chen",,ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data,,APR 2020,897,"Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. Here we propose a reliable framework for performant results for the task of semantic segmentation of monotemporal very high resolution aerial images. Our framework consists of a novel deep learning architecture, ResUNet-a, and a novel loss function based on the Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination with residual connections, atrous convolutions, pyramid scene parsing pooling and multi-tasking inference. ResUNet-a infers sequentially the boundary of the objects, the distance transform of the segmentation mask, the segmentation mask and a colored reconstruction of the input. Each of the tasks is conditioned on the inference of the previous ones, thus establishing a conditioned relationship between the various tasks, as this is described through the architecture's computation graph. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has excellent convergence properties and behaves well even under the presence of highly imbalanced classes. The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9% over all classes for our best model.",Convolutional neural network,Loss function,Architecture,Data augmentation,Very high spatial resolution,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,
Row_1420,"Qi, Xiyu","Zhang, Yidan","Wang, Lei","Wu, Yifan","Xin, Yi",DSMF-Net: Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation,,2025,0,"-Semantic segmentation of aerial images is crucial yet resource-intensive. Inspired by human ability to learn rapidly, few- shot semantic segmentation offers a promising solution by utilizing limited labeled data for efficient model training and generalization. However, the intrinsic complexities of aerial images, compounded by scarce samples, often result in inadequate feature representation and semantic ambiguity, detracting from the model's performance. In this article, we propose to tackle these challenging problems via dual semantic metric learning and multisemantic features fusion and introduce a novel few-shot segmentation Network (DSMFNet). On the one hand, we consider the inherent semantic gap between the feature of graph and grid structures and metric learning of few-shot segmentation. To exploit multiscale global semantic context, we construct scale-aware graph prototypes from different stages of the feature layers based on graph convolutional networks (GCNs), while also incorporating prior-guided metric learning to further enhance context at the high-level convolution features. On the other hand, we design a pyramid-based fusion and condensation mechanism to adaptively merge and couple the multisemantic information from support and query images. The indication and fusion of different semantic features can effectively emphasize the representation and coupling abilities of the network. We have conducted extensive experiments over the challenging iSAID-5i and DLRSD benchmarks. The experiments have demonstrated our network's effectiveness and efficiency, yielding on-par performance with the state-of-the-art methods.",Aerial image,dual metric learning,few- shot learning,few- shot learning,graph convolutional network (GCN),semantic segmentation,semantic segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Chen, Zhan",semantic segmentation,,,,"Ge, Yunping",,,,,,
Row_1421,"Yan, Chuan","Fan, Xiangsuo","Fan, Jinlong","Wang, Nayi",,Improved U-Net Remote Sensing Classification Algorithm Based on Multi-Feature Fusion Perception,,MAR 2022,31,"The selection and representation of remote sensing image classification features play crucial roles in image classification accuracy. To effectively improve the classification accuracy of features, an improved U-Net network framework based on multi-feature fusion perception is proposed in this paper. This framework adds the channel attention module (CAM-UNet) to the original U-Net framework and cascades the shallow features with the deep semantic features, replaces the classification layer in the original U-Net network with a support vector machine, and finally uses the majority voting game theory algorithm to fuse the multifeature classification results and obtain the final classification results. This study used the forest distribution in Xingbin District, Laibin City, Guangxi Zhuang Autonomous Region as the research object, which is based on Landsat 8 multispectral remote sensing images, and, by combining spectral features, spatial features, and advanced semantic features, overcame the influence of the reduction in spatial resolution that occurs with the deepening of the network on the classification results. The experimental results showed that the improved algorithm can improve classification accuracy. Before the improvement, the overall segmentation accuracy and segmentation accuracy of the forestland increased from 90.50% to 92.82% and from 95.66% to 97.16%, respectively. The forest cover results obtained by the algorithm proposed in this paper can be used as input data for regional ecological models, which is conducive to the development of accurate and real-time vegetation growth change models.",multifeature fusion,U-Net,channel attention,remote sensing image classification,majority voting game,,,REMOTE SENSING,,,,,,,,,,,,
Row_1422,"Chan-Hon-Tong, Adrien","Audebert, Nicolas",,,,OBJECT DETECTION IN REMOTE SENSING IMAGES WITH CENTER ONLY,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2018,5,"There are a lot of works aiming to reduce the need of human annotations for object detection: self supervised training, interactive verification instead of annotation or weakly supervised training.For example, only pointing object centres is a faster to annotate but weaker ground truth than providing bounding boxes or detailed segmentation mask. Although not usable for large areas such as roads, vegetation and buildings, centers can be used to learn adequate detectors and segmentors. We perform a comparative analysis on four public remote sensing datasets on the task of vehicle detection and show that centre annotations is a competitive baseline compared to other more sophisticated annotations.",deep learning,semantic segmentation,weak labeling,object centres,,,,,,,,,,,,,,,,
Row_1423,"Shi, Xianzheng","Fu, Shilei","Chen, Jin","Wang, Feng","Xu, Feng",Object-Level Semantic Segmentation on the High-Resolution Gaofen-3 FUSAR-Map Dataset,,2021,24,"Land cover classification with SAR images mainly focuses on the utilization of fully polarimetric SAR (PolSAR) images. The conventional task of PolSAR classification is single-pixel-based region-level classification using polarimetric target decomposition. In recent years, a large number of high-resolution SAR images have become available, most of which are single-polarization. This article explores the potential of object-level semantic segmentation of high-resolution single-pol SAR images, in particular tailored for the Gaofen-3 (GF-3) sensor. First, a well-annotated GF-3 segmentation dataset ""FUSAR-Map"" is presented for SAR semantic segmentation. It is based on four data sources: GF-3 single-pol SAR images, Google Earth optical remote sensing images, Google Earth digital maps, and building footprint vector data. It consists of 610 high-resolution GF-3 single-pol SAR images with the size of 1024 x 1024. Second, an encoder-decoder network based on transfer learning is employed to implement semantic segmentation of GF-3 SAR images. For the FUSAR-Map dataset, an optical image pretrained deep convolution neural network (DCNN) is fine-tuned with the SAR training dataset. Experiments on the FUSAR-Map dataset demonstrate the feasibility of object-level semantic segmentation with high-resolution GF-3 single-pol SAR images. Also, our algorithm obtains fourth place about the PolSAR image semantic segmentation on the ""2020 Gaofen Challenge on Automated High-Resolution Earth Observation Image Interpretation."" The new dataset and the encoder-decoder network are intended as the benchmark data and baseline algorithm for further development of semantic segmentation with high-resolution SAR images. The FUSAR-Map and our algorithm are available at github.com/fudanxu/FUSAR-Map/.",Image segmentation,Radar polarimetry,Semantics,Synthetic aperture radar,Earth,Buildings,Image resolution,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,2020 GaoFen challenge,encoder–,decoder network,FUSAR,,,map dataset,GaoFen-3 (GF-3) single -polarization SAR images,object-level semantic segmentation,,
Row_1424,"Zhao, Shaoxuan","Zhou, Xiaoguang","Hou, Dongyang",,,An Anomaly Detection-Based Domain Adaptation Framework for Cross-Domain Building Extraction from Remote Sensing Images,,FEB 2023,1,"Deep learning-based building extraction methods have achieved a high accuracy in closed remote sensing datasets. In fact, the distribution bias between the source and target domains can lead to a dramatic decrease in their building extraction effect in the target domain. However, the mainstream domain adaptation methods that specifically address this domain bias problem require the reselection of many unlabeled samples and retraining in other target domains. This is time-consuming and laborious and even impossible at small regions. To address this problem, a novel domain adaptation framework for cross-domain building extraction is proposed from a perspective of anomaly detection. First, the initial extraction results of images in the target domain are obtained by a source domain-based pre-trained model, and then these results are classified into building mixed and non-building layers according to the predicted probability. Second, anomalous objects in the building layer are detected using the isolation forest method. Subsequently, the remaining objects in the building layer and the objects in the non-building layer are used as positive and negative samples, respectively, to reclassify the mixed layer using the random forest classifier. The newly extracted objects are fused with the remaining objects in the building layer as the final result. Four different experiments are performed on different semantic segmentation models and target domains. Some experimental results indicate that our framework can improve cross-domain building extraction compared to the pre-trained model, with an 8.7% improvement in the F1 metric when migrating from the Inria Aerial Image Labeling dataset to the Wuhan University dataset. Furthermore, experimental results show that our framework can be applied to multiple target domains without retraining and can achieve similar results to domain adaptation models based on adversarial learning.",building extraction,remote sensing image,domain adaptation,semantic segmentation,anomaly detection,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,
Row_1425,"Molina, Eric Narciso","Zhang, Zenghui",,,,SEMANTIC SEGMENTATION OF SATELLITE IMAGES USING A U-SHAPED FULLY CONNECTED NETWORK WITH DENSE RESIDUAL BLOCKS,2019 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA & EXPO WORKSHOPS (ICMEW),2019,0,"Semantic segmentation is the task of clustering pixels into an object class. In the field of remote sensing semantic segmentation has wide applications ranging from scene cover classification to change detection for scene understanding. With the success of deep learning algorithms for classification tasks, there has been much work to apply convolutional neural networks in remote sensing with much success. However, feature extraction of high resolution remote sensing imagery poses a challenge when applying such networks. In particular, there is a need to extract high level features while maintaining an objects resolution in the networks feature space. This work proposes an efficient deep fully convolution architecture that obtains high level features without loss of spatial resolution by replacing the standard convolutional layers in U-Net with dense residual blocks. By stacking identity blocks, we allow the input to flow through the network at every proceeding layer. Our network is termed DRU-Net, and is shown to outperform standard U-Net.",U-Net,dense skip connection,semantic segmentation,satellite image,,,,,,,,,,,,,,,,
Row_1426,"Jing, Wei","Cui, Binge","Lu, Yan","Huang, Ling",,BS-Net: Using Joint-Learning Boundary and Segmentation Network for Coastline Extraction from Remote Sensing Images,,DEC 2 2021,11,"The coastline extraction from remote-sensing images is of great significance to the dynamic monitoring of the coastal zone. The types of coastlines are complex and diverse, and they show different spectrum, texture, and shape features, so accurately extracting coastlines is still a challenging task. The semantic segmentation model based on deep learning has good generalization ability. However, the down sampling operation will lose the location of boundary information, resulting in the location offset between the extracted coastlines and the actual coastlines. A multi-task network, called the joint learning network of boundary and segmentation (BS-Net), was proposed in this letter. BS-Net adds a coastline positioning stream to supervise the location of the coastlines. Moreover, this letter designed a boundary-segmentation interaction (BSI) module for the mutual guidance of information between the coastline positioning stream and the sea-land segmentation stream to correct the coastline features and enhance the segmentation boundary. The experimental results on a set of Gaofen-1 remote sensing images showed that, for various natural coastlines and artificial coastlines, coastlines extracted based on BS-Net were more accurate than those extracted by other methods. Code is available at: https://github.com/weiAI1996/BS-Net. .",,,,,,,,REMOTE SENSING LETTERS,,,,,,,,,,,,
Row_1427,"Shen, Yilang","Li, Jingzhong","Zhao, Rong","Han, Fengfeng",,Multiresolution Mapping of Land Cover From Remote Sensing Images by Geometric Generalization,,2022,0,"Land cover multiresolution mapping of remote sensing images contributes greatly to land-use management, environmental protection, and city planning. In traditional mapping of this type, the representation of different land-use types depends on the image resolution, and the geometric, topologic, and semantic characteristics are not considered. This approach can cause a loss of useful information and the redundancy of useless information. In this study, we propose a superpixel-based land cover (multiresolution representation SULR) method for remote sensing images that employs multifeature fusion. In this process, we first define three basic superpixel operations, collapse, connection, and cutting, as the basic operators of multiresolution land cover mapping. Then, the topological adjacent land parcels are combined through the amalgamation of polygons with heterogeneous properties and aggregation of polygons with homogeneous properties based on the three proposed superpixel operators. Finally, the geometric boundaries of parcels are simplified by combining the superpixel collapse operator and image thinning technologies. Compared with traditional image scale transformation methods, the proposed method can more effectively achieve multiresolution mapping of land cover from remote sensing images by considering the geometric, topologic, and semantic characteristics of land parcels.",Remote sensing,Spatial resolution,Semantics,Interpolation,Sensors,Research and development,Laplace equations,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Land cover,multiresolution mapping,remote sensing images,superpixel segmentation,,,,,,,
Row_1428,"Guo, Zhiling","Shengoku, Hiroaki","Wu, Guangming","Chen, Qi","Yuan, Wei",SEMANTIC SEGMENTATION FOR URBAN PLANNING MAPS BASED ON U-NET,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2018,28,"The automatic digitizing of paper maps is a significant and challenging task for both academia and industry. As an important procedure of map digitizing, the semantic segmentation section is mainly relied on manual visual interpretation with low efficiency. In this study, we select urban planning maps as a representative sample and investigate the feasibility of utilizing U-shape fully convolutional based architecture to perform end-to-end map semantic segmentation. The experimental results obtained from the test area in Shibuya district, Tokyo, demonstrate that our proposed method could achieve a very high Jaccard similarity coefficient of 93.63% and an overall accuracy of 99.36%. For implementation on GPGPU and cuDNN, the required processing time for the whole Shibuya district can be less than three minutes. The results indicate the proposed method can serve as a viable tool for urban planning map semantic segmentation task with high accuracy and efficiency.",map digitizing,urban planning maps,U-shape fully convolutional,semantic segmentation,,,,,"Shi, Xiaodan",,,,,"Shao, Xiaowei","Xu, Yongwei",,,,,"Shibasaki, Ryosuke"
Row_1429,"He, Haoming","Chen, Yerui","Li, Mingchao","Chen, Qiang",,ForkNet: Strong Semantic Feature Representation and Subregion Supervision for Accurate Remote Sensing Change Detection,,2022,10,"In this article, we propose an effective siamese feature pyramid network (FPN), ForkNet, for remote sensing change detection (RSCD). We find that the siamese network structure, which is widely used for RSCD, contains only one downsampling network in the feature extraction stage, e.g., VGG16 and ResNet-18, to extract the deep features of a single image, such that the features have a large semantic gap between high-level feature maps and low-level feature maps. The low-level feature maps with weak semantics may become a bottleneck of network performance. Thus, we apply an FPN to the feature extraction stage to generate feature representations with strong semantics at each level. Further, we design a cross-resolution attention module (CRAM) to aggregate contextual information across resolutions and naturally serve as a bridge for exchanging information across different resolution feature maps. The siamese FPN equipped with the CRAM is called ForkNet. To better train ForkNet, we extend the Tversky loss to a novel loss, pyramid Tversky loss, which is capable of supervising subregions at different scales to obtain more fine-grained detection results. Using pyramid Tversky loss together with Focal loss, our ForkNet achieves state-of-the-art detection performance on two challenging datasets.",Feature extraction,Semantics,Remote sensing,Network architecture,Task analysis,Convolution,Image segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Feature pyramid network (FPN),pyramid Tversky loss,remote sensing change detection (RSCD),siamese network,,,,,,,
Row_1430,"Liu, Jiayun","Wang, Shengsheng","Hou, Xiaowei","Song, Wenzhuo",,A deep residual learning serial segmentation network for extracting buildings from remote sensing imagery,,2020,21,"Extracting buildings from high spatial resolution remote sensing imagery automatically is considered as an important task in many applications. The huge differences in the appearance and spatial distribution of man-made buildings make it a challenging issue. In recent years, convolutional neural networks (CNNs) have made remarkable progress in computer vision. Many published papers have applied deep CNNs to remote sensing successfully. However, most contributions require complex structure and a big number of parameters which lead to redundant computations, and limit the application of the models. To address these issues, we propose a deep residual learning serial segmentation network called SSNet, an end-to-end semantic segmentation network, to extract buildings from high spatial resolution remote sensing imagery. SSNet reduces the network complexity and computations by drawing on the advantages of U-Net and ResNet, and improves the detection accuracy. The SSNet is extensively evaluated on two large remote sensing datasets covering a wide range of urban settlement appearances. The comparison of SSNet and state-of-the-art algorithms demonstrates the effectiveness and superiority of the proposed model for building extraction.",,,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1431,"Xu, Yonghao","Bai, Tao","Yu, Weikang","Chang, Shizhen","Atkinson, Peter M.",AI Security for Geoscience and Remote Sensing: Challenges and future trends,,JUN 2023,26,"Recent advances in artificial intelligence (AI) have significantly intensified research in the geoscience and remote sensing (RS) field. AI algorithms, especially deep learning-based ones, have been developed and applied widely to RS data analysis. The successful application of AI covers almost all aspects of Earth-observation (EO) missions, from low-level vision tasks like superresolution, denoising, and inpainting, to high-level vision tasks like scene classification, object detection, and semantic segmentation. Although AI techniques enable researchers to observe and understand the earth more accurately, the vulnerability and uncertainty of AI models deserve further attention, considering that many geoscience and RS tasks are highly safety critical. This article reviews the current development of AI security in the geoscience and RS field, covering the following five important aspects: adversarial attack, backdoor attack, federated learning (FL), uncertainty, and explainability. Moreover, the potential opportunities and trends are discussed to provide insights for future research. To the best of the authors' knowledge, this article is the first attempt to provide a systematic review of AI security-related research in the geoscience and RS community. Available code and datasets are also listed in the article to move this vibrant field of research forward.",Scene classification,Uncertainty,Systematics,Semantic segmentation,Superresolution,Market research,Safety,IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE,"Ghamisi, Pedram",Artificial intelligence,Remote sensing,Algorithm design and analysis,,,,,,,,
Row_1432,"Yuan, Xiaolei","Chen, Zeqiang","Chen, Nengcheng","Gong, Jianya",,Land cover classification based on the PSPNet and superpixel segmentation methods with high spatial resolution multispectral remote sensing imagery,,AUG 10 2021,13,"Classifying land cover using high-resolution remote-sensing images is challenging. The emergence of deep learning provides improved possibilities, but owing to the limitations of network structures, traditional convolutional neural network methods lose essential information about boundaries and small ground objects. We propose a superpixel-optimized convolutional neural network (SOCNN) framework to overcome this weakness. The SOCNN includes three modules: a semantic segmentation module, a superpixel optimization module, and a fusion module. The performance of the first module was evaluated using several common networks. PSPNet outperformed other networks, obtaining a pixel accuracy of 83.25%, a Kappa coefficient of 0.7862, and a mean intersection over union of 64.19%. Weighted loss was introduced to alleviate the effect of category imbalance, and the class pixel accuracy of category 11 improved by 19.77% with a weight of 20. The subpixel model was evaluated, and the pixel accuracy reached 83.43% with the superpixel-FCN method. Our superpixel optimized module improved the pixel accuracy of the object boundary by 1.37% when the fusion factor was 0.65. These results show that the SOCNN method is effective for recovering boundary information. (C) 2021 Society of Photo-Optical Instrumentation Engineers (SPIE)",land cover classification,high spatial resolution remote sensing image,convolution neural networks,superpixel,weighted loss,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,
Row_1433,"Liu, Qing","Dong, Yongsheng","Jiang, Zhiqiang","Pei, Yuanhua","Zheng, Boshi",Multi-Pooling Context Network for Image Semantic Segmentation,,MAY 28 2023,4,"With the development of image segmentation technology, image context information plays an increasingly important role in semantic segmentation. However, due to the complexity of context information in different feature maps, simple context capture operations can easily cause context information omission. Rich context information can better classify categories and improve the quality of image segmentation. On the contrary, poor context information will lead to blurred image category segmentation and an incomplete target edge. In order to capture rich context information as completely as possible, we constructed a Multi-Pooling Context Network (MPCNet), which is a multi-pool contextual network for the semantic segmentation of images. Specifically, we first proposed the Pooling Context Aggregation Module to capture the deep context information of the image by processing the information between the space, channel, and pixel of the image. At the same time, the Spatial Context Module was constructed to capture the detailed spatial context of images at different stages of the network. The whole network structure adopted the form of codec to better extract image context. Finally, we performed extensive experiments on three semantic segmentation datasets (Cityscapes, ADE20K, and PASCAL VOC2012 datasets), which fully proved that our proposed network effectively alleviated the lack of context extraction and verified the effectiveness of the network.",semantic segmentation,context information,convolutional neural network,attention module,,,,REMOTE SENSING,"Zheng, Lintao",,,,,"Fu, Zhumu",,,,,,
Row_1434,"Du, Jing","Jiang, Zuning","Huang, Shangfeng","Wang, Zongyue","Su, Jinhe",Point Cloud Semantic Segmentation Network Based on Multi-Scale Feature Fusion,,MAR 2021,15,"The semantic segmentation of small objects in point clouds is currently one of the most demanding tasks in photogrammetry and remote sensing applications. Multi-resolution feature extraction and fusion can significantly enhance the ability of object classification and segmentation, so it is widely used in the image field. For this motivation, we propose a point cloud semantic segmentation network based on multi-scale feature fusion (MSSCN) to aggregate the feature of a point cloud with different densities and improve the performance of semantic segmentation. In our method, random downsampling is first applied to obtain point clouds of different densities. A Spatial Aggregation Net (SAN) is then employed as the backbone network to extract local features from these point clouds, followed by concatenation of the extracted feature descriptors at different scales. Finally, a loss function is used to combine the different semantic information from point clouds of different densities for network optimization. Experiments were conducted on the S3DIS and ScanNet datasets, and our MSSCN achieved accuracies of 89.80% and 86.3%, respectively, on these datasets. Our method showed better performance than the recent methods PointNet, PointNet++, PointCNN, PointSIFT, and SAN.",LIDAR point cloud,semantic segmentation,feature fusion,deep learning,computer vision,,,SENSORS,"Su, Songjian",,,,,"Wu, Yundong","Cai, Guorong",,,,,
Row_1435,"Gao, Wei","Hashemi-Sakhtsari, Ahmad","McDonnell, Mark D.",,,End-to-End Phoneme Recognition using Models from Semantic Image Segmentation,2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN),2020,0,"We train fully convolutional neural networks with no recurrent layers for the end-to-end phoneme recognition task, using the Connectionist Temporal Classification (CTC) loss function. The adopted network, U-Net, was introduced initially for semantic image segmentation tasks, and is often applied to segmenting features in medical imaging and remote sensing. The similarities between CTC-based automatic speech recognition and semantic segmentation problems are discussed. We extend the encoder-decoder architecture of U-Net and show it is capable of good performance in the acoustic modelling of a speech recognition system. We investigate the importance of the concatenation step in the design of U-net, and report results using the core test set of the TIMIT corpus.",speech recognition,semantic image segmentation,convolutional neural networks,connectionist temporal classification,,,,,,,,,,,,,,,,
Row_1436,"Cheng, Xinglu","Sun, Yonghua","Zhang, Wangkuan","Wang, Yihan","Cao, Xuyue",Application of Deep Learning in Multitemporal Remote Sensing Image Classification,,AUG 2023,15,"The rapid advancement of remote sensing technology has significantly enhanced the temporal resolution of remote sensing data. Multitemporal remote sensing image classification can extract richer spatiotemporal features. However, this also presents the challenge of mining massive data features. In response to this challenge, deep learning methods have become prevalent in machine learning and have been widely applied in remote sensing due to their ability to handle large datasets. The combination of remote sensing classification and deep learning has become a trend and has developed rapidly in recent years. However, there is a lack of summary and discussion on the research status and trends in multitemporal images. This review retrieved and screened 170 papers and proposed a research framework for this field. It includes retrieval statistics from existing research, preparation of multitemporal datasets, sample acquisition, an overview of typical models, and a discussion of application status. Finally, this paper discusses current problems and puts forward prospects for the future from three directions: adaptability between deep learning models and multitemporal classification, prospects for high-resolution image applications, and large-scale monitoring and model generalization. The aim is to help readers quickly understand the research process and application status of this field.",deep learning,multitemporal remote sensing images,remote sensing classification,,,,,REMOTE SENSING,"Wang, Yanzhao",,,,,,,,,,,
Row_1437,"Wei, Shiqing","Zhang, Tao","Ji, Shunping",,,A Concentric Loop Convolutional Neural Network for Manual Delineation-Level Building Boundary Segmentation From Remote-Sensing Images,,2022,22,"To date, accurate building footprint delineation in the surveying, mapping, and geographic information system (GIS) communities has been dependent on human labor. In this article, to address this issue, we propose a concentric loop convolutional neural network (CLP-CNN) method for the automatic segmentation of building boundaries from remote-sensing images. The proposed method consists of three components: 1) a boundary detector to extract coarse polygonal boundaries of individual regions of interest; 2) a concentric loop-shaped convolutional network with bidirectional pairing loss to fine-tune the vertices of the polygons; and 3) a refinement block, which removes redundant vertices and regularizes the boundaries to polygons at the manual delineation level. We also demonstrate that the proposed CLP-CNN method is applicable to other generic objects in natural images. Experiments on two building datasets confirmed that more than 77%/67% of the building polygons predicted by the proposed method are on par with the manual delineation level, representing a significant saving in the labor cost of manual annotation. In generic object boundary delineation tests performed on the Semantic Boundaries Dataset (SBD), the proposed method outperformed the most recent state-of-the-art methods by at least 3.1% in average precision (AP). Furthermore, compared with other vertex matching methods, the learning process of the proposed method converges faster. The source code will be available at http://gpcv.whu.edu.cn/data.",Building boundary extraction,convolutional neural network,instance segmentation,polygon regularization,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1438,"Wu, Yingbin","Zhao, Peng","Wang, Fubo","Zhou, Mingquan","Geng, Shengling",A Prior-Guided Dual Branch Multi-Feature Fusion Network for Building Segmentation in Remote Sensing Images,,JUL 2024,0,"The domain of remote sensing image processing has witnessed remarkable advancements in recent years, with deep convolutional neural networks (CNNs) establishing themselves as a prominent approach for building segmentation. Despite the progress, traditional CNNs, which rely on convolution and pooling for feature extraction during the encoding phase, often fail to precisely delineate global pixel interactions, potentially leading to the loss of vital semantic details. Moreover, conventional CNN-based segmentation models frequently neglect the nuanced semantic differences between shallow and deep features during the decoding phase, which can result in subpar feature integration through rudimentary addition or concatenation techniques. Additionally, the unique boundary characteristics of buildings in remote sensing images, which offer a rich vein of prior information, have not been fully harnessed by traditional CNNs. This paper introduces an innovative approach to building segmentation in remote sensing images through a prior-guided dual branch multi-feature fusion network (PDBMFN). The network is composed of a prior-guided branch network (PBN) in the encoding process, a parallel dilated convolution module (PDCM) designed to incorporate prior information, and a multi-feature aggregation module (MAM) in the decoding process. The PBN leverages prior region and edge information derived from superpixels and edge maps to enhance edge detection accuracy during the encoding phase. The PDCM integrates features from both branches and applies dilated convolution across various scales to expand the receptive field and capture a more comprehensive semantic context. During the decoding phase, the MAM utilizes deep semantic information to direct the fusion of features, thereby optimizing segmentation efficacy. Through a sequence of aggregations, the MAM gradually merges deep and shallow semantic information, culminating in a more enriched and holistic feature representation. Extensive experiments are conducted across diverse datasets, such as WHU, Inria Aerial, and Massachusetts, revealing that PDBMFN outperforms other sophisticated methods in terms of segmentation accuracy. In the key segmentation metrics, including mIoU, precision, recall, and F1 score, PDBMFN shows a marked superiority over contemporary techniques. The ablation studies further substantiate the performance improvements conferred by the PBN's prior information guidance and the efficacy of the PDCM and MAM modules.",building segmentation,feature fusion,prior-guided information,dual branch network,parallel dilated convolution,,,BUILDINGS,"Zhang, Dan",,,,,,,,,,,
Row_1439,"Kaushik, Saurabh","Singh, Tejpal","Bhardwaj, Anshuman","Joshi, Pawan K.","Dietz, Andreas J.",Automated Delineation of Supraglacial Debris Cover Using Deep Learning and Multisource Remote Sensing Data,,MAR 2022,11,"High-mountain glaciers can be covered with varying degrees of debris. Debris over glaciers (supraglacial debris) significantly alter glacier melt, velocity, ice geometry, and, thus, the overall response of glaciers towards climate change. The accumulated supraglacial debris impedes the automated delineation of glacier extent owing to its similar reflectance properties with surrounding periglacial debris (debris aside the glaciated area). Here, we propose an automated scheme for supraglacial debris mapping using a synergistic approach of deep learning and multisource remote sensing data. A combination of multisource remote sensing data (visible, near-infrared, shortwave infrared, thermal infrared, microwave, elevation, and surface slope) is used as input to a fully connected feed-forward deep neural network (i.e., deep artificial neural network). The presented deep neural network is designed by choosing the optimum number and size of hidden layers using the hit and trial method. The deep neural network is trained over eight sites spread across the Himalayas and tested over three sites in the Karakoram region. Our results show 96.3% accuracy of the model over test data. The robustness of the proposed scheme is tested over 900 km(2) and 1710 km(2) of glacierized regions, representing a high degree of landscape heterogeneity. The study provides proof of the concept that deep neural networks can potentially automate the debris-covered glacier mapping using multisource remote sensing data.",debris cover glacier,deep neural network,semantic segmentation,remote sensing,SAR,Himalaya,climate change,REMOTE SENSING,,,,,,,,,,,,
Row_1440,"Dong, Qi","Chen, Xiaomei","Jiang, Lili","Wang, Lin","Chen, Jiachong",Semantic Segmentation of Remote Sensing Images Depicting Environmental Hazards in High-Speed Rail Network Based on Large-Model Pre-Classification,,MAR 2024,3,"With the rapid development of China's railways, ensuring the safety of the operating environment of high-speed railways faces daunting challenges. In response to safety hazards posed by light and heavy floating objects during the operation of trains, we propose a dual-branch semantic segmentation network with the fusion of large models (SAMUnet). The encoder part of this network uses a dual-branch structure, in which the backbone branch uses a residual network for feature extraction and the large-model branch leverages the results of feature extraction generated by the segment anything model (SAM). Moreover, a decoding attention module is fused with the results of prediction of the SAM in the decoder part to enhance the performance of the network. We conducted experiments on the Inria Aerial Image Labeling (IAIL), Massachusetts, and high-speed railway hazards datasets to verify the effectiveness and applicability of the proposed SAMUnet network in comparison with commonly used semantic segmentation networks. The results demonstrated its superiority in terms of both the accuracies of segmentation and feature extraction. It was able to precisely extract hazards in the environment of high-speed railways to significantly improve the accuracy of semantic segmentation.",remote sensing images,high-speed railway,color-coated steel sheet roof buildings,segment anything model,,,,SENSORS,"Zhao, Ying",,,,,,,,,,,
Row_1441,"Gaetano, Raffaele","Scarpa, Giuseppe","Poggi, Giovanni",,,RECURSIVE TEXTURE FRAGMENTATION AND RECONSTRUCTION SEGMENTATION ALGORITHM APPLIED TO VHR IMAGES,"2009 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM, VOLS 1-5",2009,5,"The Texture Fragmentation and Reconstruction (TFR) algorithm, recently proposed for the segmentation of textured images, has been applied with promising results to high-resolution remote-sensing images. The algorithm provides a sequence of nested segmentation maps which allow the analysis at various scales of observation However, the performance which is very good at large scales, with complex semantic areas retrieved with remarkable accuracy, becomes less satisfactory at finer scales.In this paper we propose to use the TFR in a recursive fashion, segmenting the image in just two regions, initially, with each region further segmented only if relevant subregions emerge. The recursive TFR allows one to better adapt to local statistics and to extract significant textures also at finer scales.Early experimental results validate the effectiveness of the new algorithm.",Remote-sensing images,segmentation,texture,hierarchical models,,,,,,,,,,,,,,,,
Row_1442,"Fan, Jinhong","Yang, Zhengqiu",,,,Deep Residual Network Based Road Detection Algorithm for Remote Sensing Images,"2020 5TH INTERNATIONAL CONFERENCE ON MECHANICAL, CONTROL AND COMPUTER ENGINEERING (ICMCCE 2020)",2020,3,"As an important recognition target in remote sensing images, roads can be widely used in many fields and has great significance. However, there are still some difficulties in the accurate identification of roads. Firstly, remote sensing images have high-resolution features, which provide more detailed features but also generate more complex background interference; secondly, different spatial resolution leads to different morphology of roads in images, such as different sizes of rural and urban roads, different road materials. These cause great difficulties for the detection of road targets. At present, most of the methods are based on the classification of spectral features, while ignoring other high-dimensional features.In recent years, with the development of artificial intelligence, deep learning has played a unique and outstanding role in many fields, image target detection has become a hotspot field, and many outstanding technologies have emerged. In this paper, a deep residual network-based remote sensing image road detection model is adopted to address the above problems, and the model is finally trained on Massachusetts Roads Dataset to perform road detection. The experimental results show that the model is trained to effectively and accurately identify road targets in the remote sensing images.",Remote Sensing,Road Detection,Deep Residual Networks,Semantic Segmentation,,,,,,,,,,,,,,,,
Row_1443,"Irwansyah, Edy","Gunawan, Alexander Agung Santoso",,,,Deep Learning in Damage Assessment with Remote Sensing Data: A Review,"DATA SCIENCE AND ALGORITHMS IN SYSTEMS, 2022, VOL 2",2023,1,"Over the years, deep learning (DL) algorithms have been employed in a variety of applications, including damage assessment utilizing remote sensing data. More than 70 articles in the subject, the majority of which were published during the last five years, are evaluated, and analyzed in this study, which introduces various significant DL approaches that use remote sensing data. This meta-analysis examines a variety of major DL concepts in relation to a variety of disaster types, focal objects, spatial resolutions, research regions, and classification accuracy. A thorough examination of how DL has been employed for damage assessment analytical tasks is undertaken, covering the dataset used, image preprocessing with fusion and registration, image/semantic segmentation, and object recognition and classification. Finally, a conclusion on current approaches, a critical conclusion on open challenges, and research directions are offered.",Damage assessment,Remote sensing,Deep learning,,,,,,,,,,,,,,,,,
Row_1444,"Guo, Rui","Zhao, Xiaopeng","Zuo, Guanzhong","Wang, Ying","Liang, Yi",Polarimetric Synthetic Aperture Radar Image Semantic Segmentation Network with Lovasz-Softmax Loss Optimization,,OCT 2023,1,"The deep learning technique has already been successfully applied in the field of microwave remote sensing. Especially, convolutional neural networks have demonstrated remarkable effectiveness in synthetic aperture radar (SAR) image semantic segmentation. In this paper, a Lovasz-softmax loss optimization SAR net (LoSARNet) is proposed which optimizes the semantic segmentation metric intersection over union (IOU) instead of using the traditional cross-entropy loss. Meanwhile, making use of the advantages of the dual-path structure, the network extracts feature through the spatial path (SP) and the context path (CP) to achieve a balance between efficiency and accuracy. Aiming at a polarimetric SAR (PolSAR) image, the proposed network is conducted on the PolSAR datasets for terrain segmentation. Compared to the typical dual-path network, which is the bilateral segmentation network (BiSeNet), the proposed LoSARNet can obtain better mean intersection over union (MIOU). And the proposed network also shows the highest evaluation index and the best performance when compared with several typical networks.",polarimetric synthetic aperture radar (SAR),semantic segmentation network,deep learning,loss function,Lovasz-softmax loss optimization SAR net (LoSARNet),,,REMOTE SENSING,,,,,,,,,,,,
Row_1445,"Han, Wei","Li, Jun","Wang, Sheng","Zhang, Xinyu","Dong, Yusen",Geological Remote Sensing Interpretation Using Deep Learning Feature and an Adaptive Multisource Data Fusion Network,,2022,13,"Geological remote sensing interpretation can extract elements of interest from multiple types of images, which is vital in geological survey and mapping, especially in inaccessible regions. However, due to numerous classes, high interclass similarities, complex distributions, and sample imbalances of geological elements, the interpretation results of machine learning (ML)-based methods are understandably worse than manual visual interpretation. In addition, scholars in remote sensing have mainly carried out their works to interpret a single geological element category, such as mineral, lithological, soil, and structure. The interpretation of multiple geological elements is missing, which is more in line with the open world. To improve the interpretation results of ML-based methods and reduce the labor cost in geological survey and mapping, we propose a deep learning (DL)-feature-based adaptive multisource data fusion network (AMSDFNet) for the efficient interpretation of multiple geological remote sensing elements. The AMSDFNet has two branches for learning valuable spatial and spectral information from two kinds of data sources, in which the atrous spatial pyramid pooling (ASPP) operation and an attention block are applied to adaptively extract and fuse multiscale informative features. A hard example mining algorithm was also added to select important training examples to address sample imbalance. A large-scale region in western China with sufficient geological elements was set as the research area. The proposed model improved the two critical metrics by about 2% in the experiment section. As far as we know, this research work is the first time DL features and multisource remote sensing images have been utilized to simultaneously interpret geological elements of lithology, soil, surface water, and glaciers. The extensive experimental results demonstrated the superiority of DL features and our model in geological remote sensing interpretation.",Deep learning,geological remote sensing,multisource data fusion,semantic segmentation,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Fan, Runyu",,,,,"Zhang, Xiaohan","Wang, Lizhe",,,,,
Row_1446,"Gao, Mengyu","Xu, Jiping","Yu, Jiabin","Dong, Qiulei",,Distilled Heterogeneous Feature Alignment Network for SAR Image Semantic Segmentation,,2023,3,"Synthetic aperture radar (SAR) image semantic segmentation has attracted increasing attention in the remote-sensing community recently, due to SAR's all-time and all-weather imaging capability. However, SAR images are generally more difficult to be segmented than their electro-optical (EO) counterparts, since speckle noises and layovers are inevitably involved in SAR images. On the other hand, EO images could only be obtained under cloud-free conditions, which limits their applications. To this end, this letter investigates how to introduce EO features to assist the training of an SAR-segmentation model so that the model could segment SAR images without their EO counterparts in the application and proposes a distilled heterogeneous feature alignment network (DHFA-Net), where an SAR-segmentation student model learns and aligns the features from a pretrained EO-segmentation teacher model. In the proposed DHFA-Net, both the student and teacher models employ an identical architecture but different parameter configurations, and a heterogeneous feature distillation module (HFDM) is explored for transferring latent EO features from the teacher model to the student model through heterogeneous feature distillation and then supervising the training of the SAR-segmentation model. Moreover, a heterogeneous feature alignment module (HFAM) is designed to aggregate multiscale features for segmentation by the feature alignment approach in each of the student and teacher models. By enabling the multiscale heterogeneous feature aggregation, the SAR segmentation performance could be boosted. Experimental results on two public datasets demonstrate the superiority of the proposed DHFA-Net.",Heterogeneous feature alignment,heterogeneous feature distillation,synthetic aperture radar (SAR) image semantic segmentation,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,
Row_1447,"Almarzouqi, Hasan","Saoud, Lyes Saad",,,,Semantic Labeling of High-Resolution Images Using EfficientUNets and Transformers,,2023,11,"Semantic segmentation necessitates approaches that learn high-level characteristics while dealing with enormous quantities of data. Convolutional neural networks (CNNs) can learn unique and adaptive features to achieve this aim. However, due to the large size and high spatial resolution of remote sensing images, these networks cannot efficiently analyze an entire scene. Recently, deep transformers have proven their capability to record global interactions between different objects in the image. In this article, we propose a new segmentation model that combines CNNs with transformers and show that this mixture of local and global feature extraction techniques provides significant advantages in remote sensing segmentation. In addition, the proposed model includes two fusion layers that are designed to efficiently represent multimodal inputs and outputs of the network. The input fusion layer extracts feature maps summarizing the relationship between image content and elevation maps [digital surface model (DSM)]. The output fusion layer uses a novel multitask segmentation strategy where class labels are identified using class-specific feature extraction layers and loss functions. Finally, a fast-marching method (FMM) is used to convert unidentified class labels into their closest known neighbors. Our results demonstrate that the proposed method improves segmentation accuracy compared with state-of-the-art techniques.",Transformers,Feature extraction,Remote sensing,Semantics,Semantic segmentation,Image resolution,Data models,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Convolutional neural networks (CNNs),EfficientNet,fusion networks,semantic segmentation,,,transformers,,,,
Row_1448,"Abadal, Sauc","Salgueiro, Luis","Marcello, Javier","Vilaplana, Veronica",,A Dual Network for Super-Resolution and Semantic Segmentation of Sentinel-2 Imagery,,NOV 2021,12,"There is a growing interest in the development of automated data processing workflows that provide reliable, high spatial resolution land cover maps. However, high-resolution remote sensing images are not always affordable. Taking into account the free availability of Sentinel-2 satellite data, in this work we propose a deep learning model to generate high-resolution segmentation maps from low-resolution inputs in a multi-task approach. Our proposal is a dual-network model with two branches: the Single Image Super-Resolution branch, that reconstructs a high-resolution version of the input image, and the Semantic Segmentation Super-Resolution branch, that predicts a high-resolution segmentation map with a scaling factor of 2. We performed several experiments to find the best architecture, training and testing on a subset of the S2GLC 2017 dataset. We based our model on the DeepLabV3+ architecture, enhancing the model and achieving an improvement of 5% on IoU and almost 10% on the recall score. Furthermore, our qualitative results demonstrate the effectiveness and usefulness of the proposed approach.",super-resolution,semantic segmentation,deep learning,convolutional neural network,Sentinel-2,,,REMOTE SENSING,,,,,,,,,,,,
Row_1449,"Su, Zhongbin","Wang, Yue","Xu, Qi","Gao, Rui","Kong, Qingming",LodgeNet: Improved rice lodging recognition using semantic segmentation of UAV high-resolution remote sensing images,,MAY 2022,26,"Rice lodging not only causes difficulty in harvest operations, but also drastically reduces yield. Therefore, it is very important to identify rice lodging efficiently. For unmanned aerial vehicle (UAV) remote sensing images, this paper combines the advantages of dense block, DenseNet, attention mechanism, and jump connection on the basis of U-Net network to propose an end-to-end, pixel-to-pixel semantic segmentation method to identify rice lodging. And the method can process the input multi-band image. The accuracy of the model proposed in this paper was 97.30% on rice lodging images, which performed better than other comparison methods in the test. At the same time, it has good effect on small sample data set. The results show that it is feasible to use the improved U-Net network model to extract the lodging area of rice, which provide a useful reference for rice breeding and agricultural insurance claims.",Deep learning,U-Net,Small sample data set,End-to-end neural network,,,,COMPUTERS AND ELECTRONICS IN AGRICULTURE,,,,,,,,,,,,
Row_1450,"Li, Zhuqiang","Chen, Shengbo","Meng, Xiangyu","Zhu, Ruifei","Lu, Junyan",Full Convolution Neural Network Combined with Contextual Feature Representation for Cropland Extraction from High-Resolution Remote Sensing Images,,MAY 2022,13,"The quantity and quality of cropland are the key to ensuring the sustainable development of national agriculture. Remote sensing technology can accurately and timely detect the surface information, and objectively reflect the state and changes of the ground objects. Using high-resolution remote sensing images to accurately extract cropland is the basic task of precision agriculture. The traditional model of cropland semantic segmentation based on the deep learning network is to down-sample high-resolution feature maps to low resolution, and then restore from low-resolution feature maps to high-resolution ideas; that is, obtain low-resolution feature maps through a network, and then recover to high resolution by up-sampling or deconvolution. This will bring about the loss of features, and the segmented image will be more fragmented, without very clear and smooth boundaries. A new methodology for the effective and accurate semantic segmentation cropland of high spatial resolution remote sensing images is presented in this paper. First, a multi-temporal sub-meter cropland sample dataset is automatically constructed based on the prior result data. Then, a fully convolutional neural network combined with contextual feature representation (HRNet-CFR) is improved to complete the extraction of cropland. Finally, the initial semantic segmentation results are optimized by the morphological post-processing approach, and the broken spots are ablated to obtain the internal homogeneous cropland. The proposed method has been validated on the Jilin-1 data and Gaofen Image Dataset (GID) public datasets, and the experimental results demonstrate that it outperforms the state-of-the-art method in cropland extraction accuracy. We selected the comparison of Deeplabv3+ and UPerNet methods in GID. The overall accuracy of our approach is 92.03%, which is 3.4% higher than Deeplabv3+ and 5.12% higher than UperNet.",high-resolution remote sensing image,contextual features,fully convolutional neural network,cropland extraction,morphological post-processing,,,REMOTE SENSING,"Cao, Lisai",,,,,"Lu, Peng",,,,,,
Row_1451,"Zeng, Tao","Luo, Fulin","Guo, Tan","Gong, Xiuwen","Xue, Jingyun",Multilevel Context Feature Fusion for Semantic Segmentation of ALS Point Cloud,,2023,8,"Semantic segmentation of airborne laser scanning (ALS) point clouds using deep learning is a hot research in remote sensing and photogrammetry. A current trend is to aggregate contextual features from different scales for boosting network generalization and diversity discrimination capabilities. One main challenge is how to achieve effective fusion with multi-scale information. In this letter, we propose a multilevel context feature fusion network (MCFN) for semantic segmentation of ALS point cloud based on an encoder-decoder structure. More specifically, we design the squeeze-expansion shared multilayer perceptron (SE-MLP) module following kernel point convolution (KPConv) in the encoding stage, which can extend the receptive field of KPConv. To aggregate low-level features and highlevel representations, we establish channel self-attention between skip connections. In the decoding stage, we develop a crosslayer attention fusion (CAF) module to generate additional discriminative channel features by fusing multiscale features at different upsampling layers. Experiments on the ISPRS and LASDU datasets demonstrate the superiority of the proposed method. Code: https://github.com/SC-shendazt/MCFN.",Attention mechanism,encoder-decoder structure,kernel point convolution (KPConv),multilevel fusion,point cloud semantic segmentation,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,"Li, Hanshan",,,,,,,,,,,
Row_1452,"Fang, Wei","Fu, Yuxiang","Sheng, Victor S.",,,FPS-U2Net: Combining U2Net and multi-level aggregation architecture for fire point segmentation in remote sensing images,,JUL 2024,1,"Traditional methods for fire point segmentation (FPS) in satellite remote sensing images (RSIs) overly rely on threshold judgment, which are greatly affected by factors such as regional time and show poor generalization. Besides, due to the difference between natural scene images (NSIs) and RSIs, directly apply NSIs-based deep learning methods to forest fire RSIs without any modification fails to achieve satisfactory results. To address these issues, first, we construct a Landsat8 RSI-FPS dataset covering different years, seasons and regions. Then, for the first time, we apply salient object detection (SOD) to FPS in forest fire monitoring and propose a novel network FPS-U2Net to improve the performance of FPS. FPS-U2Net is based on U2Netp (a lightweight U2Net), to make better use of the multi -level features from adjacent encoders, we propose multi -level aggregation module (MAM), which is placed between the encoder and decoder at the same stage to aggregate the adjacent multi -scale features and capture richer contextual information. To make up for the weakness of BCE loss, we employ the hybrid loss, BCE + IoU, for the training of the network, which can guide the network learn the salient information from pixel and map levels. Extensive experiments on three datasets demonstrate that our FPS-U2Net significantly outperforms the state-of-the-art semantic segmentation and SOD methods. FPS-U2Net can accurately segment fire regions and predict clear local details.",Remote sensing,Fire point segmentation (FPS),Forest fire monitoring,Salient object detection (SOD),Deep learning,,,COMPUTERS & GEOSCIENCES,,,,,,,,,,,,
Row_1453,"Song, Yanjiao","Rui, Xiaoping","Li, Junjie",,,AEDNet: An Attention-Based Encoder-Decoder Network for Urban Water Extraction From High Spatial Resolution Remote Sensing Images,,2024,3,"Accurate water extraction from urban remote sensing images holds great significance in assisting the formulation of river and lake management policies and ensuring the sustainable development of urban water resources. However, urban high-resolution remote sensing images encompass complex spatial and semantic information, which leads to disparities between the extracted water body features based on local and global information, consequently affecting the accuracy of urban water extraction. To tackle this issue, an attention-based encoder-decoder network was proposed. In this network, the backbone employing atrous convolution (AC) facilitated the acquisition of low-level and high-level features of urban remote sensing images at various scales. Integrated with the attention mechanism, the encoder-decoder structure extracted global features in both the spatial and channel domains. Subsequently, these two types of features were merged to yield the urban water segmentation. Moreover, considering both intersection over union and class weights, a joint loss function (JLF) was introduced to further enhance the accuracy of urban water extraction. Experimental results demonstrated the strong performance of the proposed method on both GID and LoveDA datasets.",Feature extraction,Convolution,Remote sensing,Water resources,Data mining,Indexes,Semantics,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Atrous convolution (AC),attention mechanism,joint loss function (JLF),remote sensing,,,urban water extraction,,,,
Row_1454,"Chen, Chuan","Zhao, Huilin","Cui, Wei","He, Xin",,Dual Crisscross Attention Module for Road Extraction from Remote Sensing Images,,OCT 2021,2,"Traditional pixel-based semantic segmentation methods for road extraction take each pixel as the recognition unit. Therefore, they are constrained by the restricted receptive field, in which pixels do not receive global road information. These phenomena greatly affect the accuracy of road extraction. To improve the limited receptive field, a non-local neural network is generated to let each pixel receive global information. However, its spatial complexity is enormous, and this method will lead to considerable information redundancy in road extraction. To optimize the spatial complexity, the Crisscross Network (CCNet), with a crisscross shaped attention area, is applied. The key aspect of CCNet is the Crisscross Attention (CCA) module. Compared with non-local neural networks, CCNet can let each pixel only perceive the correlation information from horizontal and vertical directions. However, when using CCNet in road extraction of remote sensing (RS) images, the directionality of its attention area is insufficient, which is restricted to the horizontal and vertical direction. Due to the recurrent mechanism, the similarity of some pixel pairs in oblique directions cannot be calculated correctly and will be intensely dilated. To address the above problems, we propose a special attention module called the Dual Crisscross Attention (DCCA) module for road extraction, which consists of the CCA module, Rotated Crisscross Attention (RCCA) module and Self-adaptive Attention Fusion (SAF) module. The DCCA module is embedded into the Dual Crisscross Network (DCNet). In the CCA module and RCCA module, the similarities of pixel pairs are represented by an energy map. In order to remove the influence from the heterogeneous part, a heterogeneous filter function (HFF) is used to filter the energy map. Then the SAF module can distribute the weights of the CCA module and RCCA module according to the actual road shape. The DCCA module output is the fusion of the CCA module and RCCA module with the help of the SAF module, which can let pixels perceive local information and eight-direction non-local information. The geometric information of roads improves the accuracy of road extraction. The experimental results show that DCNet with the DCCA module improves the road IOU by 4.66% compared to CCNet with a single CCA module and 3.47% compared to CCNet with a single RCCA module.",remote sensing,semantic segmentation,road extraction,attention mechanism,geometric information,directionality,,SENSORS,,,,,,,,,,,,
Row_1455,"Sui, Baikai","Cao, Yungang","Zhang, Shuang",,,EGDSR: Encoder-Generator-Decoder Network for Remote Sensing Super-Resolution Reconstruction,,2023,0,"Remote sensing single-image super-resolution (SR) reconstruction process detail information is easy to be lost and prone to defocus phenomenon; in order to solve these problems, we are based on the idea of potential spatial vector mapping, focusing on the attribute control of the feature recovery process, to improve the quality of remote sensing image reconstruction. This letter proposes an encoder-generator-decoder SR reconstruction (SRR) network for remote sensing named EGDSR. We design three modules: multiscale feature extraction and latent code generation module (MS-FCGM), multiattribute control of resolution progression module (MA-RPM) (recovery of latent encoding, fusion of multiscale features, and generation of high-resolution (HR) features), and high-resolution image reconstruction module (HRRM). The experimental results show that the HR images generated by our proposed EGDSR network have a stronger sense of truth, richer texture, and more realistic details, and have a better perceptual effect compared with other state-of-the-art SR networks. In addition, we also combine the semantic segmentation task to assist in verifying the quality of the HR remote sensing images generated by EGDSR, and successfully verify the higher application value of our proposed method.",Latent code recovery,multiscale feature,remote sensing,super-resolution (SR) reconstruction,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,
Row_1456,"Breitkopf, Tom Lukas","Hackel, Leonard","Ravanbakhsh, Mahdyar","Cooke, Anne-Karin","Willkommen, Sandra",Advanced Deep Learning Architectures for Accurate Detection of Subsurface Tile Drainage Pipes from Remote Sensing Images,,2022,4,"Subsurface tile drainage pipes provide agronomic, economic and environmental benefits. By lowering the water table of wet soils, they improve the aeration of plant roots and ultimately increase the productivity of farmland. They do however also provide an entryway of agrochemicals into subsurface water bodies and increase nutrition loss in soils. For maintenance and infrastructural development, accurate maps of tile drainage pipe locations and drained agricultural land are needed. However, these maps are often outdated or not present. Different remote sensing (RS) image processing techniques have been applied over the years with varying degrees of success to overcome these restrictions. Recent developments in deep learning (DL) techniques improve upon the conventional techniques with machine learning segmentation models. In this study, we introduce two DL-based models: i) improved U-Net architecture; and ii) Visual Transformer-based encoder-decoder in the framework of tile drainage pipe detection. Experimental results confirm the effectiveness of both models in terms of detection accuracy when compared to a basic U-Net architecture. Our code and models are publicly available at https://git.tu-berlin.de/rsim/drainage-pipes-detection.",Semantic segmentation,tile drainage pipe detection,visual transformer,U-Net,remote sensing,,,"MEDICAL IMAGE COMPUTING AND COMPUTER-ASSISTED INTERVENTION, PT III","Broda, Stefan",,,,,"Demir, Beguem",,,,,,
Row_1457,"Wang, Ying","Peng, Yuexing","Li, Wei","Alexandropoulos, George C.","Yu, Junchuan",DDU-Net: Dual-Decoder-U-Net for Road Extraction Using High-Resolution Remote Sensing Images,,2022,66,"Extracting roads from high-resolution remote sensing images (HRSIs) is vital in a wide variety of applications, such as autonomous driving, path planning, and road navigation. Due to the long and thin shape as well as the shades induced by vegetation and buildings, small-sized roads are more difficult to discern. In order to improve the reliability and accuracy of small-sized road extraction when roads of multiple sizes coexist in an HRSI, an enhanced deep neural network model termed dual-decoder-U-net (DDU-Net) is proposed in this article. Motivated by the U-Net model, a small decoder is added to form a dual-decoder structure for more detailed features. In addition, we introduce the dilated convolution attention module (DCAM) between the encoder and decoders to increase the receptive field as well as to distill multiscale features through cascading dilated convolution and global average pooling. The convolutional block attention module (CBAM) is also embedded in the parallel dilated convolution and pooling branches to capture more attention-aware features. Extensive experiments are conducted on the Massachusetts Roads dataset with experimental results showing that the proposed model outperforms the state-of-the-art DenseUNet, DeepLabv3+, and D-LinkNet by 6.5%, 3.3%, and 2.1% in the mean intersection over union (mIoU), and by 4%, 4.8%, and 3.1% in the F1 score, respectively. Both ablation and heatmap analysis are presented to validate the effectiveness of the proposed model. Moreover, the designed small decoder and introduced DCAM can be used as a portable module to be embedded in other U-Net-like models with encoder-decoder structure to enhance the road detection performance, especially for small-sized roads. The high portability of the designed module is validated by embedding it in the LinkNet, which greatly improves the road segmentation performance.",Roads,Feature extraction,Convolution,Decoding,Data mining,Semantics,Remote sensing,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Ge, Daqing",High-resolution remote sensing,road extraction,semantic segmentation,U-Net,"Xiang, Wei",,,,,,
Row_1458,"Yan, Li","Huang, Jianming","Xie, Hong","Wei, Pengcheng","Gao, Zhao",Efficient Depth Fusion Transformer for Aerial Image Semantic Segmentation,,MAR 2022,18,"Taking depth into consideration has been proven to improve the performance of semantic segmentation through providing additional geometry information. Most existing works adopt a two-stream network, extracting features from color images and depth images separately using two branches of the same structure, which suffer from high memory and computation costs. We find that depth features acquired by simple downsampling can also play a complementary part in the semantic segmentation task, sometimes even better than the two-stream scheme with the same two branches. In this paper, a novel and efficient depth fusion transformer network for aerial image segmentation is proposed. The presented network utilizes patch merging to downsample depth input and a depth-aware self-attention (DSA) module is designed to mitigate the gap caused by difference between two branches and two modalities. Concretely, the DSA fuses depth features and color features by computing depth similarity and impact on self-attention map calculated by color feature. Extensive experiments on the ISPRS 2D semantic segmentation dataset validate the efficiency and effectiveness of our method. With nearly half the parameters of traditional two-stream scheme, our method acquires 83.82% mIoU on Vaihingen dataset outperforming other state-of-the-art methods and 87.43% mIoU on Potsdam dataset comparable to the state-of-the-art.",semantic segmentation,self-attention,depth fusion,transformer,,,,REMOTE SENSING,,,,,,,,,,,,
Row_1459,"Yang, Dinghao","Wang, Bin","Li, Weijia","He, Conghui",,Exploring the user guidance for more accurate building segmentation from high-resolution remote sensing images,,FEB 2024,1,"In recent years, the computer vision domain has witnessed a surge of interest in interactive object segmentation, an area of study that seeks to expedite the annotation process for pixel-wise segmentation tasks through user guidance. Despite this growing focus, existing methods mainly focus on a single type of pre-annotation and neglect the quality of boundary prediction, which significantly influences subsequent manual adjustments to segmentation boundaries. To address these limitations, we introduce a novel end-to-end network to facilitate more precise building segmentation using diverse types of user guidance. In our proposed method, a centroid map is generated to provide foreground prior information crucial to the subsequent segmentation procedure, and the boundary correction module automatically refines the segmentation mask from existing segmentation networks. Extensive experiments on two popular building extraction datasets demonstrate that our method outperforms all existing approaches given various user guidance (bounding boxes, inside-outside points, or extreme points), achieving the IoU scores of over 95% on SpaceNet-Vegas dataset and over 93% on Inria-building dataset. The remarkable performance of our method further demonstrates its immense potential to alleviate the labor-intensive annotation process associated with remote sensing datasets. The code of our proposed method is available at https://github.com/StephenDHYang/UGBS-pytorch.",User guidance,Building extraction,Semantic segmentation,Boundary correction,,,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,
Row_1460,"Zhang, Jiahao","Chen, Bo","Zhou, Jianbang","Yang, Jingkun","Chen, Zhong",A semantic segmentation method for Satellite Image Change Detection,MIPPR 2019: AUTOMATIC TARGET RECOGNITION AND NAVIGATION,2020,0,"We apply the semantic segmentation method in deep network to high precision satellite image change detection, and propose a network framework to improve the detection performance.We directly processed the image after registration, without the steps of radiometric correction, and avoided the tedious steps of manual feature design by traditional methods.We tried to use Unet and Deeplab v3 model to divide the change area, and added the structure of jumping connection on the basis of Deeplab network, which made the edge of the detection graph more accurate and improved the performance of the network.The test results show that this method is effective for detecting the change of high-precision remote sensing images.",change detection,semantic segmentation,convolutional neural network,Deep learning,,,,,"Yang, Jian",,,,,"Zhang, Yanna",,,,,,
Row_1461,"Nethala, Prasad","Um, Dugan","Vemula, Neha","Montero, Oscar Fernandez","Lee, Kiju",Techniques for Canopy to Organ Level Plant Feature Extraction via Remote and Proximal Sensing: A Survey and Experiments,,DEC 2024,0,"This paper presents an extensive review of techniques for plant feature extraction and segmentation, addressing the growing need for efficient plant phenotyping, which is increasingly recognized as a critical application for remote sensing in agriculture. As understanding and quantifying plant structures become essential for advancing precision agriculture and crop management, this survey explores a range of methodologies, both traditional and cutting-edge, for extracting features from plant images and point cloud data, as well as segmenting plant organs. The importance of accurate plant phenotyping in remote sensing is underscored, given its role in improving crop monitoring, yield prediction, and stress detection. The review highlights the challenges posed by complex plant morphologies and data noise, evaluating the performance of various techniques and emphasizing their strengths and limitations. The insights from this survey offer valuable guidance for researchers and practitioners in plant phenotyping, advancing the fields of plant science and agriculture. The experimental section focuses on three key tasks: 3D point cloud generation, 2D image-based feature extraction, and 3D shape classification, feature extraction, and segmentation. Comparative results are presented using collected plant data and several publicly available datasets, along with insightful observations and inspiring directions for future research.",remote sensing,plant phenotyping,feature extraction,segmentation,point cloud,deep learning,,REMOTE SENSING,"Bhandari, Mahendra",,,,,,,,,,,
Row_1462,"Qu, Shenming","Lu, Yongyong","Cui, Can","Duan, Jiale","Xie, Yuan",MDSC-Net: multi-directional spatial connectivity for road extraction in remote sensing images,,APR 1 2024,0,"Extracting roads from complex remote sensing images is a crucial task for applications, such as autonomous driving, path planning, and road navigation. However, conventional convolutional neural network-based road extraction methods mostly rely on square convolutions or dilated convolutions in the local spatial domain. In multi-directional continuous road segmentation, these approaches can lead to poor road connectivity and non-smooth boundaries. Additionally, road areas occluded by shadows, buildings, and vegetation cannot be accurately predicted, which can also affect the connectivity of road segmentation and the smoothness of boundaries. To address these issues, this work proposes a multi-directional spatial connectivity network (MDSC-Net) based on multi-directional strip convolutions. Specifically, we first design a multi-directional spatial pyramid module that utilizes a multi-scale and multi-directional feature fusion to capture the connectivity relationships between neighborhood pixels, effectively distinguishing narrow and scale different roads, and improving the topological connectivity of the roads. Second, we construct an edge residual connection module to continuously learn and integrate the road boundaries and detailed information of shallow feature maps into deep feature maps, which is crucial for the smoothness of road boundaries. Additionally, we devise a high-low threshold connectivity algorithm to extract road pixels obscured by shadows, buildings, and vegetation, further refining textures and road details. Extensive experiments on two distinct public benchmarks, DeepGlobe and Ottawa datasets, demonstrate that MDSC-Net outperforms state-of-the-art methods in extracting road connectivity and boundary smoothness. The source code will be made publicly available at https://github/LYY199873/MDSC-Net.",road extraction,semantic segmentation,remote sensing images,encoder-decoder,multi-direction,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,
Row_1463,"Xia, Min","Cui, Yichen","Zhang, Yonghong","Xu, Yiming","Liu, Jia",DAU-Net: a novel water areas segmentation structure for remote sensing image,,APR 3 2021,55,"Water information extraction is always an important aspect of remote sensing image analysis. However, in the actual water images of remote sensing, the backgrounds of water areas are mostly complex buildings and vegetation, which interferes with water detection. In addition, traditional water detection methods were not able to accurately identify small tributaries and edge information. In order to improve to the accuracy of water segmentation, a dense skip connections network with multi-scale features fusion and attention mechanism is proposed for water areas segmentation. The proposed method obtains water feature information by using U-shaped Network (U-Net) as the backbone network and performs dense skip connections between nested convolution modules to reduce the semantic gap between the feature maps of the codec sub-networks and reduce the gradient vanishing problem of network training. In the proposed model, deep feature information is fused with shallow feature information through multi-scale attention modules. The shallow features and large-scale attention modules are used to locate the main body of the water area, while the deep features and small-scale attention modules are used to fine segment the edge of the water area. Combined with the above features and attention modules, waters can be extracted from the backgrounds. Finally, the segmentation accuracy of edge regions is further improved by Conditional Random Field (CRF). The experimental results from China HJ-1A (HJ-1B) satellite imageries and National Aeronautics and Space Administration (NASA) Land Remote-Sensing Satellite (System, Landsat-8) imageries show that the proposed method outperforms the existing methods.",,,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,"Xu, Yiqing",,,,,,,,,,,
Row_1464,"Tong, Zhonggui","Li, Yuxia","Zhang, Jinglin","Gong, Yushu","He, Lei",MULTI-SCALE FUSION ATTENTION NETWORK FOR MULTISPECTRAL WORLDVIEW3 DATA ROAD SEGMENTATION,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"In recent years, many semantic segmentation methods based on convolutional neural networks (CNN) have been applied to road extraction, but objects with similar spectral characteristics to roads in RGB images and road occlusions cause the discontinuous output of road extraction. To ensure extraction performance, hyperspectral data is used as a supplement to RGB data to improve the ability of remote sensing image road extraction in this paper. This paper uses a multi-scale fusion attention network to combine RGB and multispectral imagery. First, band selection is used to select multispectral bands with high inter-class separability, and the Cross-Source Feature Recalibration Module (CSFR) is used to calibrate and fuse spectral features at different scales to achieve fusion of multi-source features at different scales. In addition, a multi-scale attention decoder is proposed to fuse multi-level road features and global context information. The proposed method was applied to the SpaceNet dataset and self-annotated images from Chongzhou, a representative city in China. Our method performs better over the baseline HRNet by a large margin of +6.38 IoU and +5.11 F1-score on the SpaceNet dataset, +3.61 IoU and +2.32 F1-score on the self-annotated dataset (ChongZhou dataset).",deep learning,semantic segmentation,attention mechanism,multispectral remote sensing data,,,,,,,,,,,,,,,,
Row_1465,"Song, Ahram",,,,,Deep Learning-Based Semantic Segmentation of Urban Areas Using Heterogeneous Unmanned Aerial Vehicle Datasets,,OCT 2023,2,"Deep learning techniques have recently shown remarkable efficacy in the semantic segmentation of natural and remote sensing (RS) images. However, these techniques heavily rely on the size of the training data, and obtaining large RS imagery datasets is difficult (compared to RGB images), primarily due to environmental factors such as atmospheric conditions and relief displacement. Unmanned aerial vehicle (UAV) imagery presents unique challenges, such as variations in object appearance due to UAV flight altitude and shadows in urban areas. This study analyzed the combined segmentation network (CSN) designed to train heterogeneous UAV datasets effectively for their segmentation performance across different data types. Results confirmed that CSN yielded high segmentation accuracy on specific classes and can be used on diverse data sources for UAV image segmentation. The main contributions of this study include analyzing the impact of CSN on segmentation accuracy, experimenting with structures with shared encoding layers to enhance segmentation accuracy, and investigating the influence of data types on segmentation accuracy.",unmanned aerial vehicle (UAV),dataset,semantic segmentation,combined segmentation network,semantic drone dataset (SDD),UAVid,,AEROSPACE,,,,,,,,,,,,
Row_1466,"Jiang, Shenwang","Li, Jianan","Wang, Ying","Wu, Wenxuan","Zhang, Jizhou",MetaSeg: Content-Aware Meta-Net for Omni-Supervised Semantic Segmentation,,SEP 2024,1,"Noisy labels, inevitably existing in pseudo-segmentation labels generated from weak object-level annotations, severely hamper model optimization for semantic segmentation. Previous works often rely on massive handcrafted losses and carefully tuned hyperparameters to resist noise, suffering poor generalization capability and high model complexity. Inspired by recent advances in meta-learning, we argue that rather than struggling to tolerate noise hidden behind clean labels passively, a more feasible solution would be to find out the noisy regions actively, so as to simply ignore them during model optimization. With this in mind, this work presents a novel meta-learning-based semantic segmentation method, MetaSeg, that comprises a primary content-aware meta-net (CAM-Net) to serve as a noise indicator for an arbitrary segmentation model counterpart. Specifically, CAM-Net learns to generate pixel-wise weights to suppress noisy regions with incorrect pseudo-labels while highlighting clean ones by exploiting hybrid strengthened features from image content, providing straightforward and reliable guidance for optimizing the segmentation model. Moreover, to break the barrier of time-consuming training when applying meta-learning to common large segmentation models, we further present a new decoupled training strategy that optimizes different model layers in a divide-and-conquer manner. Extensive experiments on object, medical, remote sensing, and human segmentation show that our method achieves superior performance, approaching that of fully supervised settings, which paves a new promising way for omni-supervised semantic segmentation.",Noise measurement,Semantic segmentation,Training,Annotations,Semantics,Mathematical models,Computational modeling,IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS,"Huang, Bo",Label noise,meta-learning,omni-supervised segmentation,,"Xu, Tingfa",,,,,,
Row_1467,"Yang, Haiping","Yu, Bo","Luo, Jiancheng","Chen, Fang",,Semantic segmentation of high spatial resolution images with deep neural networks,,JUL 4 2019,31,"Availability of reliable delineation of urban lands is fundamental to applications such as infrastructure management and urban planning. An accurate semantic segmentation approach can assign each pixel of remotely sensed imagery a reliable ground object class. In this paper, we propose an end-to-end deep learning architecture to perform the pixel-level understanding of high spatial resolution remote sensing images. Both local and global contextual information are considered. The local contexts are learned by the deep residual net, and the multi-scale global contexts are extracted by a pyramid pooling module. These contextual features are concatenated to predict labels for each pixel. In addition, multiple additional losses are proposed to enhance our deep learning network to optimize multi-level features from different resolution images simultaneously. Two public datasets, including Vaihingen and Potsdam datasets, are used to assess the performance of the proposed deep neural network. Comparison with the results from the published state-of-the-art algorithms demonstrates the effectiveness of our approach.",global context information,high-resolution image segmentation,deep learning,residual network,pyramid pooling,,,GISCIENCE & REMOTE SENSING,,,,,,,,,,,,
Row_1468,"Talha, Muhammad","Bhatti, Farrukh A.","Ghuffar, Sajid","Zafar, Hamza",,ADU-Net: Semantic segmentation of satellite imagery for land cover classification,,SEP 1 2023,3,"Semantic Segmentation is an important problem in many vision related tasks. Land use and land cover classification involves semantic segmentation of satellite imagery and plays a vital role in many applications. In this paper, we propose an extended U-Net architecture with dense decoder connections and attention mechanism for pixel wise classification of satellite imagery named Attention Dense UNet (ADU-Net). We further evaluate the effect of different upsampling strategies in the decoder part of the U-Net architecture. We evaluate our models on the Gaofen Image Dataset (GID) for landcover classification consisting of five classes: built-up, forest, farmland, meadow and water. The experiments on the GID dataset show better performance than the previous approaches. Our proposed architecture delivers more than 4% higher mIoU and F1-score than the baseline U-Net. Moreover, our proposed architecture achieves an F1score of 87.21% and mIoU of 77.66% on the GID dataset. Our evaluations shows that data-dependent upsampling layer achieves higher accuracy than the Transposed Convolution, Pixel Shuffle and Bilinear upsampling layers. & COPY; 2023 COSPAR. Published by Elsevier B.V. All rights reserved.",Remote sensing,GID dataset,Land cover,Classification,Semantic segmentation,Attention mechanism,,ADVANCES IN SPACE RESEARCH,,,,,,,,,,,,
Row_1469,"Luotamo, Markku","Metsamaki, Sari","Klami, Arto",,,Multiscale Cloud Detection in Remote Sensing Images Using a Dual Convolutional Neural Network,,JUN 2021,22,"Semantic segmentation by convolutional neural networks (CNN) has advanced the state of the art in pixel-level classification of remote sensing images. However, processing large images typically requires analyzing the image in small patches, and hence, features that have a large spatial extent still cause challenges in tasks, such as cloud masking. To support a wider scale of spatial features while simultaneously reducing computational requirements for large satellite images, we propose an architecture of two cascaded CNN model components successively processing undersampled and full-resolution images. The first component distinguishes between patches in the inner cloud area from patches at the clouds boundary region. For the cloud-ambiguous edge patches requiring further segmentation, the framework then delegates computation to a fine-grained model component. We apply the architecture to a cloud detection data set of complete Sentinel-2 multispectral images, approximately annotated for minimal false negatives in a land-use application. On this specific task and data, we achieve a 16% relative improvement in pixel accuracy over a CNN baseline based on patching.",Clouds,Image segmentation,Cloud computing,Semantics,Remote sensing,Image resolution,Annotations,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Cloud detection,machine learning (ML),multispectral,neural networks,,,remote sensing,,,,
Row_1470,Wang Wenqing,Hu Ruotong,He Hao,Yang Dongfang,Ma Xiaohua,Structural road extraction method for remote sensing image,,APR 25 2021,3,"The road extraction of the remote sensing image plays an important role in the intelligent understanding of the ground. According to the structural characteristics of the road features, a road extraction method with structure similarity loss function and structural descriptor was proposed. Firstly, the proportion of the road is usually small in the remote sensing image, a shallow encoder-decoder based segment network with high resolution was proposed. Secondly, the structural similarity (SSIM) was introduced to the loss function and as the existing methods of road extraction network arc mostly based on the comparison of the prediction and ground truth of each pixel value, the structural descriptor joined the task of road extraction as an optimization step which improves the ability or the network to make use of the structural information. Lastly, experiments on Massachusetts road dataset show that the proposed network gets the precision and F1-score up to 85.3% and 84.6%.",deep learning,remote sensing,road extraction,structural descriptor,semantic segmentation,,,CHINESE SPACE SCIENCE AND TECHNOLOGY,,,,,,,,,,,,
Row_1471,Luo Songqiang,Li Hao,Chen Renxi,,,Building Extraction of Remote Sensing Images Using ResUNet+ with Enhanced Multiscale Features,,APR 2022,3,"We propose ResUNet+, an enhanced multiscale features residual U- shape network, to address issues in the extraction of small and irregular buildings from remote sensing images using the ResUNet, such as low segmentation accuracy and rough boundaries. Based on the ResUNet architecture, the squeeze and excitation module is used in the encoder to improve the network's ability to learn effective features, and the atrous spatial pyramid pooling module is selected as the last layer of the encoding network to obtain context information of buildings at various scales. We evaluate the proposed ResUNet+ and compare it with SE-UNet, DeepLabv3+, DenseASPP, and ResUNet semantic segmentation networks on two commoly used public datasets: the WHU Aerial Imagery Dataset and INRIA Buildings Dataset. The results of the experiments show that ResUNet+ outperforms other networks in terms of pecision, recall, and F-1-score. The segmentation results also show that RseUNet+ excels at extracting buildings of various sizes and irregular shapes.",remote sensing,building extraction,residual network,atrous convolution,multiscale features enhancement,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,
Row_1472,"Yuan, Yirong","Cui, Jianyong","Liu, Yawen","Wu, Boyang",,A Multi-Step Fusion Network for Semantic Segmentation of High-Resolution Aerial Images,,JUN 3 2023,1,"The demand for semantic segmentation of ultra-high-resolution remote sensing images is becoming increasingly stronger in various fields, posing a great challenge with concern to the accuracy requirement. Most of the existing methods process ultra-high-resolution images using downsampling or cropping, but using this approach could result in a decline in the accuracy of segmenting data, as it may cause the omission of local details or global contextual information. Some scholars have proposed the two-branch structure, but the noise introduced by the global image will interfere with the result of semantic segmentation and reduce the segmentation accuracy. Therefore, we propose a model that can achieve ultra-high-precision semantic segmentation. The model consists of a local branch, a surrounding branch, and a global branch. To achieve high precision, the model is designed with a two-level fusion mechanism. The high-resolution fine structures are captured through the local and surrounding branches in the low-level fusion process, and the global contextual information is captured from downsampled inputs in the high-level fusion process. We conducted extensive experiments and analyses using the Potsdam and Vaihingen datasets of the ISPRS. The results show that our model has extremely high precision.",semantic segmentation,attention mechanism,multi-branch network,,,,,SENSORS,,,,,,,,,,,,
Row_1473,"Sun, Shihao","Lu, Zexin","Liu, Wenjie","Hu, Wei","Li, Ruirui",SHIPNET FOR SEMANTIC SEGMENTATION ON VHR MARITIME IMAGERY,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2018,6,"For VHR maritime images, sematic segmentation is a new research hotspot and plays an important role in coastline navigation, resource management and territory protection. Without enough labeled training data, it is a challenge to separate small objects on a large scale while segment the big area clearly. To deal with it, we propose a novel ShipNet and design a weighted loss function for simultaneous sea-land segmentation and ship detection. To prove the proposed method, we also built and opened a new dataset to the community which contains VHR multiscale maritime images. Compared with the FCN and ResNet, the proposed method got much better F1 scores 85.90% for ship class and 97.54% overall accuracy. Compared with multiscale FCN, the ShipNet could obtain details results like sharp edges. Even for images with bad quality, the ShipNet could also keep robust and get good results.",Sea-land segmentation,ship detection,CNN,remote sensing image,,,,,,,,,,,,,,,,
Row_1474,"Xu, Yonghao","Ghamisi, Pedram",,,,Consistency-Regularized Region-Growing Network for Semantic Segmentation of Urban Scenes With Point-Level Annotations,,2022,27,"Deep learning algorithms have obtained great success in semantic segmentation of very high-resolution (VHR) remote sensing images. Nevertheless, training these models generally requires a large amount of accurate pixel-wise annotations, which is very laborious and time-consuming to collect. To reduce the annotation burden, this paper proposes a consistency-regularized region-growing network (CRGNet) to achieve semantic segmentation of VHR remote sensing images with point-level annotations. The key idea of CRGNet is to iteratively select unlabeled pixels with high confidence to expand the annotated area from the original sparse points. However, since there may exist some errors and noises in the expanded annotations, directly learning from them may mislead the training of the network. To this end, we further propose the consistency regularization strategy, where a base classifier and an expanded classifier are employed. Specifically, the base classifier is supervised by the original sparse annotations, while the expanded classifier aims to learn from the expanded annotations generated by the base classifier with the region-growing mechanism. The consistency regularization is thereby achieved by minimizing the discrepancy between the predictions from both the base and the expanded classifiers. We find such a simple regularization strategy is yet very useful to control the quality of the region-growing mechanism. Extensive experiments on two benchmark datasets demonstrate that the proposed CRGNet significantly outperforms the existing state-of-the-art methods. Codes and pre-trained models are available online (https://github.com/YonghaoXu/CRGNet).",Annotations,Image segmentation,Semantics,Training,Remote sensing,Knowledge transfer,Predictive models,IEEE TRANSACTIONS ON IMAGE PROCESSING,,Semantic segmentation,very high-resolution (VHR) images,weakly supervised learning,sparse annotation,,,convolutional neural network (CNN),remote sensing,,,
Row_1475,"Hu, Leiyi","Yu, Hongfeng","Lu, Wanxuan","Yin, Dongshuo","Sun, Xian",AiRs: Adapter in Remote Sensing for Parameter-Efficient Transfer Learning,,2024,6,"Remote sensing is stepping into the era of the foundation model, where the fine-tuning paradigm is widely adopted to transfer the profound knowledge of pretrained foundation models to downstream tasks. However, the full fine-tuning method would become inefficient in terms of training and storage, as the foundation models are getting larger and larger. Recently, a lot of deep learning research has proposed various parameter-efficient fine-tuning (PEFT) methods that perform well with a few trainable parameters. However, most of them focus on fine-tuning general foundation models without considering the special properties of remote sensing. In this article, we propose an adapter in remote sensing (AiRs) to fine-tune large foundation models for remote sensing downstream tasks by introducing the adapter-tuning framework. Specifically, we construct AiRs from two aspects: more expressive adaptation modules and a more efficient integration strategy. Specialized adaptation modules are applied to different functional layers in AiRs, which encode the inductive bias of remote sensing images and enhance the semantic concepts of geography. Moreover, AiRs establishes pathways between trainable modules with residual connections, which reduces training difficulty and improves performance. We conduct extensive experiments on object detection, semantic segmentation, and scene classification tasks. By training only 4.4% parameters of the pretrained backbone, AiRs surpasses the previous state-of-the-art (SOTA) PEFT competitors on all experimental datasets and outperforms the full fine-tuning on six out of ten datasets.",Task analysis,Atmospheric modeling,Computational modeling,Adaptation models,Remote sensing,Sensors,Training,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Fu, Kun",Foundation model,parameter-efficient transfer learning,remote sensing,,,,,,,,
Row_1476,"Gao, Junbo","Zhou, Chunyi","Xu, Guansheng","Sun, Wei",,Multiscale Sea-Land Segmentation Networks for Weak Boundaries,,2024,0,"Sea-land segmentation based on remote sensing images is one of the most important means to realize dynamic monitoring of coastal zones. However, the traditional models adopt the same design and simple stacking, resulting in equal attention to each region in scenarios mixing strong and weak boundaries, and even less attention to weak boundaries. In addition, due to the complexity of the ecological environment, there is a large intraclass gap and a small interclass gap in the weak boundary coastal zone. Based on the above problems, this article proposes a new semantic segmentation network based on remote sensing images: A2RDNet. By replacing standard convolution with cam-conv, this model integrates position information into channel attention, enhancing focus on weak boundary features. Furthermore, the resUNet-6 is proposed to recover substantial location information lost in the initial stages and provide deeper semantic information. Utilizing the DAC module inside the decoder to realize the pyramid structure of multiscale dense fusion enhances the semantic information while fusing the image detail features. A2RDNet was evaluated using a set of Landsat-8 remote sensing images, containing weak boundaries of different types and regions. The experimental results show that the MIoU, mPA, Accuracy, and F1_Score of the proposed method have reached 98.92, 99.46, 99.46, and 99.46, respectively, verifying the validity and feasibility of the proposed method. The A2RDNet model has higher segmentation accuracy compared to other methods.",Multiscale,remote sensing imagery,sea-land segmentation,weakly bounded coastal zones,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1477,"Han, Lintao","Zhao, Yuchen","Lv, Hengyi","Zhang, Yisa","Liu, Hailong",Remote Sensing Image Denoising Based on Deep and Shallow Feature Fusion and Attention Mechanism,,MAR 2022,28,"Optical remote sensing images are widely used in the fields of feature recognition, scene semantic segmentation, and others. However, the quality of remote sensing images is degraded due to the influence of various noises, which seriously affects the practical use of remote sensing images. As remote sensing images have more complex texture features than ordinary images, this will lead to the previous denoising algorithm failing to achieve the desired result. Therefore, we propose a novel remote sensing image denoising network (RSIDNet) based on a deep learning approach, which mainly consists of a multi-scale feature extraction module (MFE), multiple local skip-connected enhanced attention blocks (ECA), a global feature fusion block (GFF), and a noisy image reconstruction block (NR). The combination of these modules greatly improves the model's use of the extracted features and increases the model's denoising capability. Extensive experiments on synthetic Gaussian noise datasets and real noise datasets have shown that RSIDNet achieves satisfactory results. RSIDNet can improve the loss of detail information in denoised images in traditional denoising methods, retaining more of the higher-frequency components, which can have performance improvements for subsequent image processing.",image denoising,neural network,feature fusion,attention mechanism,remote sensing,,,REMOTE SENSING,"Bi, Guoling",,,,,,,,,,,
Row_1478,"Chen, Jun","Xu, Weifeng","Yu, Yang","Peng, Chengli","Gong, Wenping",Reliable Label-Supervised Pixel Attention Mechanism for Weakly Supervised Building Segmentation in UAV Imagery,,JUL 2022,3,"Building segmentation for Unmanned Aerial Vehicle (UAV) imagery usually requires pixel-level labels, which are time-consuming and expensive to collect. Weakly supervised semantic segmentation methods for image-level labeling have recently achieved promising performance in natural scenes, but there have been few studies on UAV remote sensing imagery. In this paper, we propose a reliable label-supervised pixel attention mechanism for building segmentation in UAV imagery. Our method is based on the class activation map. However, classification networks tend to capture discriminative parts of the object and are insensitive to over-activation; therefore, class activation maps cannot directly guide segmentation network training. To overcome these challenges, we first design a Pixel Attention Module that captures rich contextual relationships, which can further mine more discriminative regions, in order to obtain a modified class activation map. Then, we use the initial seeds generated by the classification network to synthesize reliable labels. Finally, we design a reliable label loss, which is defined as the sum of the pixel-level differences between the reliable labels and the modified class activation map. Notably, the reliable label loss can handle over-activation. The preceding steps can significantly improve the quality of the pseudo-labels. Experiments on our home-made UAV data set indicate that our method can achieve 88.8% mIoU on the test set, outperforming previous state-of-the-art weakly supervised methods.",weakly supervised segmentation,building segmentation,UAV image,remote sensing,deep learning,,,REMOTE SENSING,,,,,,,,,,,,
Row_1479,"He, Lixia","She, Jiangfeng","Zhao, Qiang","Wen, Xiang","Guan, Yuzheng",Boundary-Inner Disentanglement Enhanced Learning for Point Cloud Semantic Segmentation,,MAR 2023,2,"In a point cloud semantic segmentation task, misclassification usually appears on the semantic boundary. A few studies have taken the boundary into consideration, but they relied on complex modules for explicit boundary prediction, which greatly increased model complexity. It is challenging to improve the segmentation accuracy of points on the boundary without dependence on additional modules. For every boundary point, this paper divides its neighboring points into different collections, and then measures its entanglement with each collection. A comparison of the measurement results before and after utilizing boundary information in the semantic segmentation network showed that the boundary could enhance the disentanglement between the boundary point and its neighboring points in inner areas, thereby greatly improving the overall accuracy. Therefore, to improve the semantic segmentation accuracy of boundary points, a Boundary-Inner Disentanglement Enhanced Learning (BIDEL) framework with no need for additional modules and learning parameters is proposed, which can maximize feature distinction between the boundary point and its neighboring points in inner areas through a newly defined boundary loss function. Experiments with two classic baselines across three challenging datasets demonstrate the benefits of BIDEL for the semantic boundary. As a general framework, BIDEL can be easily adopted in many existing semantic segmentation networks.",point cloud,semantic segmentation,semantic boundary,boundary-inner disentanglement,local aggregation operation,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,
Row_1480,"Fu, Yuxiang","Fang, Wei","Sheng, Victor S.",,,Burned Area Segmentation in Optical Remote Sensing Images Driven by U-Shaped Multistage Masked Autoencoder,,2024,0,"Computer vision (CV) for natural disaster monitoring from optical remote sensing images (ORSIs) has been an emerging topic in analyzing ORSIs. Recently masked autoencoder (MAE) has achieved great success in CV and shown promising potential for many downstream vision tasks. However, due to the inherent limitation of vision transformer (ViT) in MAE which has a fixed feature scale and performs poorly in modeling local spatial correlation, directly applying MAE to burned area segmentation (BAS) in ORSIs fails to achieve satisfactory results. To address this problem, we propose a novel dual-branch complement network (DCNet) driven by U-shaped multistage masked autoencoder (UMMAE) for BAS in ORSIs, which is also the first application of MAE in BAS. UMMAE has four stages and introduces skip connection between the encoder and decoder at the same stage, which improves the feature diversity and further enhances the model performance. DCNet has three major components: the ViT encoder (global branch), the convolution encoder (local branch), and the decoder. The global branch inherits visual representation learning ability from the pretrained UMMAE and captures global contextual information from the input image, while the local branch extracts local spatial information at different scales. Features from two different branches are fused in the decoder for feature complementation, which improves feature discriminability and segmentation accuracy. Besides, we build a new BAS dataset containing ORSIs of burned area in California, USA, from 2017 to 2022. Extensive experiments on two BAS datasets demonstrate that our DCNet outperforms the state-of-the-art methods.",Burned area segmentation (BAS),deep learning,forest fire monitoring,semantic segmentation,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,
Row_1481,"Zhang, Xueliang","Su, Qi","Xiao, Pengfeng","Wang, Wenye","Li, Zhenshi",FlipCAM: A Feature-Level Flipping Augmentation Method for Weakly Supervised Building Extraction From High-Resolution Remote Sensing Imagery,,2024,1,"It is time-consuming to collect a huge number of pixel-level annotations for accurately extracting buildings by deep neural networks. Supported by class activation map (CAM), weakly supervised semantic segmentation (WSSS) methods with image-level annotations serve as an efficient solution for building extraction. However, it is a great challenge to generate high-quality CAM heatmaps for buildings from high-resolution remote sensing images. On one hand, image-level labels lack spatial information, resulting in partial integrity and hollow phenomenon for building extraction. On the other hand, complex backgrounds in remote sensing images can lead to inaccurate extraction of building boundaries. In this study, we propose a novel weakly supervised building extraction method called FlipCAM to deal with these challenges. The Flip module based on feature-level flipping augmentation is designed to improve the integrity of CAM heatmaps by fusing the original and flipped feature maps. In addition, by combining the Flip module with the slice and merge (SAM) module based on consistency architecture, FlipCAM is able to generate high-quality CAM heatmaps with both boundary fineness and internal integrity in an end-to-end manner, which also alleviates special difficulties for building extraction, including adhesions in dense buildings and confusions with background and shadows, providing reliable pixel-level pseudo masks for training segmentation network to extract buildings. Extensive experiments on three high-resolution datasets show that FlipCAM achieves excellent performance and outperforms other weakly supervised methods in terms of effectiveness and robustness capabilities. Our code is public at https://github.com/NJU-LHRS/FlipCAM-master.",Building extraction,class activation map (CAM),feature-level flipping augmentation,high-resolution remote sensing imagery,weakly supervised deep learning,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"He, Guangjun",,,,,,,,,,,
Row_1482,"Li, Liangzhi","Han, Ling","Miao, Qing","Zhang, Yang","Jing, Ying",Superpixel-Based Long-Range Dependent Network for High-Resolution Remote-Sensing Image Classification,,NOV 2022,1,"Data-driven deep neural networks have demonstrated their superiority in high-resolution remote-sensing image (HRSI) classification based on superpixel-based objects. Currently, most HRSI classification methods that combine deep learning and superpixel object segmentation use multiple scales of stacking to satisfy the contextual semantic-information extraction of one analyzed object. However, this approach does not consider the long-distance dependencies between objects, which not only weakens the representation of feature information but also increases computational redundancy. To solve this problem, a superpixel-based long-range dependent network is proposed for HRSI classification. First, a superpixel segmentation algorithm is used to segment HRSI into homogeneous analysis objects as input. Secondly, a multi-channel deep convolutional neural network is proposed for the feature mapping of the analysis objects. Finally, we design a long-range dependent framework based on a long short-term memory (LSTM) network for obtaining contextual relationships and outputting classes of analysis objects. Additionally, we define the semantic range and investigate how it affects classification accuracy. A test is conducted by using two HRSI with overall accuracy (0.79, 0.76) and kappa coefficients (kappa) (0.92, 0.89). Both qualitative and quantitative comparisons are adopted to test the proposed method's efficacy. Findings concluded that the proposed method is competitive and consistently superior to the benchmark comparison method.",remote-sensing image,deep learning,image classification,long-range dependence,semantic scope,,,LAND,,,,,,,,,,,,
Row_1483,"Xia, Min","Wang, Tao","Zhang, Yonghong","Liu, Jia","Xu, Yiqing",Cloud/shadow segmentation based on global attention feature fusion residual network for remote sensing imagery,,MAR 19 2021,54,"Cloud and cloud shadow segmentation of satellite imageries is a prerequisite for many remote sensing applications. Due to the limited number of available spectral bands and the complexity of background information, the traditional detection methods have some problems such as false detection, missing detection and inaccurate boundary information in segmentation. To solve these problems, a global attention fusion residual network method is proposed to segment cloud and cloud shadow of satellite imageries. The proposed model adopts Residual Network (ResNet) as backbone to extract semantic information at different feature levels. In order to improve the ability of the network to deal with the boundary information, an improved atrous spatial pyramid pooling method is introduced to extract the multi-scale deep semantic information. Then, the deep semantic information is fused with the shallow spatial information through the Global Attention up-sample mechanism in different scales, which improves the network's ability to utilize the global and local features. Finally, a boundary refinement module is utilized to predict the boundary of cloud and shadow, consequently the boundary information is refined. The experimental results on Sentinel-2 satellite and Land Remote-Sensing Satellite (Landsat) imageries show that the segmentation accuracy and speed of proposed method are superior to the existing methods, it is of great significance for realizing practical cloud and shadow segmentation.",,,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1484,"Long, Xianxuan","Zhuang, Wei","Xia, Min","Hu, Kai","Lin, Haifeng",SASiamNet: Self-Adaptive Siamese Network for Change Detection of Remote Sensing Image,,2024,5,"With increasingly rapid development of convolutional neural networks, the field of remote sensing has experienced a significant revitalization. However, understanding and detecting surface changes, which necessitate the identification of high-resolution remote sensing images, remain substantial challenges in achieving precise change detection. Excited deep learning-based change detection techniques often exhibit limitations and lack the necessary precision to detect edge details or other nuanced information in remote sensing images. To address these limitations, we propose a unique semantic segmentation deep learning network, the self-adaptive Siamese network (SASiamNet), specifically devised for enhancing change detection in remote sensing images. The SASiamNet excels in real-time land cover segmentation, adeptly extracting local and global information from images via the backbone residual network. Furthermore, it incorporates a primary feature fusion module to extract and fuse the primary stage feature map, and a high-level information refinement module to refine the resultant feature map. This methodology effectively transmutes low-level semantic information into high-level semantic information, thereby improving the overall detection process. Aimed at empirically testing the effectiveness of the SASiamNet, we utilize two distinct datasets: the public dataset, LEVIR-CD, and a challenging dataset, CDD. The latter is composed of bitemporal images sourced from Google Earth, spanning various regions across China. The experiment results unequivocally demonstrate that our approach outperforms traditional methodologies as well as contemporary state-of-the-art change detection techniques, hence underscoring the efficacy of the SASiamNet in the context of remote sensing image change detection.",Feature extraction,Remote sensing,Task analysis,Deep learning,Training,Semantics,Image edge detection,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Change detection,deep learning,remote sensing,Siamese network,,,,,,,
Row_1485,"Yu, Ge","Zhang, Xi",,,,Land Cover Classification Based on PSPNet Using Remote Sensing Image,2021 PROCEEDINGS OF THE 40TH CHINESE CONTROL CONFERENCE (CCC),2021,3,"In recent years, land cover classification has been modeled as semantic segmentation of remote sensing images, and significant advances have been achieved. Labeled examples for some categories are difficult to obtain manually by photo interpretation or ground survey, thereby causing category imbalance problems. In some categories, intraclass variability is large, but interclass difference is small, causing hard discrimination. To segment pixels accurately, we proposed an improved land cover classification network based on Pyramid Scene Parsing Network. In our network, an adaptation loss based on focal loss is proposed to increase the focus on indistinguishable pixels for category imbalance. Moreover, the network aggregates multiscale features to obtain fused local and global context information using multiple dilated convolutions with various dilation factors, avoiding information loss causing by large intraclass variability and small interclass difference. Experiments were conducted on real land cover datasets. These experiments confirmed the superior performance of the proposed network compared with the state-of-the-art land cover classification models.",Land cover classification,Segmentation,Remote sensing image,Imbalance data,,,,,,,,,,,,,,,,
Row_1486,"Li, Junxi","Sun, Xian","Diao, Wenhui","Wang, Peijin","Feng, Yingchao",Class-Incremental Learning Network for Small Objects Enhancing of Semantic Segmentation in Aerial Imagery,,2022,18,"Due to the differences in the feature distribution between classes, when the model learns in a continuous data stream, it will encounter catastrophic forgetting. The incremental learning methods have shown great potential to solve this problem. However, most existing methods based on task-incremental learning are difficult to adapt to characteristics of remote sensing scenes with few differences in appearance but large differences in features, which is not conducive to artificially distinguish task-identity document (ID). Thus, we propose a class-incremental learning (CIL) network for small objects enhancing semantic segmentation in aerial imagery. Specifically, considering the superior accuracy of the binary classifier, we propose a twin-auxiliary (TA) model that adds an auxiliary binary classification task. Then, for expansion and contraction at the edge and small object confusion problems, we introduce a diversity distillation loss, using the results of binary-classifier to constrain the multiclass segmentation results and strengthen the attention to the locations of the segmentation results that have changed. Finally, we design a conflict reduction mechanism for multihead classifier to achieve single-head prediction for CIL. Experiments demonstrate that our method has good performance on the Vaihingen and Potsdam datasets by the International Society for Photogrammetry and Remote Sensing (ISPRS), outperforming state-of-the-art (SOTA) incremental learning methods. The code will be available soon.",Task analysis,Data models,Remote sensing,Semantics,Learning systems,Image segmentation,Pipelines,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Lu, Xiaonan",Catastrophic forgetting,edge contraction,incremental learning,knowledge distillation,"Xu, Guangluan",,small-target confusion,,,,
Row_1487,"Huan, Linxi","Zheng, Xianwei","Tang, Shengjun","Gong, Jianya",,Learning deep cross-scale feature propagation for indoor semantic segmentation,,JUN 2021,5,"Indoor semantic segmentation is a long-standing vision task that has been recently advanced by convolutional neural networks (CNNs), but this task remains challenging by high occlusion and large scale variation of indoor scenes. Existing CNN-based methods mainly focus on using auxiliary depth data to enrich features extracted from RGB images, hence, they pay less attention to exploiting multi-scale information in exracted features, which is essential for distinguishing objects in highly cluttered indoor scenes. This paper proposes a deep cross-scale feature propagation network (CSNet), to effectively learn and fuse multi-scale features for robust semantic segmentation of indoor scene images. The proposed CSNet is deployed as an encoder-decoder engine. During encoding, the CSNet propagates contextual information across scales and learn discriminative multi-scale features, which are robust to large object scale variation and indoor occlusion. The decoder of CSNet then adaptively integrates the multi-scale encoded features with fusion supervision at all scales to generate target semantic segmentation prediction. Extensive experiments conducted on two challenging benchmarks demonstrate that the CSNet can effectively learn multi-scale representations for robust indoor semantic segmentation, achieving outstanding performance with mIoU scores of 51.5 and 50.8 on NYUDv2 and SUN-RGBD datasets, respectively.",Indoor scene parsing,Semantic segmentation,Deep learning,Cross-scale feature propagation,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,
Row_1488,"Schenkel, Fabian","Middelmann, Wolfgang",,,,DOMAIN ADAPTATION FOR SEMANTIC SEGMENTATION USING CONVOLUTIONAL NEURAL NETWORKS,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),2019,4,"Semantic segmentation is an important analysis task for the investigation of aerial imagery. Recently, the arise of convolutional neural networks has increased the performance of computer vision methods considerably. But the success of deep learning applications mostly relies on the availability of sufficiently large training datasets. However, the manual annotation of images is time consuming and needs human effort. To reduce the necessary amount of training data it is possible to fine-tune a model which is pre-trained on a different larger dataset. But usually orthophotos are affected by weather and sensor dependent light conditions. Additionally, such images are composed of imbalanced classes which leads to poor pixel-wise classification results for sparsely represented labels. In this paper we propose a convolutional neural network based domain adaptation method for semantic segmentation. The encoder-decoder structure uses adaptation modules and an alternately training procedure to adapt the network to the target domain. We employ the large ISPRS Potsdam dataset as source domain to train a base model and adapt it using very few samples. We compared our method to the common fine-tuning approach and evaluated the results for a decreasing number of training samples. We observed an improvement of the average overall prediction accuracy but especially for the sparsely represented vehicle class.",Semantic Segmentation,Domain Adaptation,Convolutional Neural Networks,Imbalanced Classes,,,,,,,,,,,,,,,,
Row_1489,"Zhang, AW","Liu, LL","Zhang, XZ",,,Multi-Feature 3D Road Point Cloud Semantic Segmentation Method Based on Convolutional Neural Network,,APR 2020,10,"Aiming at the problem of low accuracy in semantic segmentation of three-dimensional laser point clouds in road scene, an end-to-end multi-feature point clouds semantic segmentation method based on convolutional neural network is proposed. Firstly, the feature images such as point cloud distance, adjacent angle and surface curvature arc calculated based on spherical projection to apply to convolutional neural network; then, a convolutional neural network is adopted to process multi-band depth images to obtain pixel-level instance segmentation results. The proposed method combines traditional point cloud features with the deep learning method to improve the result of point cloud semantic segmentation. Using KITTI point cloud data set test, simulation results show that the multi feature convolutional neural network semantic segmentation method has better performance than other semantic segmentation methods without combining with point cloud features such as SqueezeSeg V2. The precision obtained with proposed method for car, bicycle and pedestrian segmentation is 0.3, 21.4, 14.5 percentage points higher in comparison with the SqueezeSeg V2 network.",,,,,,,,CHINESE JOURNAL OF LASERS-ZHONGGUO JIGUANG,,,,,,,,,,,,
Row_1490,"Xie, Yakun","Feng, Dejun","Chen, Hongyu","Liu, Zichen","Mao, Wenfei",Damaged Building Detection From Post-Earthquake Remote Sensing Imagery Considering Heterogeneity Characteristics,,2022,21,"Damaged building detection from remote sensing imagery helps to quickly and rapidly assess losses after an earthquake. In recent years, deep learning technology has become a favorable tool for remote sensing image information detection. Based on the characteristics of damaged buildings in remote sensing images, in this article, a framework for damaged building detection that considers heterogeneity characteristics is proposed. First, a local-global context attention module is proposed to improve the feature detection ability of the network, which can extract the features of damaged buildings from different directions and effectively aggregate global and local features. In addition, the module takes the correlation between feature maps at different scales into account while extracting information. Second, a feature fusion module with self-attention is established to replace the simple connection between the encoding and decoding processes, which improves the detail feature recovery ability of the network during the upsampling process. Finally, to fully aggregate semantic and detailed features at different scales, a multibranch auxiliary classifier is established by adding two separate branches in the prediction stage. The effectiveness of the proposed approach is verified based on the data from the 2010 Haiti earthquake, and comparisons with three object-oriented methods and 16 existing excellent deep learning models are performed. The intersection over union (IOU) increase of 0.03%-7.39% is achieved using the proposed approach compared with excellent deep learning models.",Buildings,Feature extraction,Remote sensing,Earthquakes,Image segmentation,Semantics,Deep learning,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Zhu, Jun",Convolutional neural network (CNN),damaged building detection,heterogeneity characteristics,remote sensing imagery,"Hu, Ya","Baik, Sung Wook",,,,,
Row_1491,"Wan, Ling","Tian, Ye","Kang, Wenchao","Ma, Lei",,D-TNet: Category-Awareness Based Difference-Threshold Alternative Learning Network for Remote Sensing Image Change Detection,,2022,5,"Deep-learning-based change detection methods have achieved remarkable success through the feature learning capability of deep convolutions. However, the network structures of existing methods are simply modified from the semantic segmentation models, ignoring the essential characteristics of change detection, thereby limiting their applications. In this work, we propose a category-awareness-based difference-threshold alternative-learning network (D-TNet) for remote sensing image change detection. Our motivation is to characterize the different change magnitudes for different land cover changes, and represent the semantic content differences of various objects. Thus, our D-TNet consists of a difference map (DM) learning path and a threshold map (TM) learning path, realizing self-adapting threshold selection by assigning each pixel a unique threshold. The two paths are alternatively optimized to make the DM more discriminative, as well as making the TM more adaptive. In addition, a category-awareness attention mechanism is introduced in D-TNet, which learns a pixel-to-category relationship to benefit in representing the heterogeneity of land covers. Finally, experimental results on three change detection datasets verify the effectiveness of our D-TNet in both visual and quantitative analyses. Code will be available at: https://www.researchgate.net/profile/Ling-Wan-4.",Feature extraction,Task analysis,Remote sensing,Semantics,Convolutional neural networks,Deep learning,Visualization,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Category-awareness,change detection,optical remote sensing image,threshold learning,,,,,,,
Row_1492,"Ji, Shunping","Wang, Dingpan","Luo, Muying",,,Generative Adversarial Network-Based Full-Space Domain Adaptation for Land Cover Classification From Multiple-Source Remote Sensing Images,,MAY 2021,77,"The accuracy of remote sensing image segmentation and classification is known to dramatically decrease when the source and target images are from different sources; while deep learning-based models have boosted performance, they are only effective when trained with a large number of labeled source images that are similar to the target images. In this article, we propose a generative adversarial network (GAN) based domain adaptation for land cover classification using new target remote sensing images that are enormously different from the labeled source images. In GANs, the source and target images are fully aligned in the image space, feature space, and output space domains in two stages via adversarial learning. The source images are translated to the style of the target images, which are then used to train a fully convolutional network (FCN) for semantic segmentation to classify the land cover types of the target images. The domain adaptation and segmentation are integrated to form an end-to-end framework. The experiments that we conducted on a multisource data set covering more than 3500 km(2) with 51 560 256 x 256 high-resolution satellite images in Wuhan city and a cross-city data set with 11 383 256 x 256 aerial images in Potsdam and Vaihingen demonstrated that our method exceeded the recent GAN-based domain adaptation methods by at least 6.1% and 4.9% in the mean intersection over union (mIoU) and overall accuracy (OA) indexes, respectively. We also proved that our GAN is a generic framework that can be implemented for other domain transfer methods to boost their performance.",Domain adaptation,generative adversarial network (GAN),image classification,remote sensing,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1493,"Volpi, Michele","Tuia, Devis",,,,Deep multi-task learning for a geographically-regularized semantic segmentation of aerial images,,OCT 2018,62,"When approaching the semantic segmentation of overhead imagery in the decimeter spatial resolution range, successful strategies usually combine powerful methods to learn the visual appearance of the semantic classes (e.g. convolutional neural networks) with strategies for spatial regularization (e.g. graphical models such as conditional random fields).In this paper, we propose a method to learn evidence in the form of semantic class likelihoods, semantic boundaries across classes and shallow-to-deep visual features, each one modeled by a multi-task convolutional neural network architecture. We combine this bottom-up information with top-down spatial regularization encoded by a conditional random field model optimizing the label space across a hierarchy of segments with constraints related to structural, spatial and data-dependent pairwise relationships between regions.Our results show that such strategy provide better regularization than a series of strong baselines reflecting state-of-the-art technologies. The proposed strategy offers a flexible and principled framework to include several sources of visual and structural information, while allowing for different degrees of spatial regularization accounting for priors about the expected output structures.",Semantic segmentation,Semantic boundary detection,Convolutional neural networks,Conditional random fields,Multi-task learning,Decimeter resolution,Aerial imagery,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,
Row_1494,"Zheng, Daoyuan","Li, Shengwen","Fang, Fang","Zhang, Jiahui","Feng, Yuting",Utilizing Bounding Box Annotations for Weakly Supervised Building Extraction From Remote-Sensing Images,,2023,7,"Image-level weakly supervised semantic segmentation (WSSS) methods have greatly facilitated the extraction of buildings from remote-sensing (RS) images. However, the lack of the locations and extent of individual buildings in image-level labels results in some limitations of the methods, especially in the cases of cluttered backgrounds and diverse building shapes and sizes. By utilizing bounding box annotations, a novel WSSS model is developed to improve building extraction from RS images in this article. Specifically, during the training phase, a multiscale feature retrieval (MFR) module is designed to learn multiscale building features and suppress the background noise inside the bounding box. In the inference phase, multiscale class activation maps (CAMs) are generated from multiscale features to achieve accurate building localization. Finally, a pseudo-mask generation and correction (PGC) module refines the CAMs to generate and correct the building pseudo-masks. Experiments are conducted to examine the proposed model in three datasets, namely the WHU aerial building dataset, the CrowdAI building dataset, and a self-annotated building dataset. Experimental results demonstrate that the proposed method outperforms baselines, achieving 76.99%, 75.51%, and 67.35% in terms of intersection over union (IoU) scores on the three challenging datasets, respectively. This article provides a methodological reference for the application of weakly supervised learning on RS images.",Buildings,Feature extraction,Annotations,Semantics,Training,Geology,Task analysis,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Wan, Bo",Bounding box annotations,building extraction,remote-sensing (RS) images,weakly supervised semantic segmentation (WSSS),"Liu, Yuanyuan",,,,,,
Row_1495,"Chakravarthy, Anirudh S.","Sinha, Soumendu","Narang, Pratik","Mandal, Murari","Chamola, Vinay",DroneSegNet: Robust Aerial Semantic Segmentation for UAV-Based IoT Applications,,APR 2022,24,"Unmanned Aerial Vehicles (UAVs) are the promising ""Flying IoT"" devices of the future, which can be equipped with various sensors and cognitive capabilities to perform numerous tasks related to remote sensing, search and rescue operations, object tracking, segmentation of roads and buildings, surveillance, etc. However, these AI-driven tasks require heavy computation and may lead to suboptimal performance with embedded processors on a power-constrained battery-operated drone. This work proposes a novel deep learning approach for performing robust semantic segmentation of aerial scenes captured by UAVs. In our setup, the power-constrained drone is used only for data collection, while the computationally intensive tasks are offloaded to a GPU cloud server. Our architecture performs robust semantic segmentation by learning the segmentation maps from jointly utilizing of aerial scenes along with the respective ""elevation maps"" in a semi-supervised approach. We propose a three-tier deep learning architecture, wherein the first module aims at preliminary feature extraction from aerial scenes using a backbone feature extractor. The second module captures the spatial dependency between the aerial scenes and their respective elevation maps to obtain better semantic information, which is achieved by a bi-directional LSTM. The third module is aimed at enhancing the performance of semantic segmentation through a semi-supervised approach with an encoder to generate segmentation maps and a decoder to reconstruct feature maps. This semi-supervised feature learning ensures robust extraction along with scalability. The proposed architecture was validated on real-world aerial datasets and achieves state-of-the-art results for aerial image segmentation.",Image segmentation,Feature extraction,Semantics,Internet of Things,Deep learning,Drones,Computer architecture,IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY,"Yu, F. Richard",Semantic segmentation,UAVs,aerial scene analysis,deep learning,,,IoT,,,,
Row_1496,"Bao, Hanqing","Ming, Dongping","Guo, Ya","Zhang, Kui","Zhou, Keqi",DFCNN-Based Semantic Recognition of Urban Functional Zones by Integrating Remote Sensing Data and POI Data,,APR 2020,65,"The urban functional zone, as a special fundamental unit of the city, helps to understand the complex interaction between human space activities and environmental changes. Based on the recognition of physical and social semantics of buildings, combining remote sensing data and social sensing data is an effective way to quickly and accurately comprehend urban functional zone patterns. From the object level, this paper proposes a novel object-wise recognition strategy based on very high spatial resolution images (VHSRI) and social sensing data. First, buildings are extracted according to the physical semantics of objects; second, remote sensing and point of interest (POI) data are combined to comprehend the spatial distribution and functional semantics in the social function context; finally, urban functional zones are recognized and determined by building with physical and social functional semantics. When it comes to building geometrical information extraction, this paper, given the importance of building boundary information, introduces the deeper edge feature map (DEFM) into the segmentation and classification, and improves the result of building boundary recognition. Given the difficulty in understanding deeper semantics and spatial information and the limitation of traditional convolutional neural network (CNN) models in feature extraction, we propose the Deeper-Feature Convolutional Neural Network (DFCNN), which is able to extract more and deeper features for building semantic recognition. Experimental results conducted on a Google Earth image of Shenzhen City show that the proposed method and model are able to effectively, quickly, and accurately recognize urban functional zones by combining building physical semantics and social functional semantics, and are able to ensure the accuracy of urban functional zone recognition.",urban functional zones,semantic recognition,stratified scale estimation,deeper-feature CNN (DFCNN),POIs,,,REMOTE SENSING,"Du, Shigao",,,,,,,,,,,
Row_1497,"Santiago, Jonathan Gonzalez","Schenkel, Fabian","Middelmann, Wolfgang",,,SELF-SUPERVISED MULTI-TASK LEARNING FOR SEMANTIC SEGMENTATION OF URBAN SCENES,IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXVII,2021,0,"The task of semantic segmentation plays a vital role in the analysis of remotely sensed imagery. Currently, this task is mainly solved using supervised pre-training, where very Deep Convolutional Neural Networks (DCNNs) are trained on large annotated datasets for mostly solving a classification problem. They are useful for many visual recognition tasks but heavily depend on the amount and quality of the annotations to learn a mapping function for predicting on new data. Motivated by the plethora of data generated everyday, researchers have come up with alternatives such as Self-Supervised Learning (SSL). These methods play a deciding role in boosting the progress of deep learning without the need of expensive labeling. They entirely explore the data, find supervision signals and solve a challenge known as Pretext Task (PTT) to learn robust representations. Thereafter, the learned features are transferred to resolve the so-called Downstream Task (DST), which can represent a group of computer vision applications such as classification or object detection. The current work explores the conception of a DCNN and training strategy to jointly predict on multiple PTTs in order to learn general visual representations that could lead more accurate semantic segmentations. The first Pretext Task is Image Colorization (IC) that identifies different objects and related parts present in a grayscale image to paint those areas with the right color. The second task is Spatial Context Prediction (SCP), which captures visual similarity across images to discover the spatial configuration of patches generated out of an image. The DCNN architecture is constructed considering each particular objective of the Pretext Tasks. It is subsequently trained and its acquired knowledge is transferred into a SSL trunk network to build a Fully Convolutional Network (FCN) on top of it. The FCN with SSL trunk learns a compound of features through fine-tuning to ultimately predict the semantic segmentation. With the aim of evaluating the quality of the learned representations, the performance of the trained model will be compared with inference results of a FCN-ResNet101 architecture pre-trained on ImageNet. This comparison employs the F1-Score as quality metric. Experiments show that the method is capable of achieving general feature representations that can definitely be employed for semantic segmentation purposes.",Self-supervision,Multi-task Feature Learning,Transfer Learning,Deep Learning,Semantic Segmentation,,,,,,,,,,,,,,,
Row_1498,"Gao, Qiong","Liu, Dandan","Zhang, Wei","Liu, Yuntong",,Deep Learning-Based Key Indicator Estimation in Rivers by Leveraging Remote Sensing Image Analysis,,2024,1,"Estimation of key indicators in rivers was usually conducted with use of monitoring data of Internet of Things. Currently, it has been a more practical demand to extract key indexes in rivers from the perspective of visual remote sensing, rather than data-driven perspective. As consequence, this study aims to explore the deep learning-based key indicators extraction method from remote sensing images of rivers. First of all, a large amount of river-related remote sensing images, including high-resolution satellite images and aerial photographs, are collected. Then, U-Net structure is utilized as the backbone network to realize semantic segmentation via multimodal feature fusion. On this basis, fine-grained vision features are extracted to estimate values of key indicators. Finally, the width and flow velocity of the river are identified and verified. Using deep convolutional neural networks and recurrent neural networks for feature extraction and modeling, the model can infer and estimate relevant information of rivers by learning key features from images. Empirically, we have also carried out some experiments on real-world remote sensing images to evaluate the proposal. The results indicate that our method performs well in extracting key indicators of rivers, and has higher accuracy compared to traditional methods. In addition, we also conduct sensitivity analysis on the model and find that it has certain stability to factors that affect river characteristics, such as geographical environment and climate conditions.",Rivers,Feature extraction,Deep learning,Remote sensing,Data mining,Semantic segmentation,Water quality,IEEE ACCESS,,Image recognition,deep learning,image recognition,key indicator extraction,,,,,,,
Row_1499,"Landgraf, Steven","Wursthorn, Kira","Hillemann, Markus","Ulrich, Markus",,DUDES: Deep Uncertainty Distillation using Ensembles for Semantic Segmentation,,APR 2024,0,"The intersection of deep learning and photogrammetry unveils a critical need for balancing the power of deep neural networks with interpretability and trustworthiness, especially for safety-critical application like autonomous driving, medical imaging, or machine vision tasks with high demands on reliability. Quantifying the predictive uncertainty is a promising endeavour to open up the use of deep neural networks for such applications. Unfortunately, most current available methods are computationally expensive. In this work, we present a novel approach for efficient and reliable uncertainty estimation for semantic segmentation, which we call Deep Uncertainty Distillation using Ensembles for Segmentation (DUDES). DUDES applies student-teacher distillation with a Deep Ensemble to accurately approximate predictive uncertainties with a single forward pass while maintaining simplicity and adaptability. Experimentally, DUDES accurately captures predictive uncertainties without sacrificing performance on the segmentation task and indicates impressive capabilities of highlighting wrongly classified pixels and out-of-domain samples through high uncertainties on the Cityscapes and Pascal VOC 2012 dataset. With DUDES, we manage to simultaneously simplify and outperform previous work on Deep-Ensemble-based Uncertainty Distillation.",Deep Learning,Semantic Segmentation,Uncertainty Quantification,Deep Ensemble,Knowledge Distillation,,,PFG-JOURNAL OF PHOTOGRAMMETRY REMOTE SENSING AND GEOINFORMATION SCIENCE,,,,,,,,,,,,
Row_1500,"Qian, Jianguo","Ci, Jinlong","Tan, Hai","Xu, Wenwen","Jiao, Yang",Cloud Detection Method Based on Improved DeeplabV3+Remote Sensing Image,,2024,5,"Cloud cover is a phenomenon that inevitably exists in remote sensing images, and ground information is lost due to the presence of clouds. To a large extent, it causes degradation of the remote sensing image quality. Therefore, the detection of clouds in remote sensing images is the foundation and key to further emphasizing and use of remote sensing image information. To tackle the question of misjudgment, omission, and long training time of current deep learning-based cloud detection methods, this article suggests an improved cloud detection method for remote sensing images with a deeplabV3+ network. The method aims to detect cloud-covered regions in remote sensing images more accurately, quickly and efficiently. The training time is reduced by improving the Xception backbone network to reduce the amount of parameters; the improved CBAM module is added after the Atrous Spatial Pyramid Pooling (ASPP) module to strengthen the sensory field to better capture the contextual information; and the GAU module is used to replace the traditional bilinear interpolation up-sampling in the decoder part, which results in a better quality of the up-sampling, more spatial sense of the fusion, and improved accuracy of the segmentation. Experiments were conducted by using datasets from the homegrown ZY-03 satellite and comparing them with traditional DeeplabV3+, MobileNet and U-Net. And the generalization ability is verified using the public Sentinal-2 dataset. Compared to legacy deeplabV3+ networks, the accuracy rate is improved by 3.91%, the improved precision by 3.12%, the improved recall by 2.78%, and the improved Mean Intersection over Union (MIoU) by 5.75%. Compared to Mobilenet, the improved accuracy by 1.79%, the improved precision by 1.47%, the improved recall by 1.42%, and the improved Mean Intersection over Union (MIoU) by 4.15%. Compared with the Unet network, the improved accuracy by 1.28%, the improved precision by 1.36%, the improved recall by 1.32%, and the improved Mean Intersection over Union (MIoU) by 3.36%. The method presented in this article demonstrates superior performance in cloud detection compared to several other networks.",Remote sensing,Clouds,Satellites,Cloud computing,Training,Thresholding (Imaging),Optical sensors,IEEE ACCESS,"Chen, Peiyou",Cloud detection,improved deeplabV3+,semantic segmentation,ZY-03 satellite image,,,,,,,
Row_1501,"Landgraf, Steven","Wursthorn, Kira","Hillemann, Markus","Ulrich, Markus",,DUDES: Deep Uncertainty Distillation using Ensembles for Semantic Segmentation,,APR 2024,0,"The intersection of deep learning and photogrammetry unveils a critical need for balancing the power of deep neural networks with interpretability and trustworthiness, especially for safety-critical application like autonomous driving, medical imaging, or machine vision tasks with high demands on reliability. Quantifying the predictive uncertainty is a promising endeavour to open up the use of deep neural networks for such applications. Unfortunately, most current available methods are computationally expensive. In this work, we present a novel approach for efficient and reliable uncertainty estimation for semantic segmentation, which we call Deep Uncertainty Distillation using Ensembles for Segmentation (DUDES). DUDES applies student-teacher distillation with a Deep Ensemble to accurately approximate predictive uncertainties with a single forward pass while maintaining simplicity and adaptability. Experimentally, DUDES accurately captures predictive uncertainties without sacrificing performance on the segmentation task and indicates impressive capabilities of highlighting wrongly classified pixels and out-of-domain samples through high uncertainties on the Cityscapes and Pascal VOC 2012 dataset. With DUDES, we manage to simultaneously simplify and outperform previous work on Deep-Ensemble-based Uncertainty Distillation.",Deep Learning,Semantic Segmentation,Uncertainty Quantification,Deep Ensemble,Knowledge Distillation,,,PFG-JOURNAL OF PHOTOGRAMMETRY REMOTE SENSING AND GEOINFORMATION SCIENCE,,,,,,,,,,,,
Row_1502,"Chen, Chen","Ma, Hongxiang","Yao, Guorun","Lv, Ning","Yang, Hua",Remote Sensing Image Augmentation Based on Text Description for Waterside Change Detection,,MAY 2021,12,"Since remote sensing images are difficult to obtain and need to go through a complicated administrative procedure for use in China, it cannot meet the requirement of huge training samples for Waterside Change Detection based on deep learning. Recently, data augmentation has become an effective method to address the issue of an absence of training samples. Therefore, an improved Generative Adversarial Network (GAN), i.e., BTD-sGAN (Text-based Deeply-supervised GAN), is proposed to generate training samples for remote sensing images of Anhui Province, China. The principal structure of our model is based on Deeply-supervised GAN(D-sGAN), and D-sGAN is improved from the point of the diversity of the generated samples. First, the network takes Perlin Noise, image segmentation graph, and encoded text vector as input, in which the size of image segmentation graph is adjusted to 128 x 128 to facilitate fusion with the text vector. Then, to improve the diversity of the generated images, the text vector is used to modify the semantic loss of the downsampled text. Finally, to balance the time and quality of image generation, only a two-layer Unet++ structure is used to generate the image. Herein, ""Inception Score"", ""Human Rank"", and ""Inference Time"" are used to evaluate the performance of BTD-sGAN, StackGAN++, and GAN-INT-CLS. At the same time, to verify the diversity of the remote sensing images generated by BTD-sGAN, this paper compares the results when the generated images are sent to the remote sensing interpretation network and when the generated images are not added; the results show that the generated image can improve the precision of soil-moving detection by 5%, which proves the effectiveness of the proposed model.",data augmentation,deeply monitoring,GAN,remote sensing image,text description,,,REMOTE SENSING,"Li, Cong",,,,,"Wan, Shaohua",,,,,,
Row_1503,"Ibrahim, Muhammad","Akhtar, Naveed","Ullah, Khalil","Mian, Ajmal",,Exploiting Structured CNNs for Semantic Segmentation of Unstructured Point Clouds from LiDAR Sensor,,SEP 2021,10,"Accurate semantic segmentation of 3D point clouds is a long-standing problem in remote sensing and computer vision. Due to the unstructured nature of point clouds, designing deep neural architectures for point cloud semantic segmentation is often not straightforward. In this work, we circumvent this problem by devising a technique to exploit structured neural architectures for unstructured data. In particular, we employ the popular convolutional neural network (CNN) architectures to perform semantic segmentation of LiDAR data. We propose a projection-based scheme that performs an angle-wise slicing of large 3D point clouds and transforms those slices into 2D grids. Accounting for intensity and reflectivity of the LiDAR input, the 2D grid allows us to construct a pseudo image for the point cloud slice. We enhance this image with low-level image processing techniques of normalization, histogram equalization, and decorrelation stretch to suit our ultimate object of semantic segmentation. A large number of images thus generated are used to induce an encoder-decoder CNN model that learns to compute a segmented 2D projection of the scene, which we finally back project to the 3D point cloud. In addition to a novel method, this article also makes a second major contribution of introducing the enhanced version of our large-scale public PC-Urban outdoor dataset which is captured in a civic setup with an Ouster LiDAR sensor. The updated dataset (PC-Urban_V2) provides nearly 8 billion points including over 100 million points labeled for 25 classes of interest. We provide a thorough evaluation of our technique on PC-Urban_V2 and three other public datasets.",3D point cloud,point cloud dataset,large-scale dataset,convolutional neural network,semantic segmentation,LiDAR,,REMOTE SENSING,,,,,,,,,,,,
Row_1504,"Sun, Ying","Zhang, Xinchang","Huang, Jianfeng","Wang, Haiying","Xin, Qinchuan",Fine-Grained Building Change Detection From Very High-Spatial-Resolution Remote Sensing Images Based on Deep Multitask Learning,,2022,51,"Building change detection from very high-spatial-resolution (VHR) remote sensing images has gained increasing popularity in a variety of applications, such as urban planning and damage assessment. Detecting fine-grained x201C;fromx2013;tox201D; changes (change transition from one land cover type to another) of buildings from the VHR images is still challenging as multitemporal representation is complicated. Recently, fully convolutional neural networks (FCNs) have been proven to be capable of feature extraction and semantic segmentation of VHR images, but its ability in change detection is untested and unknown. In this letter, we leverage the semantic segmentation of buildings as an auxiliary source of information for the fine-grained x201C;fromx2013;tox201D; change detection. A deep multitask learning framework for change detection (MTL-CD) is proposed for detecting building changes from the VHR images. MTL-CD adopts the encoderx2013;decoder architecture and solves the main task of change detection and the auxiliary tasks of semantic segmentation simultaneously. Accordingly, the change detection loss function is constrained by the auxiliary semantic segmentation tasks and enables the back-propagation of the building footprintsx2019; detection errors for the improvement of change detection. A building change detection data set named the Guangzhou data set is also developed for model evaluation, in which the bitemporal Rx2013;Gx2013;B images were collected by airplane (2009) and unmanned aerial vehicle (UAV, 2019) with different flight heights. Experiments on the Guangzhou data set demonstrate that the MTL-CD method effectively detects fine-grained x201C;fromx2013;tox201D; changes and outperforms the postclassification methods and the direct change detection methods.",Task analysis,Buildings,Semantics,Feature extraction,Image segmentation,Remote sensing,Architecture,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Building changes,deep multitask learning,fine-grained change detection,fully convolutional neural network (FCN),,,semantic segmentation,,,,
Row_1505,"Papadomanolaki, Maria","Vakalopoulou, Maria","Paragios, Nikos","Karantzalos, Konstantinos",,STACKED ENCODER-DECODERS FOR ACCURATE SEMANTIC SEGMENTATION OF VERY HIGH RESOLUTION SATELLITE DATASETS,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2018,5,"Semantic segmentation is currently a mainstream method for addressing several remote sensing applications, achieving recently remarkable performance by employing deep learning techniques. In particular, this is the case for pixel-wise dense classification models in very high resolution remote sensing datasets. In this paper, we exploit the use of a relatively deep architecture based on repetitive downscale-upscale processes that had been previously employed for human pose estimation tasks. By integrating such a model, we are aiming to capture and extract low-level details, such as small objects, object boundaries and edges. Experimental results and quantitative evaluation has been performed on the publicly available IS-PRS (WGIII4) benchmark dataset indicating the potential of the proposed approach.",Car detection,Building detection,Fully convolutional networks,Stacked Hourglass Networks,,,,,,,,,,,,,,,,
Row_1506,"Zheng, Zhuo","Zhong, Yanfei","Wang, Junjue","Ma, Ailong",,Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery,2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR),2020,212,"Geospatial object segmentation, as a particular semantic segmentation task, always faces with larger-scale variation, larger intra-class variance of background, and foreground-background imbalance in the high spatial resolution (HSR) remote sensing imagery. However, general semantic segmentation methods mainly focus on scale variation in the natural scene, with inadequate consideration of the other two problems that usually happen in the large area earth observation scene. In this paper, we argue that the problems lie on the lack of foreground modeling and propose a foreground-aware relation network (FarSeg) from the perspectives of relation-based and optimization-based foreground modeling, to alleviate the above two problems. From perspective of relation, FarSeg enhances the discrimination of foreground features via foreground-correlated contexts associated by learning foreground-scene relation. Meanwhile, from perspective of optimization, a foreground-aware optimization is proposed to focus on foreground examples and hard examples of background during training for a balanced optimization. The experimental results obtained using a large scale dataset suggest that the proposed method is superior to the state-of-the-art general semantic segmentation methods and achieves a better trade-off between speed and accuracy.",,,,,,,,,,,,,,,,,,,,
Row_1507,"Liu, Hongyan","Ren, Shun","Ren, Dong","Liu, Xuan",,AUTOMATIC EXTRACTION OF ORCHARDS FROM REMOTE SENSING IMAGE BASED ON CATEGORY ATTENTION MECHANISM,,2022,1,"Using remote sensing images to extract orchard is one of the important methods for product evaluation. Aiming at the problem of misclassification and omission caused by the semantic complexity of the marginal area and uneven planting density, a method based on the improved DeepLabv3 network model to segment the orchards is proposed. First, down-sampling is performed through the convolutional layer and residual module in the encoding network. Then, dilated convolution is used to build multi-scale modules, and category attention mechanism modules are added to refine high-level semantic features. Finally, the resolution of the feature map is restored using a deconvolution operation in the decoding network. Comparing the proposed method with the classic semantic segmentation network DeepLabv3 and other models on the validation set, the results show that the proposed method has better segmentation performance and generalization ability, and the segmentation result of the orchards is more accurate. To be accurate, the Mean Intersection over Union reaches 0.96, which is 2.3% more accurate than the original DeepLabv3+. The proposed method can more accurately segment the orchards in the unbalanced data set and provide a reference for product evaluation.",Orchards segmentation,deep learning,remote sensing image,multi-scale category attention mechanism,,,,INTERNATIONAL JOURNAL OF ROBOTICS & AUTOMATION,,,,,,,,,,,,
Row_1508,"Touzani, Samir","Granderson, Jessica",,,,Open Data and Deep Semantic Segmentation for Automated Extraction of Building Footprints,,JUL 2021,21,"Advances in machine learning and computer vision, combined with increased access to unstructured data (e.g., images and text), have created an opportunity for automated extraction of building characteristics, cost-effectively, and at scale. These characteristics are relevant to a variety of urban and energy applications, yet are time consuming and costly to acquire with today's manual methods. Several recent research studies have shown that in comparison to more traditional methods that are based on features engineering approach, an end-to-end learning approach based on deep learning algorithms significantly improved the accuracy of automatic building footprint extraction from remote sensing images. However, these studies used limited benchmark datasets that have been carefully curated and labeled. How the accuracy of these deep learning-based approach holds when using less curated training data has not received enough attention. The aim of this work is to leverage the openly available data to automatically generate a larger training dataset with more variability in term of regions and type of cities, which can be used to build more accurate deep learning models. In contrast to most benchmark datasets, the gathered data have not been manually curated. Thus, the training dataset is not perfectly clean in terms of remote sensing images exactly matching the ground truth building's foot-print. A workflow that includes data pre-processing, deep learning semantic segmentation modeling, and results post-processing is introduced and applied to a dataset that include remote sensing images from 15 cities and five counties from various region of the USA, which include 8,607,677 buildings. The accuracy of the proposed approach was measured on an out of sample testing dataset corresponding to 364,000 buildings from three USA cities. The results favorably compared to those obtained from Microsoft's recently released US building footprint dataset.",deep learning,semantic segmentation,building footprint extraction,open data,DeepLabv3+,GIS,,REMOTE SENSING,,,,,,,,,,,,
Row_1509,"Gui, Yuanyuan","Li, Wei","Xia, Xiang-Gen","Tao, Ran","Yue, Anzhi",Infrared Attention Network for Woodland Segmentation Using Multispectral Satellite Images,,2022,16,"Semantic segmentation of the remote sensing images (RSIs) has attracted increasing interest in recent years. However, large-area segmentation of the woodland presents challenges. The wide distribution and diverse tree species of the woodland make feature extraction difficult. For this reason, an infrared attention network (InfAttNet) is proposed to extract woodland from multispectral RSIs. InfAttNet has an extra infrared spectral encoder which makes use of the sensitivity of vegetation to near-infrared and red edge spectrums. This extra encoder applies learning about vegetation to improve woodland segmentation. Several attention blocks are designed to enhance learning about vegetation features and so improve the performance. In addition, a new dataset is built, containing a large number of woodland RSIs and covering several typical woodland distribution regions in China. The experimental results demonstrate that compared with other networks, InfAttNet has the highest accuracy and is capable of rapid extraction of the woodland in RSIs.",Feature extraction,Vegetation mapping,Image segmentation,Task analysis,Semantics,Deep learning,Image edge detection,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Attention block,deep learning,infrared spectrums,remote sensing image (RSI),,,woodland segmentation,,,,
Row_1510,"Guo, Dihua","Xiong, Hui","Atluri, Vijayalakshmi","Adam, Nabil R.",,Object discovery in high-resolution remote sensing images: a semantic perspective,,MAY 2009,14,"Given its importance, the problem of object discovery in high-resolution remote-sensing (HRRS) imagery has received a lot of attention in the literature. Despite the vast amount of expert endeavor spent on this problem, more efforts have been expected to discover and utilize hidden semantics of images for object detection. To that end, in this paper, we address this problem from two semantic perspectives. First, we propose a semantic-aware two-stage image segmentation approach, which preserves the semantics of real-world objects during the segmentation process. Second, to better capture semantic features for object discovery, we exploit a hyperclique pattern discovery method to find complex objects that consist of several co-existing individual objects that usually form a unique semantic concept. We consider the identified groups of co-existing objects as new feature sets and feed them into the learning model for better performance of image retrieval. Experiments with real-world datasets show that, with reliable segmentation and new semantic features as starting points, we can improve the performance of object discovery in terms of various external criteria.",,,,,,,,KNOWLEDGE AND INFORMATION SYSTEMS,,,,,,,,,,,,
Row_1511,"Xiao, Zhiguo","Chai, Tengfei","Li, Nianfeng","Shen, Xiangfeng","Guan, Tong",Research Advances in Deep Learning for Image Semantic Segmentation Techniques,,2024,0,"Image semantic segmentation represents a significant area of research within the field of computer vision. With the advent of deep learning, image semantic segmentation techniques that integrate deep learning have demonstrated superior accuracy compared to traditional image semantic segmentation methods. Recently, the Mamba architecture has demonstrated superior semantic segmentation performance compared to the Transformer architecture, and has consequently become a research focus in this field. Nevertheless, the specifics of the Mamba architecture have remained underexplored in the extant literature. This review provides a comprehensive overview of the latest research progress in deep learning techniques for semantic segmentation. It offers a systematic review of traditional convolutional neural network (CNN)-based architectures and focuses on a series of emerging architectures, including the Transformer architecture, the Mamba architecture, and cutting-edge approaches such as self-supervised learning strategies. For each category, a detailed account is provided of the principal algorithms and techniques employed, together with a report on the performance achieved using datasets commonly used in the field.",Semantic segmentation,Remote sensing,Deep learning,Feature extraction,Accuracy,Computer architecture,Medical diagnostic imaging,IEEE ACCESS,"Tian, Jia",Convolution,Biomedical imaging,Convolutional neural networks,Image segmentation,"Li, Xinyuan",,semantic segmentation,deep learning,image processing,,
Row_1512,"Grilli, Eleonora","Daniele, Alessandro","Bassier, Maarten","Remondino, Fabio","Serafini, Luciano",Knowledge Enhanced Neural Networks for Point Cloud Semantic Segmentation,,MAY 16 2023,12,"Deep learning approaches have sparked much interest in the AI community during the last decade, becoming state-of-the-art in domains such as pattern recognition, computer vision, and data analysis. However, these methods are highly demanding in terms of training data, which is often a major issue in the geospatial and remote sensing fields. One possible solution to this problem comes from the Neuro-Symbolic Integration field (NeSy), where multiple methods have been defined to incorporate background knowledge into the neural network's learning pipeline. One such method is KENN (Knowledge Enhanced Neural Networks), which injects logical knowledge into the neural network's structure through additional final layers. Empirically, KENN showed comparable or better results than other NeSy frameworks in various tasks while being more scalable. Therefore, we propose the usage of KENN for point cloud semantic segmentation tasks, where it has immense potential to resolve issues with small sample sizes and unbalanced classes. While other works enforce the knowledge constraints in post-processing, to the best of our knowledge, no previous methods have injected inject such knowledge into the learning pipeline through the use of a NeSy framework. The experiment results over different datasets demonstrate that the introduction of knowledge rules enhances the performance of the original network and achieves state-of-the-art levels of accuracy, even with subideal training data.",point clouds,semantic segmentation,neural network,knowledge enhancement,neuro-symbolic integration,,,REMOTE SENSING,,,,,,,,,,,,
Row_1513,"Guo, Zhou","Xu, Rui","Feng, Chen-Chieh","Zeng, Zhao",,PIF-Net: A Deep Point-Image Fusion Network for Multimodality Semantic Segmentation of Very High-Resolution Imagery and Aerial Point Cloud,,2024,1,"Semantic segmentation is of great significance in many applications. However, automating such a task on single-modality data is challenging in the field of remote sensing due to complex scenes, occlusions, and homogeneous data. In this article, we propose a deep point-image fusion network, namely PIF-Net, for multimodality semantic segmentation. The proposed PIF-Net encompass an encoder-decoder structure, where the encoder uses two independent branches with Res-Pooling blocks and point attention (Pt-Atten) blocks to extract deep and condensed multimodal features, and the decoder upsamples these features. A hierarchical fusion module is proposed to adaptively fuse multimodal features at different levels to ensure they are fully mixed. It outputs joint features in both the point and pixel representations, which are further input to a classification module to fulfill the multiple classification tasks and obtain image and point cloud semantic segmentation results. The proposed network was tested on two benchmark datasets: the Urban Semantic 3-D (US3D) dataset and the ISPRS Vaihingen dataset. Evaluation results showed that PIF-Net achieved an overall accuracy (OA) of 91.5% and 97.2% for image and point segmentation on the US3D dataset, and an OA of 90.1% and 89.3% for image and point segmentation on the ISPRS Vaihingen dataset. Comparisons with existing single-modality and multimodality methods have indicated that PIF-Net outperformed most classic methods and could bring significant improvements. It has also demonstrated that deep multimodality learning exhibit great potentials in remote sensing applications.",Deep learning,multimodality,point cloud,semantic segmentation,very high-resolution (VHR) imagery,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1514,"Liu, Chunxiu","Ming, Yanfang","Zhu, Jinshan",,,IMPROVING THE PERFORMANCE OF SEABIRDS DETECTION COMBINING MULTIPLE SEMANTIC SEGMENTATION MODELS,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,1,"The loss of seabird habitats makes many precious seabirds on the brink of extinction. Monitoring seabirds by unmanned aerial vehicle (UAV) and high-resolution aerial imagery is of vital importance in the long term. The development of convolutional neural network brings great vitality to the detection of small object just like seabird. Due to the complex background and tiny size of seabird in UAV imagery, it is difficult for single semantic segmentation model to detect seabirds accurately. Ensemble learning combines multiple individual learners with a certain strategy to form a committee, which can greatly improve the performance. To realize highly precise and automatic seabird detection from UAV imagery, this paper proposes a deep learning method which combines two semantic segmentation models, as distinct from those generic object detection methods using bounding boxes. The experimental results show that compared to the models such as U-Net and UNet++, the ensemble model performed better overall.",seabird detection,deep learning,semantic segmentation,ensemble learning,,,,,,,,,,,,,,,,
Row_1515,"Yang, Ruoyu","Zhong, Yanfei","Liu, Yinhe","Lu, Xiaoyan","Zhang, Liangpei",Occlusion-Aware Road Extraction Network for High-Resolution Remote Sensing Imagery,,2024,3,"Road occlusion seriously affects the connectivity of extracted roads, and has a negative effect on practical applications. The dense road occlusion problem is caused by high-rise buildings and street trees, and is a more serious and unique problem than simple occlusion caused by low buildings and scattered trees. The existing methods mainly solve the road occlusion problem by enhancing the encoder ability to capture the long connectivity feature of roads. Unfortunately, the existing methods can only solve small and sparse road occlusion situations, and they cannot deal with the dense road occlusions caused by dense high-rise buildings or trees. In this article, to solve the dense road occlusion problem, the occlusion-aware road extraction network, namely OARENet, is proposed for road extraction from high-resolution remote sensing imagery. In OARENet, an occlusion-aware decoder (OADecoder) is designed by explicit modeling the texture feature for road regions with dense occlusions. The OADecoder is made up of a regular occlusion-aware (ROA) module and a stochastic occlusion-aware (SOA) module. The ROA module is implemented by adopting different dilation rates to fit the texture feature in the semantic feature maps. The SOA module is proposed by designing stochastic convolutions to adaptively fit the spatial details of road regions with dense occlusions. In order to evaluate the dense occlusion problem, a dense occlusion road dataset (JHWV) was built and annotated. The experimental results obtained on the DeepGlobe dataset, the newly built JHWV dataset, and large-scale urban images demonstrate the superiority of OARENet, especially when faced with a dense road occlusion situation.",Dense road occlusion,occlusion-aware,remote sensing images,road extraction,semantic segmentation,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1516,"Li, Jingtao","Wang, Xinyu","Zhao, Hengwei","Wang, Shaoyu","Zhong, Yanfei",Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors,"THIRTY-SEVENTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 37 NO 4",2023,3,"Anomaly segmentation in high spatial resolution (HSR) remote sensing imagery is aimed at segmenting anomaly patterns of the earth deviating from normal patterns, which plays an important role in various Earth vision applications. However, it is a challenging task due to the complex distribution and the irregular shapes of objects, and the lack of abnormal samples. To tackle these problems, an anomaly segmentation model based on pixel descriptors (ASD) is proposed for anomaly segmentation in HSR imagery. Specifically, deep one-class classification is introduced for anomaly segmentation in the feature space with discriminative pixel descriptors. The ASD model incorporates the data argument for generating virtual abnormal samples, which can force the pixel descriptors to be compact for normal data and meanwhile to be diverse to avoid the model collapse problems when only positive samples participated in the training. In addition, the ASD introduced a multi-level and multi-scale feature extraction strategy for learning the low-level and semantic information to make the pixel descriptors feature-rich. The proposed ASD model was validated using four HSR datasets and compared with the recent state-of-the-art models, showing its potential value in Earth vision applications.",,,,,,,,,,,,,,,,,,,,
Row_1517,"Liu, Da","Long, Hao","Liu, Zhenbao",,,Channel selection and local attention transformer model for semantic segmentation on UAV remote sensing scene,,DEC 2024,0,"Compared with common urban landscape semantic segmentation, unmanned aerial vehicle (UAV) image semantic segmentation is more challenging because small targets have very low pixel percentages and multi-scale features due to the influence of flight altitude. Yet, the commonly used successive grid downsampling strategy in the current transformer-based methods omits some important features of small targets. Furthermore, due to the complex background interference, it can lead to even worse results. In reaction to this, existing strategies aim to maintain superior resolution. Nevertheless, the application of this method incurs considerable computational costs, which brings challenges for the practical applications of UAVs. So it is significant to design a novel framework to balance retaining more pixels representing small objects during downsampling and reducing computational costs. For this, the Channel Selection and the Local Attention Transformer Model (CSLFormer) are proposed. During the overlap patch embedding process of feature maps, the model allocates half of the important channels to global attention and local attention. These two types of attention focus on different aspects: one learns the relationships and importance among various patches, while the other emphasizes the features of individual patches. The method shows superior performance on two public datasets: AeroScapes and Vaihingen, achieving mean intersection over union (mIoU) of 75.57% and 78.93%, respectively. The proposed CSLFormer has been released on GitHub: .",aircraft,computer vision,convolutional neural nets,feedforward neural nets,image segmentation,,,IET IMAGE PROCESSING,,,,,,,,,,,,
Row_1518,"Chen, Zhe","Yang, Bisheng","Ma, Ailong","Peng, Mingjun","Li, Haiting",Joint alignment of the distribution in input and feature space for cross-domain aerial image semantic segmentation,,DEC 2022,6,"Benefiting from the development of deep learning, researchers have made significant progress and achieved superior performance in the semantic segmentation of remote sensing (RS) data. However, when encountering an unseen scenario, the performance of a trained model deteriorates dramatically because of the domain shift. Unsupervised domain adaptation (UDA) provides an alternative to address the issue. Aligning the high-level representations via adversarial learning is a popular way, but it is difficult when there is a large gap in input space. With this consideration, we design a framework to jointly align the distribution in input and feature space. For input space alignment, we unify the resolution for the consistency of content and propose a lightweight module named Digital Number Transformer (DNT) to reduce the visual differences. For feature space alignment, we design a Multi-Scale Feature aggregation (MSF) module and introduce the Fine-Grained Discriminator (FGD) to conduct a category-level alignment at multiple layers so that features can be fully aligned and negative transfer can be reduced. We carried out experiments in diverse cross-domain scenarios, including the discrepancy in geographic position and the discrepancy in both geographic position and imaging mode. Comprehensive experiments demonstrate that our method outperforms other state-of-the-art methods in all scenarios.",Unsupervised domain adaptation (UDA),Remote sensing (RS) image,Semantic segmentation,Resolution uniform (RU),Digital number transformation (DNT),Multi-scale feature aggregation (MSF),Fine-grained discriminator (FGD),INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,"Chen, Tao",,,,,"Chen, Chi","Dong, Zhen",,,,,
Row_1519,"Tasar, Onur","Happy, S. L.","Tarabalka, Yuliya","Alliez, Pierre",,SEMI2I: SEMANTICALLY CONSISTENT IMAGE-TO-IMAGE TRANSLATION FOR DOMAIN ADAPTATION OF REMOTE SENSING DATA,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,28,"Although convolutional neural networks have been proven to be an effective tool to generate high quality maps from remote sensing images, their performance significantly deteriorates when there exists a large domain shift between training and test data. To address this issue, we propose a new data augmentation approach that transfers the style of test data to training data using generative adversarial networks. Our semantic segmentation framework consists in first training a U-net from the real training data and then fine-tuning it on the test stylized fake training data generated by the proposed approach. Our experimental results prove that our framework outperforms the existing domain adaptation methods.",Domain adaptation,data augmentation,semantic segmentation,dense labeling,image-to-image translation,generative adversarial networks,GANs,,,,,,,,,,,,,
Row_1520,"Xu, Qingsong","Yuan, Xin","Ouyang, Chaojun","Zeng, Yue",,Attention-Based Pyramid Network for Segmentation and Classification of High-Resolution and Hyperspectral Remote Sensing Images,,NOV 2020,16,"Unlike conventional natural (RGB) images, the inherent large scale and complex structures of remote sensing images pose major challenges such as spatial object distribution diversity and spectral information extraction when existing models are directly applied for image classification. In this study, we develop an attention-based pyramid network for segmentation and classification of remote sensing datasets. Attention mechanisms are used to develop the following modules: (i) a novel and robust attention-based multi-scale fusion method effectively fuses useful spatial or spectral information at different and same scales; (ii) a region pyramid attention mechanism using region-based attention addresses the target geometric size diversity in large-scale remote sensing images; and (iii) cross-scale attention in our adaptive atrous spatial pyramid pooling network adapts to varied contents in a feature-embedded space. Different forms of feature fusion pyramid frameworks are established by combining these attention-based modules. First, a novel segmentation framework, called the heavy-weight spatial feature fusion pyramid network (FFPNet), is proposed to address the spatial problem of high-resolution remote sensing images. Second, an end-to-end spatial-spectral FFPNet is presented for classifying hyperspectral images. Experiments conducted on ISPRS Vaihingen and ISPRS Potsdam high-resolution datasets demonstrate the competitive segmentation accuracy achieved by the proposed heavy-weight spatial FFPNet. Furthermore, experiments on the Indian Pines and the University of Pavia hyperspectral datasets indicate that the proposed spatial-spectral FFPNet outperforms the current state-of-the-art methods in hyperspectral image classification.",high-resolution and hyperspectral images,spatial object distribution diversity,spectral information extraction,attention-based pyramid network,heavy-weight spatial feature fusion pyramid network (FFPNet),spatial-spectral FFPNet,,REMOTE SENSING,,,,,,,,,,,,
Row_1521,"Zhao, Yuehua","Zhang, Jiguang","Ma, Jie","Xu, Shibiao",,Large-Scale Semantic Scene Understanding with Cross-Correction Representation,,DEC 2022,0,"Real-time large-scale point cloud segmentation is an important but challenging task for practical applications such as remote sensing and robotics. Existing real-time methods have achieved acceptable performance by aggregating local information. However, most of them only exploit local spatial geometric or semantic information dependently, few considering the complementarity of both. In this paper, we propose a model named Spatial-Semantic Incorporation Network (SSI-Net) for real-time large-scale point cloud segmentation. A Spatial-Semantic Cross-correction (SSC) module is introduced in SSI-Net as a basic unit. High-quality contextual features can be learned through SSC by correcting and updating high-level semantic information using spatial geometric cues and vice versa. Adopting the plug-and-play SSC module, we design SSI-Net as an encoder-decoder architecture. To ensure efficiency, it also adopts a random sample-based hierarchical network structure. Extensive experiments on several prevalent indoor and outdoor datasets for point cloud semantic segmentation demonstrate that the proposed approach can achieve state-of-the-art performance.",point cloud,large-scale semantic segmentation,spatial geometric,semantic context,cross-correction,,,REMOTE SENSING,,,,,,,,,,,,
Row_1522,"Song, Ahram",,,,,Semantic Segmentation of Drone Images Based on Combined Segmentation Network Using Multiple Open Datasets,,OCT 2023,1,"This study proposed and validated a combined segmentation network (CSN) designed to effectively train on multiple drone image datasets and enhance the accuracy of semantic segmentation. CSN shares the entire encoding domain to accommodate the diversity of three drone datasets, while the decoding domains are trained independently. During training, the segmentation accuracy of CSN was lower compared to U-Net and the pyramid scene parsing network (PSPNet) on single datasets because it considers loss values for all datasets simultaneously. However, when applied to domestic autonomous drone images, CSN demonstrated the ability to classify pixels into appropriate classes without requiring additional training, outperforming PSPNet. This research suggests that CSN can serve as a valuable tool for effectively training on diverse drone image datasets and improving object recognition accuracy in new regions.",Drone image,Semantic segmentation,Deep learning,Combined segmentation network,,,,KOREAN JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1523,"Wang, ZH","Li, J","Li, MJ",,,Swin-UperNet: A Semantic Segmentation Model for Mangroves and Spartina alterniflora Loisel Based on UperNet,,MAR 2023,10,"As an ecosystem in transition from land to sea, mangroves play a vital role in wind and wave protection and biodiversity maintenance. However, the invasion of Spartina alterniflora Loisel seriously damages the mangrove wetland ecosystem. To protect mangroves scientifically and dynamically, a semantic segmentation model for mangroves and Spartina alterniflora Loise was proposed based on UperNet (Swin-UperNet). In the proposed Swin-UperNet model, a data concatenation module was proposed to make full use of the multispectral information of remote sensing images, the backbone network was replaced with a Swin transformer to improve the feature extraction capability, and a boundary optimization module was designed to optimize the rough segmentation results. Additionally, a linear combination of cross-entropy loss and Lovasz-Softmax loss was taken as the loss function of Swin-UperNet, which could address the problem of unbalanced sample distribution. Taking GF-1 and GF-6 images as the experiment data, the performance of the Swin-UperNet model was compared against that of other segmentation models in terms of pixel accuracy (PA), mean intersection over union (mIoU), and frames per second (FPS), including PSPNet, PSANet, DeepLabv3, DANet, FCN, OCRNet, and DeepLabv3+. The results showed that the Swin-UperNet model achieved the best PA of 98.87% and mIoU of 90.0%, and the efficiency of the Swin-UperNet model was higher than that of most models. In conclusion, Swin-UperNet is an efficient and accurate model for mangrove and Spartina alterniflora Loise segmentation synchronously, which will provide a scientific basis for Spartina alterniflora Loise monitoring and mangrove resource conservation and management.",,,,,,,,ELECTRONICS,,,,,,,,,,,,
Row_1524,"Wang, J","Jiang, LL","Wang, YJ",,,Exploration of Semantic Geo-Object Recognition Based on the Scale Parameter Optimization Method for Remote Sensing Images,,JUN 2021,2,"Image segmentation is of significance because it can provide objects that are the minimum analysis units for geographic object-based image analysis (GEOBIA). Most segmentation methods usually set parameters to identify geo-objects, and different parameter settings lead to different segmentation results; thus, parameter optimization is critical to obtain satisfactory segmentation results. Currently, many parameter optimization methods have been developed and successfully applied to the identification of single geo-objects. However, few studies have focused on the recognition of the union of different types of geo-objects (semantic geo-objects), such as a park. The recognition of semantic geo-objects is likely more crucial than that of single geo-objects because the former type of recognition is more correlated with the human perception. This paper proposes an approach to recognize semantic geo-objects. The key concept is that a single geo-object is the smallest component unit of a semantic geo-object, and semantic geo-objects are recognized by iteratively merging single geo-objects. Thus, the optimal scale of the semantic geo-objects is determined by iteratively recognizing the optimal scales of single geo-objects and using them as the initiation point of the reset scale parameter optimization interval. In this paper, we adopt the multiresolution segmentation (MRS) method to segment Gaofen-1 images and tested three scale parameter optimization methods to validate the proposed approach. The results show that the proposed approach can determine the scale parameters, which can produce semantic geo-objects.",,,,,,,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,,,,,,,,,,,
Row_1525,"Guiotte, Florent","Pham, Minh-Tan","Dambreville, Romain","Corpetti, Thomas","Lefevre, Sebastien",Semantic Segmentation of LiDAR Points Clouds: Rasterization Beyond Digital Elevation Models,,NOV 2020,10,"LiDAR point clouds are receiving a growing interest in remote sensing as they provide rich information to be used independently or together with optical data sources, such as aerial imagery. However, their nonstructured and sparse nature make them difficult to handle, conversely to raw imagery for which many efficient tools are available. To overcome this specific nature of LiDAR point clouds, the standard approach relies on converting the point cloud into a digital elevation model, represented as a 2-D raster. Such a raster can then be used similarly as optical images, e.g., with 2-D convolutional neural networks (CNNs) for semantic segmentation. In this letter, we show that LiDAR point clouds provide more information than only the digital elevation model and that considering alternative rasterization strategies helps to achieve better semantic segmentation results. We illustrate our findings on the IEEE Data Fusion Contest (DFC) 2018 data set.",Laser radar,Three-dimensional displays,Semantics,Computer architecture,Feature extraction,Microprocessors,Training,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Digital elevation model (DEM),deep learning,LiDAR,rasterization,,,semantic segmentation,,,,
Row_1526,"Zhang, JH","Lin, LL","Liu, JY",,,S5Mars: Semi-Supervised Learning for Mars Semantic Segmentation,,2024,0,"Deep learning has become a powerful tool for Mars exploration. Mars terrain semantic segmentation is an important Martian vision task, which is the base of rover autonomous planning and safe driving. However, there is a lack of sufficient detailed and high-confidence data annotations that are exactly required by most deep learning methods to obtain a good model. To address this problem, we propose our solution from the perspective of joint data and method design. We first present a new dataset $\text{S}<^>{5}$ Mars for semi-supervised learning on Mars semantic segmentation, which contains 6k high-resolution images and is sparsely annotated based on confidence, ensuring the high quality of labels. Then, to learn from this sparse data, we propose a semi-supervised learning (SSL) framework for Mars image semantic segmentation to learn representations from limited labeled data. Different from the existing SSL methods that are mostly targeted at the Earth image data, our method takes into account Mars data characteristics. Specifically, we first investigate the impact of current widely used natural image augmentations on Mars images. Based on the analysis, we then proposed two novel and effective augmentations for SSL of Mars segmentation and augment instance normalization (AugIN) and SAM-Mix, which serve as strong augmentations to boost the model performance. Meanwhile, to fully leverage the unlabeled data, we introduce a soft-to-hard consistency learning strategy, learning from different targets based on prediction confidence. Experimental results show that our method can outperform state-of-the-art SSL approaches remarkably. Our proposed dataset is available at https://jhang2020.github.io/S5Mars.github.io/.",,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,
Row_1527,"Lin, Lujun","Liu, Lei","Liu, Ming","Zhang, Qunjia","Feng, Min",DEDNet: Dual-Encoder DeeplabV3+Network for Rock Glacier Recognition Based on Multispectral Remote Sensing Image,,JUL 2024,0,"Understanding the distribution of rock glaciers provides key information for investigating and recognizing the status and changes of the cryosphere environment. Deep learning algorithms and red-green-blue (RGB) bands from high-resolution satellite images have been extensively employed to map rock glaciers. However, the near-infrared (NIR) band offers rich spectral information and sharp edge features that could significantly contribute to semantic segmentation tasks, but it is rarely utilized in constructing rock glacier identification models due to the limitation of three input bands for classical semantic segmentation networks, like DeeplabV3+. In this study, a dual-encoder DeeplabV3+ network (DEDNet) was designed to overcome the flaws of the classical DeeplabV3+ network (CDNet) when identifying rock glaciers using multispectral remote sensing images by extracting spatial and spectral features from RGB and NIR bands, respectively. This network, trained with manually labeled rock glacier samples from the Qilian Mountains, established a model with accuracy, precision, recall, specificity, and mIoU (mean intersection over union) of 0.9131, 0.9130, 0.9270, 0.9195, and 0.8601, respectively. The well-trained model was applied to identify new rock glaciers in a test region, achieving a producer's accuracy of 93.68% and a user's accuracy of 94.18%. Furthermore, the model was employed in two study areas in northern Tien Shan (Kazakhstan) and Daxue Shan (Hengduan Shan, China) with high accuracy, which proved that the DEDNet offers an innovative solution to more accurately map rock glaciers on a larger scale due to its robustness across diverse geographic regions.",rock glacier,dual-encoder DeeplabV3+,multispectral remote sensing images,spatial-spectral features,,,,REMOTE SENSING,"Khalil, Yasir Shaheen",,,,,"Yin, Fang",,,,,,
Row_1528,"Illarionova, Svetlana","Shadrin, Dmitrii","Shukhratov, Islomjon","Evteeva, Ksenia","Popandopulo, Georgii",Benchmark for Building Segmentation on Up-Scaled Sentinel-2 Imagery,,APR 29 2023,9,"Currently, we can solve a wide range of tasks using computer vision algorithms, which reduce manual labor and enable rapid analysis of the environment. The remote sensing domain provides vast amounts of satellite data, but it also poses challenges associated with processing this data. Baseline solutions with intermediate results are available for various tasks, such as forest species classification, infrastructure recognition, and emergency situation analysis using satellite data. Despite these advances, two major issues with high-performing artificial intelligence algorithms remain in the current decade. The first issue relates to the availability of data. To train a robust algorithm, a reasonable amount of well-annotated training data is required. The second issue is the availability of satellite data, which is another concern. Even though there are a number of data providers, high-resolution and up-to-date imagery is extremely expensive. This paper aims to address these challenges by proposing an effective pipeline for building segmentation that utilizes freely available Sentinel-2 data with 10 m spatial resolution. The approach we use combines a super-resolution (SR) component with a semantic segmentation component. As a result, we simultaneously consider and analyze SR and building segmentation tasks to improve the quality of the infrastructure analysis through medium-resolution satellite data. Additionally, we collected and made available a unique dataset for the Russian Federation covering area of 1091.2 square kilometers. The dataset provides Sentinel-2 imagery adjusted to the spatial resolution of 2.5 m and is accompanied by semantic segmentation masks. The building footprints were created using OpenStreetMap data that was manually checked and verified. Several experiments were conducted for the SR task, using advanced image SR methods such as the diffusion-based SR3 model, RCAN, SRGAN, and MCGR. The MCGR network produced the best result, with a PSNR of 27.54 and SSIM of 0.79. The obtained SR images were then used to tackle the building segmentation task with different neural network models, including DeepLabV3 with different encoders, SWIN, and Twins transformers. The SWIN transformer achieved the best results, with an F1-score of 79.60.",remote sensing,semantic segmentation,image processing,super-resolution,Sentinel-2,,,REMOTE SENSING,"Sotiriadi, Nazar",,,,,"Oseledets, Ivan","Burnaev, Evgeny",,,,,
Row_1529,"Diakogiannis, FI","Furby, S","Taylor, J",,,SSG2: A new modeling paradigm for semantic segmentation,,SEP 2024,0,"State-of-the-art models in semantic segmentation primarily operate on single, static images, generating corresponding segmentation masks. This one-shot approach leaves little room for error correction, as the models lack the capability to integrate multiple observations for enhanced accuracy. Inspired by work on semantic change detection, we address this limitation by introducing a methodology that leverages a sequence of observables generated for each static input image. By adding this ""temporal""dimension, we exploit strong signal correlations between successive observations in the sequence to reduce error rates. Our framework, dubbed SSG2 (Semantic Segmentation Generation 2), employs a dual-encoder, single-decoder base network augmented with a sequence model. The base model learns to predict the set intersection, union, and difference of labels from dual-input images. Given a fixed target input image and a set of support images, the sequence model builds the predicted mask of the target by synthesizing the partial views from each sequence step and filtering out noise. We evaluate SSG2 across four diverse datasets: UrbanMonitor, featuring orthoimage tiles from Darwin, Australia with four spectral bands at 0.2 m spatial resolution and a surface model; ISPRS Potsdam, which includes true orthophoto images with multiple spectral bands and a 5 cm ground sampling distance; ISPRS Vahingen, which also includes true orthophoto images and a 9 cm ground sampling distance; and ISIC2018, a medical dataset focused on skin lesion segmentation, particularly melanoma. The SSG2 model demonstrates rapid convergence within the first few tens of epochs and significantly outperforms UNet-like baseline models with the same number of gradient updates. However, the addition of the temporal dimension results in an increased memory footprint. While this could be a limitation, it is offset by the advent of higher-memory GPUs and coding optimizations. Our code is available at https://github.com/feevos/ssg2.",,,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,
Row_1530,"Tang, ZC","Chen, CYC","Sun, HW",,,Capsule-Encoder-Decoder: A Method for Generalizable Building Extraction from Remote Sensing Images,,MAR 2022,6,"Due to the inconsistent spatiotemporal spectral scales, a remote sensing dataset over a large-scale area and over long-term time series will have large variations and large statistical distribution features, which will lead to a performance drop of the deep learning model that is only trained on the source domain. For building an extraction task, deep learning methods perform weak generalization from the source domain to the other domain. To solve the problem, we propose a Capsule-Encoder-Decoder model. We use a vector named capsule to store the characteristics of the building and its parts. In our work, the encoder extracts capsules from remote sensing images. Capsules contain the information of the buildings' parts. Additionally, the decoder calculates the relationship between the target building and its parts. The decoder corrects the buildings' distribution and up-samples them to extract target buildings. Using remote sensing images in the lower Yellow River as the source dataset, building extraction experiments were trained on both our method and the mainstream methods. Compared with the mainstream methods on the source dataset, our method achieves convergence faster, and our method shows higher accuracy. Significantly, without fine tuning, our method can reduce the error rates of building extraction results on an almost unfamiliar dataset. The building parts' distribution in capsules has high-level semantic information, and capsules can describe the characteristics of buildings more comprehensively, which are more explanatory. The results prove that our method can not only effectively extract buildings but also perform great generalization from the source remote sensing dataset to another.",,,,,,,,REMOTE SENSING,,,,,,,,,,,,
Row_1531,"Pan, Xin","Zhang, Ce","Xu, Jun","Zhao, Jian",,Simplified object-based deep neural network for very high resolution remote sensing image classification,,NOV 2021,24,"For the object-based classification of high resolution remote sensing images, many people expect that introducing deep learning methods can improve then classification accuracy. Unfortunately, the input shape for deep neural networks (DNNs) is usually rectangular, whereas the shapes of the segments output by segmentation methods are usually according to the corresponding ground objects; this inconsistency can lead to confusion among different types of heterogeneous content when a DNN processes a segment. Currently, most object-based methods utilizing convolutional neural networks (CNNs) adopt additional models to overcome the detrimental influence of such heterogeneous content; however, these heterogeneity suppression mechanisms introduce additional complexity into the whole classification process, and these methods are usually unstable and difficult to use in real applications. To address the above problems, this paper proposes a simplified object-based deep neural network (SO-DNN) for very high resolution remote sensing image classification. In SO-DNN, a new segment category label inference method is introduced, in which a deep semantic segmentation neural network (DSSNN) is used as the classification model instead of a traditional CNN. Since the DSSNN can obtain a category label for each pixel in the input image patch, different types of content are not mixed together; therefore, SO-DNN does not require an additional heterogeneity suppression mechanism. Moreover, SO-DNN includes a sample information optimization method that allows the DSSNN model to be trained using only pixel-based training samples. Because only a single model is used and only a pixel-based training set is needed, the whole classification process of SO-DNN is relatively simple and direct. In experiments, we use very high-resolution aerial images from Vaihingen and Potsdam from the ISPRS WG II/4 dataset as test data and compare SO-DNN with 6 traditional methods: O-MLP, O+CNN, OHSF-CNN, 2-CNN, JDL and U-Net. Compared with the best-performing method among these traditional methods, the classification accuracy of SO-DNN is improved by up to 7.71% and 10.78% for single images from Vaihingen and Potsdam, respectively, and the average classification accuracy is improved by 2.46% and 2.91% for the Vaihingen and Potsdam images, respectively. SO-DNN relies on fewer models and easier-to-obtain samples than traditional methods, and its stable performance makes SO-DNN more valuable for practical applications.",CNN,Very high resolution,Semantic segmentation,Classification,OBIA,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,
Row_1532,"Hizal, C.","Gulsu, G.","Akgun, H. Y.","Kulavuz, B.","Bakirman, T.",FOREST SEMANTIC SEGMENTATION BASED ON DEEP LEARNING USING SENTINEL-2 IMAGES,"8TH INTERNATIONAL CONFERENCE ON GEOINFORMATION ADVANCES, GEOADVANCES 2024, VOL. 48-4",2024,0,"Forests are invaluable for maintaining biodiversity, watersheds, rainfall levels, bioclimatic stability, carbon sequestration and climate change mitigation, and the sustainability of large-scale climate regimes. In other words, forests provide a wide range of ecosystem services and livelihoods for the people and play a critical role in influencing global atmospheric cycles. Providing sustainable, reliable, and accurate information on forest cover change is essential for an holistic forest management, efficient use of resources, neutralizing the effects of global warming and better monitoring of deforestation activities. Within the scope of this study, it is aimed to perform semantic segmentation of 5 different tree species (larch, red pine, yellow pine, oak, spruce) from Sentinel-2 satellite images. For this purpose, the regions where these tree species are densely populated in Turkey (Marmara, Aegean, Eastern Black Sea) were selected as pilot regions. A unique data set was created using the data of the selected pilot regions. As a result of the study, it was possible to determine the forest types temporally for the selected classes with more than 90% Intersection over Union score for all classes. The developed deep learning model with the created forest data set can be implemented to the other forests areas with same species in other parts of the world.",Deep Learning,Semantic Segmentation,Sentinel-2,Remote Sensing,Stand Map,,,,"Aydin, A.",,,,,"Bayram, B.",,,,,,
Row_1533,"Song, Ahram",,,,,Semantic Segmentation of Heterogeneous Unmanned Aerial Vehicle Datasets Using Combined Segmentation Network,,FEB 2023,1,"Unmanned aerial vehicles (UAVs) can capture high-resolution imagery from a variety of viewing angles and altitudes; they are generally limited to collecting images of small scenes from larger regions. To improve the utility of UAV-appropriated datasets for use with deep learning applications, multiple datasets created from various regions under different conditions are needed. To demonstrate a powerful new method for integrating heterogeneous UAV datasets, this paper applies a combined segmentation network (CSN) to share UAVid and semantic drone dataset encoding blocks to learn their general features, whereas its decoding blocks are trained separately on each dataset. Experimental results show that our CSN improves the accuracy of specific classes (e.g., cars), which currently comprise a low ratio in both datasets. From this result, it is expected that the range of UAV dataset utilization will increase.",Semantic segmentation,Deep learning,UAVid,Semantic drone dataset,,,,KOREAN JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1534,Thuy Thi Nguyen,Sang Viet Dinh,Nguyen Tien Quang,Huynh Thi Thanh Binh,,Semantic Segmentation of Objects from Airborne Imagery,2017 FOURTH ASIAN CONFERENCE ON DEFENCE TECHNOLOGY - JAPAN (ACDT),2017,0,"Extraction of objects from images acquired by airborne sensors is the one of the most important topics in Aerial Photograph Interpretation (API). The task is challenging due to the very heterogeneous appearance of man-made and natural objects on the ground. Meanwhile images acquired by airborne sensors are very high-resolution, which requires high computational costs. This paper presents an efficient approach for automated extraction of objects at pixel level. We propose to combine a powerful classifier and an efficient contextual model for semantic segmentation of objects in images. Multiple image features are used to train the classifier, other features are used to learn the contextual model. We employ Random forest (RF) as classifier which allows one to learn very fast on big data. The outputs given by RF are then combined with a fully connected conditional random field (CRF) model for improving classification performance. Experiments have been conducted on a challenging aerial image dataset from a recent ISPRS Semantic Labeling Contest. We obtained state-of-the-art performance with a reasonable computational demand.",Image segmentation,Semantic labeling,Object detection,Machine learning,Random forest,Deep learning,Aerial image,,,Remote sensing,,,,,,,,,,
Row_1535,"Tao, Mingliang","Li, Jieshuang","Chen, Junli","Liu, Yanyang","Fan, Yifei",Radio Frequency Interference Signature Detection in Radar Remote Sensing Image Using Semantic Cognition Enhancement Network,,2022,12,"Radio frequency interference (RFI) is a significant threat to accurate microwave remote sensing. The RFI signals manifest themselves in unpredictable locations and patterns in the image, which will cause measurement distortion and image degradation or even lead to wrong retrievals of the geophysical parameters. Accurate detection of RFI artifacts is a prerequisite step to preserve the overall quality of remote sensing quality. In this article, a semantic cognitive enhancement network for RFI signature detection is proposed. It employs an encoder-decoder architecture, which incorporates the atrous spatial pyramid pooling, depthwise convolution, and self-attentional mechanism. Rather than detecting the existence of RFI artifacts for an entire image, the proposed scheme can realize RFI recognition in a pixelwise manner without setting predefined thresholds. Extensive experimental results on diverse scenarios in Sentinel-1 images with various RFI types are provided, which demonstrates robust detection performance for both strong and weak interference without requiring a large number of training samples.",Interference,Semantics,Convolution,Image segmentation,Synthetic aperture radar,Spaceborne radar,Radiofrequency interference,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Su, Jia",Interference detection,radio frequency interference (RFI),semantic cognition network,Sentinel-1,"Wang, Ling",,synthetic aperture radar (SAR),,,,
Row_1536,"Nuradili, Pakezhamu","Zhou, Guiyun","Zhou, Ji","Wang, Ziwei","Meng, Yizhen",Semantic segmentation for UAV low-light scenes based on deep learning and thermal infrared image features,,JUN 17 2024,2,"With advancements in unmanned aerial vehicle (UAV) remote sensing technology, remote sensing images have emerged as a critical source of research data across various domains, including agriculture, forestry, and environmental research. UAVs fitted with diverse spectral sensors are capable of capturing diverse image modalities, presenting both challenges and opportunities for image semantic segmentation technology. Most existing semantic segmentation networks excel in processing images captured by visible light cameras and often fail to segment images captured by unmanned aerial vehicles under low-light conditions due to insufficient lighting, reduced visual clarity, high noise levels, and uneven illumination. Thermal infrared imaging sensors can capture thermal radiation information, which has the potential to improve segmentation accuracy when integrated with visible images. In this study, we introduce a novel semantic segmentation processing framework, which evaluates different fusing methods and fuses visible and thermal infrared images. The framework employs a lightweight deep learning model and is designed for accurate semantic segmentation on the fused images. Experiments are conducted on images collected in our unmanned aerial vehicle flight experiments and a public night-time dataset to assess the performance of our proposed approach. Experimental results show that our proposed framework achieves state-of-the-art performance in semantic segmentation tasks in low-light conditions.",Deep learning,UAV image segmentation,thermal infrared image,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,"Tang, Wenbin",,,,,"Melgani, Farid",,,,,,
Row_1537,"Xiong, Yonggang","Xiao, Xueming","Yao, Meibao","Liu, Haiqiang","Yang, Hong",MarsFormer: Martian Rock Semantic Segmentation With Transformer,,2023,16,"Semantic segmentation of Mars scenes has a crucial role in Mars rovers science missions. Current convolutional neural network (CNN)-based composition of U-Net has powerful information extraction capabilities; however, convolutional localization suffers from the limited global context modeling capability. Although transformer global modeling has performed well, it still encounters obstacles in the extraction and retention of low-level features. This issue is particularly relevant for Martian rocks with their varying shapes, textures, and sizes in Mars scenes. In this article, we propose a novel transformer semantic segmentation framework for Martian rock images, called MarsFormer, that consists of an encoder-decoder structure connected through a feature enhancement module (FEM) and a window transformer block (WTB). Specifically, multiscale hierarchical features are generated by the mix transformer (MiT) encoders, upgraded-FFN decoder (UFD) fuse and filter features at different scales, preserving the rich local and global contextual information. FEM enhances the inter-multiscale feature correlation from both spatial and channel perspectives. WTB captures the long-range contexts and preserves the local features. We built two datasets of synthetic and real Martian rocks. The synthetic dataset is SynMars, referencing data from the ZhuRong rover taken from its virtual terrain engine. The other dataset is MarsData-V2, from real Mars scenes, and published recently in our previous study. Extensive experiments conducted on both datasets showed that MarsFormer achieves superiority in Martian rock segmentation, obtaining state-of-the-art performance with favorable computational simplicity. The data are available at: https://github.com/CVIR-Lab/SynMars.",Mars exploration,semantic segmentation,synthetic Mars,transformer model,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Fu, Yuegang",,,,,,,,,,,
Row_1538,"Wang, Chengyi","Li, Lianfa",,,,Multi-Scale Residual Deep Network for Semantic Segmentation of Buildings with Regularizer of Shape Representation,,SEP 2020,13,"It is challenging for semantic segmentation of buildings based on high-resolution remote sensing images, given high variability of appearance and complicated backgrounds of the buildings and their images. In this communication, we proposed an ensemble multi-scale residual deep learning method with the regularizer of shape representation for semantic segmentation of buildings. Based on the U-Net architecture using residual connections and multi-scale ASPP (atrous spatial pyramid pooling) modules, our method introduced the regularizer of shape representation and ensemble learning of multi-scale models to enhance model training and reduce over-fitting. In our method, the shape representation was coded in an antoencoder that was used to encode and reconstruct the shape characteristics of the buildings. In prediction, we consider multi-scale trained models for different resolution inputs and side effects to obtain an optimal semantic segmentation. With the high-resolution image of the Changshan, an island county in China, we used two-thirds of the study region image to train the model and the remaining one-third for the independent test. We obtained the accuracy of 0.98-0.99, mean intersection over union (MIoU) of 0.91-0.93 and Jaccard coefficient of 0.89-0.92 in validation. In the independent test, our method achieved state-of-the-art performance (MIoU: 0.83; Jaccard index: 0.81). By comparing with the existing representative methods on four different data sets, the proposed method consistently improved the learning process and generalization. The study shows important contributions of ensemble learning of multi-scale residual models and regularizer of shape representation to semantic segmentation of buildings.",multiple scales,residual deep ensemble learning,regularizer,shape representation,semantic segmentation of buildings,,,REMOTE SENSING,,,,,,,,,,,,
Row_1539,"Wang, Zhipan","Xu, Minduan","Wang, Zhongwu","Guo, Qing","Zhang, Qingling",ScribbleCDNet: Change detection on high-resolution remote sensing imagery with scribble interaction,,APR 2024,3,"Change detection on high-resolution remote sensing imagery using end-to-end deep learning methods has attracted considerable attention in recent years. Nevertheless, the performance of end-to-end models on complicated scenarios still is limited. Interactive deep-learning models have proven to be a valuable technique for enhancing model performance with minimal human interaction. For instance, the clicks-based interactive models have attracted much attention recently, however, their performance on large regions or complex areas still can be further improved, because they cannot provide accurate semantics or shape prior information of the change regions for the interactive models, as we know that the shape and semantic features of changed regions in remote sensing imagery are typically irregular and complex. Scribble-based interactive form, which can accurately represent the shape or semantic features of the changed regions, thus it is quite suitable for change detection tasks in remote sensing imagery. Therefore, we proposed a novel interactive deep learning model called ScribbleCDNet in this manuscript, which pioneered the use of scribble as an interactive form for detecting change in bi-temporal high-resolution remote sensing imageries. Compared with the widely used clicks-based interactive deep learning models, the proposed ScribbleCDNet acquired superior results on four open-sourced change detection datasets. Last but not least, we also developed an interactive change detection tool with a user-friendly graphical interface, and it can aid researchers in conducting change detection or generating training samples conveniently. Moreover, the proposed ScribbleCDNet can also inspire researchers to develop other interactive deep-learning models related to semantic segmentation, landcover classification, or object extraction in highresolution remote sensing imageries.",Change detection,Interactive deep learning,Scribble interaction,High-resolution remote sensing imagery,,,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,
Row_1540,"Zhao, Chunhui","Shen, Yi","Su, Nan","Yan, Yiming","Feng, Shou",A Label Correction Learning Framework for Gully Erosion Extraction Using High-Resolution Remote Sensing Images and Noisy Labels,,2024,1,"Rapid and reliable gully erosion (GE) extraction from high-resolution remote sensing (HRRS) images is crucial for the development of land protection measures. For this task, semantic segmentation methods are widely considered the state-of-the-art solutions. Nevertheless, providing sufficient and clean training labels for segmentation models demands substantial expenses and time investment. In this context, a label correction learning (LCL) framework is proposed to effectively extract GE from HRRS images by leveraging more readily available noisy labels. The core objective of this framework is to suppress the adverse impact of noise within noisy labels on the model performance. To achieve this, we introduce three key components in the framework, including an adaptive correction loss function, a multitree refinement module, and a noise correction module. These components collaborate to rectify noisy labels during training, thereby providing the model with a training set containing less noise. To validate the effectiveness of the LCL framework, three severely eroded regions in Northeast China are selected as study areas and corresponding noisy datasets are generated. Extensive experiments on these datasets demonstrate that our framework can significantly mitigate the negative influence of label noise and ultimately achieve superior GE extraction performance. Moreover, by employing the proposed framework, we generate GE coverage maps for the study areas and obtain measurements of gully area and length that are very close to the true statistics. Such a framework that can effectively learn from noisy labels holds promise as a practical and cost-efficient means to provide reliable data references for land resource protection.",Gully erosion (GE) extraction,high-resolution remote sensing images,land resource protection,noisy labels,semantic segmentation,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Xiang, Wei",,,,,"Liu, Yong","Zhao, Tianhao",,,,,
Row_1541,"Bhandari, Kiran Ashok","Ramchandra, Manthalkar R.",,,,A New Watershed Segmentation (NWS) and Particle Swarm Optimization (PSO-SVM) Techniques in Remote Sensing Image Retrieval,"2014 3RD INTERNATIONAL CONFERENCE ON RELIABILITY, INFOCOM TECHNOLOGIES AND OPTIMIZATION (ICRITO) (TRENDS AND FUTURE DIRECTIONS)",2014,0,"In this paper, the latest watershed segmentation process is actually found in the object feature extraction process. In the proposed method, initially the actual visual features are usually taken from the images using the spatial spectral heterogeneity method. Afterwards, the object features are usually taken from the new watershed segmentation method in which segmented objects are usually grouped with the PSO-SVM method. With PSO-SVM, the actual SVM parameters are usually optimized to achieve higher classification accuracy. Then similar scene images from the data base are usually taken from the SS modelling. A further variety of remote sensing images are utilized in the overall performance analysis process. The particular implementation benefits show the effectiveness of proposed new watershed segmentation method in RSIR and the reached advancement in sensitivity and also recall measures. Moreover, the actual overall performance of the proposed technique is actually considered by comparing with all the existing RSIR and the typical SBRSIR methods.",Remote Sensing Image Retrieval (RSIR),Particle Swarm Optimization (PSO),Support Vector Machine (SVM),Scene Semantic (SS),,,,,,,,,,,,,,,,
Row_1542,"Wang, Shuyang","Mu, Xiaodong","Yang, Dongfang","He, Hao","Zhao, Peng",Road Extraction from Remote Sensing Images Using the Inner Convolution Integrated Encoder-Decoder Network and Directional Conditional Random Fields,,FEB 2021,29,"Road extraction from remote sensing images is of great significance to urban planning, navigation, disaster assessment, and other applications. Although deep neural networks have shown a strong ability in road extraction, it remains a challenging task due to complex circumstances and factors such as occlusion. To improve the accuracy and connectivity of road extraction, we propose an inner convolution integrated encoder-decoder network with the post-processing of directional conditional random fields. Firstly, we design an inner convolutional network which can propagate information slice-by-slice within feature maps, thus enhancing the learning of road topology and linear features. Additionally, we present the directional conditional random fields to improve the quality of the extracted road by adding the direction of roads to the energy function of the conditional random fields. The experimental results on the Massachusetts road dataset show that the proposed approach achieves high-quality segmentation results, with the F1-score of 84.6%, which outperforms other comparable ""state-of-the-art"" approaches. The visualization results prove that the proposed approach is able to effectively extract roads from remote sensing images and can solve the road connectivity problem produced by occlusions to some extent.",remote sensing,semantic segmentation,encoder-decoder network,inner convolution,directional conditional random fields,,,REMOTE SENSING,,,,,,,,,,,,
Row_1543,"Csönde, G","Sekimoto, Y","Kashiyama, T",,,Crowd Counting with Semantic Scene Segmentation in Helicopter Footage,,SEP 2020,3,"Continually improving crowd counting neural networks have been developed in recent years. The accuracy of these networks has reached such high levels that further improvement is becoming very difficult. However, this high accuracy lacks deeper semantic information, such as social roles (e.g., student, company worker, or police officer) or location-based roles (e.g., pedestrian, tenant, or construction worker). Some of these can be learned from the same set of features as the human nature of an entity, whereas others require wider contextual information from the human surroundings. The primary end-goal of developing recognition software is to involve them in autonomous decision-making systems. Therefore, it must be foolproof, which is, it must have good semantic understanding of the input. In this study, we focus on counting pedestrians in helicopter footage and introduce a dataset created from helicopter videos for this purpose. We use semantic segmentation to extract the required additional contextual information from the surroundings of an entity. We demonstrate that it is possible to increase the pedestrian counting accuracy in this manner. Furthermore, we show that crowd counting and semantic segmentation can be simultaneously achieved, with comparable or even improved accuracy, by using the same crowd counting neural network for both tasks through hard parameter sharing. The presented method is generic and it can be applied to arbitrary crowd density estimation methods. A link to the dataset is available at the end of the paper.",,,,,,,,SENSORS,,,,,,,,,,,,
Row_1544,"Yu, Weikang","Xu, Yonghao","Ghamisi, Pedram",,,Universal adversarial defense in remote sensing based on pre-trained denoising diffusion models,,SEP 2024,0,"Deep neural networks (DNNs) have risen to prominence as key solutions in numerous AI applications for earth observation (AI4EO). However, their susceptibility to adversarial examples poses a critical challenge, compromising the reliability of AI4EO algorithms. This paper presents a novel Universal Adversarial Defense approach in Remote Sensing Imagery (UAD-RS), leveraging pre-trained diffusion models to protect DNNs against various adversarial examples exhibiting heterogeneous adversarial patterns. Specifically, a universal adversarial purification framework is developed utilizing pre-trained diffusion models to mitigate adversarial perturbations through the introduction of Gaussian noise and subsequent purification of the perturbations from adversarial examples. Additionally, an Adaptive Noise Level Selection (ANLS) mechanism is introduced to determine the optimal noise level for the purification framework with a task-guided Fr & eacute;chet Inception Distance (FID) ranking strategy, thereby enhancing purification performance. Consequently, only a single pre-trained diffusion model is required for purifying various adversarial examples with heterogeneous adversarial patterns across each dataset, significantly reducing training efforts for multiple attack settings while maintaining high performance without prior knowledge of adversarial perturbations. Experimental results on four heterogeneous RS datasets, focusing on scene classification and semantic segmentation, demonstrate that UAD-RS outperforms state-of-the-art adversarial purification approaches, providing universal defense against seven commonly encountered adversarial perturbations. Codes and the pre-trained models are available online (https://github. com/EricYu97/UAD-RS).",Adversarial defense,Adversarial examples,Diffusion models,Remote sensing,Scene classification,Semantic segmentation,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,
Row_1545,"Xu, Leilei","Yang, Peng","Yu, Juanjuan","Peng, Fei","Xu, Jia",Extraction of cropland field parcels with high resolution remote sensing using multi-task learning,,DEC 31 2023,11,"Parcel-level farmland information contains rich spatial distribution and boundary details, which is crucial for digital agriculture and agricultural resource surveys. However, the spatial complexity and heterogeneity of features resulting from high resolution makes it difficult to obtain parcel-level information quickly and accurately. In addition, existing methods do not sufficiently take into account the spatial topological information, particularly for blurred boundaries. Here, we develop a multi-task network model to extract plot-level cropland information. Specifically, the model consists of a cascaded multi-task network with integrated semantic and edge detection, a refinement network with fixed edge local connectivity, and an integrated fusion model. To validate the performance of the model, two typical tests were conducted in Denmark (Europe) and Chongqing (Asia) with high-resolution remote sensing images provided by Sentinel-2 (10 m) and Google Earth (0.53 m) as data sources. The results show that our proposed model outperforms other baseline models and exhibits higher performance. This study is expected to provide important support for the design of new global agricultural information management systems in the future.",High-resolution image,semantic segmentation,edge detection and repair,cropland-parcel extraction,multi-task learning,,,EUROPEAN JOURNAL OF REMOTE SENSING,"Song, Shiran",,,,,"Wu, Yongxing",,,,,,
Row_1546,"Su, Lihong","Huang, Yuxia","Hu, Zhiyong",,,AN EMPIRICAL STUDY ON FULLY CONVOLUTIONAL NETWORK AND HYPERCOLUMN METHODS FOR UAV REMOTE SENSING IMAGERY CLASSIFICATION,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,1,"Fully Convolutional Network (FCN), which can adopt various Convolutional Neural Networks (CNN), are now increasingly being used in remote sensing communities. CNN are improved constantly either in accuracy or by reducing parameters for a given equivalent accuracy. This paper investigates five widely used CNNs (AlexNet, VGG16, ResNet, SqueezeNet, and a pruned VGG16) in the context of FCN for coastal beach classification of imagery acquired by Unmanned Aerial Vehicles (UAV). Our experiments show that (1) not every CNN is suitable to FCN for semantic segmentations of images though each CNN approximately achieved an equivalent accuracy for image labeling; (2) band reduced pruning of existing CNN has the least impact on implementation and accuracy. To examine the capability of convolutional layers capturing semantic features, this paper also carries out beach classification experiments using hypercolumn methods with VGG16.",Convolutional Neural Networks,Fully Convolutional Network,hypercolumn,coastal remote sensing,classification,UAV,,,,,,,,,,,,,,
Row_1547,"Dong, Hongxiang","Yi, An","Xie, Lirong","Yang, Zhiyong","Kai, Zhang",Semantic Segmentation of Large-Scale Laser Point Cloud in Mines Based on Local Feature Enhancement,,SEP 2024,0,"Objective To improve the efficiency of coal mining and ensure production safety, the construction of intelligent coal mines is being vigorously promoted. With the rapid development of three-dimensional (3D) ranging technology, lidar can acquire high-density 3D laser point clouds in a short time. Semantic segmentation of 3D laser point cloud data can provide accurate environment perception for unmanned mining trucks and realize autonomous operation of those trucks. Point cloud data are characterized by disorder, unstructuredness, and sparsity, which brings difficulties to point cloud data processing and data analysis. To fully learn the local geometric features and the contextual information of mining point clouds and improve the accuracy of semantic segmentation, we propose a new local feature enhancement-based semantic segmentation network called LFE-Net for large-scale laser point clouds in mines. Methods The LFE-Net input is Nx6 mining point cloud. Each point goes through a shared multilayer perceptron (MLP) to obtain eight dimensions as inputs to the encoder-decoder. The encoder-decoder consists of five encoding layers and five decoding layers. Each encoding layer consists of a dilated residual block and a random downsampling operation. The dilated residual block consists of two local feature extraction modules and two local feature aggregation modules, and it is used to enlarge the receptive field of each point. The local feature extraction module learns the spatial position information and surface orientation change information of a given point and its neighborhood points, and utilizes the spatial distance weights to enhance the semantic features of neighborhood points. The local feature aggregation module utilizes mixed pooling to aggregate the features of neighborhood points. Subsequently, we downsample the point cloud and reserve 1/4 of the point cloud for each downsampling. Each decoding layer contains an upsampling operation and an MLP. The decoding layers use upsampling operation to continuously restore the spatial resolution of point cloud features and connect with the intermediate features generated by the encoding layers through skip connections. Finally, the semantic segmentation results are output through three fully connected layers and a dropout layer. Moreover, to alleviate the problem of sample imbalance, a weighted cross-entropy loss function is adopted to make the network pay more attention to small sample classes to improve the accuracy of semantic segmentation. Results and Discussions To conduct experiments on semantic segmentation of the mining point clouds, we utilize a mining truck equipped with lidar sensors to collect laser point cloud data from the open pit mine. We add real semantic labels to the collected point cloud data and create a 3D laser point cloud semantic segmentation dataset of the mine. To fully evaluate the effectiveness of the LFE-Net, we compare its experimental results with other large-scale point cloud semantic segmentation methods by using the mining dataset. The overall accuracy (OA) and the mean intersection over union (mIoU) of the LFE-Net are 97.92% and 87.578%, respectively, which are higher than those of other methods. Additionally, we conduct ablation experiments on the local feature extraction module and the local feature aggregation module to verify the effectiveness of each module.Conclusion To fully learn the local geometric features and the contextual information of mining point clouds and improve the accuracy of semantic segmentation, we propose a new local feature enhancement-based semantic segmentation network LFE-Net for large-scale laser point clouds in mines. The main technical contributions of this paper are given as follows. 1) To solve the problem of difficulty in extracting the features of mining trucks, we propose a spatial position encoding unit in this paper. This unit encodes spatial position information of the point clouds and utilizes spatial distance weights to enhance the semantic features of neighborhood points to improve the segmentation accuracy of the algorithm. 2) To solve the problem of difficulty in extracting ground edge features, we propose a spatial normal vector encoding unit. By adding normal vector information, the network can enhance the perception ability of geometric structure features and improve the accuracy of ground edge segmentation. 3) To solve the problem of feature loss caused by max pooling, we propose a mixed pooling. The mixed pooling consists of max pooling and attention pooling, which enriches local features. The LFE-Net is tested on the mining dataset and excellent performance is achieved, with an OA of 97.92% and an mIoU of 87.578%. These experimental results validate the effectiveness of LFE-Net. The network proposed in this paper provides a theoretical basis for the application of unmanned mining trucks in open pit mines, which has significant implications for the unmanned operation of mining trucks.",remote sensing,three-dimensional laser point cloud,lidar,semantic segmentation,feature coding,,,CHINESE JOURNAL OF LASERS-ZHONGGUO JIGUANG,,,,,,,,,,,,
Row_1548,"Zhou, Xuanyu","Zhou, Lifan","Zhang, Haizhen","Ji, Wei","Zhou, Bei",Local-Global Multiscale Fusion Network for Semantic Segmentation of Buildings in SAR Imagery,,2024,0,"The extraction of buildings from synthetic aperture radar (SAR) images poses a challenging task in the realm of remote sensing (RS). In recent years, convolutional neural networks (CNNs) have rapidly advanced and found application in the field of RS. Researchers have investigated the potential of CNNs for the semantic segmentation of SAR images, bringing excellent improvements. However, the semantic segmentation of buildings in SAR images still encounters challenges due to the high similarity between features of ground objects and buildings in SAR images, as well as the variability in building structures. In this article, we propose the local-global multiscale fusion network (LGMFNet), based on a dual encoder-decoder structure, for the semantic segmentation of buildings in SAR images. The proposed LGMFNet introduces an auxiliary encoder with a transformer structure to address the limitation of using the main encoder with a CNN structure for global modeling. To embed global dependencies hierarchically into the CNN, we designed the global-local semantic aggregation module (GLSM). The GLSM serves as a bridge between the dual encoders to achieve semantic guidance and coupling from the local to the global level. Furthermore, to bridge the semantic gap between different scales, we designed the multiscale feature fusion network (MSFN) as the decoder. MSFN achieves the interactive fusion of semantic information between various scales by constructing the multiscale feature fusion module. Experimental results demonstrate that the proposed LGMFNet achieves the mIoU of 91.17% on the BIGSARDATA 2023 AISAR competition dataset, outperforming the second-best method by a margin of 0.78%. This evidences the superiority of LGMFNet in comparison to other state-of-the-art methods.",Radar polarimetry,Semantics,Semantic segmentation,Buildings,Transformers,Feature extraction,Task analysis,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Convolutional neural networks (CNNs),dual encoder-decoder,semantic segmentation of buildings,synthetic aperture radar (SAR) images,,,transformer,,,,
Row_1549,"Bai, Yu","Zhao, Yu","Shao, Yajing","Zhang, Xinrong","Yuan, Xuefeng",Deep learning in different remote sensing image categories and applications: status and prospects,,MAR 4 2022,21,"In recent years, the combination of deep learning and remote sensing has been a boiling state. However, because of the difference between remote sensing images and natural images, it is still an open question of whether deep learning methods can revolutionize remote sensing field. This work provides a brief review of 2674 related papers in deep learning with remote sensing (DL-RS) from 2014 to 2020. Keywords, publication years, journals, countries, and other essential characteristics of the papers were extracted. Also, we had set up some data items for information collection, such as remote sensing image categories, remote sensing applications, commonly used public datasets, and basic deep learning models. Our analysis shows that the number of research articles in DL-RS is still exploding, growing exponentially every year, and that DL methods have been applied to virtually all types of images and all applications of remote sensing. CNNs continue to be the most used deep learning model, accounting for 70% of all articles, GANs has now turned out to be the most used model after CNNs. Finally, we make some recommendations for future studies for the development of the DL-RS field.",deep learning,remote sensing image categories,remote sensing applications,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,
Row_1550,"Ye, Xiaoling","Zhang, Hu","Sheng, Tao","Zhang, Yingchao","Zou, Ruilin",Extraction network of forests and lakes along the railway based on remote sensing images,,JAN 2 2024,1,"Foreign object debris (FOD) intrusion is a common problem along high-speed railway lines, usually caused by the entry of foreign objects into the railway infrastructure during high winds. Although the capability to address FOD incidents has improved significantly, it is still necessary to predict environmental conditions along the railway lines beforehand to prevent FOD incidents. In this paper, we propose a novel dual-branch feature extraction network that takes into account the frequently occurring landforms of lakes and forests along the railway lines to improve the recognition rate of these regions and provide theoretical support for FOD prevention. The network adopts a dual-branch structure, introducing multi-branch residual weighted module, context feature refinement module (CFR), and high-low feature fusion module (HLFF) to enhance feature extraction, refine contextual information, and boundary information, thus improving the model's segmentation performance. Experiments are conducted on our self-built forest-lake dataset, a railway dataset, and a publicly available Aerial Imagery Dataset. The results show that the proposed method achieves MIOU scores of 84.77%, 86.73% and 94.09% on these three datasets, respectively, indicating strong segmentation performance for forests and lakes and good generalization performance on other datasets.",Foreign object debris,railway safety,remote sensing imagery,semantic segmentation,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,"Xiong, Xiong",,,,,,,,,,,
