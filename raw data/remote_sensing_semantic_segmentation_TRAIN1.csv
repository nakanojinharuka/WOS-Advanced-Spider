,author1,author2,author3,author4,title,conference,publish time,citation,abstract,keyword1,keyword2,keyword3,keyword4,keyword5,author5,author6,author7,author8,journal/book,author9,keyword6,keyword7,keyword8,keyword9,keyword10,keyword11,keyword12,keyword13,keyword14,author10,author11,author12,author13,author14,author15,author16,author17,author18,author19,author20,author21,keyword15,keyword16,keyword17,keyword18,keyword19,keyword20,keyword21,keyword22,keyword23
Row_1,"Ayala, C.","Sesma, R.","Aranda, C.","Galar, M.",DIFFUSION MODELS FOR REMOTE SENSING IMAGERY SEMANTIC SEGMENTATION,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,6,"Denoising Diffusion Probabilistic Models have exhibited impressive performance for generative modelling of images. This paper aims to explore the potential of diffusion models for semantic segmentation tasks in the context of remote sensing. The major challenge of employing these models for semantic segmentation tasks is the generative nature of the model, which produces an arbitrary segmentation mask from a random noise input. Therefore, the diffusion process needs to be constrained to produce a segmentation mask that matches the target image. To address this issue, the denoising process is conditioned by utilizing the input image as a reference. In the experimental study, the proposed model is compared against other state-of-the-art semantic segmentation architectures using the Massachusetts Buildings Aerial dataset. The results of this study provide valuable insights into the potential of diffusion models for semantic segmentation tasks in the field of remote sensing.",Denoising Diffusion Probabilistic Models,Semantic Segmentation,Remote Sensing,Building Segmentation,Uncertainty Estimation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_2,"Tao, Chongxin","Meng, Yizhuo","Li, Junjie","Yang, Beibei",MSNet: multispectral semantic segmentation network for remote sensing images,,DEC 31 2022,19,"In the research of automatic interpretation of remote sensing images, semantic segmentation based on deep convolutional neural networks has been rapidly developed and applied, and the feature segmentation accuracy and network model generalization ability have been gradually improved. However, most of the network designs are mainly oriented to the three visible RGB bands of remote sensing images, aiming to be able to directly borrow the mature natural image semantic segmentation networks and pre-trained models, but simultaneously causing the waste and loss of spectral information in the invisible light bands such as near-infrared (NIR) of remote sensing images. Combining the advantages of multispectral data in distinguishing typical features such as water and vegetation, we propose a novel deep neural network structure called the multispectral semantic segmentation network (MSNet) for semantic segmentation of multi-classified feature scenes. The multispectral remote sensing image bands are split into two groups, visible and invisible, and ResNet-50 is used for feature extraction in both coding stages, and cascaded upsampling is used to recover feature map resolution in the decoding stage, and the multi-scale image features and spectral features from the upsampling process are fused layer by layer using the feature pyramid structure to finally obtain semantic segmentation results. The training and validation results on two publicly available datasets show that MSNet has competitive performance. The code is available: https://github.com/taochx/MSNet.",Multispectral remote sensing images,spectral feature,feature fusion,semantic segmentation,,"Hu, Fengmin","Li, Yuanxi","Cui, Changlu","Zhang, Wen",GISCIENCE & REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_3,"Hua, Wenyi","Liu, Jia","Liu, Fang","Zhang, Wenhua",STAIR FUSION NETWORK FOR REMOTE SENSING IMAGE SEMANTIC SEGMENTATION,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"Semantic segmentation of very high spatial resolution remote sensing images plays a vital role in many fields, such as land resource management, urban planning, and biosphere monitoring. Due to the scare variance between different types of regions, it is important to fully utilize multi-scale features. Moreover, with the complexity of some ground objects, global semantic information should be specially considered. As a consequence, in this paper, we propose a stair fusion network to further refine and fuse low-level and high-level features. In addition, we propose a global information enhancement module (GIEM) to extract global semantic information from the high-level features and reduce the length of delivery chain from them to the final results via a skip connection. Experimental results demonstrate the effectiveness of our model.",semantic segmentation,feature fusion,remote sensing,,,"An, Jiaqi",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_4,"Gao, Liang","Qian, Yurong","Liu, Hui","Zhong, Xiwu",SRANet: semantic relation aware network for semantic segmentation of remote sensing images,,JAN 1 2022,5,"Remote sensing images contain complex feature information, and traditional convolutional networks cannot effectively model the contextual relationships. To address this problem, we propose a semantic segmentation network for remote sensing images based on semantic relationship aware. We construct the semantic relationship aware module to obtain the global semantic information of remote sensing images by self-attention. In addition, the separable space convergence pyramid module was constructed to effectively utilize the feature information in the high-level feature maps. By separable convolution with different dilation rates, the network can acquire multiscale semantic information. Our semantic relation aware network (SRANet) improves the overall accuracy by 0.33% over the benchmark network in the Vaihingen dataset and by 0.42% in the Potsdam dataset. The class activation maps show that the SRANet has ideal activation responses for targets at different scales in images. Furthermore, our SRANet can produce competitive segmentation performance compared with other state-of-the-art segmentation networks. (C) 2022 Society of Photo-Optical Instrumentation Engineers (SPIE)",self attention,remote sensing image,semantic segmentation,context information,,"Xiao, Zhengqing",,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_5,"Wei, Xin","Guo, Yajing","Gao, Xin","Yan, Menglong",A NEW SEMANTIC SEGMENTATION MODEL FOR REMOTE SENSING IMAGES,2017 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),2017,12,"Semantic segmentation for remote sensing images is a critical process in the workflow of object-based image analysis. Recently, convolutional neural networks(CNNs) are powerful visual models that yield hierarchies of features. In this paper, we propose a deep convolutional encoder-decoder model for remote sensing images segmentation. Specifically, we rely on the encoder network to extract the high-level semantic feature of ultra-high resolution images and the decoder network is employed to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise labeling. Also the fully connected conditional random field (CRF) is integrated into the model so that the network can be trained end-to-end. Experiments on the Vaihingen dataset demonstrate that our model can make promising performance.",Convolutional neural networks,conditional random field,semantic segmentation,remote sensing images,,"Sun, Xian",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_6,"Xu, Zhe","Geng, Jie","Jiang, Wen",,MMT: Mixed-Mask Transformer for Remote Sensing Image Semantic Segmentation,,2023,9,"Remote sensing image semantic segmentation is a crucial step in the intelligent interpretation of remote sensing. Most of the current approaches are based on the attention mechanism to enhance long-range representations. However, these works ignore the key problem of foreground-background imbalance, and their performances encounter a bottleneck. In this article, we introduce mask classification into remote sensing image interpretation for the first time and propose a novel mixed-mask Transformer (MMT) for remote sensing image semantic segmentation. Specifically, we propose a mixed-mask attention mechanism, a simple but effective module, which assists the network to learn more explicit intraclass and interclass correlations by capturing long-range interdependent representations. In addition, a progressive multiscale learning strategy (MSL) is proposed to solve the problem of large-scale-varied targets in remote sensing images, which integrates semantic and visual representations of different scale targets by efficiently utilizing large-scale feature maps in Transformer. Experimental results show that the proposed MMT exceeds the existing alternative approaches and achieves state-of-the-art performance on three semantic segmentation datasets.",Attention mechanism,foreground-background imbalance,remote sensing,semantic segmentation,Transformer,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_7,"Chen, Yuxing","Bruzzone, Lorenzo",,,TOWARD OPEN-WORLD SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"In this work, we address the challenge of open-world semantic segmentation for remote sensing (RS) images, which involves segmenting arbitrary objects in images using open RS data. Previous efforts in open-world segmentation mostly focus on Internet-scale paired image-text data with rich vocabulary of concepts. However, these works cannot be directly transferred to RS domain due to the lack of large-scale RS data-text pairs and the corresponding annotations. To overcome this limitation, we propose using text descriptions and annotations from OpenStreetMap as a source of supervision while using images from satellite images. We utilize a conditional Unet model to predict segmentation masks given a text description, and leverage the rich information contained in a pretrained CLIP model to align the images and the corresponding text embeddings using a contrastive loss. Our experimental results demonstrate the potential of open-world segmentation on open RS data.",Semantic Segmentation,Open-world,Open Remote Sensing Data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_8,"Liu, Yuheng","Mei, Shaohui","Zhang, Shun","Wang, Ye",SEMANTIC SEGMENTATION OF HIGH-RESOLUTION REMOTE SENSING IMAGES USING AN IMPROVED TRANSFORMER,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,5,"Semantic segmentation has been widely researched for high level analysis of High Spatial Resolution (HSR) remote sensing images, where Convolutional Neural Network (CNN) is the mainstream method. However, the transformer with attention mechanism has its unique capacity of extracting global information which is generally ignored by CNN models. In this paper, a Swin Transformer with UPer head (STUP) is proposed to tackle with semantic segmentation problem on a challenging remote sensing land-cover dataset called LoveDA, which owns complex background samples and inconsistent classes distributions. The proposed STUP combines the Swin Transformer with Uper Head in the form of an encoder-decoder structure, to extract features of HSR images for segmentation. Furthermore, Focal Loss is adopted to handle the unbalanced distribution problem in the training step. Experimental results demonstrate that the proposed STUP clearly outperforms several state-of-the-art models.",Remote Sensing,High Spatial Resolution,Transformer,Semantic Segmentation,,"He, Mingyi","Du, Qian",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_9,"Zhang, Changxing","Bai, Xiangyu","Wang, Dapeng","Zhou, KeXin",Context Aggregation Network for Remote Sensing Image Semantic Segmentation,,SEP 2024,0,"In recent years, remote sensing technology has been widely applied in various industries, and semantic segmentation of remote sensing images has attracted much attention. Due to the complexity and special characteristics of remote sensing images, multi-scale object detection and accurate object localization are important challenges in remote sensing image semantic segmentation. Therefore, this paper proposes a context aggregation network (CANet). The design of CANet is influenced by advanced technologies such as attention mechanisms and feature fusion and enhancement. This network first introduces nested dilated residual module (NDRM), which can fully utilize the features extracted by the backbone network. Then, improved integrated successive dilation module (IISD) is proposed to effectively aggregate a series of contextual information scales. Next, Swim Transformer module is embedded to provide global contextual information. Finally, multi-resolution fusion module (MRFM) is proposed, allowing the comprehensive fusion of feature layers from different stages of the encoder, preserving more semantic and detailed information. The experimental results show that CANet outperforms other advanced models on the Potsdam and Vaihingen datasets.",Remote sensing,semantic segmentation,context,swin transformer,,,,,,INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_10,"Liu, Shuo","Ding, Wenrui","Liu, Chunhui","Liu, Yu",ERN: Edge Loss Reinforced Semantic Segmentation Network for Remote Sensing Images,,SEP 2018,70,"The semantic segmentation of remote sensing images faces two major challenges: high inter-class similarity and interference from ubiquitous shadows. In order to address these issues, we develop a novel edge loss reinforced semantic segmentation network (ERN) that leverages the spatial boundary context to reduce the semantic ambiguity. The main contributions of this paper are as follows: (1) we propose a novel end-to-end semantic segmentation network for remote sensing, which involves multiple weighted edge supervisions to retain spatial boundary information; (2) the main representations of the network are shared between the edge loss reinforced structures and semantic segmentation, which means that the ERN simultaneously achieves semantic segmentation and edge detection without significantly increasing the model complexity; and (3) we explore and discuss different ERN schemes to guide the design of future networks. Extensive experimental results on two remote sensing datasets demonstrate the effectiveness of our approach both in quantitative and qualitative evaluation. Specifically, the semantic segmentation performance in shadow-affected regions is significantly improved.",CNN,deep learning,edge loss reinforced network,remote sensing,semantic segmentation,"Wang, Yufeng","Li, Hongguang",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_11,"Feng, Mingzhe","Sun, Xin","Dong, Junyu","Zhao, Haoran",Gaussian Dynamic Convolution for Semantic Segmentation in Remote Sensing Images,,NOV 2022,4,"Different scales of the objects pose a great challenge for the segmentation of remote sensing images of special scenes. This paper focuses on the problem of large-scale variations of the target objects via a dynamical receptive field of the deep network. We construct a Gaussian dynamic convolution network by introducing a dynamic convolution layer to enhance remote sensing image understanding. Moreover, we propose a new Gaussian pyramid pooling (GPP) for multi-scale object segmentation. The proposed network can expand the size of the receptive field and improve its efficiency in aggregating contextual information. Experiments verify that our method outperforms the popular semantic segmentation methods on large remote sensing image datasets, including iSAID and LoveDA. Moreover, we conduct experiments to demonstrate that the Gaussian dynamic convolution works more effectively on remote sensing images than other convolutional layers.",remote sensing,semantic segmentation,deep learning,convolutional neural networks,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_12,"Liu, Keming","Liu, Fang","Liu, Jia","Xiao, Liang",UNSUPERVISED DOMAIN ADAPTION FOR REMOTE SENSING SEMANTIC SEGMENTATION WITH SELF-ATTENTION MECHANISM,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"The domain shift between the source and target domains limits the performance of traditional convolutional neural networks (CNNs) for feature extraction in remote sensing tasks. We propose an image translation network that uses generative adversarial networks (GANs) to transfer spectral distributions from training to test data, enhancing cross-domain semantic segmentation. Our approach fine-tunes the DeepLab-V3 framework on synthetic training data generated by the proposed network. Experimental results show improved performance in cross-domain semantic segmentation tasks for remote sensing images.",Domain adaption,remote sensing,semantic segmentation,image translation,generative adversarial networks,"Tang, Xu",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_13,"Zhang, Xiaoqin","Xiao, Zhiheng","Li, Dongyang","Fan, Mingyu",Semantic Segmentation of Remote Sensing Images Using Multiscale Decoding Network,,SEP 2019,27,"In this letter, we propose a practical convolutional neural network architecture for semantic pixelwise segmentation of remote sensing images, named Multiscale Decoding Network. The proposed method is built on the success of fully convolutional networks (FCNs) and the transfer of pretrained networks. The decoding network of our architecture utilizes the combination of three paths, namely, unpooling path, transposed convolution path, and dilated convolution path, in the form of an inception module. The whole network is trained in the end-to-end manner and the parameters of the three paths are learned automatically. Since the proposed method transfers the feature of pretrained networks and has three simplified decoding paths with fewer parameters, it requires less training data and training time. Compared with the classical networks FCN, SegNet, and U-net, our network shows better performance on remote sensing images segmentation.",Dilated convolution,remote sensing,semantic segmentation,,,"Zhao, Li",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_14,"Dimitrovski, Ivica","Spasev, Vlatko","Loshkovska, Suzana","Kitanovski, Ivan",U-Net Ensemble for Enhanced Semantic Segmentation in Remote Sensing Imagery,,JUN 2024,3,"Semantic segmentation of remote sensing imagery stands as a fundamental task within the domains of both remote sensing and computer vision. Its objective is to generate a comprehensive pixel-wise segmentation map of an image, assigning a specific label to each pixel. This facilitates in-depth analysis and comprehension of the Earth's surface. In this paper, we propose an approach for enhancing semantic segmentation performance by employing an ensemble of U-Net models with three different backbone networks: Multi-Axis Vision Transformer, ConvFormer, and EfficientNet. The final segmentation maps are generated through a geometric mean ensemble method, leveraging the diverse representations learned by each backbone network. The effectiveness of the base U-Net models and the proposed ensemble is evaluated on multiple datasets commonly used for semantic segmentation tasks in remote sensing imagery, including LandCover.ai, LoveDA, INRIA, UAVid, and ISPRS Potsdam datasets. Our experimental results demonstrate that the proposed approach achieves state-of-the-art performance, showcasing its effectiveness and robustness in accurately capturing the semantic information embedded within remote sensing images.",remote sensing imagery,U-Net,ensemble learning,semantic segmentation,land cover,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_15,"He, Chu","Li, Shenglin","Xiong, Dehui","Fang, Peizhang",Remote Sensing Image Semantic Segmentation Based on Edge Information Guidance,,MAY 2020,53,"Semantic segmentation is an important field for automatic processing of remote sensing image data. Existing algorithms based on Convolution Neural Network (CNN) have made rapid progress, especially the Fully Convolution Network (FCN). However, problems still exist when directly inputting remote sensing images to FCN because the segmentation result of FCN is not fine enough, and it lacks guidance for prior knowledge. To obtain more accurate segmentation results, this paper introduces edge information as prior knowledge into FCN to revise the segmentation results. Specifically, the Edge-FCN network is proposed in this paper, which uses the edge information detected by Holistically Nested Edge Detection (HED) network to correct the FCN segmentation results. The experiment results on ESAR dataset and GID dataset demonstrate the validity of Edge-FCN.",remote sensing image,semantic segmentation,edge information,Edge-FCN,,"Liao, Mingsheng",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_16,Shi Fangxing,Zhou Line,Zhu Daming,Fu Zhitao,DSNet-Based Remote Sensing Image Semantic Segmentation Method,,MAR 2023,0,"In view of the problems that the traditional neural network model tends to ignore difficult samples due to the unbalanced classification of remote sensing image semantic segmentation data, and the reasoning results are hollow and the segmentation accuracy decreases, a drill-shaped neural network semantic segmentation method is proposed. First, a new bridge module is defined to fuse the shallow and deep feature information, thus more building details can be captured by the network; second, in the deep learning segmentation model training, the multi loss function is used to improve the extraction of difficult sample information; finally, to balance the differences of category training, the feature information is extracted from remote sensing images at multiple levels, and the segmentation accuracy is improved. The experimental results show that the average intersection to union ratio of the proposed method reaches 0. 849, the building missing rate and wrong recognition rate are less, and the segmentation accuracy is improved compared with the existing methods.",remote sensing,deep learning,semantic segmentation,category imbalance,DSNet,,,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_17,"Sun, Deyan","Chen, Wei","Liu, Hai","Chen, Dufeng",Attention Dual Adversarial Remote Sensing Image Semantic Segmentation,,2024,0,"Existing semi-supervised remote sensing image semantic segmentation methods neglect to improve the stability of the adversarial network, so that the adversarial network cannot be effectively used to assist segmentation network training, which limits the further improvement of semantic segmentation accuracy. To this end, this paper first introduces a dual confrontation network, and plays a three-way game with the generator to make the network converge as soon as possible, and combines the vertical and cross attention network to propose an attention dual confrontational semantic segmentation model for remote sensing images. The model can not only use dual confrontation training to improve the stability of the network, but also use the global context relationship of pixels to predict by introducing an attention mechanism, thereby improving the accuracy of remote sensing image semantic segmentation. The experimental results on the public remote sensing data set show that the MIOU of this method on US2D dataset reached 69.65%, which are higher than the existing fully-supervised and semi-supervised methods. The effectiveness of the method proposed in this paper is verified.",Remote Sensing Image,Dual Adversarial Networks,Cross Attention,Semantic Segmentation,Semi-supervised Learning,"Wang, Zehua","Wu, Yuliang","Xu, Tingting","Zhu, Pengcheng","ADVANCED INTELLIGENT COMPUTING TECHNOLOGY AND APPLICATIONS, PT I, ICIC 2024","Wang, Jiaqi",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_18,"Dong, Zhe","Liu, Tianzhu","Gu, Yanfeng",,Spatial and Semantic Consistency Contrastive Learning for Self-Supervised Semantic Segmentation of Remote Sensing Images,,2023,11,"A critical requirement for the success of supervised deep learning lies in having numerous annotated images, which is often challenging to fulfill in remote sensing semantic segmentation tasks. Self-supervised contrastive learning (CL) offers a strategy for learning general feature representations by pretraining neural networks on vast amounts of unlabeled data and subsequently fine-tuning them on downstream tasks with limited annotations. However, the vast majority of CL methods are designed based on instance discriminative pretext tasks, focusing solely on learning the global representation of the entire image while disregarding the essential spatial and semantic correlations crucial for semantic segmentation tasks. To address the above issues, in this article, we propose a spatial and semantic consistency CL (SSCCL) framework for the semantic segmentation task of remote sensing images. Specifically, a consistency branch in SSCCL is designed to learn feature representations with spatial and semantic consistency by maximizing the similarity of the overlapping regions of the two augmented views. In addition, an instance branch is introduced to learn global representations by enforcing the similarity of two augmented views from one image. Through the integration of the consistency branch and instance branch, the proposed SSCCL framework can learn robust and informative feature representations for semantic segmentation in remote sensing scenarios. The proposed method was evaluated on three publicly available remote sensing semantic segmentation datasets, and the experiment results show that our method achieves superior segmentation performance with limited annotations compared to state-of-the-art CL methods as well as the ImageNet pretraining method.",Contrastive learning (CL),remote sensing images,self-supervised,semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_19,"Zhang, Junxi","Chen, Zhenzhong","Liu, Shan",,Remote Sensing Image Coding for Machines on Semantic Segmentation via Contrastive Learning,,2024,0,"Due to the huge data volume of high-resolution remote sensing imagery (RSI) and limited transmission bandwidth, RSIs are typically compressed for efficient transmission and storage. However, most of the existing compression algorithms are developed based on optimizing for the human perceptual that are not suitable for remote sensing image applications where RSIs are usually used for machine interpretation tasks, such as semantic segmentation for ground-object recognition. In this article, we propose an image coding for machines (ICMs) paradigm based on contrastive learning in a fully supervised manner to boost semantic segmentation of compressed RSIs. Specifically, we build an end-to-end compression framework to make full use of the global semantic information by clustering intracategory projected embeddings and spacing intercategory embeddings apart, to compensate for the loss of feature discriminability during the compression process and reconstruct the decision boundaries between different categories. Compared to the state-of-the-art image compression methods, our proposed method significantly improves the performance of semantic segmentation on the remote sensing labeling benchmark datasets.",Image coding,Remote sensing,Semantic segmentation,Feature extraction,Contrastive learning,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Object segmentation,Transform coding,Standards,Codecs,Bit rate,image coding for machines (ICMs),remote sensing interpretation,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,
Row_20,"Ao, Wei","Zheng, Shunyi","Meng, Yan",,SEMPNet: enhancing few-shot remote sensing image semantic segmentation through the integration of the segment anything model,,DEC 31 2024,0,"Few-shot semantic segmentation has attracted increasing attention due to its potential for low dependence on annotated samples. While extensively explored in the computer vision community, these techniques are primarily designed for natural images, resulting in limited generalization to remote sensing images. In contrast to the mostly individual and distinct objects presented in natural images, remote sensing images often feature clustered and regular patterns of objects. To bridge this gap, we propose a novel approach for few-shot remote sensing image semantic segmentation, which takes into account the specific characteristics of remote sensing imagery. Our approach introduces a mask classification pipeline, which initially extracts all independent objects within an image and subsequently assigns specific categories to each object guided by semantic information derived from few support images. To accomplish this, a robust mask extractor is imperative. Fortunately, the impressive segment anything model (SAM) possesses the potential to fulfill this role. Leveraging its remarkable zero-shot segmentation capabilities, we present the SAM-enhanced mask parsing network (SEMPNet), a novel few-shot remote sensing image semantic segmentation model. The method generates a set of masks for each image using SAM, transforming the segmentation task into a mask classification problem. To precisely classify each mask, we calculate pixel-wise correlations between each mask and the support features through cross-image position attention. Finally, a mask parsing module is utilized to decode the correlation maps and generate the segmentation results. The experiments on two remote sensing datasets testify the superiority of our method. Our code will be available at https://github.com/TinyAway/SEMPNet.",SAM,few-shot semantic segmentation,remote sensing,mask classification,,,,,,GISCIENCE & REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_21,"Kang, Xudong","Hong, Yintao","Duan, Puhong","Li, Shutao",Fusion of hierarchical class graphs for remote sensing semantic segmentation,,SEP 2024,3,"Semantic segmentation of remote sensing images aims to assign a specific label or class to each pixel in an image, which plays an extremely important role in scene understanding. Currently, many advanced deep learning -based semantic segmentation methods have been developed. However, these methods are always based on disjoint labels to identify ground objects while ignoring the correlation (e.g., semantic, shapes, materials, etc.) among different ground objects, which limits the segmentation performance of remote sensing images. To solve this issue, we propose a hierarchical class graph for semantic segmentation of high resolution remote sensing images, which can learn structured relation among different ground objects. Specifically, first, we construct hierarchical class graphs based on different attributes and layers. Then, a three -layer hierarchical segmentation framework is developed to learn the correlation among different ground objects. Finally, a decision fusion method is designed to fuse the benefits of different hierarchical attributes and layers. More importantly, the influence of different hierarchical class graphs on the segmentation performance is detailedly analyzed. Extensive experiments on the iSAID and Vaihingen datasets reveal that all studied segmentation methods with hierarchical class graph can obtain better segmentation performance compared to ones without hierarchical class graph. The limitation of the proposed method is that the training time of the segmentation model tends to increase a bit because of considering the correlation among different ground objects.",Remote sensing images,Semantic segmentation,Hierarchical class representation,,,,,,,INFORMATION FUSION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_22,"Zhang, Yue","Yang, Ruiqi","Dai, Qinling","Zhao, Yili",Boosting Semantic Segmentation of Remote Sensing Images by Introducing Edge Extraction Network and Spectral Indices,,NOV 2023,4,"Deep convolutional neural networks have greatly enhanced the semantic segmentation of remote sensing images. However, most networks are primarily designed to process imagery with red, green, and blue bands. Although it is feasible to directly utilize established networks and pre-trained models for remotely sensed images, they suffer from imprecise land object contour localization and unsatisfactory segmentation results. These networks still need to explore the domain knowledge embedded in images. Therefore, we boost the segmentation performance of remote sensing images by augmenting the network input with multiple nonlinear spectral indices, such as vegetation and water indices, and introducing a novel holistic attention edge detection network (HAE-RNet). Experiments were conducted on the GID and Vaihingen datasets. The results showed that the NIR-NDWI/DSM-GNDVI-R-G-B (6C-2) band combination produced the best segmentation results for both datasets. The edge extraction block benefits better contour localization. The proposed network achieved a state-of-the-art performance in both the quantitative evaluation and visual inspection.",multispectral remote sensing image,deep learning,semantic segmentation,domain knowledge,edge detection,"Xu, Weiheng","Wang, Jun","Wang, Leiguang",,REMOTE SENSING,,spectral index,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_23,"Lu, Wanxuan","Jin, Jidong","Sun, Xian","Fu, Kun",SEMI-SUPERVISED SEMANTIC GENERATIVE NETWORKS FOR REMOTE SENSING IMAGE SEGMENTATION,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"Semi-supervised remote sensing semantic segmentation is an efficient way to increase the use of unlabeled data and cut labelling costs. The unlabeled-to-labeled data ratio is employed in more recent methods, which is very different from what is really used in practise. In this paper, we propose a semi-supervised semantic generative network for remote sensing images, introducing a self-supervised learning method to enhance the feature representation of the model when the data ratio is high. Specifically, we design a new branch for unlabeled data, which includes modules for both semantic reconstruction and appearance reconstruction. It can effectively alleviate the category confusion in complicated remote sensing image when there are few labeled data. Comprehensive experiments on the ISPRS POTSDAM dataset demonstrate that the proposed method achieves promising results.",Deep learning,Segmentation,Semi-Supervised,Remote Sensing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_24,"Zeng, Zhi","ZhU, Jiasong","Zhang, Weiye","Hua, Yuansheng",Noise-Aware Regional Contrast Semantic Segmentation of Remote Sensing Images Using Crowdsourcing Labels,,2024,0,"Remote sensing semantic segmentation enables automated identification and monitoring of land cover categories in remote sensing images, providing precise geographic information support for urban planning, resource management, environmental protection, and broader geographical and environmental research. With the advancement of deep learning, methods based on convolutional neural networks (CNNs) have significantly improved pixel-level automatic classification accuracy in remote sensing image semantic segmentation. However, this progress relies on extensive, high-quality fine annotations, which are costly and time-consuming to acquire. To address these challenges, we propose utilizing easily accessible and cost-effective crowdsourced data from OpenStreetMap (OSM) to generate learning labels for network training. However, due to the open nature of crowdsourced data, the generated labels may contain noise interference, leading to decreased network performance during training. Meanwhile, supervised contrastive learning has shown strong potential in handling noisy labels, with previous research proposing selective contrastive learning strategies to address noisy labels in classification tasks. However, such research has been limited to classification, as segmentation poses memory explosion issues. To further tackle this problem, we introduce a semantic segmentation algorithm based on crowdsourced annotation and selective contrastive learning. The core idea involves selecting confident regions in the regional dimension, allowing pixel samples within these regions to participate in subsequent learning, thereby better capturing meaningful pixel features for classification while reducing sample quantity. By leveraging contrastive learning of paired features and selecting confident pairs from noise pairs for supervised contrastive learning, we enhance intra-class compactness, thereby improving network robustness and reducing noise interference. Experimental validation on the proposed crowdsourced semantic segmentation datasets demonstrates favorable outcomes.",Remote sensing images,OSM,Noisy labels,Semantic segmentation,,,,,,"COMPUTER VISION - ECCV 2018, PT VII",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_25,"Pan, Shaoming","Tao, Yulong","Nie, Congchong","Chong, Yanwen",PEGNet: Progressive Edge Guidance Network for Semantic Segmentation of Remote Sensing Images,,APR 2021,37,"Owing to the rapid development of deep neural networks, prominent advances have been recently achieved in the semantic segmentation of remote sensing images. As the vital components of computer vision, semantic segmentation, and edge detection have strong correlation whether in the extracted features or task objective. Prior studies treated edge detection as a postprocessing operation to semantic segmentation, or they implicitly combined the two tasks. We consider that pixels around the edges are easy to be misdivided because of the prevalence of intraclass inconsistencies and interclass indistinctions, which reflect the discriminative ability of models to distinguish different classes. In this letter, we propose a multipath atrous module to first enrich the deep semantic information. Then, we combine the enhanced deep semantic information and dilated edge information generated by canny and morphological operations to obtain edge-region maps via edge-region detection module, which identifies pixels around the edges. Then, we relearn these error-prone pixels using a guidance module for the segmentation branch in a progressive guided manner. Combined with edge and segmentation branches, our progressive edge guidance network achieves an overall accuracy of 91.0% on the ISPRS Vaihingen test set, which is the new state-of-the-art result.",Image edge detection,Semantics,Image segmentation,Feature extraction,Remote sensing,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Iron,Fuses,Discriminative ability,edge detection,progressive guided,remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_26,"Chen, Zhong","Zhao, Jun","Deng, He",,Global Multi-Attention UResNeXt for Semantic Segmentation of High-Resolution Remote Sensing Images,,APR 2023,3,"Semantic segmentation has played an essential role in remote sensing image interpretation for decades. Although there has been tremendous success in such segmentation with the development of deep learning in the field, several limitations still exist in the current encoder-decoder models. First, the potential interdependencies of the context contained in each layer of the encoder-decoder architecture are not well utilized. Second, multi-scale features are insufficiently used, because the upper-layer and lower-layer features are not directly connected in the decoder part. In order to solve those limitations, a global attention gate (GAG) module is proposed to fully utilize the interdependencies of the context and multi-scale features, and then a global multi-attention UResNeXt (GMAUResNeXt) module is presented for the semantic segmentation of remote sensing images. GMAUResNeXt uses GAG in each layer of the decoder part to generate the global attention gate (for utilizing the context features) and connects each global attention gate with the uppermost layer in the decoder part by using the Hadamard product (for utilizing the multi-scale features). Both qualitative and quantitative experimental results demonstrate that use of GAG in each layer lets the model focus on a certain pattern, which can help improve the effectiveness of semantic segmentation of remote sensing images. Compared with state-of-the-art methods, GMAUResNeXt not only outperforms MDCNN by 0.68% on the Potsdam dataset with respect to the overall accuracy but is also the MANet by 3.19% on the GaoFen image dataset. GMAUResNeXt achieves better performance and more accurate segmentation results than the state-of-the-art models.",remote sensing,attention module,semantic segmentation,,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_27,"Mi, Li","Chen, Zhenzhong",,,Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation,,JAN 2020,109,"Semantic segmentation plays an important role in remote sensing image understanding. Great progress has been made in this area with the development of Deep Convolutional Neural Networks (DCNNs). However, due to the complexity of ground objects' spectrum, DCNNs with simple classifier have difficulties in distinguishing ground object categories even though they can represent image features effectively. Additionally, DCNN-based semantic segmentation methods learn to accumulate contextual information over large receptive fields that causes blur on object boundaries. In this work, a novel approach named Superpixel-enhanced Deep Neural Forest (SDNF) is proposed to target the aforementioned problems. To improve the classification ability, we introduce Deep Neural Forest (DNF), where the representation learning of deep neural network is conducted by a completely differentiable decision forest. Therefore, better classification accuracy is achieved by combining DCNNs with decision forests in an end-to-end manner. In addition, considering the homogeneity within superpixels and heterogeneity between superpixels, a Superpixel-enhanced Region Module (SRM) is proposed to further alleviate the noises and strengthen edges of ground objects. Experimental results on the ISPRS 2D semantic labeling benchmark demonstrate that our model significantly outperforms state-of-the-art methods thus validate the efficiency of our proposed SDNF.",Neural forest,Superpixel,Remote sensing imagery,Semantic segmentation,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_28,"Bai, Tao","Cao, Yiming","Xu, Yonghao","Wen, Bihan",Stealthy Adversarial Examples for Semantic Segmentation in Remote Sensing,,2024,1,"Deep learning methods have been proven effective in remote sensing image analysis and interpretation, where semantic segmentation plays a vital role. These deep segmentation methods are susceptible to adversarial attacks, while most of the existing attack methods tend to manipulate the image globally, leading to noticeable perturbations and chaotic segmentation. In this work, we propose a novel stealthy attack for semantic segmentation (SASS), which can largely increase the effectiveness and stealthiness from the existing attack methods on remote sensing images. SASS manipulates specific victim classes or objects of interest while preserving the original segmentation results for other classes or objects. In practice, as different inference mechanisms, overlapped inference, can be applied in segmentation, the efficacy of SASS may be degraded. To this end, we further introduce the masked SASS (MSASS), which generates augmented adversarial perturbations that only affect victim areas. We evaluate the effectiveness of SASS and MSASS using four state-of-the-art semantic segmentation models on the Vaihingen and Zurich Summer datasets. Extensive experiments demonstrate that our SASS and MSASS methods achieve superior attack performances on victim areas while maintaining high accuracies of other areas (drop less than 2%). The detection success rates of adversarial examples for segmentation, as characterized by Xiao et al., significantly drop from 97.78% for the untargeted projected gradient descent (PGD) attack to 28.71% for our MSASS method on the Zurich Summer dataset. Our work contributes to the field of adversarial attacks in semantic segmentation for remote sensing images by improving stealthiness, flexibility, and robustness. We anticipate that our findings will inspire the development of defense methods to enhance the security and reliability of semantic segmentation models against our stealthy attack.",Remote sensing,Semantic segmentation,Task analysis,Perturbation methods,Sensors,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Buildings,Semantics,Adversarial attack,deep learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_29,"Sun, Li","Zou, Huanxin","Wei, Juan","Li, Meilin",SEMANTIC SEGMENTATION OF HIGH-RESOLUTION REMOTE SENSING IMAGES BASED ON SPARSE SELF-ATTENTION,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,2,"Semantic segmentation of high-resolution optical remote sensing images is an important but challenging task. To solve the problem that many semantic segmentation networks fail to efficiently utilize global and local context information to improve the segmentation performance, this paper proposes a semantic segmentation network based on sparse self-attention (SDANet) to model the global context dependencies. Specifically, the feature maps are first divided into four regions in spatial and channel dimensions, respectively, and the divided feature maps are rearranged to form new regions. Second, the position and channel self-attention operations are performed on the rearranged regions. Third, the feature maps are restored to the original combination and the position together with channel self-attention operations are performed again to obtain the output feature maps. Finally, semantic segmentation is completed based on the output feature maps. Extensive experiments conducted on the ISPRS Vaihingen dataset demonstrate that the proposed method is superior to self-attention-based DANet, CCNet, and other general semantic segmentation networks, such as FCN, Deeplabv3+, HRNet, etc.",Semantic segmentation,self-attention,remote sensing,context modeling,,"Cao, Xu","He, Shitian","Liu, Shuo",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_30,"Li, Rui","Zheng, Shunyi","Zhang, Ce","Duan, Chenxi",Multiattention Network for Semantic Segmentation of Fine-Resolution Remote Sensing Images,,2022,261,"Semantic segmentation of remote sensing images plays an important role in a wide range of applications, including land resource management, biosphere monitoring, and urban planning. Although the accuracy of semantic segmentation in remote sensing images has been increased significantly by deep convolutional neural networks, several limitations exist in standard models. First, for encoder-decoder architectures such as U-Net, the utilization of multiscale features causes the underuse of information, where low-level features and high-level features are concatenated directly without any refinement. Second, long-range dependencies of feature maps are insufficiently explored, resulting in suboptimal feature representations associated with each semantic class. Third, even though the dot-product attention mechanism has been introduced and utilized in semantic segmentation to model long-range dependencies, the large time and space demands of attention impede the actual usage of attention in application scenarios with large-scale input. This article proposed a multiattention network (MANet) to address these issues by extracting contextual dependencies through multiple efficient attention modules. A novel attention mechanism of kernel attention with linear complexity is proposed to alleviate the large computational demand in attention. Based on kernel attention and channel attention, we integrate local feature maps extracted by ResNet-50 with their corresponding global dependencies and reweight interdependent channel maps adaptively. Numerical experiments on two large-scale fine-resolution remote sensing datasets demonstrate the superior performance of the proposed MANet. Code is available at https://github.com/lironui/Multi-Attention-Network.",Semantics,Image segmentation,Feature extraction,Remote sensing,Task analysis,"Su, Jianlin","Wang, Libo","Atkinson, Peter M.",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Kernel,Complexity theory,Attention mechanism,fine-resolution remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_31,Guo Zhichao,Xu Junming,Liu Aidong,,Remote sensing image semantic segmentation method based on improved Deeplabv3+,INTERNATIONAL CONFERENCE ON IMAGE PROCESSING AND INTELLIGENT CONTROL (IPIC 2021),2021,2,"In recent years, with the continuous development of remote sensing technology and computer vision technology, the semantic segmentation of remote sensing images is of great significance in terms of earth observation, urban planning, military simulation, etc. This paper proposes a remote sensing image semantic segmentation method based on improved Deeplabv3+. Firstly, the backbone network is improved. Xception is selected to replace the traditional ResNe101 as the backbone network for the improved Deeplabv3+, and the network structure is deepened and depth separable. Optimization methods such as product replacement improve the segmentation efficiency; then, in order to improve the feature extraction effect of small targets in remote sensing images, the expansion rate of the cavity convolution in the ASSP module is optimized and adjusted. The experimental results show that the improved Deeplabv3+ algorithm has achieved good segmentation results on the data set, miou reached 91.23%, pixel accuracy reached 93.31%, and F1-score reached 89.2%, which is an increase of 2.4%, 1.9% and 2.7% compared with the original Deeplabv3+. At the same time, compared with mainstream U-net and SegNet algorithms, this algorithm also has strong advantages in semantic segmentation of remote sensing images.",Semantic segmentation,Deeplabv3+,remote sensing image,deep learning,neural network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_32,"Zi, Wenjie","Xiong, Wei","Chen, Hao","Li, Jun",SGA-Net: Self-Constructing Graph Attention Neural Network for Semantic Segmentation of Remote Sensing Images,,NOV 2021,20,"Semantic segmentation of remote sensing images is always a critical and challenging task. Graph neural networks, which can capture global contextual representations, can exploit long-range pixel dependency, thereby improving semantic segmentation performance. In this paper, a novel self-constructing graph attention neural network is proposed for such a purpose. Firstly, ResNet50 was employed as backbone of a feature extraction network to acquire feature maps of remote sensing images. Secondly, pixel-wise dependency graphs were constructed from the feature maps of images, and a graph attention network is designed to extract the correlations of pixels of the remote sensing images. Thirdly, the channel linear attention mechanism obtained the channel dependency of images, further improving the prediction of semantic segmentation. Lastly, we conducted comprehensive experiments and found that the proposed model consistently outperformed state-of-the-art methods on two widely used remote sensing image datasets.",self-constructing graph,semantic segmentation,remote sensing,,,"Jing, Ning",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_33,"Zhao, Yang","Guo, Peng","Sun, Zihao","Chen, Xiuwan",ResiDualGAN: Resize-Residual DualGAN for Cross-Domain Remote Sensing Images Semantic Segmentation,,MAR 2023,18,"The performance of a semantic segmentation model for remote sensing (RS) images pre-trained on an annotated dataset greatly decreases when testing on another unannotated dataset because of the domain gap. Adversarial generative methods, e.g., DualGAN, are utilized for unpaired image-to-image translation to minimize the pixel-level domain gap, which is one of the common approaches for unsupervised domain adaptation (UDA). However, the existing image translation methods face two problems when performing RS image translation: (1) ignoring the scale discrepancy between two RS datasets, which greatly affects the accuracy performance of scale-invariant objects; (2) ignoring the characteristic of real-to-real translation of RS images, which brings an unstable factor for the training of the models. In this paper, ResiDualGAN is proposed for RS image translation, where an in-network resizer module is used for addressing the scale discrepancy of RS datasets and a residual connection is used for strengthening the stability of real-to-real images translation and improving the performance in cross-domain semantic segmentation tasks. Combined with an output space adaptation method, the proposed method greatly improves the accuracy performance on common benchmarks, which demonstrates the superiority and reliability of ResiDualGAN. At the end of the paper, a thorough discussion is conducted to provide a reasonable explanation for the improvement of ResiDualGAN. Our source code is also available.",ResiDualGAN,UDA,remote sensing,semantic segmentation,,"Gao, Han",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_34,"Zhao, Weiheng","Cao, Jiannong","Dong, Xueyan",,Multilateral Semantic With Dual Relation Network for Remote Sensing Images Segmentation,,2024,2,"Semantic segmentation of remote sensing images is an extensively employed and demanding task. Although deep convolutional neural networks have significantly increased the accuracy of semantic segmentation, the problems of losing detailed features in segmentation and ignoring rich contextual information of images still exist. To solve these challenges, we propose a multilateral semantic with dual relation network (MSDRNet) for remote sensing images segmentation. The proposed MSDRNet consists of two parallel modules, the detail semantic module and the global semantic module, for extracting image detail and global features, respectively. Subsequently, improved spatial relation block and channel relation block are introduced in two separate parallel modules to further enhance the contextual connection of the images. Finally, a feature refinement module is added to balance the multilateral features between the features extracted from the two branches. We display the robustness and effectiveness of the proposed MSDRNet on the publicly available ISPRS Potsdam and Vaihingen datasets. We further experimented with the Gaofen image dataset, which contains information on larger scale features, to demonstrate the validity of our model. The results of extensive experiments conducted on the aforementioned three datasets show that the proposed approach outperforms several state-of-the-art semantic segmentation methods.",Attention mechanisms,deep learning,remote sensing image,semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_35,"He, Qibin","Sun, Xian","Diao, Wenhui","Yan, Zhiyuan",Transformer-induced graph reasoning for multimodal semantic segmentation in remote sensing,,NOV 2022,45,"As a large amount of earth observation data is available on a global scale, it becomes possible to apply multimodal semantic segmentation technology to remote sensing scene analysis. However, the diversity of objects in large-scale scenes and the cross-modal gap between different images are still challenging in practical applications. To address these problems, we propose a Transformer-Induced Hierarchical Graph Network (GraFNet) for multimodal semantic segmentation in remote sensing scenes, which promotes the exploration of potential intra- and inter-modal relations by introducing a new modeling paradigm. Different from existing methods, GraFNet parses multimodal remote sensing images into semantic topological graphs, and exploits the structural information of land cover categories to learn joint representations. Specifically, an attentive heterogeneous information aggregation mechanism is presented to parse diverse objects in remote sensing scenes into semantic entities, and capture modality-specific object-object interaction patterns in a topology-aware environment. In addition, modality hierarchical dependency modeling is introduced to encode the interactive representation of cross-modal objects, and distinguish the modality-specific contribution to improve cross-modal compatibility. Extensive experiments on several multimodal remote sensing datasets demonstrate that the proposed GraFNet outperforms the state-of-the-art approaches, achieving F-1/mIoU accuracy 91.1%/82.4% on the ISPRS Vaihingen dataset, 93.4%/88.4% on ISPRS Potsdam dataset, and 91.8%/84.0% on the MSAW dataset.",Graph reasoning,Hierarchical representation,Multimodal remote sensing,Semantic segmentation,Transformer,"Yin, Dongshuo","Fu, Kun",,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_36,"Sun, Xudong","Xia, Min","Dai, Tianfang",,Controllable Fused Semantic Segmentation with Adaptive Edge Loss for Remote Sensing Parsing,,JAN 2022,12,"High-resolution remote sensing images have been put into the application in remote sensing parsing. General remote sensing parsing methods based on semantic segmentation still have limitations, which include frequent neglect of tiny objects, high complexity in image understanding and sample imbalance. Therefore, a controllable fusion module (CFM) is proposed to alleviate the problem of implicit understanding of complicated categories. Moreover, an adaptive edge loss function (AEL) was proposed to alleviate the problem of the recognition of tiny objects and sample imbalance. Our proposed method combining CFM and AEL optimizes edge features and body features in a coupled mode. The verification on Potsdam and Vaihingen datasets shows that our method can significantly improve the parsing effect of satellite images in terms of mIoU and MPA.",remote sensing parsing,satellite imagery,semantic segmentation,,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_37,"Wang, Libo","Dong, Sijun","Chen, Ying","Meng, Xiaoliang",MetaSegNet: Metadata-Collaborative Vision-Language Representation Learning for Semantic Segmentation of Remote Sensing Images,,2024,0,"Semantic segmentation of remote sensing images plays a vital role in a wide range of Earth Observation applications, such as land-use land-cover (LULC) mapping, environment monitoring, and sustainable development. Driven by rapid developments in artificial intelligence, deep learning (DL) has emerged as the mainstream for semantic segmentation and has achieved many breakthroughs in the field of remote sensing. However, most DL-based methods focus on unimodal visual data while ignoring rich multimodal information involved in the real world. Nonvisual data, such as text, can gather extra knowledge from the real world, which can strengthen the interpretability, reliability, and generalization of visual models. Inspired by this, we propose a novel metadata-collaborative segmentation network (MetaSegNet) that applies vision-language representation learning for the semantic segmentation of remote sensing images. Unlike the common model structure that only uses unimodal visual data, we extract the key characteristic (e.g., the climate zone) from freely available remote sensing image metadata and transfer it into geographic text prompts via the generic ChatGPT. Then, we construct an image encoder, a text encoder, and a crossmodal attention fusion subnetwork to extract the image and text feature and apply image-text interaction. Benefiting from such a design, the proposed MetaSegNet not only demonstrates superior generalization in zero-shot testing but also achieves competitive accuracy with the state-of-the-art semantic segmentation methods on the large-scale OpenEarthMap dataset [70.4% mean intersection over union (mIoU)] and the Potsdam dataset (93.3% mean {F}1 score) as well as the LoveDA dataset (52.0% mIoU).",Metadata-collaborative learning,multimodal remote sensing,semantic segmentation,semantic segmentation,vision-language representation learning,"Fang, Shenghui","Fei, Songlin",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,vision-language representation learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_38,"Wu, Honglin","Huang, Peng","Zhang, Min","Tang, Wenlong",CTFNet: CNN-Transformer Fusion Network for Remote-Sensing Image Semantic Segmentation,,2024,4,"Remote-sensing image semantic segmentation is usually based on convolutional neural networks (CNNs). CNNs demonstrate powerful local feature extraction capabilities through stacked convolution and pooling. However, the locality of the convolution operation limits the ability of CNNs to directly extract global information. Relying on the multihead self-attention (MHSA) mechanism, transformer shows great advantages in modeling global information. In this letter, we propose a CNN-transformer fusion network (CTFNet) for remote-sensing image semantic segmentation. CTFNet applies a U-shaped encoder-decoder structure to achieve the extraction and adaptive fusion of local features and global context information. Specifically, a lightweight W/P transformer block is proposed as the decoder to obtain global context information with low complexity and connected to the encoder through the skip connection. Finally, the channel and spatial attention fusion module (AFM) is exploited to adaptively fuse deep semantic features and shallow detail features. On the Vaihingen and Potsdam datasets of the International Society for Photogrammetry and Remote Sensing (ISPRS), the effectiveness of each module is demonstrated by ablation experiments. Compared with several classical networks, our proposed CTFNet can obtain superior performance.",Adaptive fusion,global context information,remote sensing,semantic segmentation,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_39,"Zhu, Shengyu",,,,SEMANTIC SEGMENTATION FOR REMOTE SENSING IMAGES BASED ON SWIN-TRANSFORMER AND MULTISCALE FEATURE REFINEMENT,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"Thanks to the development of deep learning, semantic segmentation of remote sensing images has made great advances. However, because of the widespread complex background and variable imaging conditions, the feature consistency of the same classes and the variability of different classes are more difficult to capture, leading to the existing methods still have the space for improvement. Thus, we combine the local convolution operation and swin transformer to design a novel semantic segmentation method (STFR). The proposed STFR can improve the multiscale feature representation and capture the global context information, which consists of three steps. Firstly, the multiscale features of the image is extracted by swin transformer; then, a parallel structure is constructed based on depth-wise separable convolution to refine the deep semantic information; finally, the semantic segmentation result can be obtained. Experimental results on a public remote sensing semantic segmentation dataset indicate the the proposed STFR can obtain a great performance.",Semantic segmentation,remote sensing images,swin transformer,parallel structure,feature refinement,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_40,"Wang, Hengyou","Li, Xiao","Huo, Lian-Zhi",,Key Feature Repairing Based on Self-Supervised for Remote Sensing Semantic Segmentation,,2024,0,"As one of the fundamental issues in remote sensing, semantic segmentation has always received widespread attention. However, different from natural images, remote sensing images contain more complex category information, which poses many challenges to researchers, e.g., the lack of large-scale labeled semantic segmentation datasets on remote sensing and the accurate distinguishment of the edge areas between different classes. Recently, self-supervised methods have tried to avoid the issue of great dependency on labeled datasets. However, existing self-supervised methods were typically based on randomly masking and repairing images to learn features from unlabeled images. Random masks cannot drive the model focus on the salient information of the image, thus the learned features are not representative. In this letter, to improve the accuracy and generalization ability of the model in remote sensing semantic segmentation, we propose a key feature repairing network based on self-supervised learning (SSL), called KFRNet. KFRNet calculates the similarity between each image patch and its surrounding patches and sorts them to find the patches with more prominent feature information for masking and repairing, effective obtaining image context information. Besides, to improve the model's ability to distinguish different classes of objects, we designed an image comparison branch to obtain the category features of the image by comparing positive and negative samples. The experimental results on the POTSDAM and LoveDA datasets show that the proposed method can effectively improve segmentation accuracy. The overall accuracy (OA), mean intersection over union (MIOU), and Fscore indices reached 89.73%, 83.96%, 91.15% (POTSDAM) and 70.81%, 53.40%, 68.86% (LoveDA), even surpassing some supervised learning methods.",Remote sensing,self-supervised learning (SSL),semantic segmentation,,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_41,"Hu, Xudong","Zhang, Penglin","Zhang, Qi","Yuan, Feng",GLSANet: Global-Local Self-Attention Network for Remote Sensing Image Semantic Segmentation,,2023,19,"Learning long-range contextual dependence is important for remote sensing (RS) image segmentation in complex patterns. Meanwhile, exploring local context information is conducive to the discrimination of fine details. Only underlining either global semantic correlations or local context details is insufficient to achieve accurate segmentation. In this letter, we propose an architecture with the global-local self-attention (GLSA) mechanism, called GLSANet, which can simultaneously consider both global and local contexts for segmentations. Particularly, the GLSA mechanism consists of the global atrous self-attention (GASA) and local window self-attention (LWSA) mechanisms. GASA can learn long-range semantic relations in a gapped manner, while LWSA can locally capture contextual details. As a bridge between the two self-attention (SA) branches, a context fusion module (CFM) is further designed to adaptively integrate global and local contexts. The experiments with public datasets show that the proposed GLSANet significantly refines semantic segmentation and outperforms other competing methods.",Semantics,Convolution,Correlation,Transformers,Semantic segmentation,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Remote sensing,Computer architecture,Deep learning,remote sensing (RS),self-attention (SA),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_42,"Ouyang, Song","Li, Yansheng",,,Combining Deep Semantic Segmentation Network and Graph Convolutional Neural Network for Semantic Segmentation of Remote Sensing Imagery,,JAN 2021,54,"Although the deep semantic segmentation network (DSSN) has been widely used in remote sensing (RS) image semantic segmentation, it still does not fully mind the spatial relationship cues between objects when extracting deep visual features through convolutional filters and pooling layers. In fact, the spatial distribution between objects from different classes has a strong correlation characteristic. For example, buildings tend to be close to roads. In view of the strong appearance extraction ability of DSSN and the powerful topological relationship modeling capability of the graph convolutional neural network (GCN), a DSSN-GCN framework, which combines the advantages of DSSN and GCN, is proposed in this paper for RS image semantic segmentation. To lift the appearance extraction ability, this paper proposes a new DSSN called the attention residual U-shaped network (AttResUNet), which leverages residual blocks to encode feature maps and the attention module to refine the features. As far as GCN, the graph is built, where graph nodes are denoted by the superpixels and the graph weight is calculated by considering the spectral information and spatial information of the nodes. The AttResUNet is trained to extract the high-level features to initialize the graph nodes. Then the GCN combines features and spatial relationships between nodes to conduct classification. It is worth noting that the usage of spatial relationship knowledge boosts the performance and robustness of the classification module. In addition, benefiting from modeling GCN on the superpixel level, the boundaries of objects are restored to a certain extent and there are less pixel-level noises in the final classification result. Extensive experiments on two publicly open datasets show that DSSN-GCN model outperforms the competitive baseline (i.e., the DSSN model) and the DSSN-GCN when adopting AttResUNet achieves the best performance, which demonstrates the advance of our method.",deep semantic segmentation network (DSSN),graph convolutional neural network (GCN),remote sensing (RS),semantic segmentation,spatial relationship,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_43,"Zheng, Chengyu","Nie, Jie","Wang, Zhaoxin","Song, Ning",High-Order Semantic Decoupling Network for Remote Sensing Image Semantic Segmentation,,2023,15,"Low-order features based on convolution kernel are easy to be distorted when encountering dramatic view angle transformation and atmospheric scattering in remote sensing (RS) images. To address this concern, this article first proposes to operate semantic segmentation of RS images based on the high-order information, which can represent the relative relationship of low-order features and is robust and stable when suffering feature distortion. Besides, semantic decouples have recently been well researched and have achieved significant improvement in image understanding. Thus, in this article, a high-order semantic decoupling network (HSDN) is proposed to disentangle features by semantics based on high-order features. Specifically, HSDN first represents each pixel by calculating the pixel-level affinity as a high-order feature and then clusters these pixels into different semantics. Afterward, an attention-like mask generation module is designed for both intra-semantic and inter-semantic groups, leading to three kinds of masks, including the semantic decoupling mask (SDM), which utilizes each high-order cluster centroid as a mask to compact features intracluster and expand different interclusters, so as to improve semantic disentangle performance to a better extent; semantic enhancement mask (SEM), which records pixel-level relative correlation within a class to sufficiently exploit high-order features and could enhance feature robustness; and boundary supplementary mask (BSM), which aims to process borderline pixels to reduce cluster errors. Finally, by applying masks on pixels both within classes and on borderlines, semantic decoupled features are generated and concatenated to realize segmentation. The quantitative and qualitative experiments are conducted on two large-scale fine-resolution RS image datasets to demonstrate the significant performance of adopting high-order representation. Besides, we also implement numerous experiments to validate the effectiveness of the proposed semantic decouple framework in dealing with complicated and distortion-prone RS image segmentation tasks.",Semantics,Remote sensing,Semantic segmentation,Feature extraction,Scattering,"Wang, Jingyu","Wei, Zhiqiang",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Convolution,Task analysis,High-order representation (HR),remote sensing (RS),semantic decoupling,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_44,"Kotaridis, Ioannis","Lazaridou, Maria",,,Remote sensing image segmentation advances: A meta-analysis,,MAR 2021,150,"The advances in remote sensing sensors during the last two decades have led to the production of very high spatial resolution multispectral images. In order to adapt to this rapid development and handle these data, object-based analysis has emerged. A critical part of such an analysis is image segmentation. The selection of optimal segmentation parameters' values generates a qualitative segmentation output and has a direct impact on feature extraction and subsequent overall classification accuracy. Even though several image segmentation methods have been developed and suggested in the literature, each of them has advantages and disadvantages. This article presents the conceptual characteristics of image segmentation methods with a special focus on semantic segmentation. In addition, a meta-analysis was conducted through a comprehensive review of recent image segmentation case studies. It includes statistics and quantitative data regarding the applied segmentation algorithm, the software utilized and the data source among others. Since there is no miraculous segmentation algorithm, the statistical results depict only the recent trend. Finally, a few interesting subjects are addressed, including identification of current problems, image segmentation on non-traditional data and hot topics for future research.",Image segmentation,Remote sensing,Semantic segmentation,Meta-analysis,Review,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_45,"Zhang, Di","Yue, Peicheng","Yan, Yuhang","Niu, Qianqian",Multi-Source Remote Sensing Images Semantic Segmentation Based on Differential Feature Attention Fusion,,DEC 2024,0,"Multi-source remote sensing image semantic segmentation can provide more detailed feature attribute information, making it an important research field for remote sensing intelligent interpretation. However, due to the complexity of remote sensing scenes and the feature redundancy caused by multi-source fusion, multi-source remote sensing semantic segmentation still faces some challenges. In this paper, we propose a multi-source remote sensing semantic segmentation method based on differential feature attention fusion (DFAFNet) to alleviate the problems of difficult multi-source discriminant feature extraction and the poor quality of decoder feature reconstruction. Specifically, we achieve effective fusion of multi-source remote sensing features through a differential feature fusion module and unsupervised adversarial loss. Additionally, we improve decoded feature reconstruction without introducing additional parameters by employing an attention-guided upsampling strategy. Experimental results show that our method achieved 2.8% and 2.0% mean intersection over union (mIoU) score improvements compared with the competitive baseline algorithm on the available US3D and ISPRS Potsdam datasets, respectively.",multi-source fusion,remote sensing semantic segmentation,differential feature,attention-guided upsampling,,"Zhao, Jiaqi","Ma, Huifang",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_46,"Jiang, Yi","Lu, Wanxuan","Guo, Zhi",,MULTI-STAGE SEMI-SUPERVISED TRANSFORMER FOR REMOTE SENSING SEMANTIC SEGMENTATION WITH VARIOUS DATA AUGMENTATION,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"Existing research in semantic segmentation heavily relies on numerous manually annotated data, while the vast amount of unlabeled data still needs to be fully utilized. To address this challenge, this paper introduces a novel multi-stage semi-supervised method for remote sensing semantic segmentation, building upon a modified classical self-training scheme that leverages pseudo-labels. By dividing the data augmentation process into two stages, we employ various data augmentation strategies and balance the size of labels and pseudo-labels validated through rigorous experimentation, which can alleviate the student model from overfitting pseudo-labels. Furthermore, we also explore the efficacy of the Vision Transformer model in semi-supervised semantic segmentation, leading to further performance enhancements. The experimental results show that our semi-supervised remote sensing semantic segmentation method exhibits a more intuitive structure, easier deployment, and superior performance.",Semi-supervised learning (SSL),selftraining,vision transformer,semantic segmentation,remote sensing (RS),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_47,"Sun, Deyan","Liu, Hai","Chen, Wei","Zhu, Pengcheng",Multi-scale Self-attention Based Semi-supervised Remote Sensing Image Semantic Segmentation,,2024,0,"Remote sensing image semantic segmentation has been an important research direction in the interpretation, due to the huge scale difference between target objects in the remote sensing images and the loss of spatial details in the semantic segmentation, the existing semi-supervised remote sensing image semantic segmentation methods often provide compromised performance. This paper proposes a multi-scale self-attention based semi-supervised remote sensing image semantic segmentation model, which consists of a multi-scale self-attention based generator and a confidence map based discriminator network. The multi-scale mutual attention module is introduced to obtain the pixel relations between different scale images and balance the weights of different target objects in order to improve the segmentation performance of small-scale objects. Experimental results on public remote sensing data sets show that the MIOU of our proposal on CCF2015 and US2D increases to 80.74% and 66.94% respectively, superior to the state-of-the-art semi-supervised methods.",Remote Sensing Image,Semantic Segmentation,Multi-scale Attention,Semi-supervised,,"Chen, Dufeng","Liu, Jueting","Wang, Jiaqi","Wu, Yuliang","ADVANCED INTELLIGENT COMPUTING TECHNOLOGY AND APPLICATIONS, PT VI, ICIC 2024",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_48,"Xiang, Shao","Xie, Quangqi","Wang, Mi",,Semantic Segmentation for Remote Sensing Images Based on Adaptive Feature Selection Network,,2022,23,"Semantic segmentation plays a vital role in the segmentation of remote sensing field for its wide range of applications. The major current method for segmentation of remotely sensed imagery is using multiple scales strategy to improve the performance of segmentation networks. However, the ground object with uncertain scale in high-resolution aerial imagery is difficult to be segmented with conventional models. To address this problem, an adaptive feature selection module is designed, in which attention module learns weight contributions of each feature blocks in different scales. We employ the pyramid scene parsing network (PSPNet), DeepLabV3, and U-Net with the proposed module to conduct experiments on two benchmarks (the Vaihingen set and the WHU Building data set). The experimental results and comprehensive analysis validate the efficiency and practicability of the proposed method in semantic segmentation of remote sensing images.",Image segmentation,Semantics,Remote sensing,Feature extraction,Training,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Adaptation models,Buildings,Adaptive feature selection (AFS),remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_49,"Mu, Juwei","Zhou, Shangbo","Sun, Xingjie",,PPMamba: Enhancing Semantic Segmentation in Remote Sensing Imagery by SS2D,,2025,0,"Remote sensing semantic segmentation is a critical technology in the field of remote sensing image processing, with broad applications in environmental monitoring, urban planning, disaster assessment, and resource exploration. Despite the transformative impact of convolutional neural networks (CNNs) on this domain, CNN-based methods often encounter limitations due to their localized receptive fields, which struggle to capture the global context necessary for accurate segmentation in complex remote sensing imagery. In this letter, a novel approach is presented for remote sensing semantic segmentation using a mamba-based model named PPmamba. The PPmamba model integrates Resblock and PPmamba within an encoder-decoder framework to effectively capture both local and global contextual information from high-resolution remote sensing images. Leveraging the strengths of the Mamba architecture, our model employs selective scanning to efficiently process long sequences, overcoming the limitations of traditional CNNs and transformers in handling large-scale images with complex scenes. Extensive experiments on two benchmark datasets (Potsdam and Vaihingen) demonstrate the superiority of our PPmamba model against state-of-the-art models, achieving significant improvements in segmentation results. The codes will be available at https://github.com/Jerrymo59/PPMambaSeg.",Feature extraction,Remote sensing,Training,Semantic segmentation,Decoding,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Convolutional neural networks,Sensors,Semantics,Deep learning,Computer architecture,Remote sensing image,semantic segmentation,visual space state model,,,,,,,,,,,,,,,,,,,,,,
Row_50,"Nie, Jie","Wang, Zhaoxin","Liang, Xinyue","Yang, Chenxue",Semantic Category Balance-Aware Involved Anti-Interference Network for Remote Sensing Semantic Segmentation,,2023,5,"In recent years, semantic segmentation technology plays an important role in land resource management tasks. However, many classic semantic segmentation methods often fail to obtain satisfactory results for remote sensing images with a large amount of interference information. To improve this situation, we propose semantic category balance-aware involved anti-interference network (SCBANet). SCBANet has an encoder-decoder structure similar to DeeplabV3+. On this basis, we propose clustering-guided semantic decoupling module (CGSDM), consistency-based anti-interference feature extraction module (CAFEM), relevance-based anti-interference feature extraction module (RAFEM), and optional decoder module based on semantic category balance (ODMSCB) to improve the accuracy of semantic segmentation. CGSDM aims to obtain the information of different semantic categories through K -means clustering algorithm. CAFEM performs an average operation on the feature vectors in each semantic category to obtain semantic consistency information. RAFEM deeply excavates the information contained in each semantic category through the modeling method with self-attention mechanism as the core, making the relationship between pixels within each semantic category to be better understood by the model. ODMSCB classifies the feature map according to the balance of different semantic categories, so that different decoders can be applied to feature maps with different semantic category balance. These four parts complement each other, greatly improving the model's anti-interference ability while also enhancing the ability to handle category imbalance issue. We compared our method with several of the most advanced deep learning methods on the Vaihingen and Potsdam datasets. The final results demonstrate the superiority of our method.",Semantics,Feature extraction,Remote sensing,Semantic segmentation,Decoding,"Zheng, Chengyu","Wei, Zhiqiang",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Interference,Convolution,Remote sensing images,semantic category,semantic segmentation,strong anti-interference,,,,,,,,,,,,,,,,,,,,,,,,
Row_51,"Ma, Xianping","Zhang, Xiaokang","Pun, Man-On","Liu, Ming",A Multilevel Multimodal Fusion Transformer for Remote Sensing Semantic Segmentation,,2024,34,"Accurate semantic segmentation of remote sensing data plays a crucial role in the success of geoscience research and applications. Recently, multimodal fusion-based segmentation models have attracted much attention due to their outstanding performance as compared to conventional single-modal techniques. However, most of these models perform their fusion operation using convolutional neural networks (CNNs) or the vision transformer (Vit), resulting in insufficient local-global contextual modeling and representative capabilities. In this work, a multilevel multimodal fusion scheme called FTransUNet is proposed to provide a robust and effective multimodal fusion backbone for semantic segmentation by integrating both CNN and Vit into one unified fusion framework. First, the shallow-level features are first extracted and fused through convolutional layers and shallow-level feature fusion (SFF) modules. After that, deep-level features characterizing semantic information and spatial relationships are extracted and fused by a well-designed fusion Vit (FVit). It applies adaptively mutually boosted attention (Ada-MBA) layers and self-attention (SA) layers alternately in a three-stage scheme to learn cross-modality representations of high interclass separability and low intraclass variations. Specifically, the proposed Ada-MBA computes SA and cross-attention (CA) in parallel to enhance intra- and cross-modality contextual information simultaneously while steering attention distribution toward semantic-aware regions. As a result, FTransUNet can fuse shallow-level and deep-level features in a multilevel manner, taking full advantage of CNN and transformer to accurately characterize local details and global semantics, respectively. Extensive experiments confirm the superior performance of the proposed FTransUNet compared with other multimodal fusion approaches on two fine-resolution remote sensing datasets, namely ISPRS Vaihingen and Potsdam. The source code in this work is available at https://github.com/sstary/SSRS.",Multilevel multimodal fusion,remote sensing,semantic segmentation,vision transformer (Vit),,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_52,"Xie, Jiajun","Pan, Bin","Xu, Xia","Shi, Zhenwei",MiSSNet: Memory-Inspired Semantic Segmentation Augmentation Network for Class-Incremental Learning in Remote Sensing Images,,2024,6,"With remote sensing images constantly being collected rapidly, the class-incremental semantic segmentation (CISS) task has attracted increasing attention. However, the semantic distribution shift problem of the background class in remote sensing images, which is a case of catastrophic forgetting, continues to limit available CISS algorithms. To address this challenge, we present a new memory-inspired semantic segmentation augmentation network (MiSSNet) for class-incremental learning in remote sensing images. The MiSSNet mainly includes two modules: the local semantic distillation (LSD) module and the class-specific regularization (CSR) module. LSD is a distillation structure that employs the local semantic features in retained memory to maintain correlation between pixels throughout the training process of incremental learning. It constructs a series of pixel-level correlation matrices and implicitly adjusts the semantic distribution shift problem of the background class. CSR is a classwise regularization term that utilizes the class-specific portion of the preserved memory to help the model keep repeating the learning of the old categories. It alleviates the background class shift problem by generating countless pixel-level instances of old classes. LSD and CSR work together to tackle the semantic distribution shift problem of background class from semantic information and class information aspects, respectively. In particular, MiSSNet only needs an additional single inference process for memory extraction and storage, and the whole algorithm does not add any new training parameters. Experimental results on three semantic segmentation datasets indicate the advantage of the proposed method.",Incremental learning,remote sensing images,semantic segmentation,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_53,"Jiang, Baode","An, Xiaoya","Xu, Shaofen","Chen, Zhanlong",Intelligent Image Semantic Segmentation: A Review Through Deep Learning Techniques for Remote Sensing Image Analysis,,SEP 2023,19,"Image semantic segmentation is an important part of fundamental in image interpretation and computer vision. With the development of convolutional neural network technology, deep learning-based image semantic segmentation methods have received more and more attention and research. At present, many excellent semantic segmentation methods have been proposed and applied in the field of remote sensing. In this paper, we summarized the semantic segmentation methods used for remote sensing image, including the traditional remote sensing image semantic segmentation methods and the methods based on deep learning, we emphasize on summarizing the remote sensing image semantic segmentation algorithms based on deep learning and classify them into different categories, and then we introduce the datasets that commonly used and data preparation methods including pre-processing and augmentation techniques. Finally, the challenges and future directions of research in this domain are analyzed and prospected. It is hoped that this study can widen the frontiers of knowledge and provide useful literature for researchers interested in advancing this field of research.",Deep learning,Image semantic segmentation,Remote sensing image,Computer vision,,,,,,JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_54,"Ding, Hao","Xia, Bo","Liu, Weilin","Zhang, Zekai",A Novel Mamba Architecture with a Semantic Transformer for Efficient Real-Time Remote Sensing Semantic Segmentation,,JUL 2024,4,"Real-time remote sensing segmentation technology is crucial for unmanned aerial vehicles (UAVs) in battlefield surveillance, land characterization observation, earthquake disaster assessment, etc., and can significantly enhance the application value of UAVs in military and civilian fields. To realize this potential, it is essential to develop real-time semantic segmentation methods that can be applied to resource-limited platforms, such as edge devices. The majority of mainstream real-time semantic segmentation methods rely on convolutional neural networks (CNNs) and transformers. However, CNNs cannot effectively capture long-range dependencies, while transformers have high computational complexity. This paper proposes a novel remote sensing Mamba architecture for real-time segmentation tasks in remote sensing, named RTMamba. Specifically, the backbone utilizes a Visual State-Space (VSS) block to extract deep features and maintains linear computational complexity, thereby capturing long-range contextual information. Additionally, a novel Inverted Triangle Pyramid Pooling (ITP) module is incorporated into the decoder. The ITP module can effectively filter redundant feature information and enhance the perception of objects and their boundaries in remote sensing images. Extensive experiments were conducted on three challenging aerial remote sensing segmentation benchmarks, including Vaihingen, Potsdam, and LoveDA. The results show that RTMamba achieves competitive performance advantages in terms of segmentation accuracy and inference speed compared to state-of-the-art CNN and transformer methods. To further validate the deployment potential of the model on embedded devices with limited resources, such as UAVs, we conducted tests on the Jetson AGX Orin edge device. The experimental results demonstrate that RTMamba achieves impressive real-time segmentation performance.",remote sensing,real-time semantic segmentation,Mamba,unmanned aerial vehicle (UAV),,"Zhang, Jinglin","Wang, Xing","Xu, Sen",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_55,"Cheng, Jian","Deng, Changjian","Su, Yanzhou","An, Zeyu",Methods and datasets on semantic segmentation for Unmanned Aerial Vehicle remote sensing images: A review,,MAY 2024,10,"Unmanned Aerial Vehicle (UAV) has seen a dramatic rise in popularity for remote-sensing image acquisition and analysis in recent years. It has brought promising results in low-altitude monitoring tasks that require detailed visual inspections. Semantic segmentation is one of the hot topics in UAV remote sensing image analysis, as its capability to mine contextual semantic information from UAV images is crucial for achieving a fine-grained understanding of scenes. However, in the remote sensing field, recent reviews have not focused on combining ""UAV remote sensing""and ""semantic segmentation""to summarize the advanced works and future trends. In this study, we focus primarily on describing various recent semantic segmentation methods applied in UAV remote sensing images and summarizing their advantages and limitations. According to the distinction in modeling contextual semantic information, we have categorized and outlined the methods based on graph-based contextual models and deep-learning-based models. Publicly available UAV-based image datasets are also gathered to encourage systematic research on advanced semantic segmentation methods. We provide quantitative results of representative methods on two high-resolution UAV-based image datasets for fair comparisons and discussions in terms of semantic segmentation accuracy and model inference efficiency. Besides, this paper concludes some remaining challenges and future directions in semantic segmentation for UAV remote sensing images and points out that methods based on deep learning will become the future research trend.",Semantic segmentation,Unmanned aerial vehicle,Remote sensing images,Deep learning,,"Wang, Qi",,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_56,"Rui, Xue","Li, Ziqiang","Cao, Yang","Li, Ziyang",DILRS: Domain-Incremental Learning for Semantic Segmentation in Multi-Source Remote Sensing Data,,MAY 12 2023,5,"With the exponential growth in the speed and volume of remote sensing data, deep learning models are expected to adapt and continually learn over time. Unfortunately, the domain shift between multi-source remote sensing data from various sensors and regions poses a significant challenge. Segmentation models face difficulty in adapting to incremental domains due to catastrophic forgetting, which can be addressed via incremental learning methods. However, current incremental learning methods mainly focus on class-incremental learning, wherein classes belong to the same remote sensing domain, and neglect investigations into incremental domains in remote sensing. To solve this problem, we propose a domain-incremental learning method for semantic segmentation in multi-source remote sensing data. Specifically, our model aims to incrementally learn a new domain while preserving its performance on previous domains without accessing previous domain data. To achieve this, our model has a unique parameter learning structure that reparametrizes domain-agnostic and domain-specific parameters. We use different optimization strategies to adapt to domain shift in incremental domain learning. Additionally, we adopt multi-level knowledge distillation loss to mitigate the impact of label space shift among domains. The experiments demonstrate that our method achieves excellent performance in domain-incremental settings, outperforming existing methods with only a few parameters.",incremental learning,multi-source remote sensing,semantic segmentation,catastrophic forgetting,,"Song, Weiguo",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_57,"Wang, Zhibao","Chang, Huan","Bai, Lu","Chen, Liangfu",A Creative Weak Supervised Semantic Segmentation for Remote Sensing Images,,2024,0,"In weakly supervised semantic segmentation (WSSS) tasks on remote sensing images, it is a common practice to train a classification network from scratch using a large batch of images with a limited number of classes. Subsequently, class activation maps are extracted from the model based on predefined class indices, and these maps are then optimized to obtain pseudolabels. To make this strategy effective when introducing a new class, a substantial amount of data needs to be provided to the model. In this article, we present an innovative framework, RS-TextWS-Seg, designed to efficiently generate high-quality segmentation results for a wide range of remote sensing objects using concise descriptions. Our proposed framework comprises three sequential stages: initially, we undertake parameter fine-tuning of the contrastive language-image pretraining (CLIP) model to swiftly strengthen its capacity for zero-shot detection of a limited number of remote sensing features. Subsequently, we introduce a text-driven background suppression mechanism aimed at deriving class activation maps from the refined CLIP model based on textual cues, while concurrently mitigating background noises. Finally, we use the segment anything model (SAM) to refine the edges of the extracted class activation map. We widely researched the leading-edge methodologies in WSSS and conducted a range of comparative experiments and ablation studies to prove the efficacy of our proposed framework. The research findings underscore that RS-TextWS-Seg outperforms other state-of-the-art methods on renowned datasets such as DLRSD and Potsdam, as well as on bespoke datasets specifically curated for overground petroleum pipelines and oil well fields.",Remote sensing,Semantic segmentation,Cams,Training,Feature extraction,"Bi, Xiuli",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Sensors,Semantics,Petroleum,Location awareness,Decoding,Fine-tuning,remote sensing image,text prompts,weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,,,,,,,
Row_58,"Lin, Baokai","Yang, Guang","Zhang, Qian","Zhang, Guixu",Semantic Segmentation Network Using Local Relationship Upsampling for Remote Sensing Images,,2022,9,"Semantic segmentation is a fundamental task in remote sensing image processing. It provides pixel-level classification, which is important for many applications, such as building extraction and land use mapping. The development of convolutional neural network has considerably improved the performance of semantic segmentation. Most semantic segmentation networks are the encoder-decoder structure. Bilinear interpolation is an ordinary upsampling method in the decoder, but bilinear interpolation only considers its own features and inserts three times its own features. This over-simple and data-independent bilinear upsampling may lead to suboptimal results. In this work, we propose an upsampling method based on local relations to replace bilinear interpolation. Upsampling is performed by correlating the local relationship of feature maps of adjacent stages, which can better integrate local and global information. We also design a fusion module based on local similarity. Our proposed method with ResNet101 as the backbone of the segmentation network can improve the average F-1 score and overall accuracy of the Vaihingen data set by 2.69% and 1.31%, respectively. Our proposed method also has fewer parameters and less inference time.",Decoder,local relationship upsampling,remote sensing,semantic segmentation,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_59,"Ben Hamida, A.","Benoit, A.","Lambert, P.","Klein, L.",DEEP LEARNING FOR SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES WITH RICH SPECTRAL CONTENT,2017 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),2017,14,"With the rapid development of Remote Sensing acquisition techniques, there is a need to scale and improve processing tools to cope with the observed increase of both data volume and richness. Among popular techniques in remote sensing, Deep Learning gains increasing interest but depends on the quality of the training data. Therefore, this paper presents recent Deep Learning approaches for fine or coarse land cover semantic segmentation estimation. Various 2D architectures are tested and a new 3D model is introduced in order to jointly process the spatial and spectral dimensions of the data. Such a set of networks enables the comparison of the different spectral fusion schemes. Besides, we also assess the use of a ""noisy ground truth"" (i.e. outdated and low spatial resolution labels) for training and testing the networks.",Remote Sensing,Multispectral,Deep Learning,Semantic Segmentation,Noisy Training,"Ben Amar, C.","Audebert, N.","Lefevre, S.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_60,"Fan, Lili","Zhou, Yu","Liu, Hongmei","Li, Yunjie",Combining Swin Transformer With UNet for Remote Sensing Image Semantic Segmentation,,2023,13,"Remote sensing semantic segmentation plays a significant role in various applications such as environmental monitoring, land use planning, and disaster response. Convolutional neural networks (CNNs) have been dominating remote sensing semantic segmentation. However, due to the limitations of convolution operations, CNNs cannot effectively model global context. The success of transformers in the natural language processing (NLP) domain provides a new solution for global context modeling. Inspired by the Swin transformer, we propose a novel remote sensing semantic segmentation model called CSTUNet. This model employs a dual-encoder structure consisting of a CNN-based main encoder and a Swin transformer-based auxiliary encoder. We first utilize a detail-structure preservation module (DPM) to mitigate the loss of detail and structure information caused by Swin transformer downsampling. Then we introduce a spatial feature enhancement module (SFE) to collect contextual information from different spatial dimensions. Finally, we construct a position-aware attention fusion module (PAFM) to fuse contextual and local information. Our proposed model obtained 70.75% mean intersection over union (MIoU) on the ISPRS-Vaihingen dataset and 77.27% MIoU on the ISPRS-Potsdam dataset.",Transformers,Remote sensing,Feature extraction,Semantic segmentation,Context modeling,"Cao, Dongpu",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Convolutional neural networks,Task analysis,Feature fusion,remote sensing image,semantic segmentation,Swin transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_61,"Broni-Bediako, Clifford","Xia, Junshi","Yokoya, Naoto",,Real-Time Semantic Segmentation A brief survey and comparative study in remote sensing,,OCT 2023,1,"Real-time semantic segmentation of remote sensing imagery is a challenging task that requires a tradeoff between effectiveness and efficiency. It has many applications, including tracking forest fires, detecting changes in land use and land cover, crop health monitoring, and so on. With the success of efficient deep learning methods [i.e., efficient deep neural networks (DNNs)] for real-time semantic segmentation in computer vision, researchers have adopted these efficient DNNs in remote sensing image analysis. This article begins with a summary of the fundamental compression methods for designing efficient DNNs and provides a brief but comprehensive survey, outlining the recent developments in real-time semantic segmentation of remote sensing imagery. We examine several seminal efficient deep learning methods, placing them in a taxonomy based on the network architecture design approach. Furthermore, we evaluate the quality and efficiency of some existing efficient DNNs on a publicly available remote sensing semantic segmentation benchmark dataset, OpenEarthMap. The experimental results of an extensive comparative study demonstrate that most of the existing efficient DNNs have good segmentation quality, but they suffer low inference speed (i.e., a high latency rate), which may limit their capability of deployment in real-time applications of remote sensing image segmentation. We provide some insights into the current trend and future research directions for real-time semantic segmentation of remote sensing imagery.",Real-time systems,Semantic segmentation,Remote sensing,Computational modeling,Semantics,,,,,IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE,,Convolution,Deep learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_62,"Meng, Xiaoliang","Yang, Yuechi","Wang, Libo","Wang, Teng",Class-Guided Swin Transformer for Semantic Segmentation of Remote Sensing Imagery,,2022,45,"Semantic segmentation of remote sensing images plays a crucial role in a wide variety of practical applications, including land cover mapping, environmental protection, and economic assessment. In the last decade, convolutional neural network (CNN) is the mainstream deep learning-based method of semantic segmentation. Compared with conventional methods, CNN-based methods learn semantic features automatically, thereby achieving strong representation capability. However, the local receptive field of the convolution operation limits CNN-based methods from capturing long-range dependencies. In contrast, Vision Transformer (ViT) demonstrates its great potential in modeling long-range dependencies and obtains superior results in semantic segmentation. Inspired by this, in this letter, we propose a class-guided Swin Transformer (CG-Swin) for semantic segmentation of remote sensing images. Specifically, we adopt a Transformer-based encoder-decoder structure, which introduces the Swin Transformer backbone as the encoder and designs a class-guided Transformer block to construct the decoder. The experimental results on ISPRS Vaihingen and Potsdam datasets demonstrate the significant breakthrough of the proposed method over ten benchmarks, outperforming both advanced CNN-based and recent Transformer-based approaches.",Transformers,Semantics,Remote sensing,Decoding,Feature extraction,"Li, Rui","Zhang, Ce",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Convolution,Residual neural networks,Class-guided mechanism,fully transformer network,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_63,"Zhou, Yongxiu","Wang, Honghui","Yang, Ronghao","Yao, Guangle",A Novel Weakly Supervised Remote Sensing Landslide Semantic Segmentation Method: Combining CAM and cycleGAN Algorithms,,AUG 2022,23,"With the development of deep learning algorithms, more and more deep learning algorithms are being applied to remote sensing image classification, detection, and semantic segmentation. The landslide semantic segmentation of a remote sensing image based on deep learning mainly uses supervised learning, the accuracy of which depends on a large number of training data and high-quality data annotation. At this stage, high-quality data annotation often requires the investment of significant human effort. Therefore, the high cost of remote sensing landslide image data annotation greatly restricts the development of a landslide semantic segmentation algorithm. Aiming to resolve the problem of the high labeling cost of landslide semantic segmentation with a supervised learning method, we proposed a remote sensing landslide semantic segmentation with weakly supervised learning method combing class activation maps (CAMs) and cycle generative adversarial network (cycleGAN). In this method, we used the image level annotation data to replace pixel level annotation data as the training data. Firstly, the CAM method was used to determine the approximate position of the landslide area. Then, the cycleGAN method was used to generate the fake image without a landslide, and to make the difference with the real image to obtain the accurate segmentation of the landslide area. Finally, the pixel-level segmentation of the landslide area on remote sensing image was realized. We used mean intersection-over-union (mIOU) to evaluate the proposed method, and compared it with the method based on CAM, whose mIOU was 0.157, and we obtain better result with mIOU 0.237 on the same test dataset. Furthermore, we made a comparative experiment using the supervised learning method of a u-net network, and the mIOU result was 0.408. The experimental results show that it is feasible to realize landslide semantic segmentation in a remote sensing image by using weakly supervised learning. This method can greatly reduce the workload of data annotation.",landslide semantic segmentation,remote sensing,weakly supervised learning,CAM,cycleGAN,"Xu, Qiang","Zhang, Xiaojuan",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_64,"Shen, Ziyang","Ni, Huan","Guan, Haiyan","Niu, Xiaonan",Optimal transport-based domain adaptation for semantic segmentation of remote sensing images,,JAN 17 2024,1,"Thanks to its great power in feature representation, deep learning (DL) is widely used in semantic segmentation tasks. However, the requirements for high distribution consistency of different domains are too tight to be met by large-scale remote sensing tasks due to the domain shift in imaging modes and geographic environments. In this case, trained models in a source domain can hardly achieve sufficient accuracy in a target domain with domain shift. To address this issue, a novel unsupervised domain adaptation (UDA) method driven by optimal transport (OT) with two-stage training is proposed to alleviate domain shift in remote sensing images (RSIs). In the first stage, a colour distribution alignment (CDA) module and a feature joint alignment (FJA) module based on OT were designed to mitigate the discrepancy between different domains. CDA transports source-domain distribution according to the target-domain colour style, and FJA aligns source and target domains in both feature and output spaces by minimizing OT-based losses. In the second stage, self-training with pseudo-label denoising (STPD) was proposed, which alleviated the interference of noises in pseudo-labels based on a joint OT distance. For the experiments, the Potsdam, Vaihingen and UAVid datasets were employed. Based on the characteristics of these datasets, five UDA tasks were introduced. The results of these UDA experiments indicate the superiority of our method. Our code will be available at https://github.com/Hcshenziyang/OT-Domain-Adaptation-Semantic-Segmentation.",Semantic segmentation,optimal transport,domain adaptation,self-training,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_65,"Wang, Qingpeng","Chen, Wei","Huang, Zhou","Tang, Hongzhao",MultiSenseSeg: A Cost-Effective Unified Multimodal Semantic Segmentation Model for Remote Sensing,,2024,9,"Semantic segmentation is an essential technique in remote sensing. Until recently, most related research has focused primarily on advancing semantic segmentation models based on monomodal imagery, and less attention has been given to models that utilize multimodal remote sensing data. Moreover, most current multimodal approaches consider only limited bimodal situations and cannot simultaneously utilize three or more modalities. The increase in expensive computational costs associated with previous feature fusion paradigms hinders their application in broader cases. How to design a unified method to cover a wide variety of quantity-agnostic modalities for multimodal semantic segmentation remains an unsolved issue. To address the aforementioned challenges, this study explores a feasible way and proposes a cost-effective multimodal sensing semantic segmentation model (MultiSenseSeg). MultiSenseSeg employs multiple lightweight modality-specific experts (MSEs), an adaptive multimodal matching (AMM) module, and a single feature extraction pipeline to efficiently model intramodal and intermodal relationships. Benefiting from these designs, the proposed MultiSenseSeg can serve as a unified multimodal model capable of addressing both monomodal and bimodal cases and readily extrapolating to scenarios with more modalities, thereby achieving semantic segmentation of arbitrary quantities of multimodal data. To evaluate the performance of our method, we select several state-of-the-art (SOTA) semantic segmentation models from the past three years and conduct extensive experiments on two public multimodal datasets. The results show that MultiSenseSeg can not only achieve higher accuracy but also exhibit user-friendly modality extrapolation, allowing end-to-end training for consumer-grade users based on limited hardware resources. The model's code will be available at https://github.com/W-qp/MultiSenseSeg.",Semantic segmentation,Feature extraction,Remote sensing,Data models,Computational modeling,"Yang, Lan",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Costs,Semantics,Deep learning,low-cost increment,multimodal,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_66,"Liu, Siyu","Cheng, Jian","Liang, Leikun","Bai, Haiwei",Light-Weight Semantic Segmentation Network for UAV Remote Sensing Images,,2021,37,"Semantic segmentation for unmanned aerial vehicle (UAV) remote sensing images has become one of the research focuses in the field of remote sensing at present, which could accurately analyze the ground objects and their relationships. However, conventional semantic segmentation methods based on deep learning require large-scale models that are not suitable for resource-constrained UAV remote sensing tasks. Therefore, it is important to construct a light-weight semantic segmentation method for UAV remote sensing images. With this motivation, we propose a light-weight neural network model with fewer parameters to solve the problem of semantic segmentation of UAV remote sensing images. The network adopts an encoder-decoder architecture. In the encoder, we build a light-weight convolutional neural network model with fewer channels of each layer to reduce the number of model parameters. Then, feature maps of different scales from the encoder are concatenated together after resizing to carry out the multiscale fusion. Moreover, we employ two attention modules to capture the global semantic information from the context and the correlation among channels in UAV remote sensing images. In the decoder part, the model obtains predictions of each pixel through the softmax function. We conducted experiments on the ISPRS Vaihingen dataset, UAVid dataset, and UDD6 dataset to verify the effectiveness of the light-weight network. Our method obtains quality semantic segmentation results evaluated on UAV remote sensing datasets with only 9 M parameters the model owns, which is competitive among popular methods with the same level of parameters.",Remote sensing,Semantics,Image segmentation,Task analysis,Unmanned aerial vehicles,"Dang, Wanli",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Feature extraction,Convolution,Attention mechanism,light-weight network,remote sensing,semantic segmentation,unmanned aerial vehicle images,,,,,,,,,,,,,,,,,,,,,,,
Row_67,"Wang, Linhan","Lei, Shuo","He, Jianfeng","Wang, Shengkun",Self-Correlation and Cross-Correlation Learning for Few-Shot Remote Sensing Image Semantic Segmentation,"31ST ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS, ACM SIGSPATIAL GIS 2023",2023,1,"Remote sensing image semantic segmentation is an important problem for remote sensing image interpretation. Although remarkable progress has been achieved, existing deep neural network methods suffer from the reliance on massive training data. Few-shot remote sensing semantic segmentation aims at learning to segment target objects from a query image using only a few annotated support images of the target class. Most existing few-shot learning methods stem primarily from their sole focus on extracting information from support images, thereby failing to effectively address the large variance in appearance and scales of geographic objects. To tackle these challenges, we propose a Self-Correlation and Cross-Correlation Learning Network for the few-shot remote sensing image semantic segmentation. Our model enhances the generalization by considering both self-correlation and cross-correlation between support and query images to make segmentation predictions. To further explore the self-correlation with the query image, we propose to adopt a classical spectral method to produce a class-agnostic segmentation mask based on the basic visual information of the image. Extensive experiments on two remote sensing image datasets demonstrate the effectiveness and superiority of our model in few-shot remote sensing image semantic segmentation. The code is available at https://github.com/linhanwang/SCCNet.",remote sensing image semantic segmentation,few-shot learning,,,,"Zhang, Min","Lu, Chang-Tien",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_68,"Liu, Jia","Hua, Wenyi","Zhang, Wenhua","Liu, Fang",Stair Fusion Network With Context-Refined Attention for Remote Sensing Image Semantic Segmentation,,2024,6,"Semantic segmentation of remote sensing images is essential in various fields, such as Earth resource census, environmental pollution monitoring, and land use planning. The segmentation performance has been significantly improved recently with the development of deep learning. However, there are still some challenges in dealing with remote sensing images. One of the main issues is that features within the same category in remote sensing images could vary significantly, while features between different categories could be more similar, leading to confusion in segmentation. Moreover, the presence of large shadow areas narrows the feature differences between categories, making segmentation even more difficult. To address these challenges, one way is to leverage contextual and multiscale information for accurate segmentation. As a consequence, in this article, we propose a stair fusion network with context-refined attention (SFCRNet). A context-based attention embedding module is proposed to enhance the representation of the processed features by utilizing the context to maximize information retention in the channel and spatial dimensions. It can retain the information on the original channel and the association between it and other channels. Furthermore, we present an SFN, where a stair-shaped architecture and corresponding fusion module are designed to ensure that rich semantic information from high-level features is continuously transmitted to low-level layers. The experimental results on three datasets demonstrate the effectiveness of our proposed method.",Attention mechanism,remote sensing images,semantic segmentation,stair fusion,,"Xiao, Liang",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_69,"Wang, Zhen","Zhang, Shanwen","Zhang, Chuanlei","Wang, Buhong",Hidden Feature-Guided Semantic Segmentation Network for Remote Sensing Images,,2023,19,"For semantic segmentation of remote sensing images, convolutional neural networks (CNNs) have proven to be powerful tools. However, the existing CNN-based methods have the problems of feature information loss, serious interference by clutter information, and ignoring the correlation between different scale features. To solve these problems, this article proposes a novel hidden feature-guided semantic segmentation network (HFGNet) for remote sensing images, which achieves accurate semantic segmentation by hierarchically extracting and fusing valuable feature information. Specifically, the hidden feature extraction module (HFE-M) is introduced to suppress the salient feature representation to mine more valuable hidden features. Meanwhile, the multifeature interactive fusion module (MIF-M) establishes the correlation between different features to achieve hierarchical feature fusion. The multiscale feature calibration module (MSFC) is constructed to enhance the diversity and refinement representation of hierarchical fusion features. Besides, the local-channel attention mechanism (LCA-M) is designed to improve the feature perception capability of the object region and suppress background information interference. We conducted extensive experiments on the widely used ISPRS 2-D Semantic Labeling dataset and the 15-Class Gaofen Image dataset. Experimental results demonstrate that the proposed HFGNet has advantages over several state-of-the-art methods. The source code and models are available at https://github.com/darkseid-arch/RS-HFGNet.",Attention mechanism,convolution neural networks (CNNs),hidden feature extraction,remote sensing image,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_70,"Aburaed, N.","Al-Saad, M.","Alkhatib, M. Q.","Zitouni, M. S.",SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGERY USING AN ENHANCED ENCODER-DECODER ARCHITECTURE,"GEOSPATIAL WEEK 2023, VOL. 10-1",2023,0,"Semantic segmentation is one of most the important computer vision tasks for the analysis of aerial imagery in many remote sensing applications, such as resource surveys, disaster detection, and urban planning. This area of research still faces unsolved challenges, especially in cluttered environments and complex sceneries. This study presents a repurposed Robust UNet (RUNet) architecture for semantic segmentation, and embeds the architecture with attention mechanism in order to enhance feature extraction and construction of segmentation maps. The attention mechanism is achieved using Squeeze-and-Excitation (SE) block. The resulting network is referred to as SE-RUNet. SE is also tested with the classical UNet, termed SE-UNet, to verify the efficiency of introducing SE. The proposed approach is trained and tested using ""Semantic Segmentation of Aerial Imagery"" dataset. The results are evaluated using Accuracy, Precision, Recall, F-score and mean Intersection over Union (mIoU) metrics. Comparative evaluation and experimental results show that using SE to embed attention mechanism into UNet and RUNet significantly improves the overall performance.",Deep Learning,Semantic Segmentation,RUNET,UNET,Remote Sensing,"Almansoori, S.","Al-Ahmad, H.",,,,,Squeeze and Excitation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_71,"Alam, Muhammad","Wang, Jian-Feng","Guangpei, Cong","Yunrong, L., V",Convolutional Neural Network for the Semantic Segmentation of Remote Sensing Images,,FEB 2021,51,"In recent years, the success of deep learning in natural scene image processing boosted its application in the analysis of remote sensing images. In this paper, we applied Convolutional Neural Networks (CNN) on the semantic segmentation of remote sensing images. We improve the Encoder- Decoder CNN structure SegNet with index pooling and U-net to make them suitable for multi-targets semantic segmentation of remote sensing images. The results show that these two models have their own advantages and disadvantages on the segmentation of different objects. In addition, we propose an integrated algorithm that integrates these two models. Experimental results show that the presented integrated algorithm can exploite the advantages of both the models for multi-target segmentation and achieve a better segmentation compared to these two models.",Convolutional Neural Networks (CNN),Deep learning,Remote sensing images,Semantic segmentation,,"Chen, Yuanfang",,,,MOBILE NETWORKS & APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_72,"Chen, Xin","Li, Dongfen","Liu, Mingzhe","Jia, Jiaru",CNN and Transformer Fusion for Remote Sensing Image Semantic Segmentation,,SEP 2023,11,"Semantic segmentation of remote sensing images has been widely used in environmental protection, geological disaster discovery, and natural resource assessment. With the rapid development of deep learning, convolutional neural networks (CNNs) have dominated semantic segmentation, relying on their powerful local information extraction capabilities. Due to the locality of convolution operation, it can be challenging to obtain global context information directly. However, Transformer has excellent potential in global information modeling. This paper proposes a new hybrid convolutional and Transformer semantic segmentation model called CTFuse, which uses a multi-scale convolutional attention module in the convolutional part. CTFuse is a serial structure composed of a CNN and a Transformer. It first uses convolution to extract small-size target information and then uses Transformer to embed large-size ground target information. Subsequently, we propose a spatial and channel attention module in convolution to enhance the representation ability for global information and local features. In addition, we also propose a spatial and channel attention module in Transformer to improve the ability to capture detailed information. Finally, compared to other models used in the experiments, our CTFuse achieves state-of-the-art results on the International Society of Photogrammetry and Remote Sensing (ISPRS) Vaihingen and ISPRS Potsdam datasets.",segmentation,remote sensing,CNN,transformer,attention,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_73,"Li, Wenyuan","Chen, Hao","Shi, Zhenwei",,Semantic Segmentation of Remote Sensing Images With Self-Supervised Multitask Representation Learning,,2021,39,"Existing deep learning-based remote sensing images semantic segmentation methods require large-scale labeled datasets. However, the annotation of segmentation datasets is often too time-consuming and expensive. To ease the burden of data annotation, self-supervised representation learning methods have emerged recently. However, the semantic segmentation methods need to learn both high-level and low-level features, but most of the existing self-supervised representation learning methods usually focus on one level, which affects the performance of semantic segmentation for remote sensing images. In order to solve this problem, we propose a self-supervised multitask representation learning method to capture effective visual representations of remote sensing images. We design three different pretext tasks and a triplet Siamese network to learn the high-level and low-level image features at the same time. The network can be trained without any labeled data, and the trained model can be fine-tuned with the annotated segmentation dataset. We conduct experiments on Potsdam, Vaihingen dataset, and cloud/snow detection dataset Levir_CS to verify the effectiveness of our methods. Experimental results show that our proposed method can effectively reduce the demand of labeled datasets and improve the performance of remote sensing semantic segmentation. Compared with the recent state-of-the-art self-supervised representation learning methods and the mostly used initialization methods (such as random initialization and ImageNet pretraining), our proposed method has achieved the best results in most experiments, especially in the case of few training data. With only 10% to 50% labeled data, our method can achieve the comparable performance compared with random initialization. Codes are available at https://github.com/flyakon/SSLRemoteSensing.",Task analysis,Remote sensing,Semantics,Image segmentation,Training,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Sensors,Snow,Cloud detection,remote sensing images,self-supervised representation learning,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_74,"Tian, Tian","Chu, Zhengquan","Hu, Qian","Ma, Li",Class-Wise Fully Convolutional Network for Semantic Segmentation of Remote Sensing Images,,AUG 2021,20,"Semantic segmentation is a fundamental task in remote sensing image interpretation, which aims to assign a semantic label for every pixel in the given image. Accurate semantic segmentation is still challenging due to the complex distributions of various ground objects. With the development of deep learning, a series of segmentation networks represented by fully convolutional network (FCN) has made remarkable progress on this problem, but the segmentation accuracy is still far from expectations. This paper focuses on the importance of class-specific features of different land cover objects, and presents a novel end-to-end class-wise processing framework for segmentation. The proposed class-wise FCN (C-FCN) is shaped in the form of an encoder-decoder structure with skip-connections, in which the encoder is shared to produce general features for all categories and the decoder is class-wise to process class-specific features. To be detailed, class-wise transition (CT), class-wise up-sampling (CU), class-wise supervision (CS), and class-wise classification (CC) modules are designed to achieve the class-wise transfer, recover the resolution of class-wise feature maps, bridge the encoder and modified decoder, and implement class-wise classifications, respectively. Class-wise and group convolutions are adopted in the architecture with regard to the control of parameter numbers. The method is tested on the public ISPRS 2D semantic labeling benchmark datasets. Experimental results show that the proposed C-FCN significantly improves the segmentation performances compared with many state-of-the-art FCN-based networks, revealing its potentials on accurate segmentation of complex remote sensing images.",semantic segmentation,fully convolutional network (FCN),remote sensing,class- wise features,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_75,"Miao, Wang","Xu, Zhe","Geng, Jie","Jiang, Wen",ECAE: Edge-Aware Class Activation Enhancement for Semisupervised Remote Sensing Image Semantic Segmentation,,2023,6,"Remote sensing image semantic segmentation (RSISS) remains challenging due to the scarcity of labeled data. Semisupervised learning can leverage pseudolabels to enhance the model ' s ability to learn from unlabeled data. However, accurately generating pseudolabels for RSISS remains a significant challenge that severely affects the model ' s performance, especially for the edges of different classes. To overcome these issues, we propose a semisupervised semantic segmentation framework for remote sensing images (RSIs) based on edge-aware class activation enhancement (ECAE). First, the baseline network is constructed based on the average teacher model, which separates the training of labeled and unlabeled data using student and teacher networks. Second, considering local continuity and global discreteness of object distribution in RSIs, the class activation mapping enhancement (CAME) network is designed to predict local areas more remarkably. Finally, the edge-aware network (EAN) is proposed to improve the performance of edge segmentation in RSIs. The combination of the CAME with the EAN further heightens the generation of high-confidence pseudolabels. Experiments were performed on two publicly available remote sensing semantic segmentation datasets, Potsdam and ISPRS Vaihingen, which verify the superiorities of the proposed ECAE model.",Class activation mapping,remote sensing images (RSIs),semantic segmentation,semisupervised learning.,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_76,"Zhao, Qi","Liu, Jiahui","Li, Yuewen","Zhang, Hong",Semantic Segmentation With Attention Mechanism for Remote Sensing Images,,2022,72,"Semantic segmentation for high-resolution remote sensing images is one of the most significant tasks in the field of remote sensing applications. Remote sensing images contain substantial detailed information of ground objects, such as shape, location, and texture. Therefore, these objects make the images exhibit large intraclass variance and small interclass variance, which makes it very difficult to be recognized. In this study, an end-to-end attention-based semantic segmentation network (SSAtNet) is proposed. A pyramid attention pooling module is proposed to introduce the attention mechanism into the multiscale module for adaptive features refinement. To correct the detailed information, the pooling index correction module integrates pooling index maps from the encoder with high-level feature maps, which can help recover the fine-grained features. In the encoder phase, a more effective ResNet-101 backbone is designed to capture detailed features. What is more, a series of data augmentation methods are proposed to enhance the model's robustness. The proposed model is compared with several previous advanced networks and achieves the state of the art on the ISPRS Vaihingen dataset. The experiment results prove the effectiveness of the SSAtNet.",Image segmentation,Task analysis,Semantics,Remote sensing,Convolution,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Indexes,Feature extraction,Attention mechanism,convolutional neural networks (CNNs),multiscale module,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_77,"Su, Yanzhou","Cheng, Jian","Wang, Wen","Bai, Haiwei",Semantic Segmentation for High-Resolution Remote-Sensing Images via Dynamic Graph Context Reasoning,,2022,4,"Semantic segmentation for high-resolution remote-sensing (HRRS) images is one of the most challenging tasks in remote-sensing images understanding. Capturing long-range dependencies in feature representations is crucial for semantic segmentation. Recent graph-based global reasoning networks (GloRe) focus on modeling the global contextual relationship between latent nodes based on fully connected graph in interaction space. However, such a dense operation is susceptible to redundant features. Most importantly, it treats each node equally, ignoring the contextual relationship between nodes in graphs. In this work, we propose to explore more effective contextual representations in semantic segmentation by introducing dynamic graph contextual reasoning module over GloRe, dubbed DGCR. It incorporates local semantic information that represents the relationships between nodes to perform long-range contextual reasoning. More specifically, to provide effectively and flexible reasoning in graph-based reasoning approaches, we construct k-nearest neighbor (KNN) graphs rather than fully connected graphs using only the k closest nodes depends on pairwise semantic distance. Extensive experiments on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam datasets demonstrate the effectiveness and superiority of our proposed DGCR module over other state-of-the-art methods.",Context,graph reasoning,high-resolution remote-sensing (HRRS) images,semantic segmentation,,"Liu, Haijun",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_78,"Nie, Jie","Wang, Chenglong","Yu, Shusong","Shi, Jinjin",MIGN: Multiscale Image Generation Network for Remote Sensing Image Semantic Segmentation,,2023,7,"With the development of computer vision, the semantic segmentation of remote sensing images, which has become an important topic, has been utilized in various applications for image content analysis and understanding, such as urban planning, natural disaster monitoring, and land resource management. Many approaches have been proposed to address these problems. However, due to obvious differences in resolution, spatial structure, and semantics between remote sensing images and ordinary images, the semantic segmentation of remote sensing images is still challenging. In this paper, we propose a novel multiscale image generation network (MIGN) that can efficiently generate high-resolution segmentation results by considering both details and boundary information. In particular, a multi-attention mechanism method for semantic segmentation of remote sensing images is designed. The attention weight is calculated by capturing the interaction of cross dimensions in a two-branch structure, which can learn the underlying feature information and guarantee the performance of each pixel feature for final classification. We also propose an edge supervised module to ensure that the segmentation boundary has a more accurate performance. A multiscale image fusion algorithm based on the Bayes model is proposed to improve the accuracy of the segmentation module. The performance of our model is evaluated on the ISPRS Vaihingen and Potsdam datasets. The results show that our method is superior to the most advanced image segmentation methods in terms of MIoU and pixel accuracy.",Semantic segmentation,remote sensing,multiscale,multi-attention,edge supervised,"Lv, Xiaowei","Wei, Zhiqiang",,,IEEE TRANSACTIONS ON MULTIMEDIA,,image fusion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_79,"Zhang, Jing","Lin, Shaofu","Ding, Lei","Bruzzone, Lorenzo",Multi-Scale Context Aggregation for Semantic Segmentation of Remote Sensing Images,,FEB 2020,124,"The semantic segmentation of remote sensing images (RSIs) is important in a variety of applications. Conventional encoder-decoder-based convolutional neural networks (CNNs) use cascade pooling operations to aggregate the semantic information, which results in a loss of localization accuracy and in the preservation of spatial details. To overcome these limitations, we introduce the use of the high-resolution network (HRNet) to produce high-resolution features without the decoding stage. Moreover, we enhance the low-to-high features extracted from different branches separately to strengthen the embedding of scale-related contextual information. The low-resolution features contain more semantic information and have a small spatial size; thus, they are utilized to model the long-term spatial correlations. The high-resolution branches are enhanced by introducing an adaptive spatial pooling (ASP) module to aggregate more local contexts. By combining these context aggregation designs across different levels, the resulting architecture is capable of exploiting spatial context at both global and local levels. The experimental results obtained on two RSI datasets show that our approach significantly improves the accuracy with respect to the commonly used CNNs and achieves state-of-the-art performance.",semantic segmentation,convolutional neural network,deep learning,image analysis,remote sensing,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_80,"Hua, Yuansheng","Marcos, Diego","Mou, Lichao","Zhu, Xiao Xiang",Semantic Segmentation of Remote Sensing Images With Sparse Annotations,,2022,59,"Training convolutional neural networks (CNNs) for very high-resolution images requires a large quantity of high-quality pixel-level annotations, which is extremely labor-intensive and time-consuming to produce. Moreover, professional photograph interpreters might have to be involved in guaranteeing the correctness of annotations. To alleviate such a burden, we propose a framework for semantic segmentation of aerial images based on incomplete annotations, where annotators are asked to label a few pixels with easy-to-draw scribbles. To exploit these sparse scribbled annotations, we propose the FEature and Spatial relaTional regulArization (FESTA) method to complement the supervised task with an unsupervised learning signal that accounts for neighborhood structures both in spatial and feature terms. For the evaluation of our framework, we perform experiments on two remote sensing image segmentation data sets involving aerial and satellite imagery, respectively. Experimental results demonstrate that the exploitation of sparse annotations can significantly reduce labeling costs, while the proposed method can help improve the performance of semantic segmentation when training on such annotations. The sparse labels and codes are publicly available for reproducibility purposes.1",Annotations,Image segmentation,Semantics,Remote sensing,Training,"Tuia, Devis",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Kernel,Image color analysis,Aerial image,convolutional neural networks (CNNs),semantic segmentation,semisupervised learning,sparse scribbled annotation,,,,,,,,,,,,,,,,,,,,,,,
Row_81,"Huang, Cong","Yang, Yao","Wang, Huajun","Ma, Yu",Semantic segmentation of remote sensing images based on deep learning methods,,2021,0,"Remote sensing image segmentation has always been an important research direction in the field of remote sensing image processing, and it is a key step in the further understanding and analysis of remote sensing images. Image semantic segmentation is the process of classifying each pixel to form several sub-regions with respective characteristics, and extracting the objects of interest among them. However, due to the complex boundary and scale difference of the remote sensing image, the traditional algorithm can not meet the actual needs well, resulting in low segmentation accuracy. In order to further improve the accuracy of remote sensing image segmentation, this paper combines deep convolutional neural network with remote sensing image, based on the U-Net, firstly compares the model's segmentation accuracy under different learning strategies, and introduces a new learning strategy to improve the learning effect of the model; secondly, in the loss function part of the model, a new compound loss function is proposed to speed up the convergence of the network and improve the segmentation accuracy. Based on full experimental research on the WHDLD remote sensing image dataset, the results show that the improved method has 1.5% accuracy improvement compare to the U-Net.",remote sensing images,semantic segmentation,composite loss function,learning strategies,,"Zhao, Jinquan","Wan, Jun",,,"MEDICAL IMAGE COMPUTING AND COMPUTER-ASSISTED INTERVENTION, PT III",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_82,"Guo, Yongjie","Wang, Feng","Xiang, Yuming","You, Hongjian",Semisupervised Semantic Segmentation With Certainty-Aware Consistency Training for Remote Sensing Imagery,,2023,1,"Semisupervised learning is a forcible method to lessen the cost of annotation for remote sensing semantic segmentation tasks. Recent related research works indicate that consistency training is one of the most effective strategies in semisupervised learning. The core of consistency training is maintaining model outputs consistent under various perturbations. However, the current consistency training-based semisupervised semantic segmentation frameworks lack the analysis of model uncertainty, which increases the generation of semantic ambiguity on remote sensing images. Therefore, we propose the certainty-aware consistency training (CACT) strategy to mitigate the influence of semantic ambiguity caused by model uncertainty. The CACT strategy consists of two novel parts: certainty-aware consistency correction (CACC) and class-balanced-adaptive threshold (CBAT) strategy. The CACC starts with generating a high-quality prediction target, then models the importance of the consistent output target and corrects the output predictions according to the certainty map, increasing the focus on reliable predictions. The CBAT strategy uses a dynamic class-balanced adaptive threshold to filter out unreliable predictions, further reducing the impact of semantic ambiguity. Finally, considerable experimental results on the DLRSD, WHDLD, and Potsdam demonstrate that our framework has an excellent performance on semisupervised remote sensing semantic segmentation scenarios.",Training,Remote sensing,Semantic segmentation,Semantics,Adaptation models,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Predictive models,Uncertainty,Consistency training,remote sensing image,semantic segmentation,semisupervised learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_83,"Huang, Liwei","Jiang, Bitao","Lv, Shouye","Liu, Yanbo",Deep-Learning-Based Semantic Segmentation of Remote Sensing Images: A Survey,,2024,14,"Semantic segmentation of remote sensing images (SSRSIs), which aims to assign a category to each pixel in remote sensing images, plays a vital role in a broad range of applications, such as environmental monitoring, urban planning, and land resource utilization. Recently, with the successful application of deep learning in remote sensing, a substantial amount of work has been aimed at developing SSRSI methods using deep learning models. In this survey, we provide a comprehensive review of SSRSI. First, we review the current mainstream semantic segmentation models based on deep learning. Next, we analyze the main challenges faced by SSRSI and comprehensively summarize the current research status of deep-learning-based SSRSI, especially some new directions in SSRSI are outlined, including semisupervised and weakly-supervised SSRSI, unsupervised domain adaption in SSRSI, multimodal data-fusion-based SSRSI, and pretrained models for SSRSI. Then, we examine the most widely used datasets and metrics and review the quantitative results and experimental performance of some representative methods of SSRSI. Finally, we discuss promising future research directions in this area.",Deep learning,multimodal fusion,pretrained models,remote sensing images,semantic segmentation,"Fu, Ying",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,semi-supervised,unsupervised domain adaptation,weakly-supervised,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_84,"Wang, Hao","Tao, Chao","Qi, Ji","Xiao, Rong",Avoiding Negative Transfer for Semantic Segmentation of Remote Sensing Images,,2022,17,"Reducing the feature distribution shift caused by the factor of visual-environment changes, named visual-environment changes (VE-changes), is a hot issue in domain adaptation learning. However, in the semantic segmentation task of remote sensing imageries, besides VE-changes, the change of semantic-scene changes (SS-changes) is another factor raising the domain gap, which brings the label distribution shift. For example, although urban and rural share the same landcover label, there is still a gap in label distribution. If there is little relation that can be found in neither feature nor label space, forcibly adapting to a new domain could have a high risk of negative transfer. Hence, we propose a new Transitive Domain Adaptation method for Remote Sensing (TDARS) images. First, we introduce an intermediate domain to enlarge the relation between the given source and target domains. Second, we learn from primary and nonprimary confident classes to increase the likelihood of transferring valuable information. As a result, TDARS enables the given source and target domains to be connected through the selected intermediate domain and performs effective knowledge transfer among all domains. The proposed method is evaluated on three domain adaptation datasets of remote sensing images. Extensive experiments show that the approach can effectively handle the domain shift problem from remote sensing images compared to other state-of-the-art domain adaptation methods.",Remote sensing,Semantics,Image segmentation,Visualization,Knowledge transfer,"Li, Haifeng",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Urban areas,Sensors,Domain adaptation,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_85,"Liu, Bo","Hu, Jinwu","Bi, Xiuli","Li, Weisheng",PGNet: Positioning Guidance Network for Semantic Segmentation of Very-High-Resolution Remote Sensing Images,,SEP 2022,12,"Semantic segmentation of very-high-resolution (VHR) remote sensing images plays an important role in the intelligent interpretation of remote sensing since it predicts pixel-level labels to the images. Although many semantic segmentation methods of VHR remote sensing images have emerged recently and achieved good results, it is still a challenging task because the objects of VHR remote sensing images show large intra-class and small inter-class variations, and their size varies in a large range. Therefore, we proposed a novel semantic segmentation framework for VHR remote sensing images, called Positioning Guidance Network (PGNet), which consists of the feature extractor, a positioning guiding module (PGM), and a self-multiscale collection module (SMCM). First, the PGM can extract long-range dependence and global context information with the help of the transformer architecture and effectively transfer them to each pyramid-level feature, thus effectively improving the segmentation effectiveness between different semantic objects. Secondly, the SMCM we designed can effectively extract multi-scale information and generate high-resolution feature maps with high-level semantic information, thus helping to segment objects in small and varying sizes. Without bells and whistles, the mIoU scores of the proposed PGNet on the iSAID dataset and ISPRS Vaihingn dataset are 1.49% and 2.40% higher than FactSeg, respectively.",remote sensing images,semantic segmentation,positioning guiding module,self-multiscale collection module,transformer,"Gao, Xinbo",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_86,"Jiang, Jionghui","Feng, Xi'an","Ye, QiLei","Hu, Zhongyi",Semantic segmentation of remote sensing images combined with attention mechanism and feature enhancement U-Net,,OCT 2 2023,4,"Target segmentation of remote sensing images has always been a hotspot in image processing. This paper proposes a new semantic segmentation technology for remote sensing images, which uses Unet as the backbone and combines attention mechanism and feature enhancement module. The feature enhancement module can enlarge the information of the region of interest (ROI) to improve the contrast of the image; the attention mechanism includes spatial and channel attention modules, which can obtain more detailed information of the desired target while suppressing other useless information. This paper improves the loss function of the traditional Unet. On the basis of the sparse categorical cross-entropy function, the mean squared logarithmic error function is added, which can effectively improve the accuracy of semantic segmentation. The experimental results show that the algorithm has higher computational accuracy than Unet, DeepLabV3, SegNet, PSPNet, CBAM and DAnet while having the computational speed of FCN and Unet in model testing and validation.",Semantic Segmentation,Remote Sensing Images,Attention Mechanism,,,"Gu, Zhiyang","Huang, Hui",,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_87,Ni Xianyang,Cheng Yinbao,Wang Zhongyu,,Remote sensing semantic segmentation with convolution neural network using attention mechanism,PROCEEDINGS OF 2019 14TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONIC MEASUREMENT & INSTRUMENTS (ICEMI),2019,4,"Semantic image segmentation is an essential part of remote sensing image processing because accurate understanding of the ground information is the first step in obtaining useful knowledge of surface coverage. The popular semantic segmentation convolutional neural network model (DeepLab v3+) cannot electively use attention information, resulting in coarse segmentation boundaries. In this work, a new type of bottleneck using attention information which can extract semantic information and more abundant features from images is proposed. Compared with original network, the model using new bottleneck finely segments the target regions, solves the problem of segmentation boundary roughness better, leading to higher mloU and accuracy. Experimental results based on the dataset in the ISPRS benchmark on urban object classification show bringing attention model into semantic segmentation neural network improves performance.",Remote sensing,semantic segmentation,convolution neural network,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_88,"Sun, Yan","Zheng, Wenxi",,,HRNet- and PSPNet-based multiband semantic segmentation of remote sensing images,,APR 2023,24,"High-resolution remote sensing images have become mainstream remote sensing data, but there is an obvious ""salt and pepper phenomenon"" in the existing semantic segmentation methods of high-resolution remote sensing images. The purpose of this paper is to propose an improved deep convolutional neural network based on HRNet and PSPNet to segment and realize deep scene analysis and improve the pixel-level semantic segmentation representation of high-resolution remote sensing images. Based on hierarchical multiscale segmentation technology research, the main method is multiband segmentation; the vegetation, buildings, roads, waters and bare land rule sets in the experimental area are established, the classification is extracted, and the category is labeled at each pixel in the image. Using the image classification network structure, different levels of feature vectors can be used to meet the judgment requirements. The HRNet and PSPNet algorithms are used to analyze the scene and obtain the category labels of all pixels in an image. Experiments have shown that artificial intelligence uses the pyramid pooling module in the classification and recognition of CCF satellite images. In the context of integrating different regions, PSPNet affects the region segmentation accuracy. FCN, DeepLab and PSPNet are now the best methods and achieve 98% accuracy. However, the PSPNet object recognition algorithm has better advantages in specific areas. Experiments show that this method has high segmentation accuracy and good generalization ability and can be used in practical engineering.",Remote sensing image,Semantic segmentation,Ensemble learning,Convolutional neural network,,,,,,NEURAL COMPUTING & APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_89,"Huang, Wei","Shi, Yilei","Xiong, Zhitong","Zhu, Xiao Xiang",Decouple and weight semi-supervised semantic segmentation of remote sensing images,,JUN 2024,2,"Semantic understanding of high-resolution remote sensing (RS) images is of great value in Earth observation, however, it heavily depends on numerous pixel-wise manually-labeled data, which is laborious and thereby limits its practical application. Semi-supervised semantic segmentation (SSS) of RS images would be a promising solution, which uses both limited labeled data and dominant unlabeled data to train segmentation models, significantly mitigating the annotation burden. The current mainstream methods of remote sensing semi-supervised semantic segmentation (RS-SSS) utilize the hard or soft pseudo-labels of unlabeled data for model training and achieve excellent performance. Nevertheless, their performance is bottlenecked because of two inherent problems: irreversible wrong pseudo-labels and long-tailed distribution among classes. Aiming at them, we propose a decoupled weighting learning (DWL) framework for RS-SSS in this study, which consists of two novel modules, decoupled learning and ranking weighting, corresponding to the above two problems, respectively. During training, the decoupled learning module separates the predictions of the labeled and unlabeled data to decrease the negative impact of the self-training of the wrongly pseudo-labeled unlabeled data on the supervised training of the labeled data. Furthermore, the ranking weighting module tries to adaptively weight each pseudo-label of the unlabeled data according to its relative confidence ranking in its pseudo-class to alleviate model bias to majority classes as a result of the long-tailed distribution. To verify the effectiveness of the proposed DWL framework, extensive experiments are conducted on three widely- used RS semantic segmentation datasets in the semi-supervised setting. The experimental results demonstrate the superiority of our method to some state-of-the-art SSS methods. Our code will be available at https: //github.com/zhu-xlab/RS-DWL.",Remote sensing,Semi-supervised semantic segmentation,Decoupled learning,Weighting learning,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_90,"Wang, Junxiao","Feng, Zhixi","Jiang, Yao","Yang, Shuyuan",Orientation Attention Network for semantic segmentation of remote sensing images?,,MAY 12 2023,14,"With the increasing resolution of remote sensing images, semantic segmentation has become a challenging task for the extremely abundant textures and edges that existed in images. In this paper, an Orientation Attention Network (OANet) is proposed to learn both orientation features and global semantic features of ground objects for accurate segmentation. Firstly, an Asymmetrical Convolution (AC) is constructed to explore the directional anisotropy of objects. Then an Orientation Attention Module (OAM) is advanced to enhance the intrinsic geometric features of objects, by defining two branches with stacked asymmetrical convolutions along the coordinate axis and adaptively selecting features which are beneficial for segmentation. Finally, the OANet, which combines OAM with a Global Feature Module (GFM), is proposed for both structural and semantic sensitive representations of images. Extensive experiments on four well-known public datasets show the effectiveness of the OANet.",Semantic segmentation,Remote sensing images,Asymmetrical convolution,Orientation attention,,"Meng, Huixiao",,,,KNOWLEDGE-BASED SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_91,"Zhou, Han","Yang, Jianyu","Zhang, Tingting","Dai, Anjin",EAS-CNN: automatic design of convolutional neural network for remote sensing images semantic segmentation,,JUL 3 2023,1,"Accurate and effective semantic segmentation methods for remote sensing are important for applications such as precision agriculture, urban planning, and disaster monitoring. Convolutional neural networks (CNN) have achieved remarkable performance in the field of remote sensing semantic segmentation. However, the design of CNNs is both time-consuming and necessitates a substantial amount of domain expertise and experience. To address the aforementioned issues, we propose a neural architecture search method called EAS-CNN. The method constructs a search space based on a U-shaped structure and utilizes a fixed-length encoding solution based on gene expression suppression to preserve potential useful information during the evolution process. Furthermore, an improved genetic strategy is proposed to enhance search efficiency and save computational resources. In this paper, we evaluate the proposed EAS-CNN method against state-of-the-art semantic segmentation methods and verify its effectiveness. Experimental results show that EAS-CNN achieves high OA values of 91.2% and 91.6% on the Vaihingen and Postman datasets, respectively. Furthermore, we conduct a thorough analysis of the experimental results and summarize effective design patterns for model architecture to enhance remote sensing semantic segmentation tasks.",Remote sensing semantic segmentation,neural network architecture search,convolutional neural network,>,,"Wu, Chunxiao",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_92,"He, Shuyi","Li, Qingyong","Liu, Yang","Wang, Wen",Semantic Segmentation of Remote Sensing Images With Self-Supervised Semantic-Aware Inpainting,,2022,5,"Semantic segmentation of remote sensing imageries plays a crucial role in resource exploration, urban planning, weather forecasting, etc. For this task, deep learning-based methods have shown significant achievement, typically trained with large-scale labeled data. However, these methods often suffer the performance deterioration facing limited labeled data in real-world applications. To address this problem, a novel self-supervised semantic segmentation framework is proposed for remote sensing imageries with limited labeled data. Specifically, image inpainting is acted as pixel-level pretext task for learning dense feature representations suitable for semantic segmentation. Furthermore, rather than trivially leveraging the conventional random inpainting strategy, a novel adversarial training scheme is proposed to drive the pretext task to adaptively mask and restore salient local regions. The adversarial training scheme consists of instructor network and inpainting network, the instructor network increasingly predicts meaningful salient regions as erased regions, and meanwhile the inpainting network seeks for restoring the corrupted image as pretext task to learn its intrinsic representation. Moreover, the structural similarity (SSIM) is applied as a patch-level loss function for semantic segmentation considering that remote sensing images are highly structured. The experimental results on the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam dataset demonstrate that our method outperforms state-of-the-art self-supervised methods and the ImageNet pre-training methods. The source code is available at https://github.com/JasmineBJTU/self-supervised_RSSS",Task analysis,Remote sensing,Training,Image restoration,Semantics,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Self-supervised learning,Image reconstruction,Adversarial training,image inpainting,self-supervised learning,semantic segmentation,structural similarity (SSIM),,,,,,,,,,,,,,,,,,,,,,,
Row_93,"Zhao, Jiaqi","Zhang, Di","Shi, Boyu","Zhou, Yong",Multi-source collaborative enhanced for remote sensing images semantic segmentation,,JUL 7 2022,22,"Remote sensing images semantic segmentation is a difficult instance of image understanding. Due to the regional variability and uncertainty of real-world ground cover features, the semantic segmentation of remote sensing images becomes a challenging task. In this paper, we propose an end-to-end multisource remote sensing image semantic segmentation network (MCENet) aiming at the problems of intra-class inconsistency and inter-class indistinguishability in remote sensing images. Firstly, we design a collaborative enhanced fusion module to mine complementary characteristics of multi-source remote sensing images. Among them, the collaborative fusion module is used to solve the problem of intra-class difference, and the enhanced aggregation module is used to solve the problem of inter-class similarity. Secondly, a multi-scale decoder is proposed to improve the robustness of the model for small targets and large-scale changes by learning scale invariance features. Experimental results show that our method achieved 2.2% and 1.11% mean intersection over union (mIoU) score improvements compared with other methods on the US3D and ISPRS Potsdam data sets, respectively. In addition, the method proposed in this paper also has strong competitiveness in terms of parameter quantity and inference speed. (c) 2022 Elsevier B.V. All rights reserved.",Semantic segmentation,Multi-source remote sensing image,Collaborative enhanced fusion,Multi-scale feature decoder,,"Chen, Jingyang","Yao, Rui","Xue, Yong",,NEUROCOMPUTING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_94,"Jia, Jintong","Song, Jiarui","Kong, Qingqiang","Yang, Huan",Multi-Attention-Based Semantic Segmentation Network for Land Cover Remote Sensing Images,,MAR 2023,5,"Semantic segmentation is a key technology for remote sensing image analysis widely used in land cover classification, natural disaster monitoring, and other fields. Unlike traditional image segmentation, there are various targets in remote sensing images, with a large feature difference between the targets. As a result, segmentation is more difficult, and the existing models retain low accuracy and inaccurate edge segmentation when used in remote sensing images. This paper proposes a multi-attention-based semantic segmentation network for remote sensing images in order to address these problems. Specifically, we choose UNet as the baseline model, using a coordinate attention-based residual network in the encoder to improve the extraction capability of the backbone network for fine-grained features. We use a content-aware reorganization module in the decoder to replace the traditional upsampling operator to improve the network information extraction capability, and, in addition, we propose a fused attention module for feature map fusion after upsampling, aiming to solve the multi-scale problem. We evaluate our proposed model on the WHDLD dataset and our self-labeled Lu County dataset. The model achieved an mIOU of 63.27% and 72.83%, and an mPA of 74.86% and 84.72%, respectively. Through comparison and confusion matrix analysis, our model outperformed commonly used benchmark models on both datasets.",remote sensing image,attention mechanism,image segmentation,deep learning,semantic segmentation,"Teng, Yunhe","Song, Xuan",,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_95,"He, Guangjun","Dong, Zhe","Feng, Pengming","Muhtar, Dilxat",Dual-Range Context Aggregation for Efficient Semantic Segmentation in Remote Sensing Images,,2023,9,"Although introducing self-attention mechanisms is beneficial to establish long-range dependencies and explore global context information in the task of remote sensing image semantic segmentation, it results in expensive computation and large memory cost. In this letter, we address this dilemma by proposing a lightweight dual-range context aggregation network (LDCANet) for efficient remote sensing image semantic segmentation. First, a dual-range context aggregation module (DCAM) is designed to aggregate the local features and the global semantic context acquired by convolutions and self-attention, respectively, where self-attention is implemented easily by applying two cascaded linear layers to reduce the computational complexity. Furthermore, a simple and lightweight decoder is employed to combine information from different levels, in which a multilayer perceptron (MLP)-based efficient linear block (ELB) is proposed to yield a strong and efficient representation. Experiments conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen dataset and the Gaofen Image dataset (GID) prove that our LDCANet achieves an excellent trade-off between segmentation accuracy and computational efficiency. In particular, our method achieves 74.12% mean intersection over union (mIoU) on the ISPRS Vaihingen dataset and 61.42% mIoU on the GID with only 4.98-M parameter size.",Convolutional neural networks,lightweight network,multilayer perceptron (MLP),remote sensing,semantic segmentation,"Zhang, Xueliang",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_96,"Broni-Bediako, Clifford","Xia, Junshi","Song, Jian","Chen, Hongruixuan",Generalized Few-Shot Semantic Segmentation in Remote Sensing: Challenge and Benchmark,,2024,0,"Learning with limited labeled data is a challenging problem in various applications, including remote sensing. Few-shot semantic segmentation is one approach that can encourage deep learning models to learn from few labeled examples for novel classes not seen during the training. The generalized few-shot segmentation setting has an additional challenge which encourages models not only to adapt to the novel classes but also to maintain strong performance on the training base classes. While previous datasets and benchmarks discussed the few-shot segmentation setting in remote sensing, we are the first to propose a generalized few-shot segmentation benchmark for remote sensing. The generalized setting is more realistic and challenging, which necessitates exploring it within the remote sensing context. We release the dataset augmenting OpenEarthMap (OEM) with additional classes labeled for the generalized few-shot evaluation setting. The dataset is released during the OEM land cover mapping generalized few-shot challenge in the learning with limited labeled data for image and video understanding (L3D-IVU) workshop in conjunction with computer vision and pattern recognition (CVPR) 2024. In this work, we summarize the dataset and challenge details in addition to providing the benchmark results on the two phases of the challenge for the validation and test sets.",Benchmark dataset,deep learning,few-shot semantic segmentation,few-shot semantic segmentation,land cover mapping,"Siam, Mennatullah","Yokoya, Naoto",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,land cover mapping,remote sensing,remote sensing,remote sensing,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_97,"Wang, Jia-Xin","Chen, Si-Bao","Ding, Chris H. Q.","Tang, Jin",RanPaste: Paste Consistency and Pseudo Label for Semisupervised Remote Sensing Image Semantic Segmentation,,2022,44,"With the development of deep learning, remote sensing (RS) image segmentation has been applied with marked success. However, in the process of model training, the large number of labeled images required more expensive annotation. A key challenge is how to make full use of extensive unlabeled images available to improve the segmentation model. In this article, we propose a semisupervised remote sensing image semantic segmentation method defined as RanPaste, which combines labeled images with unlabeled images to improve segmentation performance. First, we obtain pseudo label by randomly pasting part of the ground truth label into the predicted segmentation map. Then, we combine the labeled and unlabeled images to generate rough predictions after strong augmentation. Finally, by using the semisupervised loss, we achieve better performance on remote sensing image segmentation. Our method combines consistency regularization and pseudo label and then utilizes thresholds to gradually improve the model performance. RanPaste enables the model to learn more underlying information in the unlabeled data. Experimental results on six datasets show that RanPaste can learn more latent information from unlabeled data to improve segmentation performance. Besides, our approach achieves better segmentation results on different network structures and datasets.",Image segmentation,Remote sensing,Semantics,Training,Roads,"Luo, Bin",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Task analysis,Sensors,Convolutional neural network,remote sensing (RS),semantic segmentation,semisupervised learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_98,"Cheng, Shiwei","Li, Baozhu","Sun, Le","Chen, Yuwen",HRRNet: Hierarchical Refinement Residual Network for Semantic Segmentation of Remote Sensing Images,,MAR 2023,7,"Semantic segmentation of high-resolution remote sensing images plays an important role in many practical applications, including precision agriculture and natural disaster assessment. With the emergence of a large number of studies on convolutional neural networks, the performance of the semantic segmentation model of remote sensing images has been dramatically promoted. However, many deep convolutional network models do not fully refine the segmentation result maps, and, in addition, the contextual dependencies of the semantic feature map have not been adequately exploited. This article proposes a hierarchical refinement residual network (HRRNet) to address these issues. The HRRNet mainly consists of ResNet50 as the backbone, attention blocks, and decoders. The attention block consists of a channel attention module (CAM) and a pooling residual attention module (PRAM) and residual structures. Specifically, the feature map output by the four blocks of Resnet50 is passed through the attention block to fully explore the contextual dependencies of the position and channel of the semantic feature map, and, then, the feature maps of each branch are fused step by step to realize the refinement of the feature maps, thereby improving the segmentation performance of the proposed HRRNet. Experiments show that the proposed HRRNet improves segmentation result maps compared with various state-of-the-art networks on Vaihingen and Potsdam datasets.",deep convolution convolutional neural network,attention mechanism,semantic segmentation,remote sensing images,residual structure,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_99,"Wang, Libo","Zhang, Ce","Li, Rui","Duan, Chenxi",Scale-Aware Neural Network for Semantic Segmentation of Multi-Resolution Remote Sensing Images,,DEC 2021,14,"Assigning geospatial objects with specific categories at the pixel level is a fundamental task in remote sensing image analysis. Along with the rapid development of sensor technologies, remotely sensed images can be captured at multiple spatial resolutions (MSR) with information content manifested at different scales. Extracting information from these MSR images represents huge opportunities for enhanced feature representation and characterisation. However, MSR images suffer from two critical issues: (1) increased scale variation of geo-objects and (2) loss of detailed information at coarse spatial resolutions. To bridge these gaps, in this paper, we propose a novel scale-aware neural network (SaNet) for the semantic segmentation of MSR remotely sensed imagery. SaNet deploys a densely connected feature network (DCFFM) module to capture high-quality multi-scale context, such that the scale variation is handled properly and the quality of segmentation is increased for both large and small objects. A spatial feature recalibration (SFRM) module was further incorporated into the network to learn intact semantic content with enhanced spatial relationships, where the negative effects of information loss are removed. The combination of DCFFM and SFRM allows SaNet to learn scale-aware feature representation, which outperforms the existing multi-scale feature representation. Extensive experiments on three semantic segmentation datasets demonstrated the effectiveness of the proposed SaNet in cross-resolution segmentation.",deep convolutional neural network,multiple spatial resolutions,remote sensing,scale-aware feature representation,semantic segmentation,"Meng, Xiaoliang","Atkinson, Peter M.",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_100,"Wang, Shunli","Hu, Qingwu","Wang, Shaohua","Zhao, Pengcheng",Category attention guided network for semantic segmentation of Fine-Resolution remote sensing images,,MAR 2024,3,"The semantic segmentation task is an essential issue in various fields, including land cover classification and cultural heritage investigation. The CNN and Transformer have been widely utilized in semantic segmentation tasks due to notable advancements in deep learning technologies. However, these methodologies may not fully account for remote sensing images' distinctive attributes, including the large intra-class variation and the small inter-class variation. Driven by it, we propose a category attention guided network (CAGNet). Initially, a local feature extraction module is devised to cater to striped objects and features at different scales. Then, we propose a novel concept of category attention for remote sensing images as a feature representation of category differences between pixels. Meanwhile, we designed the Transformer-based and CNN-based category attention guided modules to integrate the proposed category attention into the global scoring functions and local category feature weights, respectively. The network is designed to give more attention to the category features by updating these weights during the training process. Finally, a feature fusion module is developed to integrate global, local, and category multi-scale features and contextual information. A series of extensive experiments along with ablation studies on the UAVid, Vaihingen, and Potsdam datasets indicate that our network outperforms existing methods, including those based on CNN and Transformer.",Category attention,Semantic segmentation,Remote sensing images,CNN,Transformer,"Li, Jiayuan","Ai, Mingyao",,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_101,"Liang, Min","Wang, XiLi",,,DOMAIN ADAPTATION AND SUPER-RESOLUTION BASED BI-DIRECTIONAL SEMANTIC SEGMENTATION METHOD FOR REMOTE SENSING IMAGES,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,1,"Image semantic segmentation methods based on convolutional neural network rely on supervised learning with ground truth, thus cannot be well extended to datasets that all of the data are unlabeled. Domain adaptation can solve the problem of inconsistent feature distribution between target and source domains. However, when the spatial resolution of remote sensing images in the source and target domains are not the same, those domain adaptation methods are not effective. In this paper, we propose a bi-directional semantic segmentation method based on super-resolution and domain adaption (BSSM-SRDA). With the help of generative adversarial learning, the method accomplishes semantic segmentation task from a low-resolution labelled data source domain to a high-resolution unlabelled data target domain by reducing differences in resolution and feature distribution. In addition, we propose a self-supervised learning algorithm that helps the domain discriminator to focus on those target data that has not been aligned with the source domain. The experiments demonstrate the superiority of the proposed method over other state-of-the-art methods on two remote sensing image datasets.",Remote sensing image,semantic segmentation,domain adaptation,super resolution,self-supervised learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_102,"Feng, Jiangfan","Chen, Panyu","Gu, Zhujun","Zeng, Maimai",MDSNet: a multiscale decoupled supervision network for semantic segmentation of remote sensing images,,DEC 31 2023,3,"Recent deep-learning successes have led to a new wave of semantic segmentation in remote sensing (RS) applications. However, most approaches rarely distinguish the role of the body and edge of RS ground objects; thus, our understanding of these semantic parts has been frustrated by the lack of detailed geometry and appearance. Here we present a multiscale decoupled supervision network for RS semantic segmentation. Our proposed framework extends a densely supervised encoder-decoder network with a feature decoupling module that can decouple semantic features with different scales into distinct body and edge components. We further conduct multiscale supervision of the original and decoupled body and edge features to enhance inner consistency and spatial boundaries in remote sensing image (RSI) ground objects, enabling new segmentation designs and semantic components that can learn to perform multiscale geometry and appearance. Our results outperform the previous algorithm and are robust to different datasets. These results demonstrate that decoupled supervision is an effective solution to semantic segmentation tasks of RS images.",Semantic segmentation,remote sensing images,edge supervision,multiscale,y,"Zheng, Wei",,,,INTERNATIONAL JOURNAL OF DIGITAL EARTH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_103,"Feng, Jiangfan","Yang, Xinyu","Gu, Zhujun","Zeng, Maimai",SMBCNet: A Transformer-Based Approach for Change Detection in Remote Sensing Images through Semantic Segmentation,,JUL 2023,6,"Remote sensing change detection (RSCD) is crucial for our understanding of the dynamic pattern of the Earth's surface and human influence. Recently, transformer-based methodologies have advanced from their powerful global modeling capabilities in RSCD tasks. Nevertheless, they remain under excessive parameterization, which continues to be severely constrained by time and computation resources. Here, we present a transformer-based RSCD model called the Segmentation Multi-Branch Change Detection Network (SMBCNet). Our proposed approach combines a hierarchically structured transformer encoder with a cross-scale enhancement module (CEM) to extract global information with lower complexity. To account for the diverse nature of changes, we introduce a plug-and-play multi-branch change fusion module (MCFM) that integrates temporal features. Within this module, we transform the change detection task into a semantic segmentation problem. Moreover, we identify the Temporal Feature Aggregation Module (TFAM) to facilitate integrating features from diverse spatial scales. These results demonstrate that semantic segmentation is an effective solution to change detection (CD) problems in remote sensing images.",change detection,remote sensing image,semantic segmentation,transformer,,"Zheng, Wei",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_104,"He, Xin","Zhou, Yong","Liu, Bing","Zhao, Jiaqi",Remote sensing image semantic segmentation via class-guided structural interaction and boundary perception,,OCT 15 2024,2,"Existing remote sensing semantic segmentation methods generally ignore the structural information of objects that is vital in the human visual recognition system. The absence of overall structural information often results in weak perceptions of subtle textures and fragmented predictions, especially for complex and variable ground object scenarios. Besides, they still suffer from the semantic ambiguity caused by the unclear object boundary features in remote sensing images. In this paper, we propose a novel remote sensing semantic segmentation framework, called CSBNet, which aims to enhance the capacity of class-guided structural interaction and boundary perception simultaneously. It consists of a class-guided structure interaction module (CSIM), a Transformer-based context aggregation module (TCAM) and a class-guided boundary supervision module (CBSM). The CSIM has the ability to progressively extract the class-specific structural features, i.e., refining the structural information of each class by iteratively exchanging information between initial coarse class tokens and contexts. Meanwhile, the TCAM is constructed to provide CSIM with more discriminative multi- scale contexts without losing spatial features. In particular, the CBSM plays an auxiliary role, which applies the boundary information obtained from the class tokens to supervise the segmentation of boundary regions. When tested on the ISPRS dataset, LoveDA dataset, UAVid dataset, our method significantly outperforms the state-of-the-art remote sensing semantic segmentation approaches.",Transformer,Remote sensing,Semantic segmentation,Structural information,Boundary learning,"Yao, Rui",,,,EXPERT SYSTEMS WITH APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_105,"Wen, Zhiqiang","Huang, Hongxu","Liu, Shuai",,Multi-scale attention fusion network for semantic segmentation of remote sensing images,,DEC 17 2023,1,"In the realm of high-resolution remote sensing image (HRSI) segmentation, convolutional neural networks have shown their effectiveness and superiority. However, there are still two problems in the segmentation model that generally adopts the encoder-decoder structure in the face of HRSI: 1) Fusing high-level feature maps and low-level feature maps directly in the decoder will make spatial detail features easy to mask; 2) Although self-attention has been used to capture the long-distance dependence of features, the consumption of computing power and memory makes it have many restrictions in practical applications. Aiming at these two problems, this paper proposes a new HRSI segmentation model (named MLWNet). First, the introduction of the maximum pooling module improves the quality of the feature map and obtains the receptive field of the whole map and rich global semantic information. Then, based on a new linear complexity self-attention mechanism, we design a multi-scale linear self-attention module to abstract the correlation between contexts. Finally, the weighted feature fusion helps the feature map restore spatial details and refine the segmentation results. On the two HRSI datasets of ISPRS Potsdam and ISPRS Vaihingen, MLWNet achieved mIOU segmentation accuracy of 78.19% and 71.61%, respectively, which not only outperforms other mainstream segmentation models but also has only 17.423 M parameters. The segmentation model in this study has high precision and small parameters, which can provide decision information for real-time use of remote sensing images.",High-resolution remote sensing images,self-attention,semantic segmentation,,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_106,"Li, Xin","Xu, Feng","Li, Linyang","Xu, Nan",AAFormer: Attention-Attended Transformer for Semantic Segmentation of Remote Sensing Images,,2024,9,"The rapid advancements in remote sensing technology have enabled the widespread availability of fine-resolution remote sensing images (RSIs), offering rich spatial details and semantics. Despite the applicability and scalability of transformers in semantic segmentation of RSIs by learning pairwise contextual affinity, they inevitably introduce irrelevant context, hindering accurate inference of patch semantics. To address this, we propose a novel multihead attention-attended module (AAM) that refines the multihead self-attention mechanism (AM). The AAM filters out irrelevant context while highlighting informative ones by considering the relevance between self-attention maps and the query vector. The AAM generates an attention gate to complement contextual affinity and emphasize the useful ones with a higher weight simultaneously. Leveraging multihead AAM as the core unit, we construct a lightweight attention-attended transformer block (ATB). Subsequently, we devise AAFormer, a pure transformer with a mask transformer decoder, for achieving semantic segmentation of RSIs. We extensively evaluate our approach on the ISPRS Potsdam and LoveDA datasets, demonstrating compelling performance compared to mainstream methods. Additionally, we conduct evaluations to analyze the effects of AAM.",Active appearance model,Transformers,Semantic segmentation,Remote sensing,Semantics,"Liu, Fan","Yuan, Chi","Chen, Ziqi","Lyu, Xin",IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Decoding,Merging,High-resolution remote sensing images (RSIs),local and global contexts,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_107,"Li, Xin","Xu, Feng","Xia, Runliang","Lyu, Xin",Hybridizing Cross-Level Contextual and Attentive Representations for Remote Sensing Imagery Semantic Segmentation,,AUG 2021,18,"Semantic segmentation of remote sensing imagery is a fundamental task in intelligent interpretation. Since deep convolutional neural networks (DCNNs) performed considerable insight in learning implicit representations from data, numerous works in recent years have transferred the DCNN-based model to remote sensing data analysis. However, the wide-range observation areas, complex and diverse objects and illumination and imaging angle influence the pixels easily confused, leading to undesirable results. Therefore, a remote sensing imagery semantic segmentation neural network, named HCANet, is proposed to generate representative and discriminative representations for dense predictions. HCANet hybridizes cross-level contextual and attentive representations to emphasize the distinguishability of learned features. First of all, a cross-level contextual representation module (CCRM) is devised to exploit and harness the superpixel contextual information. Moreover, a hybrid representation enhancement module (HREM) is designed to fuse cross-level contextual and self-attentive representations flexibly. Furthermore, the decoder incorporates DUpsampling operation to boost the efficiency losslessly. The extensive experiments are implemented on the Vaihingen and Potsdam benchmarks. In addition, the results indicate that HCANet achieves excellent performance on overall accuracy and mean intersection over union. In addition, the ablation study further verifies the superiority of CCRM.",semantic segmentation,remote sensing imagery,cross-level contextual information,representation enhancement,,"Gao, Hongmin","Tong, Yao",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_108,"Zhao, Danpei","Yuan, Bo","Gao, Yue","Qi, Xinhu",UGCNet: An Unsupervised Semantic Segmentation Network Embedded With Geometry Consistency for Remote-Sensing Images,,2022,7,"In remote-sensing image (RSI) semantic segmentation, the dependence on large-scale and pixel-level annotated data has been a critical factor restricting its development. In this letter, we propose an unsupervised semantic segmentation network embedded with geometry consistency (UGCNet) for RSIs, which imports the adversarial-generative learning strategy into a semantic segmentation network. The proposed UGCNet can be trained on a source-domain dataset and achieve accurate segmentation results on a different target-domain dataset. Furthermore, for refining the remote-sensing target geometric representation such as densely distributed buildings, we propose a geometry-consistency (GC) constraint that can be embedded in both image-domain adaptation process and semantic segmentation network. Therefore, our model could achieve cross-domain semantic segmentation with target geometric property preservation. The experimental results on Massachusetts and Inria buildings datasets prove that the proposed unsupervised UGCNet could achieve a very comparable segmentation accuracy with the fully supervised model, which validates the effectiveness of the proposed method.",Image segmentation,Semantics,Training,Adaptation models,Remote sensing,"Shi, Zhenwei",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Geometry,Decoding,Generative-adversarial learning,geometry consistency (GC),remote-sensing images (RSIs),semantic segmentation,unsupervised,,,,,,,,,,,,,,,,,,,,,,,
Row_109,"Chen, Guanzhou","He, Chanjuan","Wang, Tong","Zhu, Kun",A Superpixel-Guided Unsupervised Fast Semantic Segmentation Method of Remote Sensing Images,,2022,8,"Semantic segmentation is one of the fundamental tasks of pixel-level remote sensing image analysis. Currently, most high-performance semantic segmentation methods are trained in a supervised learning manner. These methods require a large number of image labels as support, but manual annotations are difficult to obtain. To address the problem, we propose an efficient unsupervised remote sensing image segmentation method based on superpixel segmentation and fully convolutional networks (FCNs) in this letter. Our method can achieve pixel-level images segmentation of various scales rapidly without any manual labels or prior knowledge. We use the superpixel segmentation results as synthetic ground truth to guide the gradient descent direction during FCN training. In experiments, our method achieved high performance compared with current unsupervised image segmentation methods on three public datasets. Specifically, our method achieves an adjusted mutual information (AMI) score of 0.2955 on the Gaofen Image Dataset (GID), while processing each image of size 7200 x 6800 pixels in just 30 s.",Deep learning (DL),fully convolutional networks (FCNs),remote sensing,semantic segmentation,superpixel,"Liao, Puyun","Zhang, Xiaodong",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,unsupervised learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_110,"Guo, Shichen","Yang, Qi","Xiang, Shiming","Wang, Pengfei",Dynamic High-Resolution Network for Semantic Segmentation in Remote-Sensing Images,,APR 26 2023,2,"Semantic segmentation of remote-sensing (RS) images is one of the most fundamental tasks in the understanding of a remote-sensing scene. However, high-resolution RS images contain plentiful detailed information about ground objects, which scatter everywhere spatially and have variable sizes, styles, and visual appearances. Due to the high similarity between classes and diversity within classes, it is challenging to obtain satisfactory and accurate semantic segmentation results. This paper proposes a Dynamic High-Resolution Network (DyHRNet) to solve this problem. Our proposed network takes HRNet as a super-architecture, aiming to leverage the important connections and channels by further investigating the parallel streams at different resolution representations of the original HRNet. The learning task is conducted under the framework of a neural architecture search (NAS) and channel-wise attention module. Specifically, the Accelerated Proximal Gradient (APG) algorithm is introduced to iteratively solve the sparse regularization subproblem from the perspective of neural architecture search. In this way, valuable connections are selected for cross-resolution feature fusion. In addition, a channel-wise attention module is designed to weight the channel contributions for feature aggregation. Finally, DyHRNet fully realizes the dynamic advantages of data adaptability by combining the APG algorithm and channel-wise attention module simultaneously. Compared with nine classical or state-of-the-art models (FCN, UNet, PSPNet, DeepLabV3+, OCRNet, SETR, SegFormer, HRNet+FCN, and HRNet+OCR), DyHRNet has shown high performance on three public challenging RS image datasets (Vaihingen, Potsdam, and LoveDA). Furthermore, the visual segmentation results, the learned structures, the iteration process analysis, and the ablation study all demonstrate the effectiveness of our proposed model.",semantic segmentation,remote-sensing image,neural architecture search,sparse regularization,HRNet,"Wang, Xuezhi",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_111,"Zheng, Chen","Li, Jingying","Chen, Yuncheng","Wang, Leiguang",A Generalization Sample Learning Method of Deep Learning for Semantic Segmentation of Remote Sensing Images,,2023,1,"Deep learning methods have been widely studied in the semantic segmentation field of the remote sensing image. Training images play an important role in these methods; however, each training image usually contains not only the generalization information of each land category but also the specific interclass context between different categories. The specific interclass context prevents deep learning methods from focusing on generalization information learning during training and limits the performance on different data distributions. This article proposes a generalization sampling learning method of deep convolutional neural network (GSL-CNN) to emphasize generalization information learning for the semantic segmentation of remote sensing images. The proposed method develops a new CBR sampling strategy that contains three modules: category grouping (C), basic unit extraction (B), and random combination (R). Module C collects each land category map and strips away the specific interclass context from the raw annotated image. Module B extracts basic units with different granularities from each land category map, and each basic unit can keep the generalization information of this category. Module R aims to enhance the robustness against different data distributions by randomly picking basic units of different categories and randomly generating their interclass context. The new GSL-CNN method integrates the CBR sampling strategy with the convolutional neural network (CNN) model for semantic segmentation. Experiments on different remote sensing datasets and 15 state-of-the-art CNN models validated that the proposed method has the potential of improving the generalization ability of the CNN method from a sampling perspective.",Deep learning,generalization,remote sensing,sampling,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_112,"Sun, Wenjie","Zhang, Jie","Lei, Yujie","Hong, Danfeng",RSProtoSeg: High Spatial Resolution Remote Sensing Images Segmentation Based on Non-Learnable Prototypes,,2024,3,"Semantic segmentation of high spatial resolution (HSR) remote sensing images presents unique challenges due to the imbalanced foreground-background distribution and large intraclass variance. This study proposes a novel semantic segmentation algorithm based on non-learnable prototypes, named RSProtoSeg. This approach optimizes the spatial relationship between foreground-background prototypes and intraclass prototypes. Specifically, we propose a foreground-background distance optimization loss function to enhance sparsity between these phototypes, effectively mitigating foreground-background distribution imbalances. Moreover, we introduce an online discrete clustering module that represents each class with a set of prototypes and adds an adaptive regular term penalty to promote sparse structure and reduce the variance issue. Evaluation on three remote sensing datasets (iSAID, ISPRS Potsdam, and Vaihingen) demonstrates significant accuracy improvements, aligning our approach with state-of-the-art methods. Our non-learnable prototype-based approach offers a promising solution for semantic segmentation in HSR remote sensing images.",Neural network,prototype learning,remote sensing,semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_113,"Du, Wen-Liang","Gu, Yang","Zhao, Jiaqi","Zhu, Hancheng",A Mamba-Diffusion Framework for Multimodal Remote Sensing Image Semantic Segmentation,,2024,0,"Recent advances in deep learning have made significant progress in multimodal remote sensing semantic segmentation. However, current methods face challenges in maintaining geometric consistency, particularly when dealing with large objects, resulting in fragmented segmentation masks. We propose a Mamba-diffusion framework to preserve geometric consistency in segmentation masks. This framework preserves geometric consistency by introducing a generative diffusion-based semantic segmentation pipeline and developing a Mamba-based multimodal fusion model. The fusion model fuses the multimodal images in multiple scales and scanning mechanisms by a double cross-fusion (DCF) module. Then, the cross-modal information is further integrated by a dual-splitting structured state-space (DS-S4) model. Finally, the diffusion-based segmentation pipeline predicts semantic masks by progressively refining random Gaussian noise, guided by fused multimodal features. Our experimental results, verified on WHU-OPT-SAR and Hunan datasets, demonstrate that the proposed framework surpasses state-of-the-art (SOTA) methods by a considerable margin. Our codes are available at https://github.com/WenliangDu/MambaDiffusion.",Semantics,Semantic segmentation,Pipelines,Transformers,Remote sensing,"Yao, Rui","Zhou, Yong",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Visualization,Noise measurement,Training,Shape,Diffusion-based segmentation,Mamba-based fusion,multimodal semantic segmentation,remote sensing,,,,,,,,,,,,,,,,,,,,,
Row_114,"Wang, Zhechao","Cheng, Peirui","Duan, Shujing","Chen, Kaiqiang",DCP-Net: A Distributed Collaborative Perception Network for Remote Sensing Semantic Segmentation,,JUL 2024,1,"Collaborative perception enhances onboard perceptual capability by integrating features from other platforms, effectively mitigating the compromised accuracy caused by a restricted observational range and vulnerability to interference. However, current implementations of collaborative perception overlook the prevalent issues of both limited and low-reliability communication, as well as misaligned observations in remote sensing. To address this problem, this article presents an innovative distributed collaborative perception network (DCP-Net) specifically designed for remote sensing applications. Firstly, a self-mutual information match module is proposed to identify collaboration opportunities and select suitable partners. This module prioritizes critical collaborative features and reduces redundant transmission for better adaptation to weak communication in remote sensing. Secondly, a related feature fusion module is devised to tackle the misalignment between local and collaborative features due to the multiangle observations, improving the quality of fused features for the downstream task. We conduct extensive experiments and visualization analyses using three semantic segmentation datasets, namely Potsdam, iSAID, and DFC23. The results demonstrate that DCP-Net outperforms the existing collaborative perception methods comprehensively, improving mIoU by 2.61% to 16.89% at the highest collaboration efficiency and achieving state-of-the-art performance.",collaborative perception,distributed neural network,semantic segmentation,remote sensing,,"Wang, Zhirui","Li, Xinming","Sun, Xian",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_115,"Qian, Zhaoyong","Cao, Yuhua","Shi, Zengkai","Qiu, Luyi",A Semantic Segmentation Method for Remote Sensing Images based on Deeplab v3,2021 2ND INTERNATIONAL CONFERENCE ON BIG DATA & ARTIFICIAL INTELLIGENCE & SOFTWARE ENGINEERING (ICBASE 2021),2021,5,"As a basic technology for image analysis and scene understanding, image semantic segmentation is widely used in the field of remote sensing images. It can better help humans understand the world's scenes and analyze potential changes from the top view of the earth, which has high practical value and development prospects. In this paper, a semantic segmentation method for high-resolution remote sensing images based on DeepLab v3[9] is proposed. The network structure of this method uses the atrous spatial pyramid pooling (ASPP) to extract the multi-scale feature information of high-resolution remote sensing images. Besides, the experimental part of this paper is evaluated on the public dataset of remote sensing images named ISPRS Vaihingen. Experimental results show that the proposed method can achieve an average accuracy of 81.72% on the ISPRS Vaihingen dataset. Therefore, this method can be used as an automated tool for remote sensing semantic segmentation.",remote sensing images,semantic segmentation,multi-scale feature information,ASPP module,DeepLab v3,"Shi, Chenguang",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_116,"Wang, Jia-Xin","Chen, Si-Bao","Ding, Chris H. Q.","Tang, Jin",Semi-Supervised Semantic Segmentation of Remote Sensing Images With Iterative Contrastive Network,,2022,27,"With the development of deep learning, semantic segmentation of remote sensing images has made great progress. However, segmentation algorithms based on deep learning usually require a huge number of labeled images for model training. For remote sensing images, pixel-level annotation usually consumes expensive resources. To alleviate this problem, this letter proposes a semi-supervised segmentation method of remote sensing images based on an iterative contrastive network. This method combines few labeled images and more unlabeled images to significantly improve the model performance. First, contrastive networks continuously learn more potential information by using better pseudo labels. Then, the iterative training method keeps the differences between models to better improve the segmentation performance. The semi-supervised experiments on different remote sensing datasets prove that this method has a better performance than the related methods. Code is available at https://github.com/VCISwang/ICNet.",Training,Image segmentation,Predictive models,Remote sensing,Data models,"Luo, Bin",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Iterative methods,Semantics,Contrastive network,remote sensing,semantic segmentation,semi-supervised learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_117,"Chen, Guanzhou","Tan, Xiaoliang","Guo, Beibei","Zhu, Kun",SDFCNv2: An Improved FCN Framework for Remote Sensing Images Semantic Segmentation,,DEC 2021,29,"Semantic segmentation is a fundamental task in remote sensing image analysis (RSIA). Fully convolutional networks (FCNs) have achieved state-of-the-art performance in the task of semantic segmentation of natural scene images. However, due to distinctive differences between natural scene images and remotely-sensed (RS) images, FCN-based semantic segmentation methods from the field of computer vision cannot achieve promising performances on RS images without modifications. In previous work, we proposed an RS image semantic segmentation framework SDFCNv1, combined with a majority voting postprocessing method. Nevertheless, it still has some drawbacks, such as small receptive field and large number of parameters. In this paper, we propose an improved semantic segmentation framework SDFCNv2 based on SDFCNv1, to conduct optimal semantic segmentation on RS images. We first construct a novel FCN model with hybrid basic convolutional (HBC) blocks and spatial-channel-fusion squeeze-and-excitation (SCFSE) modules, which occupies a larger receptive field and fewer network model parameters. We also put forward a data augmentation method based on spectral-specific stochastic-gamma-transform-based (SSSGT-based) during the model training process to improve generalizability of our model. Besides, we design a mask-weighted voting decision fusion postprocessing algorithm for image segmentation on overlarge RS images. We conducted several comparative experiments on two public datasets and a real surveying and mapping dataset. Extensive experimental results demonstrate that compared with the SDFCNv1 framework, our SDFCNv2 framework can increase the mIoU metric by up to 5.22% while only using about half of parameters.",fully convolutional networks (FCNs),convolutional neural networks (CNNs),deep learning,semantic segmentation,remote sensing,"Liao, Puyun","Wang, Tong","Wang, Qing","Zhang, Xiaodong",REMOTE SENSING,,SDFCN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_118,"Chen, Yaxiong","Wang, Yujie","Xiong, Shengwu","Lu, Xiaoqiang",Integrating Detailed Features and Global Contexts for Semantic Segmentation in Ultrahigh-Resolution Remote Sensing Images,,2024,4,"Semantic segmentation of ultrahigh-resolution (UHR) remote sensing images is a fundamental task for many downstream applications. Achieving precise pixel-level classification is paramount for obtaining exceptional segmentation results. This challenge becomes even more complex due to the need to address intricate segmentation boundaries and accurately delineate small objects within the remote sensing imagery. To meet these demands effectively, it is critical to integrate two crucial components: global contextual information and spatial detail feature information. In response to this imperative, the multilevel context-aware segmentation network (MCSNet) emerges as a promising solution. MCSNet is engineered to not only model the overarching global context but also extract intricate spatial detail features, thereby optimizing segmentation outcomes. The strength of MCSNet lies in its two pivotal modules, the spatial detail feature extraction (SDFE) module and the refined multiscale feature fusion (RMFF) module. Moreover, to further harness the potential of MCSNet, a multitask learning approach is employed. This approach integrates boundary detection and semantic segmentation, ensuring that the network is well-rounded in its segmentation capabilities. The efficacy of MCSNet is rigorously demonstrated through comprehensive experiments conducted on two established international society for photogrammetry and remote sensing (ISPRS) 2-D semantic labeling datasets: Potsdam and Vaihingen. These experiments unequivocally establish MCSNet stands as a pioneering solution, that delivers state-of-the-art performance, as evidenced by its outstanding mean intersection over union (mIoU) and mean $F1$ -score (mF1) metrics. The code is available at: https://github.com/WUTCM-Lab/MCSNet.",Semantics,Cascade,multilevel fusion,multitask learning,remote sensing,"Zhu, Xiao Xiang","Mou, Lichao",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_119,"Moliner, Eloi","Romero, Luis Salgueiro","Vilaplana, Veronica",,WEAKLY SUPERVISED SEMANTIC SEGMENTATION FOR REMOTE SENSING HYPERSPECTRAL IMAGING,"2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING",2020,7,"This paper studies the problem of training a semantic segmentation neural network with weak annotations, in order to be applied in aerial vegetation images from Teide National Park. It proposes a Deep Seeded Region Growing system which consists on training a semantic segmentation network from a set of seeds generated by a Support Vector Machine. A region growing algorithm module is applied to the seeds to progressively increase the pixel-level supervision. The proposed method performs better than an SVM, which is one of the most popular segmentation tools in remote sensing image applications.",Weakly-supervised segmentation,remote sensing,hyperspectral image,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_120,"Liu, Yuheng","Zhang, Yifan","Wang, Ye","Mei, Shaohui",Rethinking Transformers for Semantic Segmentation of Remote Sensing Images,,2023,45,"Transformer has been widely applied in image processing tasks as a substitute for convolutional neural networks (CNNs) for feature extraction due to its superiority in global context modeling and flexibility in model generalization. However, the existing transformer-based methods for semantic segmentation of remote sensing (RS) images are still with several limitations, which can be summarized into two main aspects: 1) the transformer encoder is generally combined with CNN-based decoder, leading to inconsistency in feature representations; and 2) the strategies for global and local context information utilization are not sufficiently effective. Therefore, in this article, a global-local transformer segmentor (GLOTS) framework is proposed for the semantic segmentation of RS images to acquire consistent feature representations by adopting transformers for both encoding and decoding, in which a masked image modeling (MIM) pretrained transformer encoder is adopted to learn semantic-rich representations of input images and a multiscale global-local transformer decoder is designed to fully exploit the global and local features. Specifically, the transformer decoder uses a feature separation-aggregation module (FSAM) to utilize the feature adequately at different scales and adopts a global-local attention module (GLAM) containing global attention block (GAB) and local attention block (LAB) to capture the global and local context information, respectively. Furthermore, a learnable progressive upsampling strategy (LPUS) is proposed to restore the resolution progressively, which can flexibly recover the fine-grained details in the upsampling process. The experiment results on the three benchmark RS datasets demonstrate that the proposed GLOTS is capable of achieving better performance with some state-of-the-art methods, and the superiority of the proposed framework is also verified by ablation studies. The code will be available at https://github.com/lyhnsn/GLOTS.",Encoder-decoder structure,global-local transformer,remote sensing (RS),semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_121,"Du, Xinran","He, Shumeng","Yang, Houqun","Wang, Chunxiao",Multi-Field Context Fusion Network for Semantic Segmentation of High-Spatial-Resolution Remote Sensing Images,,NOV 2022,4,"High spatial resolution (HSR) remote sensing images have a wide range of application prospects in the fields of urban planning, agricultural planning and military training. Therefore, the research on the semantic segmentation of remote sensing images becomes extremely important. However, large data volume and the complex background of HSR remote sensing images put great pressure on the algorithm efficiency. Although the pressure on the GPU can be relieved by down-sampling the image or cropping it into small patches for separate processing, the loss of local details or global contextual information can lead to limited segmentation accuracy. In this study, we propose a multi-field context fusion network (MCFNet), which can preserve both global and local information efficiently. The method consists of three modules: a backbone network, a patch selection module (PSM), and a multi-field context fusion module (FM). Specifically, we propose a confidence-based local selection criterion in the PSM, which adaptively selects local locations in the image that are poorly segmented. Subsequently, the FM dynamically aggregates the semantic information of multiple visual fields centered on that local location to enhance the segmentation of these local locations. Since MCFNet only performs segmentation enhancement on local locations in an image, it can improve segmentation accuracy without consuming excessive GPU memory. We implement our method on two high spatial resolution remote sensing image datasets, DeepGlobe and Potsdam, and compare the proposed method with state-of-the-art methods. The results show that the MCFNet method achieves the best balance in terms of segmentation accuracy, memory efficiency, and inference speed.",semantic segmentation,high spatial resolution remote sensing images,memory efficiency,,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_122,"Zhang, Yongjun","Wang, Yameng","Wan, Yi","Zhou, Wenming",PointBoost: LiDAR-Enhanced Semantic Segmentation of Remote Sensing Imagery,,2023,3,"Semantic segmentation of imagery is typically reliant on texture information from raster images, which limits its accuracy due to the inherently 2-D nature of the plane. To address the nonnegligible domain gap between different metric spaces, multimodal methods have been introduced that incorporate Light Detection and Ranging (LiDAR) derived feature maps. This converts multimodal joint semantic segmentation between 3-D point clouds and 2-D optical imagery into a feature extraction process for the 2.5-D product, which is achieved by concatenating LiDAR-derived feather maps, such as digital surface models, with the optical images. However, the information sources for these methods are still limited to 2-D, and certain properties of point clouds are lost as a result. In this study, we propose PointBoost, an effective sequential segmentation framework that can work directly with cross-modal data of LiDAR point clouds and imagery, which is able to extract richer semantic features from cross-dimensional and cross-modal information. Ablation experiments demonstrate that PointBoost can take full advantage of the 3-D topological structure between points and attribute information of point clouds, which is often discarded by other methods. Experiments on three multimodal datasets, namely N3C-California, ISPRS Vaihingen, and GRSS DFC 2018, show that our method achieves superior performance with good generalization.",Light Detection and Ranging (LiDAR),remote sensing imagery,semantic segmentation,,,"Zhang, Bin",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_123,"Zhao, Shuang","Feng, Zezhen","Chen, Lei","Li, Guandian",DANet: A Semantic Segmentation Network for Remote Sensing of Roads Based on Dual-ASPP Structure,,AUG 2023,6,"Semantic segmentation of roads in remote-sensing images is a challenging task. This paper proposes a semantic segmentation model, DANet, for remote-sensing image road semantic segmentation. The model addresses the problems of missing, misclassification, and strong jaggedness of segmented target edges faced by other semantic segmentation networks when dealing with complex and diverse remote-sensing images. The proposed model uses two ASPP structures for multi-scale feature fusion and combines the DarkNet network structure for downsampling with the SegNet network structure for upsampling. This improves the model's ability to extract road feature information from remote-sensing images. Using the CHN-CUG Roads Dataset, we have confirmed that the proposed network structure, Re, has demonstrated a 1.15% improvement in accuracy compared to U-Net. Furthermore, the road IoU has shown a 1.09% enhancement compared to HRNet-V2. Additionally, there is a 1.13% increase in F1-score compared to U-Net.",semantic segmentation,remote-sensing images,roads,dual-ASPP modules,,,,,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_124,"Li, Yifan","Liu, Ziqian","Yang, Junli","Zhang, Haopeng",Wavelet Transform Feature Enhancement for Semantic Segmentation of Remote Sensing Images,,DEC 2023,6,"With developments in deep learning, semantic segmentation of remote sensing images has made great progress. Currently, mainstream methods are based on convolutional neural networks (CNNs) or vision transformers. However, these methods are not very effective in extracting features from remote sensing images, which are usually of high resolution with plenty of detail. Operations including downsampling will cause the loss of such features. To address this problem, we propose a novel module called Hierarchical Wavelet Feature Enhancement (WFE). The WFE module involves three sequential steps: (1) performing multi-scale decomposition of an input image based on the discrete wavelet transform; (2) enhancing the high-frequency sub-bands of the input image; and (3) feeding them back to the corresponding layers of the network. Our module can be easily integrated into various existing CNNs and transformers, and does not require additional pre-training. We conducted experiments on the ISPRS Potsdam and ISPRS Vaihingen datasets, with results showing that our method improves the benchmarks of CNNs and transformers while performing little additional computation.",discrete wavelet transform,remote sensing images,feature enhancement,semantic segmentation,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_125,"Xiao, Hongfeng","Yao, Wei","Chen, Haobin","Cheng, Li",SCDA: A Style and Content Domain Adaptive Semantic Segmentation Method for Remote Sensing Images,,OCT 2023,2,"Due to the differences in imaging methods and acquisition areas, remote sensing datasets can exhibit significant variations in both image style and content. In addition, the ground objects can be quite different in scale even within the same remote sensing image. These differences should be considered in remote sensing image segmentation tasks. Inspired by the recently developed domain generalization model WildNet, we propose a domain adaption framework named ""Style and Content Domain Adaptation"" (SCDA) for semantic segmentation tasks involving multiple remote sensing datasets with different data distributions. SCDA uses residual style feature transfer (RSFT) in the shallow layer of the baseline network model to enable source domain images to obtain style features from the target domain and reduce the loss of source domain content information. Considering the scale difference of different ground objects in remote sensing images, SCDA uses the projection of the source domain images, the style-transferred source domain images, and the target domain images to construct a multiscale content adaptation learning (MCAL) loss. This enables the model to capture multiscale target domain content information. Experiments show that the proposed method has obvious domain adaptability in remote sensing image segmentation. When performing cross-domain segmentation tasks from VaihingenIRRG to PotsdamIRRG, mIOU is 48.64%, and the F1 is 63.11%, marking improvements of 1.21% and 0.45%, respectively, compared with state-of-the-art methods. When performing cross-domain segmentation tasks from VaihingenIRRG to PotsdamRGB, the mIOU is 44.38%, an improvement of 0.77% over the most advanced methods. In summary, SCDA improves the semantic segmentation of remote sensing images through domain adaptation for both style and content. It fully utilizes multiple innovative modules and strategies to enhance the performance and the stability of the model.",remote sensing images,domain adaptation,cross-domain semantic segmentation,contrast learning,style transfer,"Li, Bo","Ren, Longfei",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_126,"Xu, Zhiyong","Zhang, Weicun","Zhang, Tianxiang","Li, Jiangyun",HRCNet: High-Resolution Context Extraction Network for Semantic Segmentation of Remote Sensing Images,,JAN 2021,106,"Semantic segmentation is a significant method in remote sensing image (RSIs) processing and has been widely used in various applications. Conventional convolutional neural network (CNN)-based semantic segmentation methods are likely to lose the spatial information in the feature extraction stage and usually pay little attention to global context information. Moreover, the imbalance of category scale and uncertain boundary information meanwhile exists in RSIs, which also brings a challenging problem to the semantic segmentation task. To overcome these problems, a high-resolution context extraction network (HRCNet) based on a high-resolution network (HRNet) is proposed in this paper. In this approach, the HRNet structure is adopted to keep the spatial information. Moreover, the light-weight dual attention (LDA) module is designed to obtain global context information in the feature extraction stage and the feature enhancement feature pyramid (FEFP) structure is promoted and employed to fuse the contextual information of different scales. In addition, to achieve the boundary information, we design the boundary aware (BA) module combined with the boundary aware loss (BAloss) function. The experimental results evaluated on Potsdam and Vaihingen datasets show that the proposed approach can significantly improve the boundary and segmentation performance up to 92.0% and 92.3% on overall accuracy scores, respectively. As a consequence, it is envisaged that the proposed HRCNet model will be an advantage in remote sensing images segmentation.",semantic segmentation,remote sensing,deep learning,high resolution,global context information,,,,,REMOTE SENSING,,boundary,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_127,"Chen, Hui","Qin, Yuanshou","Liu, Xinyuan","Wang, Haitao",An improved DeepLabv3+lightweight network for remote-sensing image semantic segmentation,,APR 2024,4,"To improve the accuracy of remote-sensing image semantic segmentation in complex scenario, an improved DeepLabv3+ lightweight neural network is proposed. Specifically, the lightweight network MobileNetv2 is used as the backbone network. In atrous spatial pyramid pooling (ASPP), to alleviate the gridding effect, the Dilated Convolution in original DeepLabv3+ network is replaced with the Hybrid Dilated Convolution (HDC) module. In addition, the traditional spatial mean pooling is replaced by the strip pooling module (SPN) to improve the local segmentation effect. In the decoder, to obtain the rich low-level target edge information, the ResNet50 residual network is added after the low-level feature fusion. To enhance the shallow semantic information, the efficient and lightweight Normalization-based Attention Module (NAM) is added to capture the feature information of small target objects. The results show that, under the INRIA Aerial Image Dataset and same parameter setting, the Mean Pixel Accuracy (MPA) and Mean Intersection over Union (MIoU) are generally best than DeepLabv3+ , U-Net, and PSP-Net, which are respectively improved by 1.22%, - 0.22%, and 2.22% and 2.17%, 1.35%, and 3.42%. Our proposed method has also a good performance on the small object segmentation and multi-object segmentation. What's more, it significantly converges faster with fewer model parameters and stronger computing power while ensuring the segmentation effect. It is proved to be robust and can provide a methodological reference for high-precision remote-sensing image semantic segmentation.",Remote-sensing image,Semantic segmentation,DeepLabv3+,Deep learning,Lightweight network,"Zhao, Jinling",,,,COMPLEX & INTELLIGENT SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_128,"Zhang, Guangzhen","Jiang, Wangyang",,,Remote Sensing Image Semantic Segmentation Method Based on a Deep Convolutional Neural Network and Multiscale Feature Fusion,,2023,1,"There are many problems with remote sensing images, such as large data scales, complex illumination conditions, occlusion, and dense targets. The existing semantic segmentation methods for remote sensing images are not accurate enough for small and irregular target segmentation results, and the edge extraction results are poor. The authors propose a remote sensing image segmentation method based on a DCNN and multiscale feature fusion. Firstly, an end-to-end remote sensing image segmentation model using complete residual connection and multiscale feature fusion was designed based on a deep convolutional encoder-decoder network. Secondly, weighted high-level features were obtained using an attention mechanism, which better preserved the edges, texture, and other information of remote sensing images. The experimental results on ISPRS Potsdam and Urban Drone datasets show that compared with the comparison methods, this method has better segmentation effect on small and irregular objects and achieves the best segmentation performance while ensuring the computation speed.",Attention Mechanism,Deep Learning,Multiscale Feature Fusion,Remote Sensing Images,Semantic Segmentation,,,,,INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_129,"Zhao, Yuanhao","Sun, Genyun","Ling, Ziyan","Zhang, Aizhu",Point-Based Weakly Supervised Deep Learning for Semantic Segmentation of Remote Sensing Images,,2024,0,"Weakly supervised semantic segmentation methods can effectively alleviate the problem of high cost and difficult access to annotation in traditional methods. Among these approaches, point annotated semantic label not only offers a more affordable option but also provides accurate location and category information, playing an indispensable role in current research. However, point annotation labeling encounters challenges such as missing global and texture information, and limiting segmentation accuracy and efficiency while being susceptible to noise interference. For the above problems, a weakly supervised remote sensing image classification framework based on point annotated semantic label is proposed, which consists of three components: data augmentation, Pixel-Net, and iterative superpixel-based sample expansion (ISSE). First, the data augmentation method is used to generate a sufficient number of training samples. Subsequently, the weakly supervised network Pixel-Net is trained using point annotated semantic labels. PixelNet incorporates traditional image processing techniques such as edge detection and blurring into deep learning, enabling effective learning of edge and spectral semantic details while reducing the impact of noise on classification results. Finally, ISSE leverages contextual information from superpixels and pseudo-labels to enrich the valuable information in weakly supervised labels, thereby improving the model's classification performance. In the experiments, existing semantic segmentation methods and Pixel-Net are evaluated on the Vaihingen and Zurich Summer datasets, and the effectiveness of ISSE is verified. The results show that Pixel-Net achieves the best segmentation accuracy on both datasets, while ISSE can effectively utilize the existing point annotation labels to mitigate the effect of noise and thus improve the accuracy of weakly supervised semantic segmentation.",Point annotated semantic label,remote sensing,semantic segmentation,weakly supervised learning,,"Jia, Xiuping",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_130,"Li, Boyang","Zhang, Yu","Zhang, Youmei","Li, Bin",Dual-Path Feature Fusion Network for Semantic Segmentation of Remote Sensing Images,,2024,0,"Both global contextual information and local texture information are of vital importance for the semantic segmentation of remote sensing images due to the high spatial resolution of remote sensing images and large variations in intraclass object size. In this letter, we propose a novel dual-path feature fusion semantic segmentation network for remote sensing images. A pure convolutional module called dual-path feature extraction (DPFE) module is applied to model global contextual and local texture features simultaneously with low complexity. Inspired by ConvNeXt with comparable global contextual modeling capacity with Transformer, the global path of DPFE draws some successful strategies of ConvNeXt to generate powerful global feature. Meanwhile, an attention feature fusion (AFF) module is proposed, which achieves the global and local feature comprehensive fusion by exploring the correlation of channels through attention mechanism. The proposed network is evaluated on Vaihingen and Potsdam benchmarks and the quantitative results show the proposed network can achieve overall accuracy (OA) of 91.3% and 89.7%, respectively, which are better than several representative semantic segmentation approaches.",Feature extraction,Remote sensing,Transformers,Semantic segmentation,Correlation,"Li, Zhenhao",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Convolutional neural networks,Sensors,Attention mechanism,ConvNeXt,global contextual information,local texture feature,remote sensing image,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,
Row_131,"Long, Wei","Zhang, Yongjun","Cui, Zhongwei","Xu, Yujie",Threshold Attention Network for Semantic Segmentation of Remote Sensing Images,,2023,15,"Semantic segmentation of remote sensing images is essential for various applications, including vegetation monitoring, disaster management, and urban planning. Previous studies have demonstrated that the self-attention mechanism (SA) is an effective approach for designing segmentation networks that can capture long-range pixel dependencies. SA enables the network to model the global dependencies between the input features, resulting in improved segmentation outcomes. However, the high density of attentional feature maps used in this mechanism causes exponential increases in computational complexity. In addition, it introduces redundant information that negatively impacts the feature representation. Inspired by traditional threshold segmentation algorithms, we propose a novel threshold attention mechanism (TAM). This mechanism significantly reduces computational effort while also better modeling the correlation between different regions of the feature map. Based on TAM, we present a threshold attention network (TANet) for semantic segmentation. The TANet consists of an attentional feature enhancement module (AFEM) for global feature enhancement of shallow features and a threshold attention pyramid pooling (TAPP) module for acquiring feature information at different scales for deep features. We have conducted extensive experiments on the international society for photogrammetry and remote sensing (ISPRS) Vaihingen and Potsdam datasets. The results demonstrate the validity and superiority of our proposed TANet compared with most state-of-the-art models.",Feature extraction,Semantics,Semantic segmentation,Computational modeling,Remote sensing,"Zhang, Xuexue",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Correlation,Computational complexity,Remote sensing images,self-attention mechanism (SA),semantic segmentation,threshold attention mechanism (TAM),threshold attention network (TANet),,,,,,,,,,,,,,,,,,,,,,,
Row_132,"Yang, Zhujun","Yan, Zhiyuan","Diao, Wenhui","Zhang, Qiang",Label Propagation and Contrastive Regularization for Semisupervised Semantic Segmentation of Remote Sensing Images,,2023,8,"Remarkable progress based on deep neural networks has been achieved in the semantic segmentation of remote sensing images (RSIs). However, pixel-level labeling is expensive for RSIs. Semisupervised semantic segmentation becomes an alternative approach to reduce the cost of annotation, and it is crucial to utilize efficiently a large number of unlabeled data. Nevertheless, inevitably, there is an unbalanced class distribution between labeled and unlabeled data in a remote sensing scene. Existing semisupervised methods train unlabeled images in isolation from labeled images and only learn reliable pixel pseudo-labels, leading to underutilization of unlabeled images. This article proposes a novel semisupervised semantic segmentation approach based on label propagation and contrastive regularization for RSIs. Specifically, the unlabeled images are augmented by randomly copy-pasting the class regions from labeled images. A prototype feature constraint module is used to enforce the constraint on the pixel features of unlabeled images relying on the prototype features from labeled images, achieving feature alignment on the entire dataset. Furthermore, we present the region contrastive learning (RCL) module that guides the model to learn feature consistency under different perturbations and compact feature representations over class regions on unlabeled images. Extensive experimental results on multiple remote sensing datasets demonstrate that our proposed approach achieves superior performance compared with state-of-the-art semisupervised semantic segmentation methods.",Semantic segmentation,Training,Perturbation methods,Remote sensing,Prototypes,"Kang, Yuzhuo","Li, Junxi","Li, Xinming","Sun, Xian",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Reliability,Sensors,Contrastive regularization,label propagation,remote sensing images (RSIs),semantic segmentation,semisupervised learning (SSL),,,,,,,,,,,,,,,,,,,,,,,
Row_133,"Zhou, Xuanyu","Zhou, Lifan","Gong, Shengrong","Zhang, Haizhen",Hybrid CNN and Transformer Network for Semantic Segmentation of UAV Remote Sensing Images,,MAR 2024,3,"Semantic segmentation of unmanned aerial vehicle (UAV) remote sensing images is a recent research hotspot, offering technical support for diverse types of UAV remote sensing missions. However, unlike general scene images, UAV remote sensing images present inherent challenges. These challenges include the complexity of backgrounds, substantial variations in target scales, and dense arrangements of small targets, which severely hinder the accuracy of semantic segmentation. To address these issues, we propose a convolutional neural network (CNN) and transformer hybrid network for semantic segmentation of UAV remote sensing images. The proposed network follows an encoder-decoder architecture that merges a transformer-based encoder with a CNN-based decoder. First, we incorporate the Swin transformer as the encoder to address the limitations of CNN in global modeling, mitigating the interference caused by complex background information. Second, to effectively handle the significant changes in target scales, we design the multiscale feature integration module (MFIM) that enhances the multiscale feature representation capability of the network. Finally, the semantic feature fusion module (SFFM) is designed to filter the redundant noise during the feature fusion process, which improves the recognition of small targets and edges. Experimental results demonstrate that the proposed method outperforms other popular methods on the UAVid and Aeroscapes datasets.",Remote sensing,semantic segmentation,Swin transformer,unmanned aerial vehicle (UAV),unmanned aerial vehicle (UAV),"Zhong, Shan","Xia, Yu","Huang, Yizhou",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,semantic segmentation,Swin transformer,unmanned aerial vehicle (UAV),,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_134,"Liu, Zhiqiang","Li, Jiaojiao","Song, Rui","Wu, Chaoxiong",Edge Guided Context Aggregation Network for Semantic Segmentation of Remote Sensing Imagery,,MAR 2022,8,"Semantic segmentation of remote sensing imagery (RSI) has obtained great success with the development of deep convolutional neural networks (DCNNs). However, most of the existing algorithms focus on designing end-to-end DCNNs, but neglecting to consider the difficulty of segmentation in imbalance categories, especially for minority categories in RSI, which limits the performance of RSI semantic segmentation. In this paper, a novel edge guided context aggregation network (EGCAN) is proposed for the semantic segmentation of RSI. The Unet is employed as backbone. Meanwhile, an edge guided context aggregation branch and minority categories extraction branch are designed for a comprehensive enhancement of semantic modeling. Specifically, the edge guided context aggregation branch is proposed to promote entire semantic comprehension of RSI and further emphasize the representation of edge information, which consists of three modules: edge extraction module (EEM), dual expectation maximization attention module (DEMA), and edge guided module (EGM). EEM is created primarily for accurate edge tracking. According to that, DEMA aggregates global contextual features with different scales and the edge features along spatial and channel dimensions. Subsequently, EGM cascades the aggregated features into the decoder process to capture long-range dependencies and further emphasize the error-prone pixels in the edge region to acquire better semantic labels. Besides this, the exploited minority categories extraction branch is presented to acquire rich multi-scale contextual information through an elaborate hybrid spatial pyramid pooling module (HSPP) to distinguish categories taking a small percentage and background. On the Tianzhi Cup dataset, the proposed algorithm EGCAN achieved an overall accuracy of 84.1% and an average cross-merge ratio of 68.1%, with an accuracy improvement of 0.4% and 1.3% respectively compared to the classical Deeplabv3+ model. Extensive experimental results on the dataset released in ISPRS Vaihingen and Potsdam benchmarks also demonstrate the effectiveness of the proposed EGCAN over other state-of-the-art approaches.",remote sensing imagery,semantic segmentation,deep learning,context aggregation,,"Liu, Wei","Li, Zan","Li, Yunsong",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_135,"Li, Weitao","Gao, Hui","Su, Yi","Momanyi, Biffon Manyura",Unsupervised Domain Adaptation for Remote Sensing Semantic Segmentation with Transformer,,OCT 2022,11,"With the development of deep learning, the performance of image semantic segmentation in remote sensing has been constantly improved. However, the performance usually degrades while testing on different datasets because of the domain gap. To achieve feasible performance, extensive pixel-wise annotations are acquired in a new environment, which is time-consuming and labor-intensive. Therefore, unsupervised domain adaptation (UDA) has been proposed to alleviate the effort of labeling. However, most previous approaches are based on outdated network architectures that hinder the improvement of performance in UDA. Since the effects of recent architectures for UDA have been barely studied, we reveal the potential of Transformer in UDA for remote sensing with a self-training framework. Additionally, two training strategies have been proposed to enhance the performance of UDA: (1) Gradual Class Weights (GCW) to stabilize the model on the source domain by addressing the class-imbalance problem; (2) Local Dynamic Quality (LDQ) to improve the quality of the pseudo-labels via distinguishing the discrete and clustered pseudo-labels on the target domain. Overall, our proposed method improves the state-of-the-art performance by 8.23% mIoU on Potsdam -> Vaihingen and 9.2% mIoU on Vaihingen -> Potsdam and facilitates learning even for difficult classes such as clutter/background.",unsupervised domain adaptation,semantic segmentation,remote sensing image,transformer,self-training,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_136,"Hu, Hangtao","Cai, Shuo","Wang, Wei","Zhang, Peng",A Semantic Segmentation Approach Based on DeepLab Network in High-Resolution Remote Sensing Images,,2019,2,"Recently, more and more applications for high-resolution remote sensing image intelligent processing are required. Therefore, the semantic segmentation based on deep learning has successfully attracted people's attention. In this paper, the improved Deeplabv3 network is used in the application of image semantic segmentation. The problem of segmenting objects of multiple scales of high-resolution remote sensing image is handled, and the Chinese GaoFen NO. 2(GF-2) remote sensing image is taken as the main research object. Firstly, the original image is pre-processed. Next, use data augmentation and expansion for the pre-processed training image to avoid over-fitting. Finally, it is studied the adaptability and accuracy of the model of high-resolution remote sensing images, while is found the appropriate parameters to improve the precise of the result models compared. And explore the effectiveness of the model in the case of a fewer samples. This model is demonstrated that could be achieved the well classification result.",Remote sensing image classification,Deep learning,Semantic segmentation,,,"Li, Zhiyong",,,,"IMAGE AND GRAPHICS, ICIG 2019, PT III",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_137,"Wang, Yu","Li, Yansheng","Chen, Wei","Li, Yunzhou",DNAS: Decoupling Neural Architecture Search for High-Resolution Remote Sensing Image Semantic Segmentation,,AUG 2022,8,"Deep learning methods, especially deep convolutional neural networks (DCNNs), have been widely used in high-resolution remote sensing image (HRSI) semantic segmentation. In literature, most successful DCNNs are artificially designed through a large number of experiments, which often consume lots of time and depend on rich domain knowledge. Recently, neural architecture search (NAS), as a direction for automatically designing network architectures, has achieved great success in different kinds of computer vision tasks. For HRSI semantic segmentation, NAS faces two major challenges: (1) The task's high complexity degree, which is caused by the pixel-by-pixel prediction demand in semantic segmentation, leads to a rapid expansion of the search space; (2) HRSI semantic segmentation often needs to exploit long-range dependency (i.e., a large spatial context), which means the NAS technique requires a lot of display memory in the optimization process and can be tough to converge. With the aforementioned considerations in mind, we propose a new decoupling NAS (DNAS) framework to automatically design the network architecture for HRSI semantic segmentation. In DNAS, a hierarchical search space with three levels is recommended: path-level, connection-level, and cell-level. To adapt to this hierarchical search space, we devised a new decoupling search optimization strategy to decrease the memory occupation. More specifically, the search optimization strategy consists of three stages: (1) a light super-net (i.e., the specific search space) in the path-level space is trained to get the optimal path coding; (2) we endowed the optimal path with various cross-layer connections and it is trained to obtain the connection coding; (3) the super-net, which is initialized by path coding and connection coding, is populated with kinds of concrete cell operators and the optimal cell operators are finally determined. It is worth noting that the well-designed search space can cover various network candidates and the optimization process can be done efficiently. Extensive experiments on the publicly open GID and FU datasets showed that our DNAS outperformed the state-of-the-art methods, including artificial networks and NAS methods.",decoupling neural architecture search (DNAS),high-resolution remote sensing image,semantic segmentation,deep learning,,"Dang, Bo",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_138,"Zhao, Xin","Guo, Jiayi","Zhang, Yueting","Wu, Yirong",Memory-Augmented Transformer for Remote Sensing Image Semantic Segmentation,,NOV 2021,9,"The semantic segmentation of remote sensing images requires distinguishing local regions of different classes and exploiting a uniform global representation of the same-class instances. Such requirements make it necessary for the segmentation methods to extract discriminative local features between different classes and to explore representative features for all instances of a given class. While common deep convolutional neural networks (DCNNs) can effectively focus on local features, they are limited by their receptive field to obtain consistent global information. In this paper, we propose a memory-augmented transformer (MAT) to effectively model both the local and global information. The feature extraction pipeline of the MAT is split into a memory-based global relationship guidance module and a local feature extraction module. The local feature extraction module mainly consists of a transformer, which is used to extract features from the input images. The global relationship guidance module maintains a memory bank for the consistent encoding of the global information. Global guidance is performed by memory interaction. Bidirectional information flow between the global and local branches is conducted by a memory-query module, as well as a memory-update module, respectively. Experiment results on the ISPRS Potsdam and ISPRS Vaihingen datasets demonstrated that our method can perform competitively with state-of-the-art methods.",semantic segmentation,remote sensing imagery,memory-augmented transformer,memory mechanism,self-attention,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_139,"Nie, Jie","Zheng, Chengyu","Wang, Chenglong","Zuo, Zijie",Scale-Relation Joint Decoupling Network for Remote Sensing Image Semantic Segmentation,,2022,7,"As we all know, remote sensing (RS) images contain multiscale and numerous RS objects, along with massive and complex spatial topological relationships, such as the adjacency, proximity relationships of same-scale objects, and inclusion relationships of cross-scale objects. However, the existing semantic segmentation methods have never explored the cross-scale relationships, which are especially important when it comes to the situation that the RS objects cannot be accurately identified, they could be supplemented by the surrounding contents. To address the above concern, we propose a scale-relation joint decoupling network (SRJDN) for the semantic segmentation of RS images by simultaneously considering decoupling scales and decoupling relationships to excavate more complete relationships of multiscale RS objects. The SRJDN is performed by following three steps, namely, scale decoupling (SD), relation decoupling (RD), and fine-granularity guided fusion (FGF). The SD module uses dilated convolution with different rates to decouple RS objects into different scale feature groups, from small to large scales. Afterward, the RD considers all the spatial topological relationships and decouples these relationships according to the scale, which is divided into two parts, including same-scale relation extraction (SSRE) and cross-scale relation extraction (CSRE). The SSRE establishes the graph structures at each scale independently to mine the relationships of same-scale RS objects and the CSRE constructs the graph in a unified pattern between cross-scales to explore cross-scale target relationships. Third, the FGF module regards small-scale features as fine-granularity representation and applies its attention map to guide the learning of other scale features, which could mine more reliable and comprehensive saliency information and improve the feature consistency. Numerical experiments conducted on two large-scale fine-resolution RS image datasets empirically demonstrate the robustness of the proposed joint decoupling strategy and the effectiveness of FGF in RS image semantic segmentation tasks.",Decoupling,remote sensing (RS),scale-relation joint,semantic segmentation,,"Lv, Xiaowei","Yu, Shusong","Wei, Zhiqiang",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_140,"Wu, Zhihuan","Gao, Yongming","Li, Lei","Xue, Junshi",Semantic segmentation of high-resolution remote sensing images using fully convolutional network with adaptive threshold,,APR 3 2019,50,"Semantic segmentation is an important method to implement fine-grained semantically understand for high-resolution remote sensing images by dividing images into pixel groupings which can then be labelled and classified. In the field of computer vision (CV), the methods based on fully convolutional network (FCN) are the hotspot and have achieved state-of-the-art results. Compared with popular datasets in CV such as PASCAL and COCO, class imbalance is a problem for multiclass semantic segmentation in remote sensing datasets. In this paper, an FCN-based model is proposed to implement pixel-wise classifications for remote sensing image in an end-to-end way, and an adaptive threshold algorithm is proposed to adjust the threshold of Jaccard index in each class. Experiments on DSTL dataset show that the proposed method produces accurate classifications in an end-to-end way. Results show that the adaptive threshold algorithm can increase the score of average Jaccard index from 0.614 to 0.636 and achieve better segmentation results.",Semantic segmentation,remote sensing images,fully convolutional network,class imbalance,adaptive threshold,"Li, Yuntao",,,,CONNECTION SCIENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_141,"Gao, Yupeng","Zhang, Shengwei","Zuo, Dongshi","Yan, Weihong",TMNet: A Two-Branch Multi-Scale Semantic Segmentation Network for Remote Sensing Images,,JUL 2023,3,"Pixel-level information of remote sensing images is of great value in many fields. CNN has a strong ability to extract image backbone features, but due to the localization of convolution operation, it is challenging to directly obtain global feature information and contextual semantic interaction, which makes it difficult for a pure CNN model to obtain higher precision results in semantic segmentation of remote sensing images. Inspired by the Swin Transformer with global feature coding capability, we design a two-branch multi-scale semantic segmentation network (TMNet) for remote sensing images. The network adopts the structure of a double encoder and a decoder. The Swin Transformer is used to increase the ability to extract global feature information. A multi-scale feature fusion module (MFM) is designed to merge shallow spatial features from images of different scales into deep features. In addition, the feature enhancement module (FEM) and channel enhancement module (CEM) are proposed and added to the dual encoder to enhance the feature extraction. Experiments were conducted on the WHDLD and Potsdam datasets to verify the excellent performance of TMNet.",remote sensing images,global modeling,semantic segmentation,Swin transformer,,"Pan, Xin",,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_142,"Chen, Hongkun","Luo, Huilan",,,Multilevel Feature Interaction Network for Remote Sensing Images Semantic Segmentation,,2024,0,"High-spatial resolution (HSR) remote sensing images present significant challenges due to their highly complex backgrounds, a large number of densely distributed small targets, and the potential for confusion with land targets. These characteristics render existing methods ineffective in accurately segmenting small targets and prone to boundary blurring. In response to these challenges, we introduce a novel multilevel feature interaction network (MFIN). The MFIN model was designed as a dual-branch U-shaped interactive decoding structure that effectively achieves semantic segmentation and edge detection. Notably, this study is the first to address ways to enhance the performance for HSR remote sensing image analysis by iteratively refining features at multilevels for different tasks. We designed the feature interaction module (FIM), which refines semantic features through multiscale attention and interacts with edge features of the same scale for optimization, then serving as input for iterative optimization in the next scale's FIM. In addition, a lightweight global feature module is designed to adaptively extract global contextual information from different scales features, thereby enhancing the semantic accuracy of the features. Furthermore, to mitigate the semantic dilution issues caused by upsampling, a semantic-guided fusion module is introduced to enhance the propagation of rich semantic information among features. The proposed methods achieve state-of-the-art segmentation performance across four publicly available remote sensing datasets: Potsdam, Vaihingen, LoveDA, and UAVid. Notably, our MFIN has only 15.4 MB parameters and 34.2 GB GFLOPs, achieving an optimal balance between accuracy and efficiency.",Image edge detection,Semantics,Feature extraction,Remote sensing,Semantic segmentation,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Decoding,Adaptation models,Convolution,Accuracy,Context modeling,Feature interaction,multilevel features,remote sensing images analysis,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_143,"Bai, Qinyan","Luo, Xiaobo","Wang, Yaxu","Wei, Tengfei",DHRNet: A Dual-Branch Hybrid Reinforcement Network for Semantic Segmentation of Remote Sensing Images,,2024,2,"In the field of remote sensing image processing, semantic segmentation has always been a hot research topic. Currently, deep convolutional neural networks (DCNNs) are the mainstream methods for the semantic segmentation of remote sensing image (RSI). There are two commonly used semantic segmentation methods based on DCNNs: multiscale feature extraction based on deep-level features, and global modeling. The former can better extract object features of different scales in complex scenes. However, this method lacks sufficient spatial information, resulting in poor edge segmentation ability. The latter can effectively solve the problem of limited receptive field in DCNNs obtaining more comprehensive feature extraction results. Unfortunately, this method is prone to misclassification, resulting in incorrect predictions of local pixels. To address these issues, we propose the dual-branch hybrid reinforcement network (DHRNet) for more precise semantic segmentation of RSI. This model is a dual-branch parallel structure with a multiscale feature extraction branch and a global context and detail enhancement branch. This structure decomposes the complex semantic segmentation task, allowing each branch to extract features with different emphases while retaining sufficient spatial information. The results of both branches are fused to obtain a more comprehensive segmentation result. After conducting extensive experiments on three publicly available RSI datasets, ISPRS Potsdam, ISPRS Vaihingen, and LoveDA, DHRNet demonstrates excellent results with the mean intersection over union of 86.97%, 83.53%, and 54.48% on the three datasets, respectively.",Global context modeling,multiscale feature extraction,remote sensing,semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_144,"Tong, Qixiang","Zhu, Zhipeng","Zhang, Min","Cao, Kerui",CrossFormer Embedding DeepLabv3+for Remote Sensing Images Semantic Segmentation,,2024,1,"High-resolution remote sensing image segmentation is a challenging task. In urban remote sensing, the presence of occlusions and shadows often results in blurred or invisible object boundaries, thereby increasing the difficulty of segmentation. In this paper, an improved network with a cross-region self-attention mechanism for multi-scale features based on DeepLabv3+ is designed to address the difficulties of small object segmentation and blurred target edge segmentation. First, we use CrossFormer as the backbone feature extraction network to achieve the interaction between large- and small-scale features, and establish self-attention associations between features at both large and small scales to capture global contextual feature information. Next, an improved atrous spatial pyramid pooling module is introduced to establish multi-scale feature maps with large- and small-scale feature associations, and attention vectors are added in the channel direction to enable adaptive adjustment of multi-scale channel features. The proposed network model is validated using the Potsdam and Vaihingen datasets. The experimental results show that, compared with existing techniques, the network model designed in this paper can extract and fuse multiscale information, more clearly extract edge information and small-scale information, and segment boundaries more smoothly. Experimental results on public datasets demonstrate the superiority of our method compared with several state-of-the-art networks.",Semantic segmentation,remote sensing,multiscale,self -attention,,"Xing, Haihua",,,,CMC-COMPUTERS MATERIALS & CONTINUA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_145,"Zheng, Chengyu","Jiang, Yanru","Lv, Xiaowei","Nie, Jie",SSDT: Scale-Separation Semantic Decoupled Transformer for Semantic Segmentation of Remote Sensing Images,,2024,3,"As we all know, semantic segmentation of remote sensing (RS) images is to classify the images pixel by pixel to realize the semantic decoupling of the images. Most traditional semantic decoupling methods only decouple and do not perform scale-separation operations, which leads to serious problems. In the semantic decoupling process, if the feature extractor is too large, it will ignore the small-scale targets; if the feature extractor is too small, it will lead to the separation of large-scale target objects and reduce the segmentation accuracy. To address this concern, we propose a scale-separated semantic decoupled transformer (SSDT), which first performs scale-separation in the semantic decoupling process and uses the obtained scale information-rich semantic features to guide the Transformer to extract features. The network consists of five modules, scale-separated patch extraction (SPE), semantic decoupled transformer (SDT), scale-separated feature extraction (SFE), semantic decoupling (SD), and multiview feature fusion decoder (MFFD). In particular, SPE turns the original image into a linear embedding sequence of three scales; SD divides pixels into different semantic clusters by K-means, and further obtains scale information-rich semantic features; SDT improves the intraclass compactness and interclass looseness by calculating the similarity between semantic features and image features, the core of which is decoupled attention. Finally, MFFD is proposed to fuse salient features from different perspectives to further enhance the feature representation. Our experiments on two large-scale fine-resolution RS image datasets (Vaihingen and Potsdam) demonstrate the effectiveness of the proposed SSDT strategy in RS image semantic segmentation tasks.",Semantics,Feature extraction,Transformers,Semantic segmentation,Remote sensing,"Liang, Xinyue","Wei, Zhiqiang",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Computational modeling,Vegetation mapping,Geophysical image processing,geoscience and remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_146,"Chen, Jie","Zhu, Jingru","Sun, Geng","Li, Jianhui",SMAF-Net: Sharing Multiscale Adversarial Feature for High-Resolution Remote Sensing Imagery Semantic Segmentation,,NOV 2021,11,"Semantic segmentation of high-resolution remote sensing imagery (HRSI) is a major task in remote sensing analysis. Although deep convolutional neural network (DCNN)-based semantic segmentation models have powerful capacity in pixel-wise classification, they still face challenge in obtaining intersemantic continuity and extraboundary accuracy because of the geo-object's characteristic feature of diverse scales and various distributions in HRSI. Inspired by the transfer learning, in this study, we propose an efficient semantic segmentation framework named SMAF-Net, which shares multiscale adversarial features into a U-shaped semantic segmentation model. Specifically, it uses multiscale adversarial feature representation obtained from a well-trained generative adversarial network to grasp the pixel correlation and further improve the boundary accuracy of multiscale geo-objects. Comparison experiments on the Potsdam and Vaihingen data sets demonstrate that the proposed framework can achieve considerable improvement in the semantic segmentation of HRSI.",Semantics,Feature extraction,Image segmentation,Remote sensing,Gallium nitride,"Deng, Min",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Generators,Data mining,Generative adversarial network (GAN),high-resolution remote sensing imagery (HRSI),multiscale feature,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_147,"Xiao, Ruijie","Zhong, Chuan","Zeng, Wankang","Cheng, Ming",Novel Convolutions for Semantic Segmentation of Remote Sensing Images,,2023,11,"The networks are required to be capable of learning low-level features well when applied to remote sensing image (RSI) semantic segmentation tasks. To capture accurate and abundant low-level semantic information, the early feature extractor layer is crucial to the whole network because all the subsequent features are inferred from that base. To address the low-level feature extraction issue and overcome the shortcomings of traditional convolution such as too many parameters or limited receptive field, some novel convolution units have been proposed in the literature. In this article, we propose two elaborately designed and portable yet effective convolution units, i.e., directional convolution (DC) and large field convolution (LFC), combined as the extractor of low-level semantic features. DC is designed to extract directional features from specific directions, and LFC can achieve a large receptive field with few parameters. Experimental results on two public datasets provide evidence that our convolution units can help deep learning networks improve performance stably and comprehensively compared to the baseline networks.",Feature extraction,Semantic segmentation,Kernel,Strips,Semantics,"Wang, Cheng",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Data mining,Task analysis,Low-level feature extractor,novel convolution,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_148,"Wang, Libo","Li, Rui","Duan, Chenxi","Zhang, Ce",A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images,,2022,155,"The fully convolutional network (FCN) with an encoder-decoder architecture has been the standard paradigm for semantic segmentation. The encoder-decoder architecture utilizes an encoder to capture multilevel feature maps, which are incorporated into the final prediction by a decoder. As the context is crucial for precise segmentation, tremendous effort has been made to extract such information in an intelligent fashion, including employing dilated/atrous convolutions or inserting attention modules. However, these endeavors are all based on the FCN architecture with ResNet or other backbones, which cannot fully exploit the context from the theoretical concept. By contrast, we introduce the Swin Transformer as the backbone to extract the context information and design a novel decoder of densely connected feature aggregation module (DCFAM) to restore the resolution and produce the segmentation map. The experimental results on two remotely sensed semantic segmentation datasets demonstrate the effectiveness of the proposed scheme.",Transformers,Semantics,Image segmentation,Feature extraction,Remote sensing,"Meng, Xiaoliang","Fang, Shenghui",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Decoding,Standards,Fine-resolution remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,,,
Row_149,"Yang, Kunping","Tong, Xin-Yi","Xia, Gui-Song","Shen, Weiming",Hidden Path Selection Network for Semantic Segmentation of Remote Sensing Images,,2022,11,"Targeting at depicting land covers with pixelwise semantic categories, semantic segmentation in remote sensing images needs to portray diverse distributions over vast geographical locations, which is difficult to be achieved by the homogeneous pixelwise forward paths in the architectures of existing deep models. Although specific algorithms have been designed to select pixelwise adaptive forward paths for natural image analysis, it still lacks theoretical supports on how to obtain optimal selections. In this article, we provide mathematical analyses in terms of the parameter optimization, which guides us to design a method called hidden path selection network (HPS-Net). With the help of hidden variables deriving from an extra mini-branch, HPS-Net is able to tackle the inherent problem about inaccessible global optimums by adjusting the direct relationships between feature maps and pixelwise path selections in existing algorithms, which we call hidden path selection. For the better training and evaluation, we further refine and expand the 5-class Gaofen image dataset (GID-5) to a new one with 15 land-cover categories, i.e., GID-15. The experimental results on both GID-5 and GID-15 demonstrate that the proposed modules can stably improve the performance of different deep structures, which validates the proposed mathematical analyses.",Semantics,Remote sensing,Image segmentation,Optimization,Mathematical analysis,"Zhang, Liangpei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Manifolds,Benchmark testing,Benchmark dataset,hidden path selection,remote sensing image,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_150,"Yim, Ji Hyeon","Kim, Min-A","Kim, Ku-hyeok","Lee, Jeayeol",Comparative Analysis of Methods Based on Semantic Segmentation for Cloud Detection in Remote Sensing Imagery,12TH INTERNATIONAL CONFERENCE ON ICT CONVERGENCE (ICTC 2021): BEYOND THE PANDEMIC ERA WITH ICT CONVERGENCE INNOVATION,2021,0,"Cloud coverage estimation is a fundamental step in remote sensing imagery because it can be useful information to user using imagery. Recently, cloud detection using semantic segmentation is being actively studied to automatically detect cloud regions. However, there are many limitations to obtaining accurate cloud regions; therefore, the related methods need to be analyzed. Accordingly, this study aims to identify the best network architecture that is applicable for future works, and evaluates its performance using datasets created for this research, First, we classified multiple network architectures and public datasets for the semantic segmentation of clouds in remote sensing imagery. Next, we selected the best architecture by assessing the accuracy of each architecture and evaluate it using our datasets. We believe this work will be beneficial for future research on cloud detection in the field of remote sensing imagery.",Cloud detection,Semantic segmentation,Remote sensing,Satellite,,"Lee, Myeong Shin","Chung, Dae-won","Jeon, Kyeongmi","Koo, Jamyoung",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_151,"Ma, Ailong","Wang, Junjue","Zhong, Yanfei","Zheng, Zhuo",FactSeg: Foreground Activation-Driven Small Object Semantic Segmentation in Large-Scale Remote Sensing Imagery,,2022,90,"The small object semantic segmentation task is aimed at automatically extracting key objects from high-resolution remote sensing (HRS) imagery. Compared with the large-scale coverage areas for remote sensing imagery, the key objects, such as cars and ships, in HRS imagery often contain only a few pixels. In this article, to tackle this problem, the foreground activation (FA)-driven small object semantic segmentation (FactSeg) framework is proposed from perspectives of structure and optimization. In the structure design, FA object representation is proposed to enhance the awareness of the weak features in small objects. The FA object representation framework is made up of a dual-branch decoder and collaborative probability (CP) loss. In the dual-branch decoder, the FA branch is designed to activate the small object features (activation) and suppress the large-scale background, and the semantic refinement (SR) branch is designed to further distinguish small objects (refinement). The CP loss is proposed to effectively combine the activation and refinement outputs of the decoder under the CP hypothesis. During the collaboration, the weak features of the small objects are enhanced with the activation output, and the refined output can be viewed as the refinement of the binary outputs. In the optimization stage, small object mining (SOM)-based network optimization is applied to automatically select effective samples and refine the direction of the optimization while addressing the imbalanced sample problem between the small objects and the large-scale background. The experimental results obtained with two benchmark HRS imagery segmentation datasets demonstrate that the proposed framework outperforms the state-of-the-art semantic segmentation methods and achieves a good tradeoff between accuracy and efficiency. Code will be available at: http://rsidea.whu.edu.cn/FactSeg.htm",Semantics,Image segmentation,Task analysis,Remote sensing,Optimization,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Feature extraction,Decoding,Deep learning,high-resolution remote sensing (HRS) imagery,semantic segmentation,small objects,,,,,,,,,,,,,,,,,,,,,,,,
Row_152,"Huo, Yan","Gang, Shuang","Dong, Liang","Guan, Chao",An Efficient Semantic Segmentation Method for Remote-Sensing Imagery Using Improved Coordinate Attention,,MAY 2024,1,"Semantic segmentation stands as a prominent domain within remote sensing that is currently garnering significant attention. This paper introduces a pioneering semantic segmentation model based on TransUNet architecture with improved coordinate attention for remote-sensing imagery. It is composed of an encoding stage and a decoding stage. Notably, an enhanced and improved coordinate attention module is employed by integrating two pooling methods to generate weights. Subsequently, the feature map undergoes reweighting to accentuate foreground information and suppress background information. To address the issue of time complexity, this paper introduces an improvement to the transformer model by sparsifying the attention matrix. This reduces the computing expense of calculating attention, making the model more efficient. Additionally, the paper uses a combined loss function that is designed to enhance the training performance of the model. The experimental results conducted on three public datasets manifest the efficiency of the proposed method. The results indicate that it excels in delivering outstanding performance for semantic segmentation tasks pertaining to remote-sensing images.",remote-sensing image,sparse matrix,vision transformer,coordinate attention,semantic segmentation,,,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_153,"Pereira, Matheus B.","dos Santos, Jefersson A.",,,AN END-TO-END FRAMEWORK FOR LOW-RESOLUTION REMOTE SENSING SEMANTIC SEGMENTATION,2020 IEEE LATIN AMERICAN GRSS & ISPRS REMOTE SENSING CONFERENCE (LAGIRS),2020,12,"High-resolution images for remote sensing applications are often not affordable or accessible, especially when in need of a wide temporal span of recordings. Given the easy access to low-resolution (LR) images from satellites, many remote sensing works rely on this type of data. The problem is that LR images are not appropriate for semantic segmentation, due to the need for high-quality data for accurate pixel prediction for this task, In this paper, we propose an end-to-end framework that unites a super-resolution and a semantic segmentation module in order to produce accurate thematic maps from LR inputs. It allows the semantic segmentation network to conduct the reconstruction process, modifying the input image with helpful textures, We evaluate the framework with three remote sensing datasets. The results show that the framework is capable of achieving a semantic segmentation performance close to native high-resolution data, while also surpassing the performance of a network trained with LR inputs.",Super-resolution,semantic segmentation,remotesensing,end-to-end framework,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_154,"Zhou, Yin","Li, Tianyi","Li, Xianju","Feng, Ruyi",MCNet: A Multi-scale and Cascade Network for Semantic Segmentation of Remote Sensing Images,,2024,0,"High resolution remote sensing images that can show more detailed ground information play an important role in land classification. However, existing segmentation methods have the problems of insufficient use of multi-scale feature and semantic information. In this study, a multi-scale and cascade semantic segmentation network (MCNet) was proposed and tested on the Potsdam and Vaihingen datasets. (1) Multi-scale feature extraction module: using dilated convolution and a parallel structure to fully extract multi-scale feature information. (2) Cross-layer feature selection module: adaptively selecting features in different levels to avoid the loss of key features. (3) Multi-scale object guidance module: weighting the features at different scales to express the multi-scale ground objects. (4) Cascade structure in the decoder part: increasing the information flow and enhancing the decoding capability of the network. Results show that the proposed MCNet outperformed the baseline networks, achieving an average overall accuracy of 86.91% and 87.82% on the two datasets, respectively. In conclusion, the multi-scale and cascade semantic segmentation network can improve the accuracy of land cover classification by using remote sensing images.",remote sensing,semantic segmentation,multi-scale feature,,,,,,,"WEB AND BIG DATA, PT II, APWEB-WAIM 2023",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_155,"He, Xin","Zhou, Yong","Zhao, Jiaqi","Zhang, Di",Swin Transformer Embedding UNet for Remote Sensing Image Semantic Segmentation,,2022,349,"Global context information is essential for the semantic segmentation of remote sensing (RS) images. However, most existing methods rely on a convolutional neural network (CNN), which is challenging to directly obtain the global context due to the locality of the convolution operation. Inspired by the Swin transformer with powerful global modeling capabilities, we propose a novel semantic segmentation framework for RS images called ST-U-shaped network (UNet), which embeds the Swin transformer into the classical CNN-based UNet. ST-UNet constitutes a novel dual encoder structure of the Swin transformer and CNN in parallel. First, we propose a spatial interaction module (SIM), which encodes spatial information in the Swin transformer block by establishing pixel-level correlation to enhance the feature representation ability of occluded objects. Second, we construct a feature compression module (FCM) to reduce the loss of detailed information and condense more small-scale features in patch token downsampling of the Swin transformer, which improves the segmentation accuracy of small-scale ground objects. Finally, as a bridge between dual encoders, a relational aggregation module (RAM) is designed to integrate global dependencies from the Swin transformer into the features from CNN hierarchically. Our ST-UNet brings significant improvement on the ISPRS-Vaihingen and Potsdam datasets, respectively. The code will be available at https://github.com/XinnHe/ST-UNet.",Transformers,Semantics,Image segmentation,Feature extraction,Convolutional neural networks,"Yao, Rui","Xue, Yong",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Remote sensing,Task analysis,Global information embedding,remote sensing (RS),semantic segmentation,Swin transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_156,"Liu, Yikun","Kang, Xudong","Huang, Yuwen","Wang, Kuikui",Unsupervised Domain Adaptation Semantic Segmentation for Remote-Sensing Images via Covariance Attention,,2022,5,"Semantic segmentation for remote sensing is a crucial but challenging task. Many supervised semantic segmentation methods rely heavily on a large-scale pixelwise annotated dataset, but it is time-consuming and laborious to provide manual annotation. However, due to the common domain shift of remote-sensing images, a direct transfer might not perform well. Therefore, many unsupervised domain adaptation (UDA) methods have been proposed to solve the data distribution discrepancy in remote-sensing datasets, but these methods cannot completely utilize the features extracted in the training process. In addition, the correlations between feature map channels are crucial for the pixelwise classification task. In this letter, a covariance-based channel attention module is proposed to capture correlations by covariance metric and weighting the feature map channels. To further improve the domain adaptation performance, we propose a three-stage UDA semantic segmentation method for remote-sensing images, and we fine-tune the model that has been trained on the source domain on the target domain via self-training and knowledge distillation (KD). To test the effectiveness of the proposed method, experiments are conducted on the ISPRS 2-D Semantic Labeling dataset and an urban drone dataset (UDD). Our method shows a better performance advantage compared with other state-of-the-art methods.",Feature extraction,Semantics,Remote sensing,Image segmentation,Adaptation models,"Yang, Gongping",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Task analysis,Training,Covariance attention,domain adaptation,knowledge distillation (KD),self-training,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_157,"Cheng, Xiang","Lei, Hong",,,Semantic Segmentation of Remote Sensing Imagery Based on Multiscale Deformable CNN and DenseCRF,,MAR 2023,5,"The semantic segmentation of remote sensing images is a significant research direction in digital image processing. The complex background environment, irregular size and shape of objects, and similar appearance of different categories of remote sensing images have brought great challenges to remote sensing image segmentation tasks. Traditional convolutional-neural-network-based models often ignore spatial information in the feature extraction stage and pay less attention to global context information. However, spatial context information is important in complex remote sensing images, which means that the segmentation effect of traditional models needs to be improved. In addition, neural networks with a superior segmentation performance often suffer from the problem of high computational resource consumption. To address the above issues, this paper proposes a combination model of a modified multiscale deformable convolutional neural network (mmsDCNN) and dense conditional random field (DenseCRF). Firstly, we designed a lightweight multiscale deformable convolutional network (mmsDCNN) with a large receptive field to generate a preliminary prediction probability map at each pixel. The output of the mmsDCNN model is a coarse segmentation result map, which has the same size as the input image. In addition, the preliminary segmentation result map contains rich multiscale features. Then, the multi-level DenseCRF model based on the superpixel level and the pixel level is proposed, which can make full use of the context information of the image at different levels and further optimize the rough segmentation result of mmsDCNN. To be specific, we converted the pixel-level preliminary probability map into a superpixel-level predicted probability map according to the simple linear iterative clustering (SILC) algorithm and defined the potential function of the DenseCRF model based on this. Furthermore, we added the pixel-level potential function constraint term to the superpixel-based Gaussian potential function to obtain a combined Gaussian potential function, which enabled our model to consider the features of various scales and prevent poor superpixel segmentation results from affecting the final result. To restore the contour of the object more clearly, we utilized the Sketch token edge detection algorithm to extract the edge contour features of the image and fused them into the potential function of the DenseCRF model. Finally, extensive experiments on the Potsdam and Vaihingen datasets demonstrated that the proposed model exhibited significant advantages compared to the current state-of-the-art models.",semantic segmentation of remote sensing imagery,deep learning,convolutional neural network (CNN),conditional random field (CRF),,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_158,"Zeng, Xiaopeng","Wang, Tengfei","Dong, Zhe","Zhang, Xiangrong",Superpixel Consistency Saliency Map Generation for Weakly Supervised Semantic Segmentation of Remote Sensing Images,,2023,9,"The weakly supervised semantic segmentation (WSSS) method aims to assign semantic labels to each image pixel from weak (image-level) instead of strong (pixel-level) labels, which can greatly reduce human labor costs. However, there are some problems in WSSS of remote sensing images, such as how to locate labels accurately and how to get precise segmentation edges. To address these issues, we propose a novel framework directly transferring the scene classification model to perform semantic segmentation. We first train a multilabel scene classification network as the encoder to obtain the pretrained model, and then, the feature learned by the model is transferred to the decoder. Different from other methods, we propose a saliency map generator (SMG) instead of the class activation map (CAM) for more accurate location information by making pixels belonging to the same class lie close together while different classes are separated in feature space. Meanwhile, we take the superpixel patch as processing unit to provide precise boundary inhibition for the saliency map. To assign semantic labels for each patch, combined with extracted salient region, we propose a module responsible for exploiting the consistency of spatial and semantic similarity between different patches. Finally, we incorporate the above two modules to supervise the training process of the decoder without generating pseudolabels as most methods do, thus simplifying the training process. Experimental results show that our method outperforms other weakly supervised approaches on dense labeling remote sensing dataset (DLRSD) and Wuhan dense labeling dataset (WHDLD) with at least a 3% improvement on mean intersection over union (mIoU).",Semantics,Remote sensing,Semantic segmentation,Feature extraction,Generators,"Gu, Yanfeng",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Decoding,Convolutional neural network (CNN),deep learning,semantic segmentation,weakly supervised learning (WSL),,,,,,,,,,,,,,,,,,,,,,,,
Row_159,"Cui, Wei","He, Xin","Yao, Meng","Wang, Ziwei",Knowledge and Spatial Pyramid Distance-Based Gated Graph Attention Network for Remote Sensing Semantic Segmentation,,APR 2021,15,"The pixel-based semantic segmentation methods take pixels as recognitions units, and are restricted by the limited range of receptive fields, so they cannot carry richer and higher-level semantics. These reduce the accuracy of remote sensing (RS) semantic segmentation to a certain extent. Comparing with the pixel-based methods, the graph neural networks (GNNs) usually use objects as input nodes, so they not only have relatively small computational complexity, but also can carry richer semantic information. However, the traditional GNNs are more rely on the context information of the individual samples and lack geographic prior knowledge that reflects the overall situation of the research area. Therefore, these methods may be disturbed by the confusion of ""different objects with the same spectrum"" or ""violating the first law of geography"" in some areas. To address the above problems, we propose a remote sensing semantic segmentation model called knowledge and spatial pyramid distance-based gated graph attention network (KSPGAT), which is based on prior knowledge, spatial pyramid distance and a graph attention network (GAT) with gating mechanism. The model first uses superpixels (geographical objects) to form the nodes of a graph neural network and then uses a novel spatial pyramid distance recognition algorithm to recognize the spatial relationships. Finally, based on the integration of feature similarity and the spatial relationships of geographic objects, a multi-source attention mechanism and gating mechanism are designed to control the process of node aggregation, as a result, the high-level semantics, spatial relationships and prior knowledge can be introduced into a remote sensing semantic segmentation network. The experimental results show that our model improves the overall accuracy by 4.43% compared with the U-Net Network, and 3.80% compared with the baseline GAT network.",remote sensing,semantic segmentation,knowledge,spatial relationship,spatial pyramid distance,"Hao, Yuanjie","Li, Jie","Wu, Weijie","Zhao, Huilin",REMOTE SENSING,"Xia, Cong",GAT,,,,,,,,,"Li, Jin","Cui, Wenqi",,,,,,,,,,,,,,,,,,,
Row_160,"Li, Haifeng","Li, Yi","Zhang, Guo","Liu, Ruoyun",Global and Local Contrastive Self-Supervised Learning for Semantic Segmentation of HR Remote Sensing Images,,2022,117,"Recently, supervised deep learning has achieved a great success in remote sensing image (RSI) semantic segmentation. However, supervised learning for semantic segmentation requires a large number of labeled samples, which is difficult to obtain in the field of remote sensing. A new learning paradigm, self-supervised learning (SSL), can be used to solve such problems by pretraining a general model with a large number of unlabeled images and then fine-tuning it on a downstream task with very few labeled samples. Contrastive learning is a typical method of SSL that can learn general invariant features. However, most existing contrastive learning methods are designed for classification tasks to obtain an image-level representation, which may be suboptimal for semantic segmentation tasks requiring pixel-level discrimination. Therefore, we propose a global style and local matching contrastive learning network (GLCNet) for RSI semantic segmentation. Specifically, first, the global style contrastive learning module is used to better learn an image-level representation, as we consider that style features can better represent the overall image features. Next, the local features matching the contrastive learning module is designed to learn the representations of local regions, which is beneficial for semantic segmentation. We evaluate four RSI semantic segmentation datasets, and the experimental results show that our method mostly outperforms the state-of-the-art self-supervised methods and the ImageNet pretraining method. Specifically, with 1% annotation from the original dataset, our approach improves Kappa by 6% on the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam dataset relative to the existing baseline. Moreover, our method outperforms supervised learning methods when there are some differences between the datasets of upstream tasks and downstream tasks. Our study promotes the development of SSL in the field of RSI semantic segmentation. Since SSL could directly learn the essential characteristics of data from unlabeled data, which is easy to obtain in the remote sensing field, this may be of great significance for tasks such as global mapping. The source code is available at https://github.com/GeoX-Lab/G-RSIM.",Semantics,Remote sensing,Task analysis,Image segmentation,Supervised learning,"Huang, Haozhe","Zhu, Qing","Tao, Chao",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Force,Feature extraction,Contrastive learning,remote sensing image (RSI) semantic segmentation,self-supervised learning (SSL),,,,,,,,,,,,,,,,,,,,,,,,,
Row_161,"Zhang, Zixuan","Huang, Liang","Tang, Bo-Hui","Le, Weipeng",MATNet: multiattention Transformer network for cropland semantic segmentation in remote sensing images,,DEC 31 2024,1,"Remote sensing image semantic segmentation methods have become the main approach for extracting cropland information. However, in the mountainous regions of southwestern China, croplands exhibit narrow and fragmented shapes, as well as complex planting patterns, making it difficult for traditional semantic segmentation methods to accurately delineate fine-grained cropland boundaries. To address these challenges, a multiattention Transformer network named MATNet is proposed in this paper, for fine-grained extraction of cropland at the parcel level in complex scenes. MATNet built upon the fusion of CNN encoder and Transformer decoder. In the encoder, spatial and channel reconstruction units are introduced, reducing information redundancy in the convolutional layers. The Transformer decoder incorporates multiple attention mechanisms, this design feature enhances the attention window's perception of local content and improves the model's ability to extract features from fine-grained cropland parcels through optimized computationnal al location. Taking the experimental results of the Dali cropland dataset as an illustration, MATNet achieved the highest values across five evaluation metrics, including mIoU. Specifically, the Recall, F1, and mIoU scores were 94.68%, 94.69%, and 89.92%, respectively. Compared with six other advanced models, MATNet consistently performed best in terms of extracting fine-grained cropland parcels.",Remote sensing,semantic segmentation,cropland extraction,Transformer,multiattention,"Wang, Meiqi","Cheng, Jiapei","Wu, Qiang",,INTERNATIONAL JOURNAL OF DIGITAL EARTH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_162,"Gong, Zhi","Duan, Lijuan","Xiao, Fengjin","Wang, Yuxi",MSAug: Multi-Strategy Augmentation for rare classes in semantic segmentation of remote sensing images,,SEP 2024,2,"Recently, remote sensing images have been widely used in many scenarios, gradually becoming the focus of social attention. Nevertheless, the limited annotation of scarce classes severely reduces segmentation performance. This phenomenon is more prominent in remote sensing image segmentation. Given this, we focus on image fusion and model feedback, proposing a multi-strategy method called MSAug to address the remote sensing imbalance problem. Firstly, we crop rare class images multiple times based on prior knowledge at the image patch level to provide more balanced samples. Secondly, we design an adaptive image enhancement module at the model feedback level to accurately classify rare classes at each stage and dynamically paste and mask different classes to further improve the model's recognition capabilities. The MSAug method is highly flexible and can be plug-and-play. Experimental results on remote sensing image segmentation datasets show that adding MSAug to any remote sensing image semantic segmentation network can bring varying degrees of performance improvement.",Data augmentation,Remote sensing images,Semantic segmentation,Rare classes,,,,,,DISPLAYS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_163,Song Xirui,Ge Hongwei,,,Remote Sensing Image Semantic Segmentation Algorithm Based on TransMANet,,MAY 2024,0,"Herein, we propose a Transformer multiattention network (TransMANet), a network structure based on Transformer and attention mechanisms, to address the issues of low segmentation accuracy, inadequate global feature extraction, and insufficient association between the multiattention network (MANet) algorithm and image semantic information. This network structure features a dual-branch decoder that combines local and global contexts and enhances the semantic information of shallow networks. First, we introduce a local attention embedding mechanism that enhances the embedding of context information and semantic information of high-level features into low-level features. Then, we design a dual-branch decoder that combines Transformer and convolutional neural networks, which extracts global context information and detailed information with different scales, thereby modeling global and local information. Finally, we improve the original loss function and use a joint loss function that combines cross-entropy loss and Dice loss to address the class imbalance problem often encountered in remote sensing datasets and thus improve segmentation accuracy. Our experimental results demonstrate the superiority of TransMANet over MANet and other advanced methods in terms of intersection over union on UAVid, LoveDA, Potsdam, and Vaihingen datasets. This indicates the strong generalization capability of TransMANet and its effectiveness in achieving accurate segmentation results.",image processing,semantic segmentation,attention mechanism,Transformer,high-resolution remote sensing image,,,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_164,"Yao, Hongtai","Zhao, Le","Tian, Meng","Jin, Yong",Semantic Segmentation for Remote Sensing Image Using the Multigranularity Object-Based Markov Random Field With Blinking Coefficient,,2023,3,"Semantic segmentation is one of the most important tasks in remote sensing. In the semantic segmentation of remote sensing images, some regions are repeatedly transformed between multiclasses, which affects the convergence speed and segmentation accuracy. This is because the increased spatial resolution makes the spectral distribution of geographic targets differ from the overall category distribution. The Markov random field (MRF) model is widely used for semantic segmentation of remote sensing images because of its outstanding spatial description ability. Some scholars have made improvements on MRF models to extract more information or enhance semantic inference. However, these improvements fail to capture the correlation between the multigranularity layers and the historical information. In this article, we propose a new MRF-based model that adopts multigranularity layers to realize the multigranularity correlation representation of targets and the spatial-temporal inference of segmentation labels. First, the algorithm constructs a multigrained layer structure based on remote sensing images to enhance feature extraction for targets of different sizes in images. Second, for the multilayer feature field, a cross-layer Gauss-Markov model is constructed based on intra-inter-layer feature correlation constraints. Then, for the multigranularity layer label field, a self-renewing pairwise spatial-temporal potential function with blinking coefficients is constructed based on the newly defined cross-layer augmented neighborhood system, which can accelerate the convergence of segmentation by using the history information and spatial neighborhood information. The proposed method is tested on texture images, SPOT-5, and Gaofen-2 images. Experiments show that the proposed method has a better performance compared to other state-of-the-art MRF-based methods.",Markov random field (MRF),multigranularity,remote sensing image,semantic segmentation,,"Hu, Zhentao","Peng, Qinglan","Qiu, Qian",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_165,"Ma, Xianping","Zhang, Xiaokang","Pun, Man-On",,RS3Mamba: Visual State Space Model for Remote Sensing Image Semantic Segmentation,,2024,17,"Semantic segmentation of remote sensing images is a fundamental task in geoscience research. However, convolutional neural networks (CNNs) and transformers have some significant shortcomings. The former are limited by insufficient long-range modeling capabilities, while the latter are hampered by computational complexity. Recently, a novel visual state space (VSS) model represented by Mamba has emerged, capable of modeling long-range relationships with linear computability. In this research, we propose a novel dual-branch network named remote sensing image semantic segmentation Mamba (RS(3)Mamba) designed specifically for remote sensing tasks. RS(3)Mamba uses VSS blocks to construct an auxiliary branch, providing additional global information to a convolution-based main branch. Moreover, considering the distinct characteristics of the two branches, we introduce a collaborative completion module (CCM) to refine and fuse features from the dual-encoder using a novel adaptive mechanism. Through experiments on two widely used datasets, the proposed RS(3)Mamba was found to outperform the state-of-the-art methods in terms of mIoU with 0.66% on ISPRS Vaihingen and 1.70% on LoveDA Urban, demonstrating its effectiveness and potential. The source code is available at https://github.com/sstary/SSRS.",Remote sensing,Feature extraction,Computational modeling,Semantics,Decoding,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Visualization,Transformers,semantic segmentation,visual state space (VSS) model,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_166,"Fu, Yujia","Zhang, Xiangrong","Wang, Mingyang",,DSHNet: A Semantic Segmentation Model of Remote Sensing Images Based on Dual Stream Hybrid Network,,2024,5,"Semantic segmentation is an important issue in intelligent interpretation of remote sensing, playing an important role in applications such as Earth observation and land data update. However, remote sensing images often contain complex ground objects and the boundaries between them are blurred, which poses a huge challenge to the semantic segmentation task of remote sensing images. This article proposes a dual stream hybrid network (DSHNet) model, which focuses on parallel extraction of semantic and boundary features in remote sensing images, and improves the performance of semantic segmentation by fully integrating dual stream information. In the semantic stream, the ViT model pretrained on remote sensing images is used as the backbone network for feature extraction. In the boundary stream, the boundary detection operator Sobel is used to capture the boundaries of different ground objects in the image, and a boundary enhancement mechanism is taken to optimize and enhance the feature representation of ground object boundaries. In addition, DSHNet designs a feature fusion module to cross-aggregate features from both semantic and boundary streams. Compared with the state-to-art semantic segmentation methods, DSHNet model has achieved the best performance on two datasets of Yellow River Estuary Wetland and Gaofen image dataset.",Semantics,Feature extraction,Streaming media,Remote sensing,Semantic segmentation,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Transformers,Data mining,Boundary detection,cross-fusion,dual-stream remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_167,"Li, Linhui","Zhang, Wenjun","Zhang, Xiaoyan","Emam, Mahmoud",Semi-Supervised Remote Sensing Image Semantic Segmentation Method Based on Deep Learning,,JAN 2023,20,"In this paper, we study the semi-supervised semantic segmentation problem via limited labeled samples and a large number of unlabeled samples. We propose a self-learning semi-supervised approach for the semantic segmentation of high-resolution remote sensing images. Our approach uses two networks (UNet and DeepLabV3) to predict the labels of the same unlabeled sample, and the pseudo labels samples with high prediction consistency are added to the training samples to improve the accuracy of semantic segmentation under the condition of limited labeled samples. Our method expands training data samples by using unlabeled data samples with pseudo labels. In order to verify the effectiveness of the proposed method, some experiments were conducted on the improved ISPRS Vaihingen 2D Semantic Labeling dataset using the method that we proposed. We focus on the extraction of forest and vegetation information and focus on the impact of a large number of unlabeled samples on the precision of semantic segmentation, we combine water, surface, buildings, cars, and background into one category and named others, and we call the changed dataset the improved ISPRS Vaihingen dataset. The experimental results show that the proposed method can effectively improve the semantic segmentation accuracy of high-scoring remote sensing images with limited samples than common deep semi-supervised learning.",convolutional neural network,high-resolution remote sensing images,semi-supervised,semantic segmentation,,"Jing, Weipeng",,,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_168,"Ni, Yue","Liu, Jiahang","Chi, Weijian","Wang, Xiaozhen",CGGLNet: Semantic Segmentation Network for Remote Sensing Images Based on Category-Guided Global-Local Feature Interaction,,2024,5,"As spatial resolution increases, the information conveyed by remote sensing images becomes more and more complex. Large-scale variation and highly discrete distribution of objects greatly increase the challenge of the semantic segmentation task for remote sensing images. Mainstream approaches usually use implicit attention mechanisms or transformer modules to achieve global context for good results. However, these approaches fail to explicitly extract intraobject consistency and interobject saliency features leading to unclear boundaries and incomplete structures. In this article, we propose a category-guided global-local feature interaction network (CGGLNet), which utilizes category information to guide the modeling of global contextual information. To better acquire global information, we proposed a category-guided supervised transformer module (CGSTM). This module guides the modeling of global contextual information by estimating the potential class information of pixels so that features of the same class are more aggregated and those of different classes are more easily distinguished. To enhance the representation of local detailed features of multiscale objects, we designed the adaptive local feature extraction module (ALFEM). By parallel connection of the CGSTM and the ALFEM, our network can extract rich global and local context information contained in the image. Meanwhile, the designed feature refinement segmentation head (FRSH) helps to reduce the semantic difference between deep and shallow features and realizes the full integration of different levels of information. Extensive ablation and comparison experiments on two public remote sensing datasets (ISPRS Vaihingen dataset and ISPRS Potsdam dataset) indicate that our proposed CGGLNet achieves superior performance compared to the state-of-the-art methods.",Category-guided,global contextual information,remote sensing,semantic segmentation,transformer,"Li, Deren",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_169,"Xu, Fan","Shang, Zhigao","Wu, Qihui","Zhang, Xiaofei",MUFNet: Toward Semantic Segmentation of Multi-spectral Remote Sensing Images,AICCC 2021: 2021 4TH ARTIFICIAL INTELLIGENCE AND CLOUD COMPUTING CONFERENCE,2021,2,"In this paper, a new convolutional neural network called multi-U fusion networks (MUFNet) is proposed for accurate semantic segmentation of multi-spectral remote sensing. Essentially, MUFNet is inspired by UNet, MFNet and CAM and fully combines their advantages. First, MUFNet introduces the skip connections into a multi-encoder-to-mono-decoder architecture, thereby facilitating the fusion of multi-scale and multi-channel spectral information. Second, the shortcut module in the decoder is revised by concatenating multiple spectral features from different encoders and then feeding the concatenated data into a CAM unit. Thus, the multispectral context semantics are fused and also the redundant feature maps are attention-compressed. Extensive simulations were conducted by testing UNet, UNet-4ch, MFNet and MUFNet on the 8400 RGB-NIR multi-spectral images with five categories from the GID image dataset. The visual results clearly showed that the proposed MUFNet can achieve more smoothing and complete segmentation performance than the other networks. Moreover, the measure values of mIoU, FWIoU and PA indicate that the proposed MUFNet can outperform the other networks in average semantic segmentation accuracy.",MUFNet,Semantic segmentation,Multi-spectral remote sensing images,Revised shortcut,,"Lin, Zebin","Shao, Shuning",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_170,"Lv, Ning","Zhang, Zenghui","Li, Cong","Deng, Jiaxuan",A hybrid-attention semantic segmentation network for remote sensing interpretation in land-use surveillance,,FEB 2023,35,"Remote sensing interpretation for surveillance of land use often needs to mark out construction disturbance on satellite imagery, such as illegal buildings or spoil area. These disturbance region annotated by a set of surveillance rules contain the corresponding image characteristics which are regarded as semantic information in computer vision. Different from the natural Landscapes interpretation, the semantic information of construciton disturbance region shows more complex to extract with lack of available training dataset and interference of the various sizes of targets. This paper proposes a hybrid attention semantic segmentation network (HAssNet) which can extract the target and its surroundings through a large receptive field for multi-scale targets. Based on the full convolutional networks (FCN), spatial attention mechanism is firstly introduced to acquire the position of segmentation target with the global correlations, so that the small targets in large scale scene are guaranteed not to be omitted in semantic features extraction. Secondly, channel attention mechanism is designed to assign higher weights to task-related channels for semantic consistency. Experimental results on an open remote sensing dataset show that HAssNet achieves average 6.7% improvement in mIoU than the state-of-the-art segmentation networks. In a land use surveillance project, HAssNet shows considerable performance compared with manual interpretation.",Remote sensing,Image interpretation,Semantic segmentation,Attention mechanism,,"Su, Tao","Chen, Chen","Zhou, Yang",,INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_171,"Liu, Honghao","Yang, Ruixia","Xu, Yue","Chen, Zhengchao",DiffRSS: A Diffusion-Guided Multi-Scale Features Remote Sensing Image Segmentation Method,,2025,0,"Semantic segmentation in remote sensing is a fundamental task with crucial applications across various domains. Traditional approaches primarily utilize bottom-up discriminative methods, where network architectures learn image features to generate segmentation masks. However, the complexity of remote sensing images, characterized by diverse ground object types and intricate scenes, often results in information redundancy and confusion during feature extraction, impacting segmentation accuracy. To address these challenges, we introduce a novel segmentation framework, DiffRSS, based on the denoising model paradigm. This top-down generative approach learns the data distribution of sample labels and uses image features as guiding priors for generating segmentation masks. We conceptualize the semantic segmentation of remote sensing images as a conditional generation task and design a Multiscale Cyclic Denoising Module (MSCDM), which effectively leverages multiscale features of remote sensing images, leading to superior segmentation outcomes. Inspired by diffusion models, our denoising structure, MSCDM, can be reused multiple times during inference, enhancing the quality of segmentation masks. This method allows for more precise capture and utilization of image features, resulting in finer and more accurate segmentation masks. Extensive testing on three public remote sensing datasets the ISPRS Vaihingen, ISPRS Potsdam, and GID Fine Land Cover Classification datasets demonstrates that our method achieves competitive results.",Remote sensing,Semantic segmentation,diffusion model,Semantic segmentation,diffusion model,"Zheng, Yuyang",,,,IEEE ACCESS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_172,"Zhao, Danpei","Wang, Chenxu","Gao, Yue","Shi, Zhenwei",Semantic Segmentation of Remote Sensing Image Based on Regional Self-Attention Mechanism,,2022,22,"In remote sensing images (RSIs), accurate semantic segmentation faces more challenges because of small targets, unbalanced categories, and complex scenes. Restricted by local receptive field of convolution layers, the traditional semantic segmentation models cannot use global information of RSIs. According to the characteristics of RSIs, we propose an RSANet based on regional self-attention mechanism. Our model is no longer limited by the locality of convolution, but transfers the information flow in the whole image. It can mine out the relationship between pixels in the surrounding areas, which is more logical for understanding images content. Moreover, compared with the traditional self-attention mechanism, RSANet can effectively reduce the noise of feature maps and the interference of redundant features. Our model can get better semantic segmentation results than other current models on the DroneDeploy data set and the Chreos semantic segmentation data set. The experiments show that our RSANet achieves 2% higher mean intersection over union (mIoU) than the baseline model, especially in terms of fineness, edge integrity, and classification accuracy.",Convolution,Semantics,Feature extraction,Correlation,Image segmentation,"Xie, Fengying",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Roads,Remote sensing,Convolutional neural network,region descriptors,remote sensing image (RSI),self-attention mechanism,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_173,"Liu, Yutong","Gao, Kun","Wang, Hong","Wang, Junwei",TRANSFORMER AND CNN HYBRID NETWORK FOR SUPER-RESOLUTION SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGERY,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"Super-resolution semantic segmentation (SRSS) based on Convolutional neural network (CNN) cannot establish long-range dependencies due to limited receptive field, which limits the SRSS to obtain accurate high-resolution (HR) segmentation results from the low-resolution (LR) input images. In this paper, we design a Transformer and CNN hybrid SRSS network that consists of two branches: Transformer and CNN hybrid SRSS branch and super-resolution guided branch. In the Transformer and CNN hybrid SRSS branch, Transformer extracts global context information from the feature map of the CNN, while skip connection is used to retain the local context information extracted from the CNN and combines both features to further improve the segmentation performance. In addition, the super-resolution guided branch is designed to supplement rich structure information and guide the semantic segmentation (SS). We test the proposed method on the ISPRS Vaihingen benchmark data set, and our network is superior to other state-of-the-art methods.",Remote Sensing,Semantic Segmentation,Super-Resolution Semantic Segmentation,Transformer,,"Zhang, Xiaodian","Wang, Pengyu","Li, Shuzhong",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_174,"Yuan, Xiaohui","Shi, Jianfang","Gu, Lichuan",,A review of deep learning methods for semantic segmentation of remote sensing imagery,,MAY 1 2021,377,"Semantic segmentation of remote sensing imagery has been employed in many applications and is a key research topic for decades. With the success of deep learning methods in the field of computer vision, researchers have made a great effort to transfer their superior performance to the field of remote sensing image analysis. This paper starts with a summary of the fundamental deep neural network architectures and reviews the most recent developments of deep learning methods for semantic segmentation of remote sensing imagery including non conventional data such as hyperspectral images and point clouds. In our review of the literature, we identified three major challenges faced by researchers and summarize the innovative development to address them. As tremendous efforts have been devoted to advancing pixel-level accuracy, the emerged deep learning methods demonstrated much-improved performance on several public data sets. As to handling the non-conventional, unstructured point cloud and rich spectral imagery, the performance of the state-of-the-art methods is, on average, inferior to that of the satellite imagery. Such a performance gap also exists in learning from small data sets. In particular, the limited non-conventional remote sensing data sets with labels is an obstacle to developing and evaluating new deep learning methods.",Semantic image segmentation,Deep neural networks,Remote sensing imagery,,,,,,,EXPERT SYSTEMS WITH APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_175,"Wang, Jiaqi","Liu, Bing","Zhou, Yong","Zhao, Jiaqi",Semisupervised Multiscale Generative Adversarial Network for Semantic Segmentation of Remote Sensing Image,,2022,4,"Semantic segmentation of remote sensing images based on deep neural networks has gained wide attention recently. Although many methods have achieved amazing performance, they need large amounts of labeled images to distinguish the differences in angle, color, size, and other aspects for small targets in remote sensing data sets. However, with a few labeled images, it is difficult to extract the key features of small targets. We propose a semisupervised multiscale generative adversarial network (GAN), which not only utilizes the multipath input and atrous spatial pyramid pooling (ASPP) module but leverages unlabeled images and semisupervised learning strategy to improve the performance of small target segmentation in semantic segmentation when labeled data amount is small. Experimental results show that our model outperforms state-of-the-art methods with insufficient labeled data.",Image segmentation,Semantics,Remote sensing,Feature extraction,Generative adversarial networks,"Xia, Shixiong","Yang, Yuancan","Zhang, Man","Ming, Liu Ming",IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Gallium nitride,Training,Generative adversarial network (GAN),multiscale,remote sensing,semantic segmentation,semisupervised,,,,,,,,,,,,,,,,,,,,,,,
Row_176,"Zhang, Hua","Jiang, Zhengang","Xu, Jun","Pan, Xin",Advancing high-resolution remote sensing: a compact and powerful approach to semantic segmentation,,SEP 16 2024,0,"Deep learning (DL)-based approaches are notable for their ability to establish feature associations without relying on physical constraints, unlike traditional strategies that are complex and dependent on expert experience. However, three main challenges hinder the versatility of semantic segmentation models. First, the targets in these images are dense and exist at varying spatial scales, which imposes higher demands on the model for accurate segmentation across scales. Second, the segmentation of small targets in the images is often overlooked, leading to a compromise between fine segmentation and model efficiency. Lastly, the data-intensive nature of remote sensing images and the resource-intensive operations of large-scale networks impose significant communication and computation burdens on edge devices, which may not have sufficient resources to handle them effectively. To address these challenges, this paper proposes a lightweight semantic segmentation method for remote sensing images to achieve high-precision segmentation for multi-scale targets while maintaining low computational complexity. The main components include: (1) embedding the inverted residual block structure to minimize the number of model parameters and computational costs; (2) introducing the parallel irregular space pyramid pooling module to efficiently aggregate multi-scale contextual information for fine-grained recognition of small targets; and (3) embedding transfer learning into the encoder-decoder structure to speed up the convergence rate and improve multi-scale feature fusion capability, thereby reducing semantic information loss. The proposed lightweight method has been extensively tested on real-world high-resolution remote sensing datasets. It achieved PA, MPA, MIoU, and FWIoU scores of 87.90%, 75.76%, 66.29%, and 78.81% on the Vaihingen dataset; 87.03%, 85.31%, 74.85%, and 77.54% on the Potsdam dataset; and 95.37%, 83.33%, 75.70%, and 91.31% on the Aeroscapes dataset. Compared to other popular semantic segmentation models, the proposed method achieved the highest values in all four evaluation indicators, demonstrating its effectiveness and superiority.",Remote sensing,image analysis,neural networks,semantic,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_177,"Wang, Yan","Cao, Li","Deng, He",,MFMamba: A Mamba-Based Multi-Modal Fusion Network for Semantic Segmentation of Remote Sensing Images,,NOV 2024,0,"Semantic segmentation of remote sensing images is a fundamental task in computer vision, holding substantial relevance in applications such as land cover surveys, environmental protection, and urban building planning. In recent years, multi-modal fusion-based models have garnered considerable attention, exhibiting superior segmentation performance when compared with traditional single-modal techniques. Nonetheless, the majority of these multi-modal models, which rely on Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs) for feature fusion, face limitations in terms of remote modeling capabilities or computational complexity. This paper presents a novel Mamba-based multi-modal fusion network called MFMamba for semantic segmentation of remote sensing images. Specifically, the network employs a dual-branch encoding structure, consisting of a CNN-based main encoder for extracting local features from high-resolution remote sensing images (HRRSIs) and of a Mamba-based auxiliary encoder for capturing global features on its corresponding digital surface model (DSM). To capitalize on the distinct attributes of the multi-modal remote sensing data from both branches, a feature fusion block (FFB) is designed to synergistically enhance and integrate the features extracted from the dual-branch structure at each stage. Extensive experiments on the Vaihingen and the Potsdam datasets have verified the effectiveness and superiority of MFMamba in semantic segmentation of remote sensing images. Compared with state-of-the-art methods, MFMamba achieves higher overall accuracy (OA) and a higher mean F1 score (mF1) and mean intersection over union (mIoU), while maintaining low computational complexity.",semantic segmentation,multi-modal remote sensing data,feature fusion,,,,,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_178,"Boulila, Wadii",,,,A top-down approach for semantic segmentation of big remote sensing images,,SEP 2019,28,"The increasing amount of remote sensing data has opened the door to new challenging research topics. Nowadays, significant efforts are devoted to pixel and object based classification in case of massive data. This paper addresses the problem of semantic segmentation of big remote sensing images. To do this, we proposed a top-down approach based on two main steps. The first step aims to compute features at the object-level. These features constitute the input of a multi-layer feed-forward network to generate a structure for classifying remote sensing objects. The goal of the second step is to use this structure to label every pixel in new images. Several experiments are conducted based on real datasets and results show good classification accuracy of the proposed approach. In addition, the comparison with existing classification techniques proves the effectiveness of the proposed approach especially for big remote sensing data.",Semantic segmentation,Remote sensing images,Neural networks,Big data,,,,,,EARTH SCIENCE INFORMATICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_179,"Li, Aijin","Jiao, Licheng","Zhu, Hao","Li, Lingling",Multitask Semantic Boundary Awareness Network for Remote Sensing Image Segmentation,,2022,77,"In remote sensing images, boundary information plays a crucial role in land-cover segmentation. However, it is a challenging problem that sufficiently extracts complete and sharp boundaries from complex very-high-resolution (VHR) remote sensing images. To tackle this problem, we propose a semantic boundary awareness network (SBANet). The SBANet captures refined boundary information of land covers in feature extraction and then supervises its learning with a designed boundary loss. The key of SBANet includes boundary attention module (BA-module) and adaptive weights of multitask learning (AWML). The BA-module is proposed to capture land-cover boundary information from hierarchical features aggregation in a bottom-up manner. It emphasizes useful boundary information and relieves noise information in low-level features with the guidance of high-level features. To directly learn the boundary information, AWML adds a boundary loss to the original semantic loss by an adaptive fusion manner. This multitask learning enables the semantic information and the boundary information to work collaboratively and promote each other. Note that the BA-module and AWML are plug-and-play. Experimental results demonstrate the effectiveness of the proposed SBANet on the available ISPRS 2-D semantic labeling Potsdam and Vaihingen data sets. The SBANet also achieves the state-of-the-art performance in terms of overall accuracy (OA) and mean score (m-).",Semantics,Feature extraction,Image segmentation,Remote sensing,Task analysis,"Liu, Fang",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Convolution,Spatial resolution,Boundary attention,multilevel aggregation,multitask learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_180,"Muhtar, Dilxat","Zhang, Xueliang","Xiao, Pengfeng",,Index Your Position: A Novel Self-Supervised Learning Method for Remote Sensing Images Semantic Segmentation,,2022,27,"Learning effective visual representations without human supervision is a critical problem for the task of semantic segmentation of remote sensing images (RSIs), where pixel-level annotations are difficult to obtain. Self-supervised learning (SSL), which learns useful representations by creating artificial supervised learning problems, has recently emerged as an effective method to learn from unlabelled data. Current SSL methods are generally trained on ImageNet through image-level prediction tasks. We argue that this is suboptimal for application in semantic segmentation of RSIs since it does not take into account spatial position information between objects, which is critical for the segmentation of RSIs characterized by multiobject. In this study, we propose a novel self-supervised dense representation learning method, IndexNet, for the semantic segmentation of RSIs. On the one hand, considering the multiobject characteristics of RSIs, IndexNet learns pixel-level representations by tracking object positions, while maintaining sensitivity to object position changes to ensure that no mismatches are caused. On the other hand, by combining image-level contrast and pixel-level contrast, IndexNet can learn spatiotemporal invariant features. Experimental results show that our method works better than ImageNet pretraining and outperforms state-of-the-art (SOTA) SSL methods. Code and pretrained models will be available at https://github.com/pUmpKin-Co/offical-IndexNet.",Image segmentation,Semantics,Task analysis,Indexes,Remote sensing,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Computer architecture,Crops,Remote sensing images (RSIs),self-supervised learning (SSL),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_181,"Li, Zhuoxuan","Yang, Junli","Wang, Bin","Li, Yaqi",MASKFORMER WITH IMPROVED ENCODER-DECODER MODULE FOR SEMANTIC SEGMENTATION OF FINE-RESOLUTION REMOTE SENSING IMAGES,"2022 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",2022,6,"In 2021, the Transformer based models have demonstrated extraordinary achievement in the field of computer vision. Among which, Maskformer, a Transformer based model adopting the mask classification method, is an outstanding model in both semantic segmentation and instance segmentation. Considering the specific characteristics of semantic segmentation of remote sensing images (RSIs), we design CADA-MaskFormer(a Mask classification-based model with Cross-shaped window self-Attention and Densely connected feature Aggregation) based on Maskformer by improving its encoder and pixel decoder. Concretely, the mask classification that generates one or even more masks for specific category to perform the elaborate segmentation is especially suitable for handling the characteristic of large within-class and small between-class variance of RSIs. Furthermore, we apply the Cross-Shaped Window self-attention mechanism to model the long-range context information contained in RSIs at maximum extent without the increasing of computational complexity. In addition, the Densely Connected Feature Aggregation Module (DCFAM) is used as the pixel decoder to incorporate multi-level feature maps from the encoder to get a finer semantic segmentation map. Extensive experiments conducted on two remotely sensed semantic segmentation datasets Potsdam and Vaihingen achieves 91.88% and 91.01% in OA index respectively, outperforming most of competitive models designed for RSIs. The code is available from https://github.com/lqwrl542293/JL-Yang_CV/tree/master/CADA_Maskformer",Semantic segmentation,remote sensing images,Transformer,,,"Pan, Ting",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_182,"Nie, Jie","Huang, Lei","Zheng, Chengyu","Lv, Xiaowei",Cross-scale Graph Interaction Network for Semantic Segmentation of Remote Sensing Images,,NOV 2023,8,"Semantic segmentation of remote sensing (RS) images plays a vital role in a variety of fields, including urban planning, natural disaster monitoring, and land resource management. Due to the complexity and low resolution of RS images, many approaches have been proposed to handle the related task. However, these previously developed approaches dedicate to contextual interaction but ignore the cross-scale semantic correlation and multi-scale boundary information. Therefore, we propose a Cross-scale Graph Interaction Network (CGIN) to address semantic segmentation problems of RS images, which consists of a semantic branch and a boundary branch. In the semantic branch, we first apply atrous convolution to extract multi-scale semantic features of RS images. Particularly, based on the multi-scale semantic features, a Cross-scale Graph Interaction (CGI) module is introduced, which establishes cross-scale graph structures and performs adaptive graph reasoning to capture the cross-scale semantic correlation of RS objects. In the boundary branch, we propose a Multiscale Boundary Feature Extraction (MBFE) module that utilizes atrous convolutions with different dilation rates to extract multi-scale boundary features. Finally, to address the problem of sparse boundary pixels in the fusion process of the two branches, we propose a Multi-scale Similarity-guided Aggregation (MSA) module by calculating the similarity of semantic features and boundary features at the corresponding scale, which can emphasize the boundary information in semantic features. Our proposed CGIN outperforms state-of-the-art approaches in numerical experiments conducted on two benchmark remote sensing datasets.",Remote sensing,semantic segmentation,cross-scale,graph convolutional network,boundary,"Wang, Rui",,,,ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_183,"Yang, Xue","Fan, Xiang","Peng, Mingjun","Guan, Qingfeng",Semantic segmentation for remote sensing images based on an AD-HRNet model,,DEC 31 2022,10,"Semantic segmentation for remote sensing images faces challenges of unbalanced category weight, rich context causing difficulties of recognition, blurred boundaries of multi-scale objects, and so on. To address these problems, we propose a new model by combining HRNet with attention mechanisms and dilated convolution, denoted as: AD-HRNet for the semantic segmentation of remote sensing images. In the framework of AD-HRNet, we obtained the weight value of each category based on an improved weighted cross-entropy function by introducing the median frequency balance method to solve the issue of class weight imbalance. The Shuffle-CBAM module with channel attention and spatial attention in AD-HRNet framework was applied to extract more global context information of images through slightly increasing the amount of computation. To address the problem of blurred boundaries caused by multi-scale object segmentation and edge segmentation, we developed an MDC-DUC module in AD-HRNet framework to capture the context information of multi-scale objects and the edge information of many irregular objects. Taking Postdam, Vaihingen, and SAMA-VTOL datasets as materials, we verified the performance of AD-HRNet by comparing with eight typical semantic segmentation models. Experimental results shown that AD-HRNet increases the mIoUs to 75.59% and 71.58% based on the Postdam and Vaihingen datasets, respectively.",Semantic segmentation,convolutional neural networks,dilated convolution,attention mechanism,remote sensing,"Tang, Luliang",,,,INTERNATIONAL JOURNAL OF DIGITAL EARTH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_184,"Duan, Yiping","Tao, Xiaoming","Han, Chaoyi","Lu, Jianhua",HIERARCHICAL MULTINOMIAL LATENT MODEL WITH G0 DISTRIBUTION FOR REMOTE SENSING IMAGE SEMANTIC SEGMENTATION,2017 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP 2017),2017,5,"Considering the scattering statistics and multi scale characteristics of the remote sensing images, this paper presents a hierarchical multinomial latent model with G(0) distribution (HML-G(0)) for remote sensing image semantic segmentation. In the proposed approach, hierarchical multinomial latent model is proposed to capture the multi scale information of the remote sensing images. Moreover, the flexibility of G(0) distribution is plugged into the hierarchical multinomial latent model for the segmentation of various types of land covers. Then, the developed Bayesian inference on the quadtree is incorporated in our approach, and the semantic segmentation map is achieved by bottom-up and top-down probability computation. Experimental results demonstrate that our proposed hierarchical scheme produces the semantic segmentation maps, and the exhibiting performance improvements in terms of labeling consistency and the detail preservation.",Remote sensing images,semantic segmentation,hierarchical multinomial latent model,G(0) distribution,Bayesian inference,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_185,Liu Yu-Xi,Zhang Bo,Wang Bin,,Semi-supervised semantic segmentation based on Generative Adversarial Networks for remote sensing images,,AUG 2020,5,"Semantic segmentation of very high resolution (VHR) remote sensing images is one of the hot topics in the field of remote sensing image processing. Traditional supervised segmentation methods demand a huge mass of labeled data while the labeling process is very consuming. To solve this problem, a semi-supervised semantic segmentation method for VHR remote sensing images based on Generative Adversarial Networks (GANs) is proposed, and only a few labeled samples are needed to obtain pretty good segmentation results. A fully convolutional auxiliary adversarial network is added to the segmentation network, conducing to keeping the consistency of labels in the segmentation results of VHR remote sensing images. Furthermore, a novel adversarial loss with attention mechanism is proposed in the paper in order to solve the problem of easy sample over-whelming during the updating process of the segmentation network constrained by the discriminator when the segmentation results can confuse the discriminator. The experimental results on ISPRS Vaihingen 2D Semantic Labeling Challenge Dataset show that the proposed method can greatly improve the segmentation accuracy of remote sensing images compared with other state-of-the-art methods.",very high resolution remote sensing images,semantic segmentation,deep learning,generative adversarial networks,loss function,,,,,JOURNAL OF INFRARED AND MILLIMETER WAVES,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_186,"Liu, Songlin","Gao, Kai","Qin, Jinchun","Gong, Hui",SE2Net: semantic segmentation of remote sensing images based on self-attention and edge enhancement modules,,MAY 19 2021,4,"The semantic segmentation of optical satellite remote sensing images is more challenging than that of natural images, owing to the considerable differences in the texture, shape, topology, and scale of ground features in different areas and the coexistence of dense and sparse arrangements. To alleviate these difficulties and ensure a high accuracy, a semantic segmentation framework named the self-attention and edge enhancement network (SE(2)Net) is constructed considering two aspects. First, because the self-attention mechanism can capture more useful semantic information by modeling large neighborhood correlations, we embed a self-attention module known as spatial expectation maximization attention (SEMA) in the considered network. Second, the Laplace operator is adopted to explore the significant edge information to design an edge enhancement module (EEM). Finally, both SEMA and EEM are embedded in the proposed SE(2)Net, thereby forming an end-to-end network. To validate the performance of the proposed approach, we construct a semantic segmentation dataset (SSD) using Tian-Hui 1 satellite images and conduct extensive experiments on both the SSD and the gaofen image dataset (GID). The results demonstrate the superiority of the proposed method over other state-of-the-art approaches and the effectiveness of the constructed SSD. (C) 2021 Society of Photo-Optical Instrumentation Engineers (SPIE)",semantic segmentation,optical satellite remote sensing images,edge enhancement,self-attention mechanism,,"Wang, Haiyan","Zhang, Li","Gong, Danchao",,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_187,"Hu, Zaiyi","Gao, Junyu","Yuan, Yuan","Li, Xuelong",Contrastive Tokens and Label Activation for Remote Sensing Weakly Supervised Semantic Segmentation,,2024,0,"In recent years, there has been remarkable progress in weakly supervised semantic segmentation (WSSS), with vision transformer (ViT) architectures emerging as a natural fit for such tasks due to their inherent ability to leverage global attention for comprehensive object information perception. However, directly applying ViT to WSSS tasks can introduce challenges. The characteristics of ViT can lead to an oversmoothing problem, particularly in dense scenes of remote sensing images, significantly compromising the effectiveness of class activation maps (CAMs) and posing challenges for segmentation. Moreover, existing methods often adopt multistage strategies, adding complexity and reducing training efficiency. To overcome these challenges, a comprehensive framework Contrastive Token and Foreground Activation (CTFA) based on the ViT architecture for WSSS of remote sensing images is presented. Our proposed method includes a contrastive token learning module (CTLM), incorporating both patch-wise and class-wise token learning to enhance model performance. In patch-wise learning, we leverage the semantic diversity preserved in intermediate layers of ViT and derive a relation matrix from these layers and employ it to supervise the final output tokens, thereby improving the quality of CAM. In class-wise learning, we ensure the consistency of representation between global and local tokens, revealing more entire object regions. Additionally, by activating foreground features in the generated pseudo label using a dual-branch decoder, we further promote the improvement of CAM generation. Our approach demonstrates outstanding results across three well-established datasets, providing a more efficient and streamlined solution for WSSS. Code will be available at: https://github.com/ZaiyiHu/CTFA.",Remote sensing,Semantic segmentation,Training,Task analysis,Semantics,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Convolutional neural networks,Transformers,Deep learning,remote sensing images,vision transformer (ViT),weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,,,,,,,,,,
Row_188,"Zhong, Bo","Du, Jiang","Liu, Minghao","Yang, Aixia",Region-Enhancing Network for Semantic Segmentation of Remote-Sensing Imagery,,NOV 2021,1,"Semantic segmentation for high-resolution remote-sensing imagery (HRRSI) has become increasingly popular in machine vision in recent years. Most of the state-of-the-art methods for semantic segmentation of HRRSI usually emphasize the strong learning ability of deep convolutional neural network to model the contextual relationship in the image, which takes too much consideration on every pixel in images and subsequently causes the problem of overlearning. Annotation errors and easily confused features can also lead to the confusion problem while using the pixel-based methods. Therefore, we propose a new semantic segmentation network-the region-enhancing network (RE-Net)-to emphasize the regional information instead of pixels to solve the above problems. RE-Net introduces the regional information into the base network, to enhance the regional integrity of images and thus reduce misclassification. Specifically, the regional context learning procedure (RCLP) can learn the context relationship from the perspective of regions. The region correcting procedure (RCP) uses the pixel aggregation feature to recalibrate the pixel features in each region. In addition, another simple intra-network multi-scale attention module is introduced to select features at different scales by the size of the region. A large number of comparative experiments on four different public datasets demonstrate that the proposed RE-Net performs better than most of the state-of-the-art ones.",semantic segmentation,remote sensing imagery (HRRSI),deep convolutional neural network,regional integrity of images,,"Wu, Junjun",,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_189,"Zhou, Zheng","Zheng, Change","Liu, Xiaodong","Tian, Ye",A Dynamic Effective Class Balanced Approach for Remote Sensing Imagery Semantic Segmentation of Imbalanced Data,,APR 2023,12,"The wide application and rapid development of satellite remote sensing technology have put higher requirements on remote sensing image segmentation methods. Because of its characteristics of large image size, large data volume, and complex segmentation background, not only are the traditional image segmentation methods difficult to apply effectively, but the image segmentation methods based on deep learning are faced with the problem of extremely unbalanced data between categories. In order to solve this problem, first of all, according to the existing effective sample theory, the effective sample calculation method in the context of semantic segmentation is firstly proposed in the highly unbalanced dataset. Then, a dynamic weighting method based on the effective sample concept is proposed, which can be applied to the semantic segmentation of remote sensing images. Finally, the applicability of this method to different loss functions and different network structures is verified on the self-built Landsat8-OLI remote sensing image-based tri-classified forest fire burning area dataset and the LoveDA dataset, which is for land-cover semantic segmentation. It has been concluded that this weighting algorithm can enhance the minimal-class segmentation accuracy while ensuring that the overall segmentation performance in multi-class segmentation tasks is verified in two different semantic segmentation tasks, including the land use and land cover (LULC) and the forest fire burning area segmentation In addition, this proposed method significantly improves the recall of forest fire burning area segmentation by as much as about 30%, which is of great reference value for forest fire research based on remote sensing images.",remote sensing,segmentation,imbalanced dataset,dynamic weighting,effective sample,"Chen, Xiaoyi","Chen, Xuexue","Dong, Zixun",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_190,"Nong, Zhixian","Su, Xin","Liu, Yi","Zhan, Zongqian",Boundary-Aware Dual-Stream Network for VHR Remote Sensing Images Semantic Segmentation,,2021,11,"Semantic segmentation for very-high-resolution remote sensing images has been a research hotspot in the field of remote sensing image analysis. However, most existing methods still suffer from a challenge that object boundaries cannot be finely recovered. To tackle the problem, we develop a dual-stream network based on the U-Net architecture, Instead of the traditional skip connections, a boundary attention module is proposed to introduce the boundary information from the EDN module to the SSN module. Experiments on ISPRS Potsdam and Vaihingen datasets show the effectiveness of the proposed network, especially in man-made objects with distinct boundaries.",Semantics,Image edge detection,Image segmentation,Remote sensing,Feature extraction,"Yuan, Qiangqiang",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Convolution,Task analysis,Attention module,edge detection subnetwork,semantic segmentation,very high spatial resolution,,,,,,,,,,,,,,,,,,,,,,,,
Row_191,"Wei, Haoran","Xu, Xiangyang","Ou, Ni","Zhang, Xinru",DEANet: Dual Encoder with Attention Network for Semantic Segmentation of Remote Sensing Imagery,,OCT 2021,21,"Remote sensing has now been widely used in various fields, and the research on the automatic land-cover segmentation methods of remote sensing imagery is significant to the development of remote sensing technology. Deep learning methods, which are developing rapidly in the field of semantic segmentation, have been widely applied to remote sensing imagery segmentation. In this work, a novel deep learning network-Dual Encoder with Attention Network (DEANet) is proposed. In this network, a dual-branch encoder structure, whose first branch is used to generate a rough guidance feature map as area attention to help re-encode feature maps in the next branch, is proposed to improve the encoding ability of the network, and an improved pyramid partial decoder (PPD) based on the parallel partial decoder is put forward to make fuller use of the features form the encoder along with the receptive filed block (RFB). In addition, an edge attention module using the transfer learning method is introduced to explicitly advance the segmentation performance in edge areas. Except for structure, a loss function composed with the weighted Cross Entropy (CE) loss and weighted Union subtract Intersection (UsI) loss is designed for training, where UsI loss represents a new region-based aware loss which replaces the IoU loss to adapt to multi-classification tasks. Furthermore, a detailed training strategy for the network is introduced as well. Extensive experiments on three public datasets verify the effectiveness of each proposed module in our framework and demonstrate that our method achieves more excellent performance over some state-of-the-art methods.",remote sensing,land cover classification,deep learning,semantic segmentation,encoder-decoder,"Dai, Yaping",,,,REMOTE SENSING,,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_192,"Zhang, Zehua","Liu, Bailin","Li, Yani",,FURSformer: Semantic Segmentation Network for Remote Sensing Images with Fused Heterogeneous Features,,JUL 2023,3,"Semantic segmentation of remote sensing images poses a formidable challenge within this domain. Our investigation commences with a pilot study aimed at scrutinizing the advantages and disadvantages of employing a Transformer architecture and a CNN architecture in remote sensing imagery (RSI). Our objective is to substantiate the indispensability of both local and global information for RSI analysis. In this research article, we harness the potential of the Transformer model to establish global contextual understanding while incorporating an additional convolution module for localized perception. Nonetheless, a direct fusion of these heterogeneous information sources often yields subpar outcomes. To address this limitation, we propose an innovative hierarchical fusion feature information module that this model can fuse Transformer and CNN features using an ensemble-to-set approach, thereby enhancing information compatibility. Our proposed model, named FURSformer, amalgamates the strengths of the Transformer architecture and CNN. The experimental results clearly demonstrate the effectiveness of this approach. Notably, our model achieved an outstanding accuracy of 90.78% mAccuracy on the DLRSD dataset.",semantic segmentation,remote sensing images,Transformer,CNN,,,,,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_193,"Liu, Rui","Mi, Li","Chen, Zhenzhong",,AFNet: Adaptive Fusion Network for Remote Sensing Image Semantic Segmentation,,SEP 2021,77,"Semantic segmentation of remote sensing images plays an important role in many applications. However, a remote sensing image typically comprises a complex and heterogenous urban landscape with objects in various sizes and materials, which causes challenges to the task. In this work, a novel adaptive fusion network (AFNet) is proposed to improve the performance of very high resolution (VHR) remote sensing image segmentation. To coherently label size-varied ground objects from different categories, we design multilevel architecture with the scale-feature attention module (SFAM). By SFAM, at the location of small objects, low-level features from the shallow layers of convolutional neural network (CNN) are enhanced, whilst for large objects, high-level features from deep layers are enhanced. Thus, the features of size-varied objects could be preserved during fusing features from different levels, which helps to label size-varied objects. As for labeling the category with high intra-class difference and varied scales, the multiscale structure with a scale-layer attention module (SLAM) is utilized to learn representative features, where an adjacent score map refinement module (ACSR) is employed as the classifier. By SLAM, when fusing multiscale features, based on the interested objects scale, feature map from appropriate scale is given greater weights. With such a scale-aware strategy, the learned features can be more representative, which is helpful to distinguish objects for semantic segmentation. Besides, the performance is further improved by introducing several nonlinear layers to the ACSR. Extensive experiments conducted on two well-known public high-resolution remote sensing image data sets show the effectiveness of our proposed model. Code and predictions are available at https://github.com/athauna/AFNet/",Image segmentation,Simultaneous localization and mapping,Adaptive systems,Semantics,Optical imaging,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Labeling,Convolutional neural networks,Attention mechanism,convolutional neural network (CNN),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_194,Su Zhipeng,Li Jingwen,Jiang Jianwu,Lu Yanling,Semantic Segmentation Method for Remote Sensing Images Based on Improved DeepLabV3+,,MAR 2023,1,"A remote sensing image segmentation network called AFSM-Net, which combines a feature map segmentation module and an attention mechanism module, is proposed to address the issues of low recognition and low segmentation accuracy of small targets in remote sensing image segmentation using conventional convolutional neural networks. First, the feature map segmentation module is introduced in the coding stage to enlarge each segmented feature map and extract features by sharing parameters; then, the extracted features are fused with the original output image of the network; and finally, the attention mechanism module is introduced into the network model to make it pay more attention to the effective feature information in the image and ignore the irrelevant background information, to improve the feature extraction ability of the model for small target objects. The experimental results show that the average intersection ratio of the proposed method is 86. 42%, which is 3. 94 percentage points higher than that of the DeepLabV3+ model. The proposed method fully considers the attention of small and medium targets in image segmentation, and improves the segmentation accuracy of remote sensing images.",remote sensing,remote sensing image,DeepLabV3+,feature image cut,attention mechanism,Zhu Ming,,,,LASER & OPTOELECTRONICS PROGRESS,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_195,"Meng, Xichen","Zhu, Liqun","Han, Yilong","Zhang, Hanchao",We Need to Communicate: Communicating Attention Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,JUL 2023,2,"Traditional models that employ CNNs as encoders do not sufficiently combine high-level features and low-level features. However, high-level features are rich in semantic information but lack spatial detail, while low-level features are the opposite. Therefore, the integrated utilization of multi-level features and the bridging of the gap between them is crucial to promote the accuracy of semantic segmentation. To address this issue, we presented communicating mutual attention (CMA) and communicating self-attention (CSA) modules to enhance the interaction and fusion of different levels of feature maps. On the one hand, CMA aggregates the global context information of high-level features into low-level features and embeds the spatial detail localization characteristics of low-level features in high-level features. On the other hand, the CSA module is deployed to integrate the spatially detailed representation of low-level features into the attention map of high-level features. We have experimented with the communicating attention network (CANet), a U-net-like network composed of multiple CMA and CSA modules, on the ISPRS Vaihingen and Potsdam datasets with mean F1-scores of 89.61% and 92.60%, respectively. The results demonstrate that CANet embodies superior performance in the semantic segmentation task of remote sensing of images.",attention mechanism,remote sensing,semantic segmentation,,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_196,"Zheng, Guoxun","Jiang, Zhengang","Zhang, Hua","Yao, Xuekun",Deep semantic segmentation of unmanned aerial vehicle remote sensing images based on fully convolutional neural network,,MAR 28 2023,1,"In the era of artificial intelligence and big data, semantic segmentation of images plays a vital role in various fields, such as people's livelihoods and the military. The accuracy of semantic segmentation results directly affects the subsequent data analysis and intelligent applications. Presently, semantic segmentation of unmanned aerial vehicle (UAV) remote-sensing images is a research hotspot. Compared with manual segmentation and object-based segmentation methods, semantic segmentation methods based on deep learning are efficient and highly accurate segmentation methods. The author has seriously studied the implementation principle and process of the classical deep semantic segmentation model-the fully convolutional neural network (FCN), including convolution and pooling in the encoding stage, deconvolution and upsampling, etc., in the decoding stage. The author has applied the three structures (i.e., FCN-32s, FCN-16s, and FCN-8s) to the UAV remote sensing image dataset AeroScapes. And the results show that the accuracy of vegetation recognition is stable at about 94%. The accuracy of road recognition can reach up to more than 88%. The mean pixel accuracy rate of the whole test dataset is above 91%. Applying full convolution neural network to semantic segmentation of UAV remote sensing images can improve the efficiency and accuracy of semantic segmentation significantly.",deep learning,unmanned aerial vehicle,remote sensing,semantic segmentation,FCN,,,,,FRONTIERS IN EARTH SCIENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_197,He Shumeng,Xu Gaodi,Yang Houqun,,A semantic segmentation method for remote sensing images based on multiple contextual feature extraction,,JAN 25 2023,2,"Semantic segmentation of remote sensing images plays a significant role in many applications such as urban planning and ecological protection, but its semantic segmentation suffers from large intra-category variation and large differences in the scale of objects, so it is prone to misclassification. To cope with this issue, an embedded channel's categorical attention module (ECCA) is proposed to extract contextual information from the perspective of categories, and a channel attention module is embedded in it to achieve multiple contextual information extraction. Combined with the remote sensing atrous spatial pyramid pooling module (RSASPP), which is composed of atrous convolution with different expansion rates, feature fusion of objects at different scales is achieved. The refinement module (RM) is added for boundary refinement to achieve finer segmentation. Experiments are conducted on the WHDLD dataset to prove the effectiveness of the method.",attention mechanism,contextual information,remote sensing images,semantic segmentation,,,,,,CONCURRENCY AND COMPUTATION-PRACTICE & EXPERIENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_198,"Hwang, Gyutae","Jeong, Jiwoo","Lee, Sang Jun",,SFA-Net: Semantic Feature Adjustment Network for Remote Sensing Image Segmentation,,SEP 2024,2,"Advances in deep learning and computer vision techniques have made impacts in the field of remote sensing, enabling efficient data analysis for applications such as land cover classification and change detection. Convolutional neural networks (CNNs) and transformer architectures have been utilized in visual perception algorithms due to their effectiveness in analyzing local features and global context. In this paper, we propose a hybrid transformer architecture that consists of a CNN-based encoder and transformer-based decoder. We propose a feature adjustment module that refines the multiscale feature maps extracted from an EfficientNet backbone network. The adjusted feature maps are integrated into the transformer-based decoder to perform the semantic segmentation of the remote sensing images. This paper refers to the proposed encoder-decoder architecture as a semantic feature adjustment network (SFA-Net). To demonstrate the effectiveness of the SFA-Net, experiments were thoroughly conducted with four public benchmark datasets, including the UAVid, ISPRS Potsdam, ISPRS Vaihingen, and LoveDA datasets. The proposed model achieved state-of-the-art accuracy on the UAVid, ISPRS Vaihingen, and LoveDA datasets for the segmentation of the remote sensing images. On the ISPRS Potsdam dataset, our method achieved comparable accuracy to the latest model while reducing the number of trainable parameters from 113.8 M to 10.7 M.",remote sensing image,segmentation,transformer,hybrid architecture,feature adjustment module,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_199,"Zhang, Xiaolu","Wang, Zhaoshun","Zhang, Jianheng","Wei, Anlei",MSANet: an improved semantic segmentation method using multi-scale attention for remote sensing images,,DEC 2 2022,5,"With the development of deep learning technology in the field of computer vision, the land cover classification of remote sensing images has gradually developed. In this paper, a deep learning algorithm for land cover classification of remote sensing images is proposed. Two dual-attention mechanism modules with multi-scale spatial attention and channel attention are designed and integrated into the DeepLabv3+ network, called multi-scale convolutional attention network (MCAN) and multi-scale atrous convolutional attention network (MACAN). Besides, Inception-Squeeze-and-Excitation network (ISNet) is added. So, the network can focus on the parts of remote sensing images that need attention at different scales, and it can improve the accuracy of remote sensing semantic segmentation. The experimental results show that the optimized network can achieve a Mean Intersection over Union (mIoU) of 74.03% on the DeepGlobe datasets, which is 14.55%, 13.47%, and 5.73% higher than that of UNet, SegNet, and DeepLabv3+. It also achieved good results on Vaihingen datasets. This result indicates that the optimized network can effectively improve the accuracy of remote sensing land cover classification.",Remote sensing semantic segmentation,land cover classification,attention mechanism,deep learning,,,,,,REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_200,"Li, Jiarui","Cheng, Shuli",,,AFENet: An Attention-Focused Feature Enhancement Network for the Efficient Semantic Segmentation of Remote Sensing Images,,DEC 2024,0,"The semantic segmentation of high-resolution remote sensing images (HRRSIs) faces persistent challenges in handling complex architectural structures and shadow occlusions, limiting the effectiveness of existing deep learning approaches. To address these limitations, we propose an attention-focused feature enhancement network (AFENet) with a novel encoder-decoder architecture. The encoder architecture combines ResNet50 with a parallel multistage feature enhancement group (PMFEG), enabling robust feature extraction through optimized channel reduction, scale expansion, and channel reassignment operations. Building upon this foundation, we develop a global multi-scale attention mechanism (GMAM) in the decoder that effectively synthesizes spatial information across multiple scales by learning comprehensive global-local relationships. The architecture is further enhanced by an efficient feature-weighted fusion module (FWFM) that systematically integrates remote spatial features with local semantic information to improve segmentation accuracy. Experimental results across diverse scenarios demonstrate that AFENet achieves superior performance in building structure detection, exhibiting enhanced segmentation connectivity and completeness compared to state-of-the-art methods.",remote sensing,semantic segmentation,multi-scale feature,attention mechanism,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_201,"Zhu, Jiahang","Zhou, Yuan","Xu, Nuo","Huo, Chunlei",Collaborative Learning Network for Change Detection and Semantic Segmentation of Remote Sensing Images,,2023,1,"Change detection of high-resolution remote sensing images is more attractive, since it can not only identify areas of changes but also identify types of changes. In this context, simultaneous change detection and semantic segmentation are natural and necessary. However, traditional methods put less emphasis on the cooperation of the above two tasks. In this letter, a novel method is proposed to realize the collaborative learning of change detection and semantic segmentation. By elaborately exploring the relevance and consistency between change detection and semantic segmentation, the proposed method synchronously enhanced feature separability of two tasks, and it outperformed a single change detection network or semantic segmentation network. Specifically, the proposed approach extracts multilevel bitemporal features by a backbone network, followed by two layer-by-layer decoders for learning change features and semantic features. On one hand, the interactive fusion module (IFM) fuses the changing features and semantic features together to increase the collaboration between the two tasks. On the other hand, the contrastive loss (CL) enhances the constraints between the two tasks. The advantages of the proposed method are demonstrated with respect to change region detection and change-type identification.",Change detection,collaborative learning,semantic segmentation,,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_202,"Chen, Yuhan","Liu, Pengyuan","Zhao, Jiechen","Huang, Kaijian",Shallow-Guided Transformer for Semantic Segmentation of Hyperspectral Remote Sensing Imagery,,JUL 2023,10,"Convolutional neural networks (CNNs) have achieved great progress in the classification of surface objects with hyperspectral data, but due to the limitations of convolutional operations, CNNs cannot effectively interact with contextual information. Transformer succeeds in solving this problem, and thus has been widely used to classify hyperspectral surface objects in recent years. However, the huge computational load of Transformer poses a challenge in hyperspectral semantic segmentation tasks. In addition, the use of single Transformer discards the local correlation, making it ineffective for remote sensing tasks with small datasets. Therefore, we propose a new Transformer layered architecture that combines Transformer with CNN, adopts a feature dimensionality reduction module and a Transformer-style CNN module to extract shallow features and construct texture constraints, and employs the original Transformer Encoder to extract deep features. Furthermore, we also designed a simple Decoder to process shallow spatial detail information and deep semantic features separately. Experimental results based on three publicly available hyperspectral datasets show that our proposed method has significant advantages compared with other traditional CNN, Transformer-type models.",vision transformer,convolutional neural networks (CNNs),feature representations,hyperspectral images (HSIs),semantic segmentation,"Yan, Qingyun",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_203,"Zheng, Chen","Zhang, Yun","Wang, Leiguang",,Multigranularity Multiclass-Layer Markov Random Field Model for Semantic Segmentation of Remote Sensing Images,,DEC 2021,16,"Semantic segmentation is one of the most important tasks in remote sensing. However, as spatial resolution increases, distinguishing the homogeneity of each land class and the heterogeneity between different land classes are challenging. The Markov random field model (MRF) is a widely used method for semantic segmentation due to its effective spatial context description. To improve segmentation accuracy, some MRF-based methods extract more image information by constructing the probability graph with pixel or object granularity units, and some other methods interpret the image from different semantic perspectives by building multilayer semantic classes. However, these MRF-based methods fail to capture the relationship between different granularity features extracted from the image and hierarchical semantic classes that need to be interpreted. In this article, a new MRF-based method is proposed to incorporate the multigranularity information and the multilayer semantic classes together for semantic segmentation of remote sensing images. The proposed method develops a framework that builds a hybrid probability graph on both pixel and object granularities and defines a multiclass-layer label field with hierarchical semantic over the hybrid probability graph. A generative alternating granularity inference is suggested to provide the result by iteratively passing and updating information between different granularities and hierarchical semantics. The proposed method is tested on texture images, different remote sensing images obtained by the SPOT5, Gaofen-2, GeoEye, and aerial sensors, and Pavia University hyperspectral image. Experiments demonstrate that the proposed method shows a better segmentation performance than other state-of-the-art methods.",Semantics,Image segmentation,Remote sensing,Feature extraction,Biological system modeling,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Spatial resolution,Nonhomogeneous media,Granularity,Markov random field (MRF),remote sensing,segmentation,semantic,,,,,,,,,,,,,,,,,,,,,,,
Row_204,"Liu, Jiachao","Xiong, Xinyue","Li, Jiaojiao","Wu, Chaoxiong",DILATED RESIDUAL NETWORK BASED ON DUAL EXPECTATION MAXIMIZATION ATTENTION FOR SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,3,"Compared with common RGB images, remote sensing images (RSIs) have larger size and lower spatial resolution. RSIs are usually cropped into sub-images for training convolutional neural networks (CNNs), which loses amounts of context information, thus limiting the extraction of feature interdependencies and reducing the accuracy of semantic segmentation. In this paper, a novel dilated residual network based on dual expectation maximization attention (DE-MANet) is proposed for semantic segmentation of RSIs. In specific, we append a dual expectation maximization attention (DEMA) module on top of the dilated CNN. The spatial expectation maximization attention (SEMA) can model spatial feature interdependencies to acquire rich long-range contextual information. The channel expectation maximization attention (CEMA) enhances discriminant ability of channel-wise feature representations through extracting the channel dependencies. We evaluate the model on the dataset released in the Tianzhi Cup Artificial Intelligence Challenge and achieve 85.60% pixel accuracy and 69.00% mean intersection over union (mIoU).",Remote Sensing Images,Semantic Segmentation,DEMANet,Dual Expectation Maximization Attention,,"Song, Rui",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_205,"Wang, Huaijun","Qiao, Luqi","Li, He","Li, Xiujuan",Remote sensing image semantic segmentation method based on small target and edge feature enhancement,,OCT 1 2023,5,"Semantic segmentation of high-resolution remote sensing images based on deep learning has become a hot research topic and has been widely applied. At present, based on the structure of the convolutional neural network, when extracting target features through multiple layer convolutional layers, it is easy to cause the loss of small target features and fuzzy boundary of ground object classification. Therefore, we propose a remote sensing image semantic segmentation method P-Net to detect small target and enhance edge feature. The proposed network was based on an Encoder-Decoder structure. The decoder included the following components: a progressive small target feature enhancement network (IFEN), a boundary thinning module (BRM), and a feature aggregation module (FIAM). Firstly, the dense side output features of the encoder network were utilized to learn and acquired small target feature information and target edge features. Secondly, the pyramid segmentation attention module was introduced to effectively extract fine-grained and multi-scale spatial information. This module enhanced the feature expression of small targets and obtained high-level semantic feature information. The boundary refinement module was designed to refine the low-level spatial feature information extracted by the encoder. Finally, in order to improve the accuracy of remote sensing image object segmentation boundaries, skip connections were used to fuse high-level semantic information and low-level spatial information acrossed layers. These skip connections had the same spatial resolution but different semantic information. In this paper, six evaluation indices including mean intersection over union, frequency weighted intersection over union, pixel accuracy, F1, recall, and precision were used to verify on two public datasets of high-resolution remote sensing images, Gaofen image dataset (GID) and wuhan dense labeling dataset (WHDLD). In the GID dataset, each index reached 78.90%, 78.87%, 87.76%, 87.74%, 87.51%, and 88.04%, respectively; in the WHDLD dataset, each index reached 63.21%, 75.20%, 84.67%, 75.79%, 76.56%, and 75.45%, respectively. The results show that the performance of proposed method is better than that of DeepLabv3+, U-NET, PSPNet, and DUC_HDC methods. More precisely, the recognition performance of small target features is better, and the boundary obtained between object categories is clearer.",remote sensing image,semantic segmentation,pyramidal syncopated attention module,feature fusion,small target,"Li, Junhuai","Cao, Ting","Zhang, Chunyi",,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_206,"Huang, Wubiao","Ding, Mingtao","Deng, Fei",,Domain-Incremental Learning for Remote Sensing Semantic Segmentation With Multifeature Constraints in Graph Space,,2024,0,"The use of deep learning techniques for semantic segmentation in remote sensing has been increasingly prevalent. Effectively modeling remote contextual information and integrating high-level abstract features with low-level spatial features are critical challenges for semantic segmentation tasks. This article addresses these challenges by constructing a graph space reasoning (GSR) module and a dual-channel cross-attention upsampling (DCAU) module. Meanwhile, a new domain-incremental learning (DIL) framework is designed to alleviate catastrophic forgetting when the deep learning model is used in cross-domain. This framework makes a balance between retaining prior knowledge and acquiring new information through the use of frozen feature layers and multifeature joint loss optimization. Based on this, a new DIL of remote sensing semantic segmentation with multifeature constraints in graph space (GSMF-RS-DIL) framework is proposed. Extensive experiments, including ablation experiments on the ISPRS and LoveDA datasets, demonstrate that the proposed method achieves superior performance and optimal computational efficiency in both single-domain and cross-domain tasks. The code is publicly available at https://github.com/Huang WBill/GSMF-RS-DIL.",Cross attention,domain-incremental learning (DIL),graph space reasoning (GSR),remote sensing image,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Cross attention,domain-incremental learning (DIL),graph space reasoning (GSR),remote sensing image,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_207,"Zheng, Yalan","Yang, Mengyuan","Wang, Min","Qian, Xiaojun",Semi-Supervised Adversarial Semantic Segmentation Network Using Transformer and Multiscale Convolution for High-Resolution Remote Sensing Imagery,,APR 2022,16,"Semantic segmentation is a crucial approach for remote sensing interpretation. High-precision semantic segmentation results are obtained at the cost of manually collecting massive pixelwise annotations. Remote sensing imagery contains complex and variable ground objects and obtaining abundant manual annotations is expensive and arduous. The semi-supervised learning (SSL) strategy can enhance the generalization capability of a model with a small number of labeled samples. In this study, a novel semi-supervised adversarial semantic segmentation network is developed for remote sensing information extraction. A multiscale input convolution module (MICM) is designed to extract sufficient local features, while a Transformer module (TM) is applied for long-range dependency modeling. These modules are integrated to construct a segmentation network with a double-branch encoder. Additionally, a double-branch discriminator network with different convolution kernel sizes is proposed. The segmentation network and discriminator network are jointly trained under the semi-supervised adversarial learning (SSAL) framework to improve its segmentation accuracy in cases with small amounts of labeled data. Taking building extraction as a case study, experiments on three datasets with different resolutions are conducted to validate the proposed network. Semi-supervised semantic segmentation models, in which DeepLabv2, the pyramid scene parsing network (PSPNet), UNet and TransUNet are taken as backbone networks, are utilized for performance comparisons. The results suggest that the approach effectively improves the accuracy of semantic segmentation. The F1 and mean intersection over union (mIoU) accuracy measures are improved by 0.82-11.83% and 0.74-7.5%, respectively, over those of other methods.",semantic segmentation,semi-supervised learning,transformer,adversarial learning,remote sensing,"Yang, Rui","Zhang, Xin","Dong, Wen",,REMOTE SENSING,,building extraction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_208,"Qin, Yiqing","Chi, Mingmin",,,RSImageNet: A Universal Deep Semantic Segmentation Lifecycle for Remote Sensing Images,,2020,5,"In real applications, there is a lack of labeled data to train a proper deep neural network (DNN) model for map generation of remote sensing images. The aim of newly acquired data in spaceborne or airborne platforms is often to consistently observe the Earth for new tasks in the applications such as disaster monitoring, climate change, disease control. To fulfill the tasks, the corresponding classification maps should be obtained traditionally based on the assumption that a classification model should be learnt by the labeled data for the same task from the same scene or at least from the historical labeled remote sensing image pixels provided by domain experts in the same areas by the same sensor, which is denoted as labeled target data. In the paper, a universal deep semantic segmentation lifecycle is proposed against the assumption aforementioned, i.e., there is no need to have the labeled data for the same/similar task from the same locations and sensors to define a proper DNN model. In particular, a general labeled dataset is generated through a feature binding strategy in terms of real-world existed remote sensing images, which is named RSImageNet. In addition, a special training strategy is proposed by using the RSImageNet dataset to train a universal deep semantic segmentation model with a balanced constraint for the loss function. Without the labeled target data from the area observed, we gain an average overall accuracy of 77.32% in the range of 67.28-94.63% on 6 real world datasets by taking advantage of the proposed universal deep semantic segmentation lifecycle and the generated RSImageNet dataset.",Semantics,Image segmentation,Remote sensing,Training,Task analysis,,,,,IEEE ACCESS,,Data models,Machine learning,Semantic segmentation,RSImageNet,remote sensing images,fully convolutional network,big data,,,,,,,,,,,,,,,,,,,,,,,
Row_209,"Zhang, Qian","Yang, Guang","Zhang, Guixu",,Collaborative Network for Super-Resolution and Semantic Segmentation of Remote Sensing Images,,2022,30,"In the past few years, multitask learning (MTL) has been widely used in a single model to solve the problems of multiple businesses. MTL enables each task to achieve high performance and greatly reduces computational resource overhead. In this work, we designed a collaborative network that simultaneously solves the super-resolution semantic segmentation and super-resolution image reconstruction. This algorithm can obtain high-resolution semantic segmentation and super-resolution reconstruction results by taking relatively low-resolution images as input when high-resolution data are inconvenient or computing resources are limited. The framework consists of three parts: the semantic segmentation branch (SSB), the super-resolution branch (SRB), and the structural affinity block (SAB). Specifically, the SSB, SRB, and SAB are responsible for completing super-resolution semantic segmentation, image super-resolution reconstruction, and associated features, respectively. Our proposed method is simple and efficient, and it can replace the different branches with most of the state-of-the-art models. The International Society for Photogrammetry and Remote Sensing (ISPRS) segmentation benchmarks were used to evaluate our models. In particular, super-resolution semantic segmentation on the Potsdam dataset reduced Intersection over Union (IoU) by only 1.8% when the resolution of the input image was reduced by a factor of two. The experimental results showed that our framework can obtain more accurate semantic segmentation and super-resolution reconstruction results than the single model.",Image segmentation,Semantics,Superresolution,Task analysis,Remote sensing,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Image reconstruction,Image resolution,Multitask learning (MTL),remote sensing,semantic segmentation,super resolution,,,,,,,,,,,,,,,,,,,,,,,,
Row_210,"Zhang, Libao","Ma, Jie","Lv, Xiruan","Chen, Donghui",Hierarchical Weakly Supervised Learning for Residential Area Semantic Segmentation in Remote Sensing Images,,JAN 2020,35,"Residential-area segmentation is one of the most fundamental tasks in the field of remote sensing. Recently, fully supervised convolutional neural network (CNN)-based methods have shown superiority in the field of semantic segmentation. However, a serious problem for those CNN-based methods is that pixel-level annotations are expensive and laborious. In this study, a novel hierarchical weakly supervised learning (HWSL) method is proposed to realize pixel-level semantic segmentation in remote sensing images. First, a weakly supervised hierarchical saliency analysis is proposed to capture a sequence of class-specific hierarchical saliency maps by computing the gradient maps with respect to the middle layers of the CNN. Then, superpixels and low-rank matrix recovery are introduced to highlight the common salient areas and fuse class-specific saliency maps with adaptive weights. Finally, a subtraction operation between class-specific saliency maps is conducted to generate hierarchical residual saliency maps and fulfill residential-area segmentation. Comprehensive evaluations with two remote sensing data sets and comparison with seven methods validate the superiority of the proposed HWSL model.",Image segmentation,Semantics,Remote sensing,Task analysis,Image color analysis,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Fuses,Deep learning,remote sensing,saliency analysis,semantic segmentation,weakly supervised,,,,,,,,,,,,,,,,,,,,,,,
Row_211,"Guo, Shichen","Yang, Qi","Xiang, Shiming","Wang, Shuwen",Mask2Former with Improved Query for Semantic Segmentation in Remote-Sensing Images,,MAR 2024,3,"Semantic segmentation of remote sensing (RS) images is vital in various practical applications, including urban construction planning, natural disaster monitoring, and land resources investigation. However, RS images are captured by airplanes or satellites at high altitudes and long distances, resulting in ground objects of the same category being scattered in various corners of the image. Moreover, objects of different sizes appear simultaneously in RS images. For example, some objects occupy a large area in urban scenes, while others only have small regions. Technically, the above two universal situations pose significant challenges to the segmentation with a high quality for RS images. Based on these observations, this paper proposes a Mask2Former with an improved query (IQ2Former) for this task. The fundamental motivation behind the IQ2Former is to enhance the capability of the query of Mask2Former by exploiting the characteristics of RS images well. First, we propose the Query Scenario Module (QSM), which aims to learn and group the queries from feature maps, allowing the selection of distinct scenarios such as the urban and rural areas, building clusters, and parking lots. Second, we design the query position module (QPM), which is developed to assign the image position information to each query without increasing the number of parameters, thereby enhancing the model's sensitivity to small targets in complex scenarios. Finally, we propose the query attention module (QAM), which is constructed to leverage the characteristics of query attention to extract valuable features from the preceding queries. Being positioned between the duplicated transformer decoder layers, QAM ensures the comprehensive utilization of the supervisory information and the exploitation of those fine-grained details. Architecturally, the QSM, QPM, and QAM as well as an end-to-end model are assembled to achieve high-quality semantic segmentation. In comparison to the classical or state-of-the-art models (FCN, PSPNet, DeepLabV3+, OCRNet, UPerNet, MaskFormer, Mask2Former), IQ2Former has demonstrated exceptional performance across three publicly challenging remote-sensing image datasets, 83.59 mIoU on the Vaihingen dataset, 87.89 mIoU on Potsdam dataset, and 56.31 mIoU on LoveDA dataset. Additionally, overall accuracy, ablation experiment, and visualization segmentation results all indicate IQ2Former validity.",semantic segmentation,remote-sensing image,transformer,Mask2Former,query,"Wang, Xuezhi",,,,MATHEMATICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_212,"Jin, Jianhui","Zhou, Wujie","Yang, Rongwang","Ye, Lv",Edge Detection Guide Network for Semantic Segmentation of Remote-Sensing Images,,2023,37,"The acquisition of high-resolution satellite and airborne remote sensing images has been significantly simplified due to the rapid development of sensor technology. Several practical applications of high-resolution remote sensing images (HRRSIs) are based on semantic segmentation. However, single-modal HRRSIs are difficult to classify accurately in the complex situation of some scene objects; therefore, the semantic segmentation of multi-source information fusion is gaining popularity. The inherent difference between multimodal features and the semantic gap between multi-level features typically affect the performance of existing multi-mode fusion methods. We propose a multimodal fusion network based on edge detection to address these issues. This method aids multimodal information fusion by utilizing spatial information contained in the boundary. An edge detection guide module is included in the feature extraction stage to realize the boundary information through the fusion of details and semantics between high-level and low-level features. The boundary information is extended into the well-designed multimodal adaptive fusion block (MAFB) to obtain the multimodal fusion features. Furthermore, a residual adaptive fusion block (RAFB) and a spatial position module (SPM) in the feature decoding stage have been designed to fuse multi-level features from the standpoint of local and global dependence. We compared our method to several state-of-the-art (SOTA) methods using the International Society for Photogrammetry and Remote Sensing's (ISPRS) Vaihingen and Potsdam datasets. The final results demonstrate that our method achieves excellent performance.",Feature extraction,Semantics,Semantic segmentation,Remote sensing,Image edge detection,"Yu, Lu",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Convolution,Optical sensors,Edge detection,multi-level,multimodal,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_213,"Yang, Zimeng","Wu, Qiulan","Zhang, Feng","Zhang, Xueshen",A New Semantic Segmentation Method for Remote Sensing Images Integrating Coordinate Attention and SPD-Conv,,MAY 8 2023,6,"Semantic segmentation is an important task for the interpretation of remote sensing images. Remote sensing images are large in size, contain substantial spatial semantic information, and generally exhibit strong symmetry, resulting in images exhibiting large intraclass variance and small interclass variance, thus leading to class imbalance and poor small-object segmentation. In this paper, we propose a new remote sensing image semantic segmentation network, called CAS-Net, which includes coordinate attention (CA) and SPD-Conv. In the model, we replace stepwise convolution with SPD-Conv convolution in the feature extraction network and add a pooling layer into the network to avoid the loss of detailed information, effectively improving the segmentation of small objects. The CA is introduced into the atrous spatial pyramid pooling (ASPP) module, thus improving the recognizability of classified objects and target localization accuracy in remote sensing images. Finally, the Dice coefficient was introduced into the cross-entropy loss function to maximize the gradient optimization of the model and solve the classification imbalance problem in the image. The proposed model is compared with several state-of-the-art models on the ISPRS Vaihingen dataset. The experimental results demonstrate that the proposed model significantly optimizes the segmentation effect of small objects in remote sensing images, effectively solves the problem of class imbalance in the dataset, and improves segmentation accuracy.",remote sensing,semantic segmentation,coordinate attention mechanism,SPD-Conv,small objects,"Chen, Xuefei","Gao, Yue",,,SYMMETRY-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_214,"Zheng, Xianwei","Wu, Xiujie","Huan, Linxi","He, Wei",A Gather-to-Guide Network for Remote Sensing Semantic Segmentation of RGB and Auxiliary Image,,2022,23,"Convolutional neural network (CNN)-based feature fusion of RGB and auxiliary remote sensing data is known to enable improved semantic segmentation. However, such fusion is challengeable because of the substantial variance in data characteristics and quality (e.g., data uncertainties and misalignment) between two modality data. In this article, we propose a unified gather-to-guide network (G2GNet) for remote sensing semantic segmentation of RGB and auxiliary data. The key aspect of the proposed architecture is a novel gather-to-guide module (G2GM) that consists of a feature gatherer and a feature guider. The feature gatherer generates a set of cross-modal descriptors by absorbing the complementary merits of RGB and auxiliary modality data. The feature guider calibrates the RGB feature response by using the channel-wise guide weights extracted from the cross-modal descriptors. In this way, the G2GM can perform RGB feature calibration with different modality data in a gather-to-guide fashion, thus preserving the informative features while suppressing redundant and noisy information. Extensive experiments conducted on two benchmark datasets show that the proposed G2GNet is robust to data uncertainties while also improving the semantic segmentation performance of RGB and auxiliary remote sensing data.",Semantics,Image segmentation,Remote sensing,Feature extraction,Convolutional neural networks,"Zhang, Hongyan",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Calibration,Task analysis,Deep learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_215,"Lv, Jinna","Shen, Qi","Lv, Mingzheng","Li, Yiran",Deep learning-based semantic segmentation of remote sensing images: a review,,JUL 14 2023,14,"Semantic segmentation is a fundamental but challenging problem of pixel-level remote sensing (RS) data analysis. Semantic segmentation tasks based on aerial and satellite images play an important role in a wide range of applications. Recently, with the successful applications of deep learning (DL) in the computer vision (CV) field, more and more researchers have introduced and improved DL methods to the task of RS data semantic segmentation and achieved excellent results. Although there are a large number of DL methods, there remains a deficiency in the evaluation and advancement of semantic segmentation techniques for RS data. To solve the problem, this paper surveys more than 100 papers in this field in the past 5 years and elaborates in detail on the aspects of technical framework classification discussion, datasets, experimental evaluation, research challenges, and future research directions. Different from several previously published surveys, this paper first focuses on comprehensively summarizing the advantages and disadvantages of techniques and models based on the important and difficult points. This research will help beginners quickly establish research ideas and processes in this field, allowing them to focus on algorithm innovation without paying too much attention to datasets, evaluation indicators, and research frameworks.",remote sensing,deep learning,convolutional neural network,semantic segmentation,satellite image,"Shi, Lei","Zhang, Peiying",,,FRONTIERS IN ECOLOGY AND EVOLUTION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_216,"Ren, Dong","Li, Falin","Sun, Hang","Liu, Li",Local-enhanced multi-scale aggregation swin transformer for semantic segmentation of high-resolution remote sensing images,,JAN 2 2024,1,"Semantic segmentation of remote sensing images is crucial for various practical applications. In the field of deep learning, convolutional neural network (CNN) has been the primary approach for semantic segmentation over the past decade. Recently, Transformer-based models have achieved superior segmentation performance due to their exceptional global modelling capabilities. However, the Transformer-based models tend to focus more on extracting global contextual information, leading to suboptimal performance in segmenting local edges and difficulties in preserving fine-grained details during the patch token downsampling process. Inspired by the local receptive field of CNN, this article proposes a Local-Enhanced Multi-Scale Aggregation Swin Transformer (LMA-Swin) for semantic segmentation of high-resolution remote sensing images. Specifically, we adopt Swin Transformer as main encoder, introduce convolutional blocks as auxiliary encoder, and design a feature modulation module (FMM) to integrate the local contextual modelling ability of CNN into the Transformer backbone. Additionally, we propose a novel cross-aggregation decoder (CAD) to effectively aggregate shallow edge information and deep semantic information, thereby enhancing the discriminative ability for multi-scale objects. On the ISPRS Vaihingen and Potsdam datasets, experimental results illustrate noteworthy improvement in segmentation performance accomplished through the proposed approach. Code: https://github.com/patricklee16/LMA-Swin.",Local enhancement,multi-scale aggregation,feature modulation,semantic segmentation,remote sensing,"Ren, Shun","Yu, Mei",,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_217,"Zhang, Zheng","Liu, Fanchen","Liu, Changan","Tian, Qing",ACTNet: A Dual-Attention Adapter with a CNN-Transformer Network for the Semantic Segmentation of Remote Sensing Imagery,,APR 29 2023,8,"In recent years, the application of semantic segmentation methods based on the remote sensing of images has become increasingly prevalent across a diverse range of domains, including but not limited to forest detection, water body detection, urban rail transportation planning, and building extraction. With the incorporation of the Transformer model into computer vision, the efficacy and accuracy of these algorithms have been significantly enhanced. Nevertheless, the Transformer model's high computational complexity and dependence on a pre-training weight of large datasets leads to a slow convergence during the training for remote sensing segmentation tasks. Motivated by the success of the adapter module in the field of natural language processing, this paper presents a novel adapter module (ResAttn) for improving the model training speed for remote sensing segmentation. The ResAttn adopts a dual-attention structure in order to capture the interdependencies between sets of features, thereby improving its global modeling capabilities, and introduces a Swin Transformer-like down-sampling method to reduce information loss and retain the original architecture while reducing the resolution. In addition, the existing Transformer model is limited in its ability to capture local high-frequency information, which can lead to an inadequate extraction of edge and texture features. To address these issues, this paper proposes a Local Feature Extractor (LFE) module, which is based on a convolutional neural network (CNN), and incorporates multi-scale feature extraction and residual structure to effectively overcome this limitation. Further, a mask-based segmentation method is employed and a residual-enhanced deformable attention block (Deformer Block) is incorporated to improve the small target segmentation accuracy. Finally, a sufficient number of experiments were performed on the ISPRS Potsdam datasets. The experimental results demonstrate the superior performance of the model described in this paper.",remote sensing,semantic segmentation,transformer,adapter,,"Qu, Hongquan",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_218,"Wu, Jiayi","Qin, Chuan","Ren, Yanli","Feng, Guorui",EPFNet: Edge-Prototype Fusion Network Toward Few-Shot Semantic Segmentation for Aerial Remote-Sensing Images,,2023,3,"Few-shot semantic segmentation is a technique that is receiving increasing attention. The aim of this approach is to enable models to segment objects with a few support images (usually 1, 5, 10, etc.). At present, few-shot semantic segmentation has made great progress in the field of natural scene images (NSIs), but these methods cannot be applied directly to the field of remote-sensing images (RSIs). To overcome this challenge, we propose a novel semantic segmentation network structure that integrates prototype information with global edge information to achieve more accurate prototype-matching results. In addition, we design a comprehensive weighted loss function to monitor the training process to help overcome the challenges. Results of the performance comparison with state-of-the-art few-shot semantic segmentation methods demonstrate the superiority of the proposed method.",Edge-prototype fusion,few-shot semantic segmentation,remote-sensing image (RSI),,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_219,"Ulku, Irem","Akagunduz, Erdem",,,Semantic Segmentation of Crop Areas in Remote Sensing Imagery using Spectral Indices and Multiple Channels,"FIFTEENTH INTERNATIONAL CONFERENCE ON MACHINE VISION, ICMV 2022",2023,0,"This study focuses on pixel-wise semantic segmentation of crop production regions by using satellite remote sensing multispectral imagery. One of the principal aims of the study is to find out whether the raw multiple channel inputs are more effective in the training process of the semantic segmentation models or if the formularized counterparts as the spectral indices are more effective. For this purpose, the vegetation indices NDVI, ARVI and SAVI and the water indices NDWI, NDMI, and WRI are employed as inputs. Additionally, using 8, 10 and 16 channels, multiple channel inputs are utilized. Moreover, all spectral indices are taken as separate channels to form a multiple channel input. We conduct deep learning experiments using two semantic segmentation architectures, namely U-Net and DeepLabV3+. Our results show that, in general, feeding raw multiple channel inputs to semantic segmentation models performs much better than feeding the spectral indices. Hence, regarding crop production region segmentation, deep learning models are capable of encoding multispectral information. When the spectral indices are compared among themselves, ARVI, which reduces the atmospheric scattering effects, achieves better accuracy for both architectures. The results also reveal that spatial resolution of multispectral data has a significant effect on the semantic segmentation performance, and therefore the RGB band, which has the lowest ground sample distance (0.31 m) outperforms multispectral bands and shortwave infrared bands.",Semantic Segmentation,Remote Sensing,Spectral Indices,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_220,"Li, Jinsong","Zhang, Shujun","Sun, Yukang","Han, Qi",Frequency-Driven Edge Guidance Network for Semantic Segmentation of Remote Sensing Images,,2024,3,"Semantic segmentation plays a significant role in parsing remote sensing images. However, mainstream segmentation models lack a thorough understanding of the complex structures and scale differences, and struggle to effectively locate and emphasize diverse edges. Aiming at these limitations, we propose a frequency-driven edge guidance network, named FDEG-Net, for semantic segmentation of remote sensing images. First, we design a joint sparse context aggregation module that integrates both dense local context and sparse long-range context to improve the analysis of intricate and multiscale objects. Second, an edge guidance module is developed for strong interclass edge acquisition. It applies a 2-D discrete wavelet transform, coefficient superposition method, and adaptive edge feature enhancement algorithm to reduce low-frequency information and highlight salient boundaries in spatial features. This module has two significant advantages. 1) The edge positions are defined in pixel intensity with high interpretability. 2) The modular design without additional edge labels is plug-and-play. The effectiveness and robustness of this module are validated through edge visualization results. The proposed FDEG-Net is evaluated on the Potsdam, Vaihingen, and GID datasets, demonstrating its excellent performance in accurately capturing the rich semantics of geographic space features.",Feature extraction,Image edge detection,Semantics,Wavelet transforms,Decoding,"Sun, Yuanyuan","Wang, Yimin",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Semantic segmentation,Remote sensing,Context extraction,edge guidance,remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_221,"He, Xin","Zhou, Yong","Zhao, Jiaqi","Zhang, Man",Semantic Segmentation of Remote-Sensing Images Based on Multiscale Feature Fusion and Attention Refinement,,2022,11,"In recent years, the automatic extraction of remote-sensing image information has attracted full attention. However, the particularity of remote-sensing images and the scarcity of data sets with label information have brought new challenges to existing methods. Therefore, we develop a lightweight semantic segmentation network based onmultiscale feature fusion (MFF) and attention refinement (MFFANet). Our network relies on three crucial modules for improved performance. The multiscale attention refinement module strengthens the representation ability of feature maps extracted by the deep residual network. The MFF module aggregates the information carried by the high-level and low-level features while restoring the image resolution. Furthermore, the boundary enhancement module captures boundary details to solve the semantic ambiguity problem. We achieve 83.5% mean intersection over union (MIoU) on the Urban Semantic 3-D (US3D) data set and 69.3% MIoU on the Vaihingen data set with only 8.2M parameters.",Semantics,Image segmentation,Remote sensing,Feature extraction,Image resolution,"Yao, Rui","Liu, Bing","Li, Haichao",,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Decoding,Training,Attention,feature fusion,multiscale feature,remote-sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_222,"Brilhador, Anderson","Lazzaretti, Andre Eugenio","Lopes, Heitor Silverio",,A Prototypical Metric Learning Approach for Open-Set Semantic Segmentation on Remote Sensing Images,,2024,1,"Semantic segmentation has received wide attention as a feasible solution to effectively interpret the information in remote sensing images. Solutions are typically built with a static closed-set perception, where all labels are known a priori. However, in real-world applications, such as remote sensing images, one has to handle objects from unknown classes. Open-set semantic segmentation (OSSS) is an approach that incorporates open-set perception into semantic segmentation, allowing the recognition of unknown classes of objects. Different studies have explored the use of OSSS in remote sensing images. However, their performance is limited due to the poor and overlapped representation of the features extracted from images. This results in an embedding space with low discrimination among the classes. This article introduces a novel loss function called prototypical triplet loss, which uses prototype representation and metric learning to improve open-set recognition. In addition, two open-set classifiers, one based on principal components and the other on prototypical distance, were also proposed once they took advantage of the features obtained by the prototypical triplet loss. Experiments were done with two public remote sensing image datasets: Vaihingen and Potsdam. The results demonstrate that the proposed methods improve OSSS compared to other state-of-the-art approaches. These results reinforce the importance of this type of approach, enabling applications in real systems that require open-set recognition. All codes are freely available at https://github.com/Brilhador/tgrs2023 to foster further research in this area.",Semantic segmentation,Measurement,Remote sensing,Buildings,Vegetation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Automobiles,Training,Metric learning,open-set,prototype representation,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_223,"Liu, Runrui","Tao, Fei","Liu, Xintao","Na, Jiaming",RAANet: A Residual ASPP with Attention Framework for Semantic Segmentation of High-Resolution Remote Sensing Images,,JUL 2022,68,"Classification of land use and land cover from remote sensing images has been widely used in natural resources and urban information management. The variability and complex background of land use in high-resolution imagery poses greater challenges for remote sensing semantic segmentation. To obtain multi-scale semantic information and improve the classification accuracy of land-use types in remote sensing images, the deep learning models have been wildly focused on. Inspired by the idea of the atrous-spatial pyramid pooling (ASPP) framework, an improved deep learning model named RAANet (Residual ASPP with Attention Net) is constructed in this paper, which constructed a new residual ASPP by embedding the attention module and residual structure into the ASPP. There are 5 dilated attention convolution units and a residual unit in its encoder. The former is used to obtain important semantic information at more scales, and residual units are used to reduce the complexity of the network to prevent the disappearance of gradients. In practical applications, according to the characteristics of the data set, the attention unit can select different attention modules such as the convolutional block attention model (CBAM). The experimental results obtained from the land-cover domain adaptive semantic segmentation (LoveDA) and ISPRS Vaihingen datasets showed that this model can enhance the classification accuracy of semantic segmentation compared to the current deep learning models.",semantic segmentation,remote sensing,convolutional block attention module,dual attention module,residual structure,"Leng, Hongjun","Wu, Junjie","Zhou, Tong",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_224,"Li, Xin","Xu, Feng","Liu, Fan","Lyu, Xin",A Synergistical Attention Model for Semantic Segmentation of Remote Sensing Images,,2023,58,"In remotely sensed images, high intraclass variance and interclass similarity are ubiquitous due to complex scenes and objects with multivariate features, making semantic segmentation a challenging task. Deep convolutional neural networks can solve this problem by modeling the context of features and improving their discriminability. However, current learning paradigms model the feature affinity in spatial dimension and channel dimension separately and then fuse them in a sequential or parallel manner, leading to suboptimal performance. In this study, we first analyze this problem practically and summarize it as attention bias that reduces the capability of network in distinguishing weak and discretely distributed objects from wide-range objects with internal connectivity, when modeled only in spatial or channel domain. To jointly model both spatial and channel affinity, we design a synergistic attention module (SAM), which allows for channelwise affinity extraction while preserving spatial details. In addition, we propose a synergistic attention perception neural network (SAPNet) for the semantic segmentation of remote sensing images. The hierarchical-embedded synergistic attention perception module aggregates SAM-refined features and decoded features. As a result, SAPNet enriches inference clues with desired spatial and channel details. Experiments on three benchmark datasets show that SAPNet is competitive in accuracy and adaptability compared with state-of-the-art methods. The experiments also validate the hypothesis of attention bias and the efficiency of SAM.",Remote sensing,Convolution,Semantic segmentation,Feature extraction,Task analysis,"Tong, Yao","Xu, Zhennan","Zhou, Jun",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantics,Correlation,Attention bias,contextual affinity,remote sensing images,semantic segmentation,synergistic attention,,,,,,,,,,,,,,,,,,,,,,,
Row_225,"Liu, Qianqian","Wang, Xili",,,Bidirectional Feature Fusion and Enhanced Alignment Based Multimodal Semantic Segmentation for Remote Sensing Images,,JUL 2024,1,"Image-text multimodal deep semantic segmentation leverages the fusion and alignment of image and text information and provides more prior knowledge for segmentation tasks. It is worth exploring image-text multimodal semantic segmentation for remote sensing images. In this paper, we propose a bidirectional feature fusion and enhanced alignment-based multimodal semantic segmentation model (BEMSeg) for remote sensing images. Specifically, BEMSeg first extracts image and text features by image and text encoders, respectively, and then the features are provided for fusion and alignment to obtain complementary multimodal feature representation. Secondly, a bidirectional feature fusion module is proposed, which employs self-attention and cross-attention to adaptively fuse image and text features of different modalities, thus reducing the differences between multimodal features. For multimodal feature alignment, the similarity between the image pixel features and text features is computed to obtain a pixel-text score map. Thirdly, we propose a category-based pixel-level contrastive learning on the score map to reduce the differences among the same category's pixels and increase the differences among the different categories' pixels, thereby enhancing the alignment effect. Additionally, a positive and negative sample selection strategy based on different images is explored during contrastive learning. Averaging pixel values across different training images for each category to set positive and negative samples compares global pixel information while also limiting sample quantity and reducing computational costs. Finally, the fused image features and aligned pixel-text score map are concatenated and fed into the decoder to predict the segmentation results. Experimental results on the ISPRS Potsdam, Vaihingen, and LoveDA datasets demonstrate that BEMSeg is superior to comparison methods on the Potsdam and Vaihingen datasets, with improvements in mIoU ranging from 0.57% to 5.59% and 0.48% to 6.15%, and compared with Transformer-based methods, BEMSeg also performs competitively on LoveDA dataset with improvements in mIoU ranging from 0.37% to 7.14%.",remote sensing image,multimodal feature fusion,multimodal feature alignment,semantic segmentation,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_226,"Shen, Weihao","Ma, Ailong","Wang, Junjue","Zheng, Zhuo",Adaptive Self-Supporting Prototype Learning for Remote Sensing Few-Shot Semantic Segmentation,,2024,1,"The semantic segmentation of remote sensing images with few shots has important theoretical and application value. Most of the existing few-shot semantic segmentation frameworks are based on prototype learning methods, in which a single support prototype is designed to guide the query set for prediction. However, the visual differences between the support set and the query set make it difficult for a single support prototype, generated from the support set, to comprehensively encapsulate the semantic information of all the query images. This article introduces an adaptive self-supporting prototype learning network designed for few-shot segmentation (FSS), in order to tackle the challenges mentioned earlier. We propose adaptive hyperprototype representation (HPR), which consists of hyperprototype clustering (HPC) and guided prototype matching (GPM), to generate and assign multiple representative prototypes to compensate for the limitations of a single prototype in representing the semantic information of the query images. Specifically, HPC is a parameter-free and adaptive approach, which can extract more representative prototypes by aggregating similar feature vectors utilizing superpixel feature clustering. Meanwhile, GPM can select matched prototypes to provide more accurate guidance, allowing for uniformly aligned representation of multiple prototypes and complex image semantic information. We also introduce self-supporting matching (SSM) prototype learning, which can accurately guide the query set segmentation by acquiring query set prototypes. SSM generates initial pseudo labels for the query set based on the support set prototypes, and further guides the query set using the pseudo labels, along with the query prototypes generated by its own features, thus effectively avoiding visual differences between the support set and query set. The proposed adaptive self-supporting prototype learning network substantially improves the prototype quality and achieves a superior performance on object-level remote sensing datasets.",Prototypes,Remote sensing,Semantic segmentation,Feature extraction,Semantics,"Zhong, Yanfei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Task analysis,Measurement,Few-shot learning,few-shot segmentation (FSS),prototype learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_227,"Rong, Xuee","Sun, Xian","Diao, Wenhui","Wang, Peijin",Historical Information-Guided Class-Incremental Semantic Segmentation in Remote Sensing Images,,2022,25,"Despite the extraordinary success of the deep architectures on semantic segmentation for remote sensing (RS) images, they have difficulties in learning new classes from a sequential data stream because of catastrophic forgetting. Continual learning for semantic segmentation (CSS) is an emerging trend for its capability to cope with the above problems effectively. However, old classes from previous steps are collapsed into the background, which further aggravates the challenge of CSS in the RS scene. In this article, we revisit the knowledge distillation (KD) strategy and the characteristics of class-incremental semantic segmentation (CISS) and then present a generalized and effective framework to learn new classes while preserving knowledge of the learned classes. In particular, we propose two novel historical information-guided modules: the feature global perception module and the label reconstruction (LR) module. The former enables the current model to pay more attention to the region related to the old categories identified by the historical information when learning new classes. Meanwhile, the latter retrieves pixels belonging to the learned classes from the background to handle the background shift problem and maintain the high performance of old classes. We have conducted comprehensive experiments on two RS semantic segmentation datasets of Instance Segmentation in Aerial Images Dataset (iSAID) and Gao Fen (GF) challenge semantic segmentation dataset (GCSS). The experimental results outperform the current state-of-the-art methods in most incremental settings, which demonstrates the effectiveness of the proposed framework.",Task analysis,Semantics,Image segmentation,Image reconstruction,Transformers,"Yuan, Zhiqiang","Wang, Hongqi",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Data models,Catastrophic forgetting (CF),continual learning,remote sensing (RS),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_228,"Yao, Min","Zhang, Yaozu","Liu, Guofeng","Pang, Dongdong",SSNet: A Novel Transformer and CNN Hybrid Network for Remote Sensing Semantic Segmentation,,2024,11,"There are still various challenges in remote sensing semantic segmentation due to objects diversity and complexity. Transformer-based models have significant advantages in capturing global feature dependencies for segmentation. However, it unfortunately ignores local feature details. On the other hand, convolutional neural network (CNN), with a different interaction mechanism from transformer-based models, captures more small-scale local features instead of global features. In this article, a new semantic segmentation net framework named SSNet is proposed, which incorporates an encoder-decoder structure, optimizing the advantages of both local and global features. In addition, we build feature fuse module and feature inject module to largely fuse these two-style features. The former module captures the dependencies between different positions and channels to extract multiscale features, which promotes the segmentation precision on similar objects. The latter module condenses the global information in transformer and injects it into CNN to obtain a broad global field of view, in which the depthwise strip convolution improves the segmentation accuracy on tiny objects. A CNN-based decoder progressively recovers the feature map size, and a block called atrous spatial pyramid pooling is adopted in decoder to obtain a multiscale context. The skip connection is established between the decoder and the encoder, which retains important feature information of the shallow layer network and is conducive to achieving flow of multiscale features. To evaluate our model, we compare it with current state-of-the-art models on WHDLD and Potsdam datasets. The experimental results indicate that our proposed model achieves more precise semantic segmentation.",Fusion features,multiscale features,remote sensing (RS),semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_229,"Xu, Qingsong","Yuan, Xin","Ouyang, Chaojun",,Class-Aware Domain Adaptation for Semantic Segmentation of Remote Sensing Images,,2022,40,"Unsupervised domain adaptation (UDA) for the semantic segmentation of remote sensing images is challenging since the same class of objects may have different spectra while the different class of objects may have the same spectrum. To address this issue, we propose a class-aware generative adversarial network (CaGAN) for UDA semantic segmentation of multisource remote sensing images, which explicitly models the discrepancies of intraclass and the interclass between the source domain images with labels and the target domain images without labels. Specifically, first, to enhance the global domain alignment (GDA), we propose a transferable attention alignment (TAA) procedure to add more fine-grained features into the adversarial learning framework. Then, we propose a novel class-aware domain alignment (CDA) approach in semantic segmentation. CDA mainly includes two parts: the first one is adaptive category selection, which is to alleviate the class imbalance and select the reliable per-category centers in the source and target domains; the second one is adaptive category alignment, which is to model the intraclass compactness and interclass separability from source-only, target-only, and joint source and target images. Finally, the CDA plays as a penalty of GDA to train GaGAN in an alternating and iterative manner. Experiments on domain adaptation of space to space, spectrum to spectrum, both space-to-space and spectrum-to-spectrum data sets demonstrate that CaGAN outperforms the current state-of-the-art methods, which may serve as a starting point and baseline for the comprehensive applications of semantic segmentation in cross-space and cross-spectrum remote sensing images.",Semantics,Image segmentation,Remote sensing,Adaptation models,Generative adversarial networks,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Gallium nitride,Training,Class-aware domain alignment (CDA),class-aware generative adversarial network (CaGAN),cross-scene and cross-spectrum remote sensing images,global domain alignment (GDA),unsupervised domain adaptation (UDA) semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_230,"Rong, Xuee","Wang, Peijin","Diao, Wenhui","Yang, Yiran",MiCro: Modeling Cross-Image Semantic Relationship Dependencies for Class-Incremental Semantic Segmentation in Remote Sensing Images,,2023,7,"Continual learning is an effective way to overcome catastrophic forgetting (CF) in incremental learning for semantic segmentation. The existing continual semantic segmentation (CSS) methods of remote sensing (RS) ignore the semantic relationships among pixels across different images, which will lead to disappointing segmentation results, such as edge pixel misclassification and small object omission. In this article, we propose a framework for modeling cross-image semantic relationship dependencies (MiCro), which aims to learn an interclass separable and intraclass cohesive feature space from the pixel relationships across various images to ensure that learned categories can prevent CF in the incremental process. Specifically, we exploit the relationships among pixels of images in minibatch to construct three losses: 1) cross-image feature relationship distillation (CFRD) loss, which builds a well-structured feature space; 2) cross-image intraclass feature cohesion (CIFC) loss, which is devised to make intraclass features more cohesive; and 3) cross-image class-area weighted cross-entropy (CCWCE) loss, which is mainly employed to inversely weight the proportion of category area in minibatch. The effectiveness of the proposed approach is demonstrated by extensive experiments on three RS semantic segmentation datasets from ISPRS Vaihingen, ISPRS Potsdam, and iSAID. MiCro is superior to the current most advanced methods in most incremental settings, especially improving mIoU by 11.59% on ISPRS Vaihingen, 13.17% on ISPRS Potsdam, and 15.01% on iSAID in the most difficult incremental settings, which promotes the CSS to a state-of-the-art (SOTA) level. The code will be available at https://github.com/RongXueE/MiCro.",Catastrophic forgetting (CF),class-incremental learning (CIL),remote sensing (RS),semantic segmentation,,"Yin, Wenxin","Zeng, Xuan","Wang, Hongqi","Sun, Xian",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_231,"Shang, Ronghua","Zhang, Jiyu","Jiao, Licheng","Li, Yangyang",Multi-scale Adaptive Feature Fusion Network for Semantic Segmentation in Remote Sensing Images,,MAR 2020,73,"Semantic segmentation of high-resolution remote sensing images is highly challenging due to the presence of a complicated background, irregular target shapes, and similarities in the appearance of multiple target categories. Most of the existing segmentation methods that rely only on simple fusion of the extracted multi-scale features often fail to provide satisfactory results when there is a large difference in the target sizes. Handling this problem through multi-scale context extraction and efficient fusion of multi-scale features, in this paper we present an end-to-end multi-scale adaptive feature fusion network (MANet) for semantic segmentation in remote sensing images. It is a coding and decoding structure that includes a multi-scale context extraction module (MCM) and an adaptive fusion module (AFM). The MCM employs two layers of atrous convolutions with different dilatation rates and global average pooling to extract context information at multiple scales in parallel. MANet embeds the channel attention mechanism to fuse semantic features. The high- and low-level semantic information are concatenated to generate global features via global average pooling. These global features are used as channel weights to acquire adaptive weight information of each channel by the fully connected layer. To accomplish an efficient fusion, these tuned weights are applied to the fused features. Performance of the proposed method has been evaluated by comparing it with six other state-of-the-art networks: fully convolutional networks (FCN), U-net, UZ1, Light-weight RefineNet, DeepLabv3+, and APPD. Experiments performed using the publicly available Potsdam and Vaihingen datasets show that the proposed MANet significantly outperforms the other existing networks, with overall accuracy reaching 89.4% and 88.2%, respectively and with average of F1 reaching 90.4% and 86.7% respectively.",multi-scale context,adaptive fusion,remote sensing image,semantic segmentation,CNN,"Marturi, Naresh","Stolkin, Rustam",,,REMOTE SENSING,,deep learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_232,"Li, Bo","Lv, Pengyuan","Zhong, Yanfei","Zhang, Liangpei",HIGH RESOLUTION REMOTE SENSING IMAGE SEMANTIC SEGMENTATION BASED ON ULTRA-LIGHTWEIGHT FULLY CONVOLUTION NEURAL NETWORK,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,2,"In recent years, fully convolutional neural networks (FCNs) have been widely used in the field of remote sensing image semantic segmentation. However, these networks have huge amount of parameters and cost much computational efficiency. In this paper, an ultra-lightweight network (ULN) is proposed to overcome this problem. The proposed ULN model uses the encoder-decoder architecture to acquire the pixelwise result. In ULN, the efficient spatial pyramid network (ESPNet) is used to extract deep semantic features with fewer parameters. Considering the dilated convolutions will lose some semantic information in the encoding process, the feature enhancement block (FEB) is proposed. The recurrent criss-cross attention module is added at the end of skip connection to acquire the global contextual information. The proposed ULN is tested on the ISPRS Vaihingen dataset, the results show that our network achieves competitive results with fewer parameters(1.5M).",Remote sensing image,high spatial resolution semantic segmentation,ultra-lightweight network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_233,"An, Ke","Wang, Yupei","Chen, Liang",,Encouraging the Mutual Interact Between Dataset-Level and Image-Level Context for Semantic Segmentation of Remote Sensing Image,,2024,0,"Recently, semantic segmentation of remote sensing images has witnessed rapid advancement with the adoption of deep neural networks. Contextual cues, referring to the long-range correlation between pixels, are crucial for achieving accurate segmentation results, particularly for objects with less discriminative characteristics in these images. Currently, most studies are centered on incorporating contextual cues by aggregating context information at the dataset level or image level. However, current research often treats contextual cue modeling at the dataset-level and image level as independent procedures, neglecting the intrinsic correlation between these two feature levels. Consequently, the obtained contextual cues are suboptimal. This issue is particularly critical in the semantic segmentation of remote sensing images. To address this, we propose to encourage mutual interaction between dataset-level and image-level contextual cues. Firstly, we propose an interactive dataset-image context aggregation scheme to obtain complementary and consistent multilevel contextual cues. Additionally, we introduce a parallel feature interaction network (PFI-Net) that progressively extracts and fuses features across multiple layers, enabling effective integration of multilevel contexts. Furthermore, we introduce an enhanced shifted window-based cross-attention mechanism to augment model efficiency. Extensive experimental results on the widely used Vaihingen dataset, GaoFen-2 dataset, and instance segmentation in aerial images dataset (iSAID) effectively demonstrate the superiority of our proposed method over the other state-of-the-art methods.",Contextual cue,remote sensing image,semantic segmentation,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_234,"Xia, Liegang","Zhang, Junxia","Zhang, Xiongbo","Yang, Haiping",Precise Extraction of Buildings from High-Resolution Remote-Sensing Images Based on Semantic Edges and Segmentation,,AUG 2021,14,"Building extraction is a basic task in the field of remote sensing, and it has also been a popular research topic in the past decade. However, the shape of the semantic polygon generated by semantic segmentation is irregular and does not match the actual building boundary. The boundary of buildings generated by semantic edge detection has difficulty ensuring continuity and integrity. Due to the aforementioned problems, we cannot directly apply the results in many drawing tasks and engineering applications. In this paper, we propose a novel convolutional neural network (CNN) model based on multitask learning, Dense D-LinkNet (DDLNet), which adopts full-scale skip connections and edge guidance module to ensure the effective combination of low-level information and high-level information. DDLNet has good adaptability to both semantic segmentation tasks and edge detection tasks. Moreover, we propose a universal postprocessing method that integrates semantic edges and semantic polygons. It can solve the aforementioned problems and more accurately locate buildings, especially building boundaries. The experimental results show that DDLNet achieves great improvements compared with other edge detection and semantic segmentation networks. Our postprocessing method is effective and universal.",building extraction,high-resolution remote-sensing image,semantic edge detection,semantic segmentation,,"Xu, Meixia",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_235,"Fang, Leyuan","Zhou, Peng","Liu, Xinxin","Ghamisi, Pedram",Context Enhancing Representation for Semantic Segmentation in Remote Sensing Images,,MAR 2024,11,"As the foundation of image interpretation, semantic segmentation is an active topic in the field of remote sensing. Facing the complex combination of multiscale objects existing in remote sensing images (RSIs), the exploration and modeling of contextual information have become the key to accurately identifying the objects at different scales. Although several methods have been proposed in the past decade, insufficient context modeling of global or local information, which easily results in the fragmentation of large-scale objects, the ignorance of small-scale objects, and blurred boundaries. To address the above issues, we propose a contextual representation enhancement network (CRENet) to strengthen the global context (GC) and local context (LC) modeling in high-level features. The core components of the CRENet are the local feature alignment enhancement module (LFAEM) and the superpixel affinity loss (SAL). The LFAEM aligns and enhances the LC in low-level features by constructing contextual contrast through multilayer cascaded deformable convolution and is then supplemented with high-level features to refine the segmentation map. The SAL assists the network to accurately capture the GC by supervising semantic information and relationship learned from superpixels. The proposed method is plug-and-play and can be embedded in any FCN-based network. Experiments on two popular RSI datasets demonstrate the effectiveness of our proposed network with competitive performance in qualitative and quantitative aspects.",Semantics,Context modeling,Image segmentation,Convolution,Decoding,"Chen, Siwei",,,,IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS,,Remote sensing,Predictive models,Context modeling,deep learning,feature alignment and enhancement,remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_236,"Wang, Zhen","Guo, Jianxin","Huang, Wenzhun","Zhang, Shanwen",High-resolution remote sensing image semantic segmentation based on a deep feature aggregation network,,SEP 2021,13,"Semantic segmentation of high-resolution remote sensing images has a wide range of applications, such as territorial planning, geographic monitoring and smart cities. The proper operation of semantic segmentation for remote sensing images remains challenging due to the complex and diverse transitions between different ground areas. Although several convolution neural networks (CNNs) have been developed for remote sensing semantic segmentation, the performance of CNNs is far from the expected target. This study presents a deep feature aggregation network (DFANet) for remote sensing image semantic segmentation. It is composed of a basic feature representation layer, an intermediate feature aggregation layer, a deep feature aggregation layer and a feature aggregation module (FAM). Specially, the basic feature representation layer is used to obtain feature maps with different resolutions: the intermediate feature aggregation layer and deep feature aggregation layer can fuse various resolution features and multi-scale features; the FAM is used to splice the features and form more abundant spatial feature maps; and the conditional random field module is used to optimize semantic segmentation results. We have performed extensive experiments on the ISPRS two-dimensional Vaihingen and Potsdam remote sensing image datasets and compared the proposed method with several variations of semantic segmentation networks. The experimental results show that DFANet outperforms the other state-of-the-art approaches.",high resolution remote sensing image,semantic segmentation,convolution neural networks (CNNs),deep feature aggregation network (DFANet),conditional random field (CRF),,,,,MEASUREMENT SCIENCE AND TECHNOLOGY,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_237,"Cui, Hao","Zhang, Guo","Qi, Ji","Li, Haifeng","MDANet: Unsupervised, Mixed-Domain Adaptation for Semantic Segmentation of Remote Sensing Images",,2022,3,"The imaging process of optical remote sensing images (RSIs) is easily affected by external conditions. Therefore, RSIs under different imaging conditions often show color differences, resulting in feature distribution differences between the source and target domains, hindering the migration of semantic segmentation models between domains. Currently, most domain adaptation (DA) methods are for single-source and single-target domains. Here, we proposed a novel and concise method, coined mixed-DA network (MDANet), for the adaptation of patch images of multisource and multitarget domains and for reducing the distribution differences of different patch images by projecting them onto the virtual center of a mixed domain. MDANet is a lightweight and self-supervised network that can be grafted with any semantic segmentation model. Our method significantly improved the segmentation accuracy of semantic segmentation models and showed higher stability and competitiveness than the existing methods.",Image reconstruction,Remote sensing,Semantics,Adaptation models,Decoding,"Tao, Chao","Li, Xue","Hou, Shasha","Li, Deren",IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Training,Image edge detection,Domain adaptation (DA),mixed domain,remote sensing image (RSI),self-supervision,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_238,"Pan, Shaoming","Tao, Yulong","Chen, Xiaoshu","Chong, Yanwen",Progressive Guidance Edge Perception Network for Semantic Segmentation of Remote-Sensing Images,,2022,8,"Remarkable improvements have been seen in the semantic segmentation of remote-sensing images. As an effective structure to aggregate shallow information and deep information, encoder-decoder structure has been widely used in many state-of-the-art models, but it possesses two drawbacks that have not been fully addressed. On the one hand, encoder-decoder structure fuses the features obtained from shallow and deep layers directly; despite harvesting some detailed information, it also brings in noisy features owing to the poor discriminant ability of the shallow layers. On the other hand, existing encoder-decoder structure merely fuses the high-level information generated by the last layer of encoder once, which neglects its guidance ability to the feature aggregation process in the decoder. In this letter, we first propose an edge perception module (EPM) to eliminate the noisy features in the shallow information, as well as enhance features' structural information. And then, we generate the most suitable guidance information adaptively for different stages in the decoder through high-level information module (HIM). Finally, we apply the guidance information to achieve feature aggregation in the feature aggregation module (FAM). Combined with EPM, HIM, and FAM, our proposed model achieves 89.5% overall accuracy (OA) on the challenging ISPRS Vaihingen test set, which is the new state-of-the-art in the semantic segmentation of remote-sensing images.",Semantics,Convolution,Noise measurement,Remote sensing,Image segmentation,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Decoding,Encoder-decoder structure,high-level information,noisy features,remote-sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_239,"Igonin, Dmitry M.","Tiumentsev, Yury V.",,,Semantic Segmentation of Images Obtained by Remote Sensing of the Earth,"ADVANCES IN NEURAL COMPUTATION, MACHINE LEARNING, AND COGNITIVE RESEARCH III",2020,1,"In the last decade, computer vision algorithms, including those related to the problem of understanding images, have developed a lot. One of the tasks within the framework of this problem is semantic segmentation of images, which provides the classification of objects available in the image at the pixel level. This kind of segmentation is essential as a source of information for robotic UAV behavior control systems. One of the types of pictures that are used in this case is the images obtained by remote sensing of the earth's surface. A significant number of various neuroarchitecture based on convolutional neural networks were proposed for solving problems of semantic segmentation of images. However, for some reasons, not all of them are suitable for working with pictures of the earth's surface obtained using remote sensing. Neuroarchitectures that are potentially suitable for solving the problem of semantic segmentation of images of the earth's surface are identified, a comparative analysis of their effectiveness as applied to this task is carried out.",Earth remote sensing,Aerial and satellite imaging,2D image,Semantic segmentation,Convolutional neural networks,,,,,,,Comparative analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_240,"Zhang, Xiaoyan","Li, Linhui","Di, Donglin","Wang, Jian",SERNet: Squeeze and Excitation Residual Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,OCT 2022,22,"The semantic segmentation of high-resolution remote sensing images (HRRSIs) is a basic task for remote sensing image processing and has a wide range of applications. However, the abundant texture information and wide imaging range of HRRSIs lead to the complex distribution of ground objects and unclear boundaries, which bring huge challenges to the segmentation of HRRSIs. To solve this problem, in this paper we propose an improved squeeze and excitation residual network (SERNet), which integrates several squeeze and excitation residual modules (SERMs) and a refine attention module (RAM). The SERM can recalibrate feature responses adaptively by modeling the long-range dependencies in the channel and spatial dimensions, which enables effective information to be transmitted between the shallow and deep layers. The RAM pays attention to global features that are beneficial to segmentation results. Furthermore, the ISPRS datasets were processed to focus on the segmentation of vegetation categories and introduce Digital Surface Model (DSM) images to learn and integrate features to improve the segmentation accuracy of surface vegetation, which has certain prospects in the field of forestry applications. We conduct a set of comparative experiments on ISPRS Vaihingen and Potsdam datasets. The results verify the superior performance of the proposed SERNet.",remote sensing,forestry technology,smart forestry,residual module,semantic segmentation,"Chen, Guangsheng","Jing, Weipeng","Emam, Mahmoud",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_241,"Chong, Qianpeng","Xu, Jindong","Ding, Yang","Dai, Zhe",A multiscale bidirectional fuzzy-driven learning network for remote sensing image segmentation,,NOV 2 2023,4,"Semantic segmentation is a fundamental but meaningful task in the remote sensing image understanding community. Great progress has been made in optical sensor photography technology, which poses an opportunity and a challenge for remote sensing image segmentation task. But, in fact, a longstanding and intractable problem is that many hard pixels in special position, i.e. the prevalent intra-class noise and a poor boundary delineation, is difficult to classify due to their inherent uncertainty. In this paper, we comprehensively consider the characteristics of deep learning and introduce traditional pattern recognition methods to drive structure learning, which can leverage the corresponding fuzzy logic model to alleviate the aforementioned problem in remote sensing images. Specifically, this paper designs a multiscale bidirectional fuzzy-driven learning network (MBFNet), which takes advantage of both deep learning and fuzzy logic to effectively alleviate the inherent uncertainty of these hard pixels. The structure of convolutional neural networks driven by fuzzy systems also provides a new modelling paradigm for solving the uncertain problem in remote sensing images. Meanwhile, multiscale techniques and bidirectional fusion are introduced to enhance feature aggregation and avoid the potential adverse effects of fuzzy systems, respectively. Experimental results on two datasets demonstrate qualitatively and quantitatively that the proposed MBFNet is competitive.",Remote sensing,semantic segmentation,fuzzy learning,bidirectional fusion,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_242,"Bai, Haiwei","Cheng, Jian","Huang, Xia","Liu, Siyu",HCANet: A Hierarchical Context Aggregation Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,2022,15,"Many practical applications of high-resolution remote sensing images (HRRSIs) are based on semantic segmentation. However, due to the complex ground object information contained in remote sensing images, it is difficult to make precise semantic segmentation of HRRSIs. In this letter, we proposed a hierarchical context aggregation network (HCANet) for the semantic segmentation of HRRSIs. The HCANet has an encoder-decoder structure which is similar to UNet. In the HCANet, we designed two Compact Atrous Spatial Pyramid Pooling (CASPP and CASPP+) modules. The CASPP modules replace the copy and crop operation in UNet to extract the multiscale context information of the multisemantic features of ResNet. The CASPP+ module is embedded in the middle layer of HCANet's decoder to provide a strong aggregation path of contextual information. In the decoder of HCANet, the multiscale context information obtained by CASPP modules is hierarchically merged layer by layer for the semantic segmentation of HRRSIs. We compared our method with several of the most advanced methods on the ISPRS Vaihingen and Potsdam data sets. The final results demonstrate that our method can achieve outstanding performance.",Semantics,Feature extraction,Image segmentation,Remote sensing,Convolution,"Deng, Changjian",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Data mining,Decoding,Hierarchical context aggregation,high-resolution remote sensing images (HRRSIs),multiscale context information,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_243,"Wang, Feiting","Zhang, Yuan","Hu, Qiongqiong","Zhu, Yu",Remote sensing image semantic segmentation network based on multi-scale feature enhancement fusion,,JAN 1 2024,0,"Semantic segmentation is a crucial method for recognizing and classifying objects in high-resolution remote sensing images (HRRSIs). However, due to the problems of varying target scale and difficulty in determining the edges of small-scale targets in remote sensing images, traditional semantic segmentation models perform poorly. To address this issue, we propose a multi-scale feature enhancement network (MFENet) to improve the segmentation accuracy of small-scale objects in HRRSIs. MFENet considers the differences between objects of different scales and selects more suitable receptive fields to enhance the extraction of multi-scale semantic features. We propose a composite atrous multi-scale feature fusion (CAMFF) module to enhance the extraction of spatial detail and semantic information of features at different scales. In addition, we propose an improved composite atrous spatial pyramid pooling (C-ASPP) module to enhance the network feature extraction capability across multiple scales. We also propose a network structure that combines the C-ASPP module with the efficient channel attention (ECA) module in parallel, which performs better to extract contextual information. Our experimental evaluations on the Potsdam and Vaihingen datasets demonstrate the effectiveness of our Network, It F1 score reaching 93.33% and 94.66% respectively.",Semantic segmentation,remote sensing images,multi-scale objects,spatial detail information,,,,,,GEOCARTO INTERNATIONAL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_244,"Li, Jinglun","Xiu, Jiapeng","Yang, Zhengqiu","Liu, Chen",Dual Path Attention Net for Remote Sensing Semantic Image Segmentation,,OCT 2020,20,"Semantic segmentation plays an important role in being able to understand the content of remote sensing images. In recent years, deep learning methods based on Fully Convolutional Networks (FCNs) have proved to be effective for the sematic segmentation of remote sensing images. However, the rich information and complex content makes the training of networks for segmentation challenging, and the datasets are necessarily constrained. In this paper, we propose a Convolutional Neural Network (CNN) model called Dual Path Attention Network (DPA-Net) that has a simple modular structure and can be added to any segmentation model to enhance its ability to learn features. Two types of attention module are appended to the segmentation model, one focusing on spatial information the other focusing upon the channel. Then, the outputs of these two attention modules are fused to further improve the network's ability to extract features, thus contributing to more precise segmentation results. Finally, data pre-processing and augmentation strategies are used to compensate for the small number of datasets and uneven distribution. The proposed network was tested on the Gaofen Image Dataset (GID). The results show that the network outperformed U-Net, PSP-Net, and DeepLab V3+ in terms of the mean IoU by 0.84%, 2.54%, and 1.32%, respectively.",remote sensing image,semantic segmentation,fully convolutional network,convolutional neural network,self-attention mechanism,,,,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_245,"Chang, Wanjun","Zhang, Dongfang",,,A Novel Semantic Segmentation Approach Using Improved SegNet and DSC in Remote Sensing Images,,2023,0,"An improved SegNet semantic segmentation model is proposed to address the issue of traditional classification algorithms and shallow learning algorithms not being suitable for extracting information from high-resolution remote sensing images. During the research process, space remote sensing images obtained from the GF-1 satellite were used as the data source. In order to improve the operational efficiency of the encoding network, the pooling layer in the encoding network is removed and the ordinary convolutional layer is replaced with a depth-wise separable convolution. By decoding the last layer of the network to obtain the reshaped output results, and then calculating the probability of each classification using a Softmax classifier, the classification of pixels can be achieved. The output result of the classifier is the final result of the remote sensing image semantic segmentation model. The results showed that the proposed algorithm had the highest Kappa coefficient of 0.9531, indicating good classification performance.",Deep Learning,Depth-Wise Separable Convolution,Remote Sensing Image,SegNet,Semantic Segmentation,,,,,INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_246,"Gao, Liang","Liu, Hui","Yang, Minhang","Chen, Long",STransFuse: Fusing Swin Transformer and Convolutional Neural Network for Remote Sensing Image Semantic Segmentation,,2021,133,"The applied research in remote sensing images has been pushed by convolutional neural network (CNN). Because of the fixed size of the perceptual field, CNN is unable to model global semantic relevance. Modeling global semantic information is possible with the self-attentive Transformer-based model. However, the method of patch computation used by Transformer for self-attentive computation ignores the spatial information inside each patch. To address these issues, we offer the STransFuse model as a new semantic segmentation method for remote sensing images. It is a model that combines the benefits of Transformer with CNN to improve the segmentation quality of various remote sensing images. We employ a staged model to extract coarse-grained and fine-grained feature representations at various semantic scales, unlike earlier techniques based on Transformer model fusion. In order to take full advantage of the features acquired at different stages, we designed an adaptive fusion module. This module adaptively fuses the semantic information between features at different scales employing a self-attentive mechanism. The overall accuracy (OA) of our proposed model on the Vaihingen dataset is 1.36% higher than the baseline, and 1.27% improvement in OA over baseline on the Potsdam dataset. When compared to other advanced models, the STransFuse model performs admirably.",Remote sensing,Transformers,Semantics,Image segmentation,Computational modeling,"Wan, Yaling","Xiao, Zhengqing","Qian, Yurong",,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Feature extraction,Context modeling,Remote sensing,self-attention,semantic segmentation,Transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_247,"Liu, Hongrong","Liu, Minghua","Song, Shuhua","Guo, Guolong",ECGNet: edge and class guided semantic segmentation network for remote sensing urban scene images,,JUL 1 2024,0,"Semantic segmentation of remote sensing images in urban scenes suffers from blurred multi-scale target boundaries, insufficient use of global context, and classification errors caused by high inter-class variance and low intra-class variance. Therefore, we propose a semantic segmentation network with edge and class guidance (ECGNet). First, ECGNet introduces multi-scale edge prior knowledge to address the problem of blurred target boundaries. Second, ECGNet applies synergistic class augmented attention to introduce class prior knowledge while retaining rich spatial dimensional localization information to alleviate the problem of classification errors caused by low intra-class variance and high inter-class variance. Finally, the multi-scale large receptive field attention in ECGNet simulates a large convolutional kernel to capture multi-scale global context information. Experiments conducted on the ISPRS Vaihingen and ISPRS Potsdam datasets show that the proposed method is competitive.",remote sensing images of urban scenes,computer vision,semantic segmentation,priori knowledge,large convolutional kernel,"Yuan, Zhengyi","Chen, Kai","Yang, Shuai","Yu, Jiangfeng",JOURNAL OF APPLIED REMOTE SENSING,"Zhang, Hongwei",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_248,"Jiang, Jionghui","Feng, Xi'an","Huang, Hui",,Semantic segmentation of remote sensing images based on dual-channel attention mechanism,,JUL 2024,3,"Due to the inadequate utilization of data correlation and complementarity in the feature extraction process of multimodal remote sensing images, the paper proposes a deep learning semantic segmentation algorithm based on the Dual Channel Attention Mechanism (DCAM). This algorithm uses U-Net as the backbone, combining the RGB remote sensing image as one input channel with the Convolutional Block Attention Module to extract colour space features. Simultaneously, it utilizes near-infrared (NIR) as another input channel with the Self-Attention Module (SAM) to extract shape space features. Finally, by concatenating the multi-scale attention features of the RGB remote sensing image channel and the NIR remote sensing image channel, it achieves the correlation and complementarity of contextual features between the two modal remote sensing images. Experimental results on the GID-15 dataset demonstrate that the DCAM algorithm significantly improves the segmentation accuracy, edge segmentation quality, and object segmentation integrity for various types of targets compared to current mainstream segmentation methods.The proposed algorithm takes RGB remote sensing images as input to a U-Net downsampling network combined with the Convolutional Block Attention Module to extract colour space features. Simultaneously, it takes near-infrared remote sensing images as input to another U-Net downsampling network combined with the Self-Attention Module to extract shape space features. image",convolutional neural nets,image segmentation,remote sensing,,,,,,,IET IMAGE PROCESSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_249,"Ding, Rong-Xing","Xu, Yi-Han","Liu, Jie","Zhou, Wen",LSENet: Local and Spatial Enhancement to Improve the Semantic Segmentation of Remote Sensing Images,,2024,0,"The semantic segmentation of remote sensing images is extensively used in crop cover and type analysis and environmental monitoring. In the semantic segmentation of remote sensing images, owning to the specificity of remote sensing images, not only the local context is required, but also the global context information makes an important role in it. Inspired by the powerful global modeling capability of Swin Transformer, we propose the Local and Spatial Enhancement Net (LSENet) network, which follows the encoder-decoder architecture of the UNet network. In the encoding phase, we propose spatial enhancement module (SEM), which helps Swin Transformer further enhance feature extraction by encoding spatial information. In the decoding stage, we propose local enhancement module (LEM), which is embedded in the Swin Transformer to improve the Swin Transformer to assist the network to obtain more local semantic information so as to classify pixels more accurately, especially in the edge region, the adding of LEM enables to obtain smoother edges. The experimental results on the Vaihingen and Potsdam datasets demonstrate the effectiveness of our proposed method. Specifically, the mIoU metric is 78.58% on the Potsdam dataset and 72.59% on the Vaihingen dataset.",Remote sensing,Transformers,Feature extraction,Convolution,Semantics,"Chen, Chen",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Forestry,Training,Deep learning,remote sensing,semantic segmentation,Swin Transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_250,"Xu, Rongtao","Wang, Changwei","Zhang, Jiguang","Xu, Shibiao",RSSFormer: Foreground Saliency Enhancement for Remote Sensing Land-Cover Segmentation,,2023,62,"High spatial resolution (HSR) remote sensing images contain complex foreground-background relationships, which makes the remote sensing land cover segmentation a special semantic segmentation task. The main challenges come from the large-scale variation, complex background samples and imbalanced foreground-background distribution. These issues make recent context modeling methods sub-optimal due to the lack of foreground saliency modeling. To handle these problems, we propose a Remote Sensing Segmentation framework (RSSFormer), including Adaptive TransFormer Fusion Module, Detail-aware Attention Layer and Foreground Saliency Guided Loss. Specifically, from the perspective of relation-based foreground saliency modeling, our Adaptive Transformer Fusion Module can adaptively suppress background noise and enhance object saliency when fusing multi-scale features. Then our Detail-aware Attention Layer extracts the detail and foreground-related information via the interplay of spatial attention and channel attention, which further enhances the foreground saliency. From the perspective of optimization-based foreground saliency modeling, our Foreground Saliency Guided Loss can guide the network to focus on hard samples with low foreground saliency responses to achieve balanced optimization. Experimental results on LoveDA datasets, Vaihingen datasets, Potsdam datasets and iSAID datasets validate that our method outperforms existing general semantic segmentation methods and remote sensing segmentation methods, and achieves a good compromise between computational overhead and accuracy.",Remote sensing,Transformers,Semantic segmentation,Task analysis,Buildings,"Meng, Weiliang","Zhang, Xiaopeng",,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,Background noise,Convolution,Remote sensing segmentation,foreground saliency enhancement,transformer,,,,,,,,,,,,,,,,,,,,,,,,,
Row_251,"Li, Yansheng","Shi, Te","Zhang, Yongjun","Ma, Jiayi",SPGAN-DA: Semantic-Preserved Generative Adversarial Network for Domain Adaptive Remote Sensing Image Semantic Segmentation,,2023,28,"Unsupervised domain adaptation for remote sensing semantic segmentation seeks to adapt a model trained on the labeled source domain to the unlabeled target domain. One of the most promising ways is to translate images from the source domain to the target domain to align the spectral information or imaging mode by the generative adversarial network (GAN). However, source-to-target translation often brings bias in the translated images causing limited performance, as semantic information is not well considered in the translation procedure. To overcome this limitation, we present an innovative semantic-preserved generative adversarial network (SPGAN), designed to mitigate the image translation bias and then leverage the translated images as well as unlabeled target images by class distribution alignment (CDA) module to train a domain adaptive semantic segmentation model. The above two stages are coupled together to form a unified framework called SPGAN-DA. Specifically, we first conduct semantic invariant translation from source to target domain, which is achieved by introducing representation-invariant and semantic-preserved constraints to the GAN model. To further narrow the landscape layout gap between the translated and target images, CDA semantic segmentation is proposed. CDA semantic segmentation consists of two aspects. At the model input level, object discrepancy is eliminated by introducing the ClassMix operation. At the model output level, boundary enhancement is proposed to refine the performance of object boundaries. Extensive experiments on three typical remote sensing cross-domain semantic segmentation benchmarks demonstrate the effectiveness and generality of our proposed method, which competes favorably against existing state-of-the-art methods.",Index Terms-Class distribution alignment (CDA),domain adaptive semantic segmentation,generative adversarial network (GAN),semantic-preserved generative adversarial network (SPGAN),unbiased image translation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_252,"Wang, Yupei","Shi, Hao","Dong, Shan","Zhuang, Yin",Dual-Path Sparse Hierarchical Network for Semantic Segmentation of Remote Sensing Images,,2022,4,"Semantic segmentation of remote sensing images aims to label every pixel with the correct semantic category. The core challenge of the current deep convolutional network (ConvNet)-based methods lies in the difficulty of effectively aggregating high-level categorical semantics and low-level local details along the hierarchy of backbone. Most current approaches consider only fusing adjacent feature layers gradually with short-range feature connections, which lack the diversity of feature interactions, such as long-range cross-scale connections. To this end, we propose a novel dual-path sparse hierarchical network that is characterized by rich cross-scale feature interactions. Multiscale features are first sparsely grouped with a predefined interval, which is then aggregated via both long-range and short-range cross-scale connections in a hierarchical manner. Moreover, in order to further enrich the diversity of feature interactions, we also introduce another fusion path in parallel but with different sparsity for feature grouping, forming a dual-path network. In this way, our model is able to effectively aggregate multilevel features by incorporating both long-range and short-range feature interactions in both parallel and hierarchical manner. Meanwhile, the semantic and resolution gap between multilevel features can also be bridged.",Semantics,Image segmentation,Remote sensing,Spatial resolution,Location awareness,"Chen, Liang",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Aggregates,Deep learning,remote sensing image understanding,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_253,"Chong, Qianpeng","Xu, Jindong","Jia, Fei","Liu, Zhaowei",A multiscale fuzzy dual-domain attention network for urban remote sensing image segmentation,,JUL 18 2022,10,"Semantic segmentation of high-resolution remote sensing images plays an important role in the remote sensing community. However, many indistinguishable objects are prevalent within urban remote sensing images, and some objects belonging to the same class are different and many objects that do not belong to the same class are similar. These tricky objects make the images exhibit low-interclass variance and high-intraclass variance, which significantly limits segmentation performance. Therefore, a fresh insight was presented to alleviate this issue by incorporating the fuzzy pattern recognition method and deep-learning method. Specifically, we proposed a multiscale fuzzy dual-domain attention network (MFDAN). In MFDAN, a two-dimensional Gaussian fuzzy learning module is proposed to eliminate those factors that influence the intraclass and interclass variance. In addition, a dual-domain attention module is proposed to derive more informative semantic representations in the channel and spatial domains, respectively. These two modules will be integrated in a multiscale perspective. Extensive experiments on the benchmark datasets illustrate qualitatively and quantitatively that the proposed MFDAN is competitive.",Attention mechanism,fuzzy learning,remote-sensing imagery,semantic segmentation,,"Yan, Weiqing","Wang, Xuan","Song, Yongchao",,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_254,"de Carvalho, Osmar Luiz Ferreira","de Carvalho Junior, Osmar Abilio","Silva, Cristiano Rosa e","de Albuquerque, Anesmar Olino",Panoptic Segmentation Meets Remote Sensing,,FEB 2022,24,"Panoptic segmentation combines instance and semantic predictions, allowing the detection of countable objects and different backgrounds simultaneously. Effectively approaching panoptic segmentation in remotely sensed data is very promising since it provides a complete classification, especially in areas with many elements as the urban setting. However, some difficulties have prevented the growth of this task: (a) it is very laborious to label large images with many classes, (b) there is no software for generating DL samples in the panoptic segmentation format, (c) remote sensing images are often very large requiring methods for selecting and generating samples, and (d) most available software is not friendly to remote sensing data formats (e.g., TIFF). Thus, this study aims to increase the operability of panoptic segmentation in remote sensing by providing: (1) a pipeline for generating panoptic segmentation datasets, (2) software to create deep learning samples in the Common Objects in Context (COCO) annotation format automatically, (3) a novel dataset, (4) leverage the Detectron2 software for compatibility with remote sensing data, and (5) evaluate this task on the urban setting. The proposed pipeline considers three inputs (original image, semantic image, and panoptic image), and our software uses these inputs alongside point shapefiles to automatically generate samples in the COCO annotation format. We generated 3400 samples with 512 x 512 pixel dimensions and evaluated the dataset using Panoptic-FPN. Besides, the metric analysis considered semantic, instance, and panoptic metrics, obtaining 93.865 mean intersection over union (mIoU), 47.691 Average (AP) Precision, and 64.979 Panoptic Quality (PQ). Our study presents the first effective pipeline for generating panoptic segmentation data for remote sensing targets.",deep learning,aerial image,dataset,semantic segmentation,instance segmentation,"Santana, Nickolas Castro","Borges, Dibio Leandro","Gomes, Roberto Arnaldo Trancoso","Guimaraes, Renato Fontes",REMOTE SENSING,,panoptic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_255,"Zhang, Yijie","Cheng, Jian","Su, Yanzhou","Wu, Yuheng",ORBNet: Original Reinforcement Bilateral Network for High-Resolution Remote Sensing Image Semantic Segmentation,,2024,0,"Semantic segmentation of high-resolution remote sensing images (HRRSIs) is a basic research in the field of remote sensing image processing. Many current CNN-based methods complete detailed segmentation by building an encoder-decoder network. However, the representative selection features of ground objects are often ignored and the semantic gap between high-level features and low-level features, resulting in redundant information and erroneous annotation results. In this article, we propose an original reinforcement bilateral network (ORBNet) to improve the performance of HRRSIs semantic segmentation. The ORBNet consists of two branches-the detail branch and the semantic branch, which are responsible for extracting low-level features and high-level features, respectively. The feature alignment and fusion (FAF) modules are used to align features at different levels between two branches and produce shallow features and deep features. Furthermore, we use the detail loss in the detail branch to supervise the generation of low-level features. The class-specific discriminative loss is used to help the semantic branch distinguish features of different ground objects. The spatial-channel attention (SCA) modules are used in the feature fusion stage to select representative features. We conducted extensive experiments on two open-source ISPRS remote sensing datasets, and the experimental results verified the superior performance of our ORBNet.",Semantic segmentation,Feature extraction,Semantics,Remote sensing,Transformers,"Ma, Qijun",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Task analysis,Decoding,Deep learning,feature fusion,semantic segmentation,spatial-channel attention,,,,,,,,,,,,,,,,,,,,,,,,
Row_256,"Yu, Bo","Yang, Lu","Chen, Fang",,Semantic Segmentation for High Spatial Resolution Remote Sensing Images Based on Convolution Neural Network and Pyramid Pooling Module,,SEP 2018,141,"Semantic segmentation provides a practical way to segment remotely sensed images into multiple ground objects simultaneously, which can be potentially applied to multiple remote sensed related aspects. Current classification algorithms in remotely sensed images are mostly limited by different imaging conditions, the multiple ground objects are difficult to be separated from each other due to high intraclass spectral variances and interclass spectral similarities. In this study, we propose an end-to-end framework to semantically segment high-resolution aerial images without postprocessing to refine the segmentation results. The framework provides a pixel-wise segmentation result, comprising convolutional neural network structure and pyramid pooling module, which aims to extract feature maps at multiple scales. The proposed model is applied to the ISPRS Vaihingen benchmark dataset from the ISPRS 2D Semantic Labeling Challenge. Its segmentation results are compared with previous state-of-the-art method UZ _1, UPB and three other methods that segment images into objects of all the classes (including clutter/background) based on true orthophoto tiles, and achieve the highest overall accuracy of 87.8% over the published performances, to the best of our knowledge. The results validate the efficiency of the proposed model in segmenting multiple ground objects from remotely sensed images simultaneously.",Remotely sensed images,semantic segmentation,,,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_257,"Li, Jiahao","Sun, Bin","Li, Shutao","Kang, Xudong",Semisupervised Semantic Segmentation of Remote Sensing Images With Consistency Self-Training,,2022,30,"Semisupervised semantic segmentation is an effective way to reduce the expensive manual annotation cost and take advantage of the unlabeled data for remote sensing (RS) image interpretation. Recent related research has mainly adopted two strategies: self-training and consistency regularization. Self-training tries to acquire accurate pseudo-labels to explicitly expand the train set. However, the existing methods cannot accurately identify false pseudo-labels, suffering from their negative impact on model optimization. The consistency regularization constrains the model by producing consistent predictions robust to the perturbations introduced in the sample or feature domain but requires a sufficient number of training data. Therefore, we propose a strategy for the semisupervised semantic segmentation of the RS images. The proposed model in the generative adversarial network (GAN) framework is optimized by consistency self-training, learning the distributions of both labeled and unlabeled data. The discriminator is optimized by accurate pixel-level training labels instead of the image-level ones, thereby assessing the confidence for the prediction of each pixel, which is then used to reweight the loss of the unlabeled data in self-training. The generator is optimized with the consistency constraint with respect to all random perturbations on the unlabeled data, which increases the sample diversity and prompts the model to learn the underlying distribution of the unlabeled data. Experimental results on the the large-scale and densely annotated Instance Segmentation in Aerial Images Dataset (iSAID) datasets and the International Society for Photogrammetry and Remote Sensing (ISPRS) datasets show that our framework outperforms several state-of-the-art semisupervised semantic segmentation methods.",Semantics,Predictive models,Image segmentation,Generative adversarial networks,Perturbation methods,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Semisupervised learning,Consistency self-training,generative adversarial network (GAN),remote sensing (RS) image,semantic segmentation,semisupervised learning,,,,,,,,,,,,,,,,,,,,,,,
Row_258,"Jung, Hoin","Choi, Han-Soo","Kang, Myungjoo",,Boundary Enhancement Semantic Segmentation for Building Extraction From Remote Sensed Image,,2022,80,"Image processing via convolutional neural network (CNN) has been developed rapidly for remote sensing technology. Moreover, techniques for accurately extracting building footprints from remote sensed images have attracted considerable interest owing to their wide variety of common applications, including monitoring natural disasters and urban development. Extraction of building footprints can be performed easily by semantic segmentation using U-Net-like CNN architectures. However, obtaining precise boundaries of segmentation masks remains challenging due to various impediments surrounding target objects. In this study, we propose a method to elaborate edges of buildings detected in remote sensed images to enhance the boundaries of segmentation masks. The proposed method adopts holistically nested edge detection (HED), which extracts edge features at an encoder of a given architecture. In the proposed boundary enhancement (BE) module, an extracted edge and segmentation mask are combined, sharing mutual information. To enable the proposed method efficiently to adapt to a wide variety of conditions, we design a distinctive approach adopting a HED unit and BE module, which is applicable to various semantic segmentation networks containing encoder-decoder structures. Experiments were conducted on five different datasets (DeepGlobe, Urban3D, WHU [high-resolution (HR), low-resolution (LR)], and Massachusetts). The results demonstrate that our proposed approaches improved on the performance of prior methods for extracting building footprints. Comparative experiments were conducted on various backbone architectures including U-Net, ResUNet++, TernausNet, and U-shape spatial pyramid pooling (USPP) to ensure the effectiveness of the proposed method. Based on various evaluation metrics and qualitative analysis, our results show that the proposed method achieved improved performance compared with prior methods for all datasets and backbone networks.",Semantics,Remote sensing,Buildings,Feature extraction,Image edge detection,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Image segmentation,Convolutional neural networks,Boundary enhancement (BE),building footprint extraction,convolutional neural network (CNN),remote sensing,satellite imagery,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,
Row_259,"Yin, Peng","Zhang, Dongmei","Han, Wei","Li, Jiang",High-Resolution Remote Sensing Image Semantic Segmentation via Multiscale Context and Linear Self-Attention,,2022,7,"Remote sensing image semantic segmentation, which aims to realize pixel-level classification according to the content of remote sensing images, has broad applications in various fields. Thanks to the superiority of deep learning (DL), the semantic segmentation model based on the convolutional neural network (CNN) dramatically promotes the development of remote sensing image semantic segmentation. Due to the high resolution, comprehensive coverage, extensive data, and sizeable spectral difference of high-resolution remote sensing images (HRRSI), the existing GPU is not suitable for directly semantic segmentation of the whole image. Cutting the image into small patches will lead to the loss of context information, resulting in the decline of accuracy. To address this issue, we propose the multiscale context self-attention network (MSCSANet). It combines the benefits of the self-attention mechanism with CNN to improve the segmentation quality of various remote sensing images. The MSCSANet extracts multiscale features from multiscale context images to solve the problem of feature loss caused by image segmentation. In addition, in order to make use of the feature of large-scale context, the multiscale context patches are used to guide the local image patch to focus on different fine-grained objects to enhance the feature of the local image patch. Moreover, considering the limited computing resources, we designed a linear self-attention module to reduce the computational complexity. Compared with other DL models, our proposed model can enhance the ability of multiscale features in complex scenes, and realizes improvements of 1.56% mean intersection over union (MIoU) on the Gaofen Image Dataset and 1.93% MIoU on the ISPRS Potsdam Dataset, respectively.",Feature extraction,Remote sensing,Correlation,Complexity theory,Context modeling,"Cheng, Jianmei",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Computational modeling,Deep learning,Context,remote sensing,self-attention,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_260,"Dong, Xingjun","Zhang, Changsheng","Fang, Lei","Yan, Yuxiao",A deep learning based framework for remote sensing image ground object segmentation,,NOV 2022,7,"Semantic segmentation of very-high-resolution (VHR) remote sensing images is of great significance, in which remote sensing can be applied to numerous fields. However, VHR remote sensing images are taken at different seasons and regions, causing the large intra-class and low inter-class variations of pixels. Thus, the state-of-the-art semantic segmentation network have considerable misclassifications and blurring of object boundaries. To solve the above problems, a deep learning-based semantic segmentation framework (DLSS) of VHR remote sensing images is proposed in this study, which comprising three stages. At the pre-processing stage, a novel data pre-processing method named Image Block Segmentation (IBS) is proposed to coarse segmentation of VHR remote sensing images at the image block scale. At the image segmentation stage, the different strategies are adopted to segment image blocks of different categories for fine segmentation. Through these two stages, this study implements a coarse-to-fine segmentation strategy, which reduces the phenomenon of misclassification by using different network models for low inter class variations of pixels. At the post-processing stage, a novel post-processing method termed Superpixel Cluster (SPC) is proposed to modify the segmentation results. SPC can capture fine details of objects and aggregating continuous pixels with similar characteristics into a set of superpixels, so as to ensure the boundary accuracy and internal consistency of ground object. Extensive experiments, including a comprehensive ablation study, confirm that IBS is capable of effectively reducing the misclassification, and SPC can correct the segmentation results significantly. The experimental results on the Gaofen Image Dataset (GID) suggest that the overall accuracy (OA) of the commonly utilized models combined with DLSS framework can increase, and the average is 2.05%. The code of DLSS is available at https://github.com/dxj620/Deep-learning-semantic-segementation.(c) 2022 Elsevier B.V. All rights reserved.",Convolutional Neural Networks,Semantic segmentation,Remote sensing images,Superpixel segmentation,Landcover classification,,,,,APPLIED SOFT COMPUTING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_261,"Gu, Yuhang","Hao, Jie","Chen, Bing","Deng, Hai",Top-Down Pyramid Fusion Network for High-Resolution Remote Sensing Semantic Segmentation,,OCT 2021,3,"In recent years, high-resolution remote sensing semantic segmentation based on data fusion has gradually become a research focus in the field of land classification, which is an indispensable task of a smart city. However, the existing feature fusion methods with bottom-up structures can achieve limited fusion results. Alternatively, various auxiliary fusion modules significantly increase the complexity of the models and make the training process intolerably expensive. In this paper, we propose a new lightweight model called top-down pyramid fusion network (TdPFNet) including a multi-source feature extractor, a top-down pyramid fusion module and a decoder. It can deeply fuse features from different sources in a top-down structure using high-level semantic knowledge guiding the fusion of low-level texture information. Digital surface model (DSM) data and open street map (OSM) data are used as auxiliary inputs to the Potsdam dataset for the proposed model evaluation. Experimental results show that the network proposed in this paper not only notably improves the segmentation accuracy, but also reduces the complexity of the multi-source semantic segmentation model.",semantic segmentation,data fusion,deep learning,open street map,digital surface model,,,,,REMOTE SENSING,,high-resolution remote sensing image,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_262,"Liang, Min","Wang, Xili",,,A bidirectional semantic segmentation method for remote sensing image based on super-resolution and domain adaptation,,JAN 17 2023,3,"Image semantic segmentation methods based on convolutional neural networks rely on supervised learning with labels, and their performance often drops significantly when applied to unlabelled datasets from different sources. The domain adaptation methods can reduce the inconsistency of feature distribution between the unlabelled target domain data used for testing and the labelled source domain data used for training, thus improve the segmentation performance and have more practical applications. However, in the field of remote sensing image processing, if the spatial resolutions of the source domain and the target domain are different and this problem is not to be solved, the performance of the transferred model will be affected. In this paper, we propose a bidirectional semantic segmentation method based on super-resolution and domain adaption (BSSM-SRDA), which is suitable for the transfer learning task of a semantic segmentation model from a low-resolution source domain data to a high-resolution target domain data. BSSM-SRDA mainly consists of three parts: a shared feature extraction network; a super-resolution image translation module, which incorporates a super-resolution approach to reduce spatial resolution differences and visual style differences of the two domains; a domain-adaptive semantic segmentation module, which combines an adversarial domain adaptation approach to reduce differences at the output level. At the same time, we design a new bidirectional self-supervised learning algorithm for BSSM-SRDA that facilitates mutually beneficial learning of the super-resolution image translation module and the domain-adaptive semantic segmentation module. The experiments demonstrate the superiority of the proposed method over other state-of-the-art methods on two remote-sensing image datasets, with mIoU improvements of 2.5% and 3.2%, respectively.",remote-sensing image,semantic segmentation,domain adaptation,super resolution,self-supervised learning,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_263,Niu Mengjia,Zhang Yongjun,Li Zhi,Yang Gang,Remote Sensing Image Segmentation Network Based on Adaptive Multiscale and Contour Gradient,,JAN 2023,1,"Remote sensing image segmentation algorithms are susceptible to interference from environmental factors, such as object occlusion and uneven illumination. Existing deep learning remote sensing image semantic segmentation methods usually adopt an end-to-end codec structure. However, they still suffer from inaccurate segmentation for the structure and contours of high similarity objects. Therefore, to improve the algorithm robustness and classification accuracy, a deep convolutional neural network remote sensing image semantic segmentation algorithm based on contour gradient learning is proposed. To improve the quality of the predicted feature maps, the adaptive attention-based multichannel multiscale feature fusion network (D-MMA Net) is proposed based on the SegNet model network. The D-MA block uses an attention-based adaptive multiscale module to adaptively extract different scale features according to the learned weights to obtain more effective high level semantic features. To further refine the extracted object boundaries, the contour extraction module, a learnable contour extraction module, is proposed based on the principle of the Sobel edge detection operator. Finally, the contour information is combined with multi-scale semantic features to enhance the robustness of the spatial resolution of the image. The experimental results show that the proposed method improves the segmentation accuracy and produces good segmentation results for irregular object boundaries.",remote sensing,remote sensing image,multi-channel feature extraction,contour gradient,feature fusion,Cui Zhongwei,Liu Junwen,,,LASER & OPTOELECTRONICS PROGRESS,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_264,"Ma, Bifang","Chang, Chih-Yung",,,Semantic Segmentation of High-Resolution Remote Sensing Images Using Multiscale Skip Connection Network,,FEB 15 2022,15,"Semantic segmentation of remote sensing images plays a vital role in land resource management, yield estimation, and economic evaluation. Therefore, this paper proposes a multi-scale skip connection network with the Atrous convolution to deal with the segmentation problems of the multi-modal and multi-scale high-resolution remote sensing images. Firstly, we applied the Atrous convolution in the encoder to enlarge the convolution kernel's receptive field. Secondly, based on the U-Net network, we merged the light and deep features of different scales by redesigning the skip connection and combining multi-scale features in each U-Net layer. Finally, we applied a pixel-by-pixel classification method and obtained the semantic segmentation results of remote sensing images. The effectiveness of the proposed algorithm is verified. The experimental results show that the mF1 scores are 89.4% and 90.3% on the open dataset of ISPRS Vaihingen and ISPRS Potsdam, respectively, which are better than the state-of-the-art algorithms.",Image segmentation,Semantics,Remote sensing,Feature extraction,Sensors,,,,,IEEE SENSORS JOURNAL,,Convolution,Task analysis,Deep convolutional neural network,high-resolution remote sensing image,multi-scale skip connection,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_265,"Li, Chunhua","Li, Xin","Xia, Runliang","Li, Tao",Hierarchical Self-Attention Embedded Neural Network With Dense Connection for Remote-Sensing Image Semantic Segmentation,,2021,1,"Semantic segmentation of remote-sensing imagery strives to assign a pixel-wise semantic label. Since encoder-decoder networks have demonstrated tremendous success in natural image semantic segmentation, the adoption and extension of this kind of method are transferring such superior performance for the problems in remote-sensing. Facing the high-altitude angle of imaging and complex and diverse ground objects of remote-sensing data, it is necessary to strengthen the features' distinguishability by enhancing the network's capability. Nevertheless, the existing methods suffer from the structural stereotype, leveraging the short-range and long-range contextual information insufficiently. Attempting to address the problems mentioned above, a hierarchical self-attention embedded neural network with dense connection for remote sensing image semantic segmentation (HSDCN) is proposed. In the encoder stage, multiple self-attention modules (SAM) are embedded to model pixel-wise and channel-wise relationships at various scales hierarchically, making the representations more refined and discriminative. Then the dense connections are used to fuse the heterogeneous features. Thus, the network could produce logical and reasonable clues for labeling pixels. The extensive experiments are conducted on ISPRS Vaihingen and Potsdam benchmarks. And the results reveal significant improvements in comparison with other state-of-the-art methods.",Semantics,Image segmentation,Remote sensing,Task analysis,Decoding,"Lyu, Xin","Tong, Yao","Zhao, Liancheng","Wang, Xinyuan",IEEE ACCESS,,Computer architecture,Costs,Semantic segmentation,remote-sensing imagery,self-attention,dense connection,ISPRS benchmarks,,,,,,,,,,,,,,,,,,,,,,,
Row_266,"Xu, Zhiyong","Zhang, Weicun","Zhang, Tianxiang","Yang, Zhifang",Efficient Transformer for Remote Sensing Image Segmentation,,SEP 2021,128,"Semantic segmentation for remote sensing images (RSIs) is widely applied in geological surveys, urban resources management, and disaster monitoring. Recent solutions on remote sensing segmentation tasks are generally addressed by CNN-based models and transformer-based models. In particular, transformer-based architecture generally struggles with two main problems: a high computation load and inaccurate edge classification. Therefore, to overcome these problems, we propose a novel transformer model to realize lightweight edge classification. First, based on a Swin transformer backbone, a pure Efficient transformer with mlphead is proposed to accelerate the inference speed. Moreover, explicit and implicit edge enhancement methods are proposed to cope with object edge problems. The experimental results evaluated on the Potsdam and Vaihingen datasets present that the proposed approach significantly improved the final accuracy, achieving a trade-off between computational complexity (Flops) and accuracy (Efficient-L obtaining 3.23% mIoU improvement on Vaihingen and 2.46% mIoU improvement on Potsdam compared with HRCNet_W48). As a result, it is believed that the proposed Efficient transformer will have an advantage in dealing with remote sensing image segmentation problems.",semantic segmentation,remote sensing,deep learning,pure Efficient transformer,Swin transformer,"Li, Jiangyun",,,,REMOTE SENSING,,edge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_267,"Lu, Yujie","Zhang, Yongjun","Cui, Zhongwei","Long, Wei",Multi-Dimensional Manifolds Consistency Regularization for semi-supervised remote sensing semantic segmentation,,SEP 5 2024,2,"Semi -supervised semantic segmentation in remote sensing is critical for urban planning, environmental monitoring and disaster response. The high cost and time required for high -quality data annotation limits its wider application. Traditional semi -supervised deep learning methods, which operate in a single dimension, limit model robustness and generalization. Our study addresses this issue by proposing an effective semisupervised learning method. This method improves model robustness and generalization in remote sensing semantic segmentation. We introduce the Multi -Dimensional Manifolds Consistency Regularization (MDMCR) approach. It applies multi -dimensional perturbations to input images and features, expanding the sample library and improving learning efficiency. Our method has been rigorously tested on various datasets. With only 1/8 of the data labeled, it achieved mean Intersection over Union (mIoU) scores of 74.48% on ISPRS Vaihingen and 78.80% on Potsdam. With only 5% labeled data, it reached 49.93% mIoU on DeepGlobe Roads and 57.90% on Massachusetts Roads. These results show the superiority of our method over existing techniques.",Semi-supervised remote sensing semantic,segmentation,Consistency regularization,Manifold hypothesis,Multi-dimensional manifolds,"Chen, Ziyang",,,,KNOWLEDGE-BASED SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_268,"Xiong, Shun","Ma, Chao","Yang, Guang","Song, Yaodong",Semantic segmentation of remote sensing imagery for road extraction via joint angle prediction: comparisons to deep learning,,DEC 28 2023,1,"Accurate road network information is required to study and analyze the relationship between land usage type and land subsidence, and road extraction from remote sensing images is an important data source for updating road networks. This task has been considered a significant semantic segmentation problem, given the many road extraction methods developed for remote sensing images in recent years. Although impressive results have been achieved by classifying each pixel in the remote sensing image using a semantic segmentation network, traditional semantic segmentation methods often lack clear constraints of road features. Consequently, the geometric features of the results might deviate from actual roads, leading to issues like road fractures, rough edges, inconsistent road widths, and more, which hinder their effectiveness in road updates. This paper proposes a novel road semantic segmentation algorithm for remote sensing images based on the joint road angle prediction. By incorporating the angle prediction module and the angle feature fusion module, constraints are added to the angle features of the road. Through the angle prediction and angle feature fusion, the information contained in the remote sensing images can be better utilized. The experimental results show that the proposed method outperforms existing semantic segmentation methods in both quantitative evaluation and visual effects. Furthermore, the extracted roads were consecutive with distinct edges, making them more suitable for mapping road updates.",angle prediction,semantic segmentation,road extraction,remote sensing image,map cartography,"Liang, Shuaizhe","Feng, Jing",,,FRONTIERS IN EARTH SCIENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_269,"Lyu, Xinran","Zheng, Ruohui","Zhang, Libao",,Semantic Segmentation of Weakly Annotated Remote Sensing Images Based on Feature Adversary and Uncertainty Perception,,2024,0,"The rich details in remote sensing images require the expertise of domain experts for annotation, making precise labeling across multiple scenarios challenging. Currently, there is an increasing interest in low-precision image-level annotations. However, errors and omissions are difficult to avoid during the labeling process. How to achieve accurate semantic segmentation under noisy annotations has become an urgent problem that requires resolution. In this letter, we consider the rich features of remote sensing targets and design a weakly labeled semantic segmentation model for remote sensing images based on feature adversary and uncertainty perception. First, we introduce a confidence model voting method to handle missing or incorrect image-level labels. Subsequently, we perform multilevel feature fusion to obtain initial pixel-level pseudo labels. Finally, leveraging the significant differences in image features across different categories, we design a feature adversarial model and introduce an uncertainty analysis method, improving the utilization of remote sensing image features and enhancing the accuracy of semantic segmentation. The effectiveness of this approach is validated through a comprehensive evaluation of four datasets.",Remote sensing,Semantic segmentation,Annotations,Uncertainty,Training,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Accuracy,Feature extraction,Feature adversary,noise cleaning,remote sensing,uncertainty perception,weak annotation,,,,,,,,,,,,,,,,,,,,,,,
Row_270,"Kang, Yuhan","Ji, Jian","Xu, Hekai","Yang, Yong",Swin-CDSA: The Semantic Segmentation of Remote Sensing Images Based on Cascaded Depthwise Convolution and Spatial Attention Mechanism,,2024,1,"As an important task in remote sensing image processing, semantic segmentation of remote sensing images has broad application prospects in many fields such as disaster warning and rescue, environmental protection, and road planning. Research on semantic segmentation of remote sensing images based on deep learning has made some progress, but there are still problems such as poor perception of small object features, loss of detailed information in deep feature extraction, and imprecise segmentation contours of small objects. To this end, we propose a new remote sensing semantic segmentation model Swin-CDSA, which copes these problems to some extent by designing cascaded deep convolutional modules (CDCMs) and spatial attention mechanisms (SAMs). CDCM extracts multiscale features by using multilayer convolutions with different layers but parallel fixed small-sized kernels, while SAM supplements the model's understanding of local and global information through a dual attention mechanism. We conducted experiments on the Potsdam and LoveDA datasets and achieved good results.",Convolution,Remote sensing,Feature extraction,Semantic segmentation,Attention mechanisms,"Chen, Peng","Zhao, Hui",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Transformers,Semantics,Attention mechanism,remote sensing,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_271,Zhang Zhehan,Fang Wei,Du Lili,Qiao Yanli,Semantic Segmentation of Remote Sensing Image Based on Encoder-Decoder Convolutional Neural Network,,FEB 10 2020,19,"The remote sensing image semantic segmentation in rural areas is the basis for urban and rural planning, vegetation and agricultural land detection. Segmentation of a high-resolution remote sensing image of rural areas is difficult because of the complex image information. Herein, we designed a complete symmetric network structure that includes a pooled index and a convolution used to fuse semantic information and image features. The Bottleneck layer is constructed using 1x1 convolution and employed to extract the details and reduce the parameter quantity, deepen the filter depth to build an end-to-end semantic segmentation network, and improve the activation function to further enhance network performance. The experimental results show that the accuracies of the proposed method and the classical semantic segmentation networks U-Net and SegNet are 98.4%, 80.3%, and 98.1%, respectively on the CCF dataset. Thus, the proposed method achieves better performance than the other two methods.",image processing,agricultural land detection,remote sensing images,semantic segmentation,encoder-decoder network,Zhang Dongying,Ding Guoshen,,,ACTA OPTICA SINICA,,deep learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_272,"Liao, Lingcen","Liu, Wei","Liu, Shibin",,Effect of Bit Depth on Cloud Segmentation of Remote-Sensing Images,,MAY 12 2023,2,"Due to the cloud coverage of remote-sensing images, the ground object information will be attenuated or even lost, and the texture and spectral information of the image will be changed at the same time. Accurately detecting clouds from remote-sensing images is of great significance to the field of remote sensing. Cloud detection utilizes semantic segmentation to classify remote-sensing images at the pixel level. However, previous studies have focused on the improvement of algorithm performance, and little attention has been paid to the impact of bit depth of remote-sensing images on cloud detection. In this paper, the deep semantic segmentation algorithm UNet is taken as an example, and a set of widely used cloud labeling dataset ""L8 Biome"" is used as the verification data to explore the relationship between bit depth and segmentation accuracy on different surface landscapes when the algorithm is used for cloud detection. The research results show that when the image is normalized, the effect of cloud detection with a 16-bit remote-sensing image is slightly better than that of an 8-bit remote sensing image; when the image is not normalized, the gap will be widened. However, using 16-bit remote-sensing images for training will take longer. This means data selection and classification do not always need to follow the highest possible bit depth when doing cloud detection but should consider the balance of efficiency and accuracy.",bit depth,remote sensing,semantic segmentation,cloud,deep learning,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_273,"Wu, Honglin","Huang, Peng","Zhang, Min","Tang, Wenlong",CMTFNet: CNN and Multiscale Transformer Fusion Network for Remote-Sensing Image Semantic Segmentation,,2023,52,"Convolutional neural networks (CNNs) are powerful in extracting local information but lack the ability to model long-range dependencies. In contrast, the transformer relies on multihead self-attention mechanisms to effectively extract the global contextual information and thus model long-range dependencies. In this article, we propose a novel encoder-decoder structured semantic segmentation network, named CNN and multiscale transformer fusion network (CMTFNet), to extract and fuse local information and multiscale global contextual information of high-resolution remote-sensing images. Specifically, to further process the output features from the CNN encoder, we build a transformer decoder based on the multiscale multihead self-attention (M2SA) module for extracting rich multiscale global contextual information and channel information. Additionally, the transformer block introduces an efficient feed-forward network (E-FFN) to enhance the information interaction between different channels of the feature. Finally, the multiscale attention fusion (MAF) module fully fuses the feature information from different levels. We have conducted extensive comparison experiments and ablation experiments on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam datasets. The extensive experimental results demonstrate that our proposed CMTFNet can obtain superior performance compared to the currently popular methods. The codes will be available at https://github.com/DrWuHonglin/CMTFNet.",Global contextual information,multiscale transformer,remote-sensing image,semantic segmentation,,"Yu, Xinyu",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_274,"Su, Zhongbin","Li, Wei","Ma, Zheng","Gao, Rui",An improved U-Net method for the semantic segmentation of remote sensing images,,FEB 2022,35,"Foremost deep neural network models trained in natural scenes cannot transfer and apply to remote sensing image semantic segmentation well. Studies have shown that fine-tuning methods containing model fusion can alleviate this dilemma. In this paper, we provide an approach used to improve U-Net and propose an end-to-end deep convolutional neural network (DCNN) combining the superiorities of DenseNet, U-Net, dilated convolution, and DeconvNet. We evaluated the proposed method and model on the Potsdam orthophoto data set. Compared with U-Net, our approach increases the PA, mPA, and mIoU evaluation indexes by 11.1%, 14.0%, and 13.5%, respectively; the segmentation speed increases by approximately 1.18 times and the number of parameters is 59.0% that of U-Net. The experiments demonstrate that for the semantic segmentation of high-resolution remote sensing images, using the combined dilated convolutions as the primary feature extractor, using the transposed convolution to restore the size of the feature maps, and reducing the number of layers is an effective method to improve the comprehensive performance of U-Net. This research enriches the models based on DCNNs and the modes of using DCNNs in a specific scene.",Deep learning,Artificial neural network,Remote sensing,U-Net,Semantic segmentation,,,,,APPLIED INTELLIGENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_275,"Yi, Yaning","Zhang, Zhijie","Zhang, Wanchang","Zhang, Chuanrong",Semantic Segmentation of Urban Buildings from VHR Remote Sensing Imagery Using a Deep Convolutional Neural Network,,AUG 2019,165,"Urban building segmentation is a prevalent research domain for very high resolution (VHR) remote sensing; however, various appearances and complicated background of VHR remote sensing imagery make accurate semantic segmentation of urban buildings a challenge in relevant applications. Following the basic architecture of U-Net, an end-to-end deep convolutional neural network (denoted as DeepResUnet) was proposed, which can effectively perform urban building segmentation at pixel scale from VHR imagery and generate accurate segmentation results. The method contains two sub-networks: One is a cascade down-sampling network for extracting feature maps of buildings from the VHR image, and the other is an up-sampling network for reconstructing those extracted feature maps back to the same size of the input VHR image. The deep residual learning approach was adopted to facilitate training in order to alleviate the degradation problem that often occurred in the model training process. The proposed DeepResUnet was tested with aerial images with a spatial resolution of 0.075 m and was compared in performance under the exact same conditions with six other state-of-the-art networks-FCN-8s, SegNet, DeconvNet, U-Net, ResUNet and DeepUNet. Results of extensive experiments indicated that the proposed DeepResUnet outperformed the other six existing networks in semantic segmentation of urban buildings in terms of visual and quantitative evaluation, especially in labeling irregular-shape and small-size buildings with higher accuracy and entirety. Compared with the U-Net, the F1 score, Kappa coefficient and overall accuracy of DeepResUnet were improved by 3.52%, 4.67% and 1.72%, respectively. Moreover, the proposed DeepResUnet required much fewer parameters than the U-Net, highlighting its significant improvement among U-Net applications. Nevertheless, the inference time of DeepResUnet is slightly longer than that of the U-Net, which is subject to further improvement.",semantic segmentation,urban building extraction,deep convolutional neural network,VHR remote sensing imagery,U-Net,"Li, Weidong","Zhao, Tian",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_276,"Li, Xin","Xu, Feng","Tao, Feifei","Tong, Yao",A Cross-Domain Coupling Network for Semantic Segmentation of Remote Sensing Images,,2024,0,"Semantic segmentation of remote sensing images (RSIs) is critical for various applications, including urban planning, agriculture, and disaster management. Existing methods often fail to capture fine-grained textures and periodic patterns in RSIs, leading to suboptimal results in complex terrains. To address these challenges, we propose a cross-domain coupling network (CDCNet) that leverages both domain-specific extraction and cross-domain coupling (CDC) to enrich contextual cues for semantic inference. Our CDCNet integrates a CDC layer within the encoder-decoder architecture to simultaneously refine representations in the frequency and spatial domains. This approach effectively models fine-grained textures and periodic patterns in the frequency domain, as well as edges, shapes, and broad structural elements in the spatial domain. Extensive experiments on the ISPRS Potsdam and LoveDA datasets demonstrate the superiority of CDCNet over several state-of-the-art methods. Ablation studies confirm the significant impact of the CDC layer, validating the effectiveness of our approach in handling RSIs.",Cross-domain coupling (CDC),frequency domain,remote sensing images (RSIs),semantic segmentation,Cross-domain coupling (CDC),"Gao, Hongmin","Liu, Fan","Chen, Ziqi","Lyu, Xin",IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,frequency domain,remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_277,"Li, Rui","Zheng, Shunyi","Duan, Chenxi","Su, Jianlin",Multistage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images,,2022,131,"The attention mechanism can refine the extracted feature maps and boost the classification performance of the deep network, which has become an essential technique in computer vision and natural language processing. However, the memory and computational costs of the dot-product attention mechanism increase quadratically with the spatiotemporal size of the input. Such growth hinders the usage of attention mechanisms considerably in application scenarios with large-scale inputs. In this letter, we propose a linear attention mechanism (LAM) to address this issue, which is approximately equivalent to dot-product attention with computational efficiency. Such a design makes the incorporation between attention mechanisms and deep networks much more flexible and versatile. Based on the proposed LAM, we refactor the skip connections in the raw U-Net and design a multistage attention ResU-Net (MAResU-Net) for semantic segmentation from fine-resolution remote sensing images. Experiments conducted on the Vaihingen data set demonstrated the effectiveness and efficiency of our MAResU-Net. Our code is available at https://github.com/lironui/MAResU-Net.",Semantics,Complexity theory,Remote sensing,Task analysis,Image segmentation,"Zhang, Ce",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Decoding,Fine-resolution remote sensing images,linear attention mechanism (LAM),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_278,"Xiang, Xuyang","Gong, Wenping","Li, Shuailong","Chen, Jun",TCNet: Multiscale Fusion of Transformer and CNN for Semantic Segmentation of Remote Sensing Images,,2024,10,"Semantic segmentation of remote sensing images plays a critical role in areas such as urban change detection, environmental protection, and geohazard identification. Convolutional Neural Networks (CNNs) have been excessively employed for semantic segmentation over the past few years; however, a limitation of the CNN is that there exists a challenge in extracting the global context of remote sensing images, which is vital for semantic segmentation, due to the locality of the convolution operation. It is informed that the recently developed Transformer is equipped with powerful global modeling capabilities. A network called TCNet is proposed in this article, and a parallel-in-branch architecture of the Transformer and the CNN is adopted in the TCNet. As such, the TCNet takes advantage of both Transformer and CNN, and both global context and low-level spatial details could be captured in a much shallower manner. In addition, a novel fusion technique called Interactive Self-attention is advanced to fuse the multilevel features extracted from both branches. To bridge the semantic gap between regions, a skip connection module called Windowed Self-attention Gating is further developed and added to the progressive upsampling network. Experiments on three public datasets (i.e., Bijie Landslide Dataset, WHU Building Dataset, and Massachusetts Buildings Dataset) depict that TCNet yields superior performance over state-of-the-art models. The IoU values obtained by TCNet for these three datasets are 75.34% (ranked first among 10 models compared), 91.16% (ranked first among 13 models compared), and 76.21% (ranked first among 13 models compared), respectively.",Convolutional Neural Network (CNN),feature fusion,remote sensing images,semantic segmentation,Transformer,"Ren, Tianhe",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_279,"Li, Xin","Yong, Xi","Li, Tao","Tong, Yao",A Spectral-Spatial Context-Boosted Network for Semantic Segmentation of Remote Sensing Images,,APR 2024,6,"Semantic segmentation of remote sensing images (RSIs) is pivotal for numerous applications in urban planning, agricultural monitoring, and environmental conservation. However, traditional approaches have primarily emphasized learning within the spatial domain, which frequently leads to less than optimal discrimination of features. Considering the inherent spectral qualities of RSIs, it is essential to bolster these representations by incorporating the spectral context in conjunction with spatial information to improve discriminative capacity. In this paper, we introduce the spectral-spatial context-boosted network (SSCBNet), an innovative network designed to enhance the accuracy semantic segmentation in RSIs. SSCBNet integrates synergetic attention (SYA) layers and cross-fusion modules (CFMs) to harness both spectral and spatial information, addressing the intrinsic complexities of urban and natural landscapes within RSIs. Extensive experiments on the ISPRS Potsdam and LoveDA datasets reveal that SSCBNet surpasses existing state-of-the-art models, achieving remarkable results in F1-scores, overall accuracy (OA), and mean intersection over union (mIoU). Ablation studies confirm the significant contribution of SYA layers and CFMs to the model's performance, emphasizing the effectiveness of these components in capturing detailed contextual cues.",semantic segmentation,remote sensing images,spectral-spatial context,synergetic attention,cross-fusion module,"Gao, Hongmin","Wang, Xinyuan","Xu, Zhennan","Fang, Yiwei",REMOTE SENSING,"You, Qian",,,,,,,,,,"Lyu, Xin",,,,,,,,,,,,,,,,,,,,
Row_280,"Liu, Bing","Chen, Xiaohui","Yu, Anzhu","Feng, Fan",Large multimodal model for open vocabulary semantic segmentation of remote sensing images,,DEC 31 2025,0,"Conventional remote sensing image semantic segmentation tasks require training specialized models for specific categories of ground objects, which often fail to recognize unseen ground object categories during the training process. The generalization ability of the model is the key to achieving open vocabulary semantic segmentation of remote sensing images. Recently, large multimodal models that have been pre-trained with massive amounts of image and text data have demonstrated strong generalization capabilities. Inspired by the success of large multimodal models, we propose an open vocabulary segmentation method for remote sensing images. The proposed method uses the large multimodal model LLAVA and the vision large model SAM to achieve segmentation of open vocabulary. Specifically, LLAVA is used to understand the remote sensing images and the open vocabulary, and SAM is used to extract visual features of the remote sensing images. Finally, the features extracted from SAM and LLAVA are input into the mask decoder to complete semantic segmentation tasks. In order to verify the effectiveness of the proposed method, we conducted a large number of experiments on multiple ground object categories such as airplane, ship, river and lake. The qualitative and quantitative evaluation results fully verified the effectiveness of our proposed method.",Open vocabulary Semantic Segmentation,large multimodal model,deep learning,segment anything,,"Yue, Jiaying","Yu, Xuchu",,,EUROPEAN JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_281,"Jiang, Na","Li, Jiyuan",,,An Improved Semantic Segmentation Method for Remote Sensing Images Based on Neural Network,,APR 2020,8,"Traditional semantic segmentation methods cannot accurately classify high-resolution remote sensing images, due to the difficulty in acquiring the correlations between geophysical objects in these images. To solve the problem, this paper proposes an improved semantic segmentation method for remote sensing images based on neural network. Based on residual network, the proposed algorithm changes the dilated convolution kernels in the dilated spatial pyramid pooling (SPP) module before extracting the correlations between geophysical objects, thus improving the accuracy of segmentation. Next, the high resolution of the input image was maintained through deconvolution, and the semantic segmentation was realized by the pixel-level method. To enhance the robustness of our algorithm, the dataset was expanded through random cropping and stitching of images. Finally, our algorithm was trained and tested on the Potsdam dataset provided by the International Society for Photogrammetry and Remote Sensing (ISPRS). The results show that our algorithm was 1.4% more accurate than the DeepLab v3 Plus. The research results shed new light on the semantic segmentation of high-resolution remote sensing images.",remote sensing images,pixel-level method,residual network (ResNet),dilated spatial pyramid pooling (SPP),sub pixel up-sampling,,,,,TRAITEMENT DU SIGNAL,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_282,"Ni, Yue","Liu, Jiahang","Cui, Jian","Yang, Yuze",Edge Guidance Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,2023,5,"With the improvement of spatial resolution, the conveyed information of remote sensing images has become increasingly intricate. The semantic content of pixels within the same object exhibits considerable variability, whereas the semantic content of pixels between different objects exhibits significant overlap. However, most existing approaches focus solely on establishing the internal consistency of objects by aggregating global or multiscale contextual information without adequately considering the orientation and spatially detailed features of the target. Moreover, these methods often overlook the potential of edge information in achieving accurate edge positioning. These defects will adversely affect the accuracy of segmentation. In this article, we present an edge information guided network, which leverages edge information to guide the aggregation of rich contextual information for semantic segmentation to improve the segmentation accuracy of high-resolution remote sensing images. Specifically, an orientation convolution module is proposed to construct a spatial detail branch for acquiring precise edge information and spatial detail information. To effectively guide the aggregation of spatial detail features and semantic features, we propose a spatial-semantic feature aggregation module. Moreover, to enhance the extraction of long-range dependencies of irregular objects, we propose the orientation atrous convolution module, which facilitates the extraction of multiphase long-range dependencies of objects. The ISPRS Vaihingen and Potsdam datasets are employed to validate the efficacy of the proposed methodology and draw comparisons with various state-of-the-art techniques. The experimental results demonstrate that the proposed method offers distinct advantages.",Feature extraction,Semantics,Semantic segmentation,Image edge detection,Remote sensing,"Wang, Xiaozhen",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Convolution,Data mining,Edge information,orientation astrous convolution,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_283,"Liu, Yan","Ren, Qirui","Geng, Jiahui","Ding, Meng",Efficient Patch-Wise Semantic Segmentation for Large-Scale Remote Sensing Images,,OCT 2018,60,"Efficient and accurate semantic segmentation is the key technique for automatic remote sensing image analysis. While there have been many segmentation methods based on traditional hand-craft feature extractors, it is still challenging to process high-resolution and large-scale remote sensing images. In this work, a novel patch-wise semantic segmentation method with a new training strategy based on fully convolutional networks is presented to segment common land resources. First, to handle the high-resolution image, the images are split as local patches and then a patch-wise network is built. Second, training data is preprocessed in several ways to meet the specific characteristics of remote sensing images, i.e., color imbalance, object rotation variations and lens distortion. Third, a multi-scale training strategy is developed to solve the severe scale variation problem. In addition, the impact of conditional random field (CRF) is studied to improve the precision. The proposed method was evaluated on a dataset collected from a capital city in West China with the Gaofen-2 satellite. The dataset contains ten common land resources (Grassland, Road, etc.). The experimental results show that the proposed algorithm achieves 54.96% in terms of mean intersection over union (MIoU) and outperforms other state-of-the-art methods in remote sensing image segmentation.",remote sensing,image segmentation,fully convolutional network,patch-wise,multi-scale,"Li, Jiangyun",,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_284,"Fan, Junyu","Li, Jinjiang","Liu, Yepeng","Zhang, Fan",Frequency-aware robust multidimensional information fusion framework for remote sensing image segmentation,,MAR 2024,7,"Urban scene image segmentation is an important research area in high-resolution remote sensing image processing. However, due to its complex three-dimensional structure, interference factors such as occlusion, shadow, intra-class inconsistency, and inter-class indistinction affect segmentation performance. Many methods have combined local and global information using CNNs and Transformers to achieve high performance in remote sensing image segmentation tasks. However, these methods are not stable when dealing with these interference factors. Recent studies have found that semantic segmentation is highly sensitive to frequency information, so we introduced frequency information to make the model learn more comprehensively about different categories of targets from multiple dimensions. By modeling the target with local features, global information, and frequency information, the target features can be learned in multiple dimensions to reduce the impact of interference factors on the model and improve its robustness. In this paper, we consider frequency information in addition to combining CNNs and Transformers for modeling and propose a Multidimensional Information Fusion Network (MIFNet) for high-resolution remote sensing image segmentation of urban scenes. Specifically, we design an information fusion Transformer module that can adaptively associate local features, global semantic information, and frequency information and a relevant semantic aggregation module for aggregating features at different scales to construct the decoder. By aggregating image features at different depths, the specific representation of the target and the correlation between targets can be modeled in multiple dimensions, allowing the network to better recognize and understand the features of each class of targets to resist various interference factors that affect segmentation performance. We conducted extensive ablation experiments and comparative experiments on the ISPRS Vaihingen and ISPRS Potsdam benchmarks to verify our proposed method. In a large number of experiments, our method achieved the best results, with 84.53% and 87.3% mIoU scores on the Vaihingen and Potsdam datasets, respectively, proving the superiority of our method. The source code will be available at https://github.com/JunyuFan/MIFNet.",Remote sensing,Semantic segmentation,Frequency information,Transformer,,,,,,ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_285,"Dong, He","Yu, Baoguo","Wu, Wanqing","He, Chenglong",Enhanced Lightweight End-to-End Semantic Segmentation for High-Resolution Remote Sensing Images,,2022,4,"Deep learning based methods have shown promising performance in semantic segmentation of high-resolution remote sensing (HRRS) images. However, due to the multi-scale property and complexity of HRRS images, it still faces many challenges in tackling the scale variance problem and obtaining global context information. In this paper, we propose an enhanced lightweight end-to-end semantic segmentation (ELES2) framework for HRRS images, where a superpixel segmentation pooling (SSP) module is embedded with the framework for result refinement, leading to a more accurate end-to-end semantic segmentation. Besides, compensation connections (CC) are applied between encoder blocks to establish long-range dependencies. In addition, a dense dilated convolutional pyramid (DDCP) module is proposed to generate dense features under different scales and capture global context information. Experiments conducted showed that our ELES2 respectively achieves the mean pixel intersection-over-union (mIoU) values of 80.16% and 73.20% on the ISPRS Potsdam and Vaihingen benchmark datasets using only 12.62M parameters and 13.09G floating-point operations (FLOPs). Experimental results prove that our method achieves a promising balance between segmentation accuracy and computational efficiency compared with the state-of-the-art semantic segmentation models.",Image segmentation,Semantics,Remote sensing,Computational efficiency,Feature extraction,,,,,IEEE ACCESS,,Deep learning,Decoding,High-resolution remote sensing images,semantic segmentation,superpixel pooling,,,,,,,,,,,,,,,,,,,,,,,,,
Row_286,"Chen, Xi","Li, Zhiqiang","Jiang, Jie","Han, Zhen",Adaptive Effective Receptive Field Convolution for Semantic Segmentation of VHR Remote Sensing Images,,APR 2021,48,"Convolutional neural networks (CNNs) have facilitated impressive improvements in the semantic segmentation of very high-resolution (VHR) remote sensing images. The success of semantic segmentation depends on an effective receptive field (RF) large enough to cover the entire object. Popular methods to enlarge the effective RF include dilated filters, subsampling operations, and stacking layers. Unfortunately, the methods are inefficient or able to cause grid artifacts. Moreover, although the object sizes vary greatly in remote sensing images, the size of the RF cannot reach a compromise between small and large objects. To tackle these problems, we propose adaptive effective receptive convolution (AERFC) for VHR remote sensing images. AERFC adaptively controls the sampling location of convolution and automatically adjusts the effective RF without significantly increasing the parameter number and computational cost. Thus, AERFC reduces the training difficulty, decreases overfitting risk, and reserves details in VHR images. AERFC is also integrated with spatial pyramid pooling (SPP) to aggregate diverse multiscale features for exploring contextual information. Experimental results of the quantitative and qualitative evaluation over four benchmark data sets show that AERFC outperforms state-of-the-art methods.",Radio frequency,Convolution,Feature extraction,Image segmentation,Semantics,"Deng, Shiyi","Li, Zhihong","Fang, Tao","Huo, Hong",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Li, Qingli",Remote sensing,Shape,Field of view,filter,kernel,semantic contextual information,,,,"Liu, Min",,,,,,,,,,,,,,,,,,,,
Row_287,"Dong, Sijun","Chen, Zhengchao",,,A Multi-Level Feature Fusion Network for Remote Sensing Image Segmentation,,FEB 2021,16,"High-resolution remote sensing image segmentation is a mature application in many industrial-level image applications and it also has military and civil applications. The scene analysis needs to be automated as much as possible with high-resolution remote sensing images. This plays a significant role in environmental disaster monitoring, forestry industry, agricultural farming, urban planning, and road analysis. This study proposes a multi-level feature fusion network (MFNet) that can integrate the multi-level features in the backbone to obtain different types of image information. Finally, the experiments in this study demonstrate that the proposed network can achieve good segmentation results in the Vaihingen and Potsdam datasets. By aiming to achieve a large difference in the scale of the target objects in remote sensing images and achieving a poor recognition result for small objects, a multi-level feature fusion solution is proposed in this study. This investigation improves the recognition results of the remote sensing image segmentation to a certain extent.",remote sensing image,image semantic segmentation,scale difference,feature fusion,,,,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_288,"Xi, Zhihao","Meng, Yu","Chen, Jingbo","Deng, Yupeng",Learning to Adapt Adversarial Perturbation Consistency for Domain Adaptive Semantic Segmentation of Remote Sensing Images,,DEC 2023,1,"Semantic segmentation techniques for remote sensing images (RSIs) have been widely developed and applied. However, most segmentation methods depend on sufficiently annotated data for specific scenarios. When a large change occurs in the target scenes, model performance drops significantly. Therefore, unsupervised domain adaptation (UDA) for semantic segmentation is proposed to alleviate the reliance on expensive per-pixel densely labeled data. In this paper, two key issues of existing domain adaptive (DA) methods are considered: (1) the factors that cause data distribution shifts in RSIs may be complex and diverse, and existing DA approaches cannot adaptively optimize for different domain discrepancy scenarios; (2) domain-invariant feature alignment, based on adversarial training (AT), is prone to excessive feature perturbation, leading to over robust models. To address these issues, we propose an AdvCDA method that guides the model to adapt adversarial perturbation consistency. We combine consistency regularization to consider interdomain feature alignment as perturbation information in the feature space, and thus propose a joint AT and self-training (ST) DA method to further promote the generalization performance of the model. Additionally, we propose a confidence estimation mechanism that determines network stream training weights so that the model can adaptively adjust the optimization direction. Extensive experiments have been conducted on Potsdam, Vaihingen, and LoveDA remote sensing datasets, and the results demonstrate that the proposed method can significantly improve the UDA performance in various cross-domain scenarios.",unsupervised domain adaptation,adversarial perturbation consistency,self-training,semantic segmentation,remote sensing,"Liu, Diyou","Kong, Yunlong","Yue, Anzhi",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_289,"Zhou, Nan","Hong, Jin","Cui, Wenyu","Wu, Shichao",A Multiscale Attention Segment Network-Based Semantic Segmentation Model for Landslide Remote Sensing Images,,MAY 2024,5,"Landslide disasters have garnered significant attention due to their extensive devastating impact, leading to a growing emphasis on the prompt and precise identification and detection of landslides as a prominent area of research. Previous research has primarily relied on human-computer interactions and visual interpretation from remote sensing to identify landslides. However, these methods are time-consuming, labor-intensive, subjective, and have a low level of accuracy in extracting data. An essential task in deep learning, semantic segmentation, has been crucial to automated remote sensing image recognition tasks because of its end-to-end pixel-level classification capability. In this study, to mitigate the disadvantages of existing landslide detection methods, we propose a multiscale attention segment network (MsASNet) that acquires different scales of remote sensing image features, designs an encoder-decoder structure to strengthen the landslide boundary, and combines the channel attention mechanism to strengthen the feature extraction capability. The MsASNet model exhibited an average accuracy of 95.13% on the test set from Bijie's landslide dataset, a mean accuracy of 91.45% on the test set from Chongqing's landslide dataset, and a mean accuracy of 90.17% on the test set from Tianshui's landslide dataset, signifying its ability to extract landslide information efficiently and accurately in real time. Our proposed model may be used in efforts toward the prevention and control of geological disasters.",deep learning,remote sensing images,landslide identification,semantic segmentation,feature extraction,"Zhang, Ziheng",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_290,"Sun, Wei","Zhou, Rong","Nie, Congchong","Wang, Livan",Farmland Segmentation from Remote Sensing Images Using Deep Learning Methods,,2020,6,"Farmland segmentation is crucial and getting increasingly important role in the field of agricultural insurance and digital agriculture. To accurately achieve insured crop area and disaster loss assessment, precision smallholders' farmland segmentation and mapping are necessary. Deep learning technology has demonstrated its strength and has out-performed state-of-the-art alternatives in many fields. In this study, we aim to explore the effectiveness of five classic deep semantic segmentation models on the smallholder-wise farmland segmentation from remote sensing images. Five FCN-based segmentation models were selected including U-Net, PSPNet, DeepLabV3+, DANet, and CCNet, which were originally proposed for natural or medical image segmentation. The study area locates in Jiaxiang County in the north China. We used GF-1 at 2m spatial resolution with 4-band multispectral images (red, green, blue, and near-infrared) acquired based on image fusion. Comparative experiment results showed that DeepLabv3+ achieved the best mIoU of 89.82%. PSPNet, DANet, CCNet and U-Net obtained lower but similar mIoU with: 89.63%, 89.66%, 89.59%, and 89.15%. The OA metric of the all the five models were above 94.8%. The results indicate that the FCN-based deep semantic segmentation networks are effective for larger-scale smallholder-wise farmland segmentation with high spatial resolution multispectral satellite images.",Farmland segmentation,remote sensing,satellite imagery,deep semantic segmentation,agriculture insurance,"Sun, Jun",,,,"MEDICAL IMAGE COMPUTING AND COMPUTER-ASSISTED INTERVENTION, PT III",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_291,"Zhang, Wenbo","Wang, Achuan",,,Research on Semantic Segmentation Method of Remote Sensing Image Based on Self-supervised Learning,,AUG 2023,0,"To address the challenge of requiring a large amount of manually annotated data for semantic segmentation of remote sensing images using deep learning, a method based on self-supervised learning is proposed. Firstly, to simultaneously learn the global and local features of remote sensing images, a self-supervised learning network structure called TBSNet (Triple-Branch Self-supervised Network) is constructed. This network comprises an image transformation prediction branch, a global contrastive learning branch, and a local contrastive learning branch. The contrastive learning part of the network employs a novel data augmentation method to simulate positive pairs of the same remote sensing images under different weather conditions, enhancing the model's performance. Meanwhile, the model integrates channel attention and spatial attention mechanisms in the projection head structure of the global contrastive learning branch, and replaces a fully connected layer with a convolutional layer in the local contrastive learning branch, thus improving the model's feature extraction ability. Secondly, to mitigate the high computational cost during the pre-training phase, an algorithm optimization strategy is proposed using the TracIn method and sequential optimization theory, which increases the efficiency of pre-training. Lastly, by fine-tuning the model with a small amount of annotated data, effective semantic segmentation of remote sensing images is achieved even with limited annotated data. The experimental results indicate that with only 10% annotated data, the overall accuracy (OA) and recall of this model have improved by 4.60% and 4.88% respectively, compared to the traditional self-supervised model SimCLR (A Simple Framework for Contrastive Learning of Visual Representations). This provides significant application value for tasks such as semantic segmentation in remote sensing imagery and other computer vision domains.",Computer vision,deep learning,self-supervised learning,remote sensing image,semantic segmentation,,,,,INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_292,"Cui, Wei","Wang, Fei","He, Xin","Zhang, Dongyou",Multi-Scale Semantic Segmentation and Spatial Relationship Recognition of Remote Sensing Images Based on an Attention Model,,MAY 1 2019,46,"A comprehensive interpretation of remote sensing images involves not only remote sensing object recognition but also the recognition of spatial relations between objects. Especially in the case of different objects with the same spectrum, the spatial relationship can help interpret remote sensing objects more accurately. Compared with traditional remote sensing object recognition methods, deep learning has the advantages of high accuracy and strong generalizability regarding scene classification and semantic segmentation. However, it is difficult to simultaneously recognize remote sensing objects and their spatial relationship from end-to-end only relying on present deep learning networks. To address this problem, we propose a multi-scale remote sensing image interpretation network, called the MSRIN. The architecture of the MSRIN is a parallel deep neural network based on a fully convolutional network (FCN), a U-Net, and a long short-term memory network (LSTM). The MSRIN recognizes remote sensing objects and their spatial relationship through three processes. First, the MSRIN defines a multi-scale remote sensing image caption strategy and simultaneously segments the same image using the FCN and U-Net on different spatial scales so that a two-scale hierarchy is formed. The output of the FCN and U-Net are masked to obtain the location and boundaries of remote sensing objects. Second, using an attention-based LSTM, the remote sensing image captions include the remote sensing objects (nouns) and their spatial relationships described with natural language. Finally, we designed a remote sensing object recognition and correction mechanism to build the relationship between nouns in captions and object mask graphs using an attention weight matrix to transfer the spatial relationship from captions to objects mask graphs. In other words, the MSRIN simultaneously realizes the semantic segmentation of the remote sensing objects and their spatial relationship identification end-to-end. Experimental results demonstrated that the matching rate between samples and the mask graph increased by 67.37 percentage points, and the matching rate between nouns and the mask graph increased by 41.78 percentage points compared to before correction. The proposed MSRIN has achieved remarkable results.",multi-scale,semantic segmentation,image caption,remote sensing,LSTM,"Xu, Xuxiang","Yao, Meng","Wang, Ziwei","Huang, Jiejun",REMOTE SENSING,,U-Net,upscaling,downscaling,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_293,"Wang, Yiqin","Dong, Yunyun",,,A Semantic Segmentation Method of Remote Sensing Image Based on Feature Fusion and Attention Mechanism,,OCT 2024,0,"Current methods for semantic segmentation of remote-sensing images, especially for irregular and small targets, often result in low precision and incomplete feature extraction. To address this issue, an improved semantic segmentation method was developed utilizing DeepLabv3+. First, DeepLabv3+ is combined with the proposed feature fusion module to make full use of the complementary information of low- and high-level features. Second, the channel attention module helps extract effective features while suppressing irrelevant features, thereby enabling the extraction of more meaningful global information from high-level features. Finally, rich spatial information is selected using guided spatial attention, which improves the accuracy of edge segmentation of target objects. The results of the comparison show that the mean F1 score (MF1) and overall accuracy (OA) of the proposed method on the ISPRS Potsdam dataset are 89.81% and 88.45%, respectively. The MF1 of the proposed method is 89.90% and the OA is 89.14% for the UAVid dataset, which are higher than those of the other comparison algorithms. The proposed method exhibits superior semantic segmentation capabilities for remote-sensing images.",Channel Attention,DeepLabv3+,Feature Fusion Module,Remote-Sensing Images,Semantic Segmentation,,,,,JOURNAL OF INFORMATION PROCESSING SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_294,"Liu, Bin","Li, Bing","Sreeram, Victor","Li, Shuofeng",MBT-UNet: Multi-Branch Transform Combined with UNet for Semantic Segmentation of Remote Sensing Images,,AUG 2024,0,"Remote sensing (RS) images play an indispensable role in many key fields such as environmental monitoring, precision agriculture, and urban resource management. Traditional deep convolutional neural networks have the problem of limited receptive fields. To address this problem, this paper introduces a hybrid network model that combines the advantages of CNN and Transformer, called MBT-UNet. First, a multi-branch encoder design based on the pyramid vision transformer (PVT) is proposed to effectively capture multi-scale feature information; second, an efficient feature fusion module (FFM) is proposed to optimize the collaboration and integration of features at different scales; finally, in the decoder stage, a multi-scale upsampling module (MSUM) is proposed to further refine the segmentation results and enhance segmentation accuracy. We conduct experiments on the ISPRS Vaihingen dataset, the Potsdam dataset, the LoveDA dataset, and the UAVid dataset. Experimental results show that MBT-UNet surpasses state-of-the-art algorithms in key performance indicators, confirming its superior performance in high-precision remote sensing image segmentation tasks.",transformer,semantic segmentation,convolutional neural network,remote sensing,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_295,"Dong, Rongsheng","Pan, Xiaoquan","Li, Fengying",,DenseU-Net-Based Semantic Segmentation of Objects in Urban Remote Sensing Images,,2019,108,"Class imbalance is a serious problem that plagues the semantic segmentation task in urban remote sensing images. Since large object classes dominate the segmentation task, small object classes are usually suppressed, so the solutions based on optimizing the overall accuracy are often unsatisfactory. In the light of the class imbalance of the semantic segmentation in urban remote sensing images, we developed the concept of the Down-sampling Block (DownBlock) for obtaining context information and the Up-sampling Block (UpBlock) for restoring the original resolution. We proposed an end-to-end deep convolutional neural network (DenseU-Net) architecture for pixel-wise urban remote sensing image segmentation. The main idea of the DenseU-Net is to connect convolutional neural network features through cascade operations and use its symmetrical structure to fuse the detail features in shallow layers and the abstract semantic features in deep layers. A focal loss function weighted by the median frequency balancing (MFB_Focal(loss)) is proposed; the accuracy of the small object classes and the overall accuracy are improved effectively with our approach. Our experiments were based on the 2016 ISPRS Vaihingen 2D semantic labeling dataset and demonstrated the following outcomes. In the case where boundary pixels were considered (GT), MFB_Focal(loss) achieved a good overall segmentation performance using the same U-Net model, and the F1-score of the small object class ""car"" was improved by 9.28% compared with the cross-entropy loss function. Using the same MFB_Focal(loss) loss function, the overall accuracy of the DenseU-Net was better than that of U-Net, where the F1-score of the ""car"" class was 6.71% higher. Finally, without any post-processing, the DenseU-Net+MFB_Focal(loss) achieved the overall accuracy of 85.63%, and the F1-score of the ""car"" class was 83.23%, which is superior to HSN+OI+WBP both numerically and visually.",Class imbalance,deep convolutional neural networks,median frequency balancing,semantic segmentation,urban remote sensing images,,,,,IEEE ACCESS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_296,"Li, Yansheng","Shi, Te","Zhang, Yongjun","Chen, Wei",Learning deep semantic segmentation network under multiple weakly-supervised constraints for cross-domain remote sensing image semantic segmentation,,MAY 2021,151,"Due to its wide applications, remote sensing (RS) image semantic segmentation has attracted increasing research interest in recent years. Benefiting from its hierarchical abstract ability, the deep semantic segmentation network (DSSN) has achieved tremendous success on RS image semantic segmentation and has gradually become the mainstream technology. However, the superior performance of DSSN highly depends on two conditions: (I) massive quantities of labeled training data exist; (II) the testing data seriously resemble the training data. In actual RS applications, it is difficult to fully meet these conditions due to the RS sensor variation and the distinct landscape variation in different geographic locations. To make DSSN fit the actual RS scenario, this paper exploits the cross-domain RS image semantic segmentation task, which means that DSSN is trained on one labeled dataset (i.e., the source domain) but is tested on another varied dataset (i.e., the target domain). In this setting, the performance of DSSN is inevitably very limited due to the data shift between the source and target domains. To reduce the disadvantageous influence of data shift, this paper proposes a novel objective function with multiple weakly-supervised constraints to learn DSSN for cross-domain RS image semantic segmentation. Through carefully examining the characteristics of cross-domain RS image semantic segmentation, multiple weakly-supervised constraints include the weakly-supervised transfer invariant constraint (WTIC), weakly-supervised pseudo-label constraint (WPLC) and weakly-supervised rotation consistency constraint (WRCC). Specifically, DualGAN is recommended to conduct unsupervised style transfer between the source and target domains to carry out WTIC. To make full use of the merits of multiple constraints, this paper presents a dynamic optimization strategy that dynamically adjusts the constraint weights of the objective function during the training process. With full consideration of the characteristics of the cross-domain RS image semantic segmentation task, this paper gives two cross-domain RS image semantic segmentation settings: (I) variation in geographic location and (II) variation in both geographic location and imaging mode. Extensive experiments demonstrate that our proposed method remarkably outperforms the state-of-the-art methods under both of these settings. The collected datasets and evaluation benchmarks have been made publicly available online (htt ps://github.com/te-shi/MUCSS).",Cross-domain remote sensing (RS) image semantic segmentation,Weakly-supervised transfer invariant constraint (WTIC),Weakly-supervised pseudo-label constraint (WPLC),Weakly-supervised rotation consistency constraint (WRCC),DualGAN,"Wang, Zhibin","Li, Hao",,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,Dynamic optimization strategy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_297,"Dang, Yuanyuan","Gao, Yu","Liu, Bing",,MFAFNet: A Multiscale Fully Attention Fusion Network for Remote Sensing Image Semantic Segmentation,,2024,0,"The semantic segmentation of high-resolution remote sensing images is widely used in various precision agriculture, urban planning, and environmental detection are some examples of these industries. Convolutional neural networks (CNNs) are excellent in the semantic segmentation of remote sensing images. CNN excels in extracting local feature details but lacks the ability to model global context data. Therefore, to obtain rich local-global information about context, we describe in this work a semantic segmentation network design technique for remote sensing, based on an encoder-decoder structure, which is named Multiscale Fully Attention Fusion Network for Remote Sensing Image Semantic Segmentation (MFAFNet). In particular, to improve the segmentation efficiency, the encoder's extractor of features was ResNet18, after which the explicit visual center module EVC and the full attention network FANB are intended to retrieve the detailed global context data. Finally, the gated channel attention fusion module (GCF) tries to augment channel interaction information in the decoder stage while fusing low-level characteristics for efficient aggregation. During our research and testing, we used the publicly available Vaihingen and Potsdam datasets from the International Society for Photogrammetry and Remote Sensing (ISPRS), as well as the LoveDA dataset. Meanwhile, it demonstrates that MFAFNet outperforms other well-liked methods in terms of competition. We further validated the efficiency of the network components in the study by conducting ablation experiments on the Vaihingen dataset.",Feature extraction,Semantic segmentation,Remote sensing,Convolutional neural networks,Decoding,,,,,IEEE ACCESS,,Semantics,Data mining,Attention mechanisms,remote sensing,global-local context,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,
Row_298,"Chen, Yan","Dong, Quan","Wang, Xiaofeng","Zhang, Qianchuan",Hybrid Attention Fusion Embedded in Transformer for Remote Sensing Image Semantic Segmentation,,2024,6,"In the context of fast progress in deep learning, convolutional neural networks have been extensively applied to the semantic segmentation of remote sensing images and have achieved significant progress. However, certain limitations exist in capturing global contextual information due to the characteristics of convolutional local properties. Recently, Transformer has become a focus of research in computer vision and has shown great potential in extracting global contextual information, further promoting the development of semantic segmentation tasks. In this article, we use ResNet50 as an encoder, embed the hybrid attention mechanism into Transformer, and propose a Transformer-based decoder. The Channel-Spatial Transformer Block further aggregates features by integrating the local feature maps extracted by the encoder with their associated global dependencies. At the same time, an adaptive approach is employed to reweight the interdependent channel maps to enhance the feature fusion. The global cross-fusion module combines the extracted complementary features to obtain more comprehensive semantic information. Extensive comparative experiments were conducted on the ISPRS Potsdam and Vaihingen datasets, where mIoU reached 78.06% and 76.37%, respectively. The outcomes of multiple ablation experiments also validate the effectiveness of the proposed method.",Feature extraction,Semantic segmentation,Semantics,Transformers,Remote sensing,"Kang, Menglei","Jiang, Wenxiang","Wang, Mengyuan","Xu, Lixiang",IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Zhang, Chen",Task analysis,Decoding,Global cross fusion,hybrid attention,remote sensing image,semantic segmentation,Transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_299,"Kemker, Ronald","Luu, Ryan","Kanan, Christopher",,Low-Shot Learning for the Semantic Segmentation of Remote Sensing Imagery,,OCT 2018,46,"Recent advances in computer vision using deep learning with RGB imagery (e.g., object recognition and detection) have been made possible thanks to the development of large annotated RGB image data sets. In contrast, multispectral image (MSI) and hyperspectral image (HSI) data sets contain far fewer labeled images, in part due to the wide variety of sensors used. These annotations are especially limited for semantic segmentation, or pixelwise classification, of remote sensing imagery because it is labor intensive to generate image annotations. Low-shot learning algorithms can make effective inferences despite smaller amounts of annotated data. In this paper, we study low-shot learning using self-taught feature learning for semantic segmentation. We introduce: 1) an improved self-taught feature learning framework for HSI and MSI data and 2) a semisupervised classification algorithm. When these are combined, they achieve the state-of-the-art performance on remote sensing data sets that have little annotated training data available. These low-shot learning frameworks will reduce the manual image annotation burden and improve semantic segmentation performance for remote sensing imagery.",Deep learning,feature learning,hyperspectral imaging,self-taught learning,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,semisupervised,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_300,"Liu, Zhuoran","Li, Zizhen","Liang, Ying","Persello, Claudio",RSPS-SAM: A Remote Sensing Image Panoptic Segmentation Method Based on SAM,,NOV 2024,0,"Satellite remote sensing images contain complex and diverse ground object information and the images exhibit spatial multi-scale characteristics, making the panoptic segmentation of satellite remote sensing images a highly challenging task. Due to the lack of large-scale annotated datasets for panoramic segmentation, existing methods still suffer from weak model generalization capabilities. To mitigate this issue, this paper leverages the advantages of the Segment Anything Model (SAM), which can segment any object in remote sensing images without requiring any annotations and proposes a high-resolution remote sensing image panoptic segmentation method called Remote Sensing Panoptic Segmentation SAM (RSPS-SAM). Firstly, to address the problem of global information loss caused by cropping large remote sensing images for training, a Batch Attention Pyramid was designed to extract multi-scale features from remote sensing images and capture long-range contextual information between cropped patches, thereby enhancing the semantic understanding of remote sensing images. Secondly, we constructed a Mask Decoder to address the limitation of SAM requiring manual input prompts and its inability to output category information. This decoder utilized mask-based attention for mask segmentation, enabling automatic prompt generation and category prediction of segmented objects. Finally, the effectiveness of the proposed method was validated on the high-resolution remote sensing image airport scene dataset RSAPS-ASD. The results demonstrate that the proposed method achieves segmentation and recognition of foreground instances and background regions in high-resolution remote sensing images without the need for prompt input, while providing smooth segmentation boundaries with a panoptic segmentation quality (PQ) of 57.2, outperforming current mainstream methods.",panoptic segmentation,segment anything model,remote sensing,deep learning,,"Sun, Bo","He, Guangjun","Ma, Lei",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_301,"He, Sheng","Liu, Jin",,,Semantic segmentation of very high resolution remote sensing images with residual logic deep fully convolutional networks,"MIPPR 2019: REMOTE SENSING IMAGE PROCESSING, GEOGRAPHIC INFORMATION SYSTEMS, AND OTHER APPLICATIONS",2020,0,"This paper describes a deep learning approach to semantic segmentation of very high resolution remote sensing images. We introduce RLFCN, a fully convolutional architecture based on residual logic blocks, to model the ambiguous mapping between remote sensing images and classification maps. In order to recover the output resolution to the original size, we adopt a special way to efficiently learn feature map up-sampling within the network. For optimization, we employ the equally-weighted focal loss which is particularly suitable for the task for it reduces the impact of class imbalance. Our framework consists of only one single architecture which is trained end-to-end and doesn't rely on any post-processing techniques and needs no extra data except images. Based on our framework, we conducted experiments on a ISPRS dataset: Vaihingen. The results indicate that our framework achieves better performance than the current state of the art, while containing fewer parameters and requires fewer training data.",deep learning,remote sensing,semantic segmentation,fully convolutional networks,RLFCN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_302,"Ding, Lei","Bruzzone, Lorenzo",,,A DEEP ARCHITECTURE BASED ON A TWO-STAGE LEARNING FOR SEMANTIC SEGMENTATION OF LARGE-SIZE REMOTE SENSING IMAGES,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),2019,4,"Remote sensing images (RSIs) usually have much larger size compared to typical natural images used in computer vision applications. This makes the computational cost of training convolutional neural networks with full-size images unaffordable. Commonly used methodologies for semantic segmentation of RSIs perform training and prediction on cropped local image patches. Thus they fail to model the potential dependencies between ground objects at a higher level of abstraction. In order to better exploit global context information in RSIs, a deep architecture based on a two-stage training approach that is specially tailored to training large-size RSIs is proposed. In the first training stage, down-scaled images are used as input to learn high-level features from a large image area. In the second training stage, a local feature extraction network is designed to extract low-level information from cropped image patches. The complementary information learned from different levels is fused to make the prediction. As a result, the proposed two-stage training approach is able to exploit the context information of RSIs from a larger perspective without losing spatial details. Experimental results on a benchmark remote sensing dataset demonstrate the effectiveness of the proposed approach.",Semantic Segmentation,Convolutional Neural Network,Deep Learning,Remote Sensing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_303,"Xiao, Sining","Wang, Peijin","Diao, Wenhui","Rong, Xuee",MoCG: Modality Characteristics-Guided Semantic Segmentation in Multimodal Remote Sensing Images,,2023,3,"The rapid development of satellite platforms has yielded copious and diverse multisource data for earth observation, greatly facilitating the growth of multimodal semantic segmentation (MSS) in remote sensing. However, MSS also suffers from numerous challenges: 1) existing inherent defects in each modality due to the different imaging mechanisms; 2) insufficient exploration of the intrinsic characteristics of modalities; and 3) the existence of the huge semantic gap between heterogeneous data causes difficulties in feature fusion. The inability to effectively utilize the rich and diverse information provided by each modality and ignorance of the heterogeneity between modalities will hinder the feature enhancement, and further significantly impacts the semantic segmentation accuracy. Furthermore, neglecting the huge gap makes feature fusion challenging. In this study, we introduce a novel framework for MSS that effectively mitigates the aforementioned problems. Our approach employs a pseudo-Siamese structure for feature extraction. Specifically, we propose a simple yet effective geometric topology structure modeling (GTSM) module to extract geometric relationships and texture information from optical data. Additionally, we present a modality intrinsic noise suppression (MINS) module to fully exploit radiation information and alleviate the effects of unique geometric distortions for synthetic aperture radar (SAR). Furthermore, we present an adaptive multimodal feature fusion (AMFF) module for fully fusing different modality features. Extensive experiments on both WHU-OPT-SAR and DFC23 datasets validate the robustness and effectiveness of the proposed Modality Characteristics-Guided (MoCG) semantic segmentation network compared to other state-of-the-art semantic segmentation methods, including multimodal and single-modal approaches. Our approach achieves the best performance on both datasets, resulting in mean intersection over union (mIoU)/overall accuracy (OA) gains 69.1%/87.5% on WHU-OPT-SAR and 86.7%/97.3% on DFC23.",Optical sensors,Optical imaging,Semantic segmentation,Feature extraction,Semantics,"Li, Xuexue","Fu, Kun","Sun, Xian",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Adaptive optics,Optical distortion,Cross-modal feature fusion,multimodal semantic segmentation (MSS),optical,remote sensing,synthetic aperture radar (SAR),,,,,,,,,,,,,,,,,,,,,,,
Row_304,"Che, Rui","Ma, Xiaowen","Hong, Tingfeng","Wang, Xinyu",DBDAN: Dual-Branch Dynamic Attention Network for Semantic Segmentation of Remote Sensing Images,,2024,0,"Attention mechanism is capable to capture long-range dependence. However, its independent calculation of correlations can hardly consider the complex background of remote sensing images, which causes noisy and ambiguous attention weights. To address this issue, we design a correlation attention module (CAM) to enhance appropriate correlations and suppress erroneous ones by seeking consensus among all correlation vectors, which facilitates feature aggregation. Simultaneously, we introduce the CAM into a local dynamic attention (LDA) branch and a global dynamic attention (GDA) branch to obtain the information on local texture details and global context, respectively. In addition, considering the different demands of complex and diverse geographical objects for both local texture details and global context, we devise a dynamic weighting mechanism to adaptively adjust the contributions of both branches, thereby constructing a more discriminative feature representation. Experimental results on three datasets suggest that the proposed dual-branch dynamic attention network (DBDAN), which integrates the CAM and both branches, can considerably improve the performance for semantic segmentation of remote sensing images and outperform representative state-of-the-art methods.",Remote Sensing,Semantic Segmentation,Attention Mechanism,Deep Learning,,"Feng, Tian","Zhang, Wei",,,"PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2023, PT IV",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_305,Wu Shengwei,Fang Jiaoli,Zhu Daming,,Semantic Segmentation of Remote Sensing Imagery Based on Improved Squeeze and Excitaion Block,,JUN 2024,0,"Aiming to solve the semantic recognition error of traditional methods in semantic segmentation of remote sensing imagery with complex background, we propose a simple but effective convolutional attention module, region squeeze and excitation block (RSE-block), based on squeeze and excitation block (SE-block). This block can squeeze regional context information of features, guides the network to screen more important features and excite features expression in both spatial and channel dimensions. In addition, it can be added to any convolutional neural network and trained end-to-end with the network. Meanwhile, we propose a multi-scale integration method supported by this block to solve the recognition problem of different size ground objects, and a new semantic segmentation network, RSENet, is constructed on these bases. The experimental results show that RSENet is superior to the baseline in terms of mean F1-score and mean intersection over union by 0. 028 and 0. 021 respectively on the Potsdam dataset, and is more competitive with some current advanced methods.",remote sensing imagery,semantic segmentation,attention mechanism,convolutional neural network,,,,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_306,"Yuan, Wei","Xu, Wenbo",,,NeighborLoss: A Loss Function Considering Spatial Correlation for Semantic Segmentation of Remote Sensing Image,,2021,15,"House segmentation of remote sensing image based on deep learning has become the main segmentation method because it can automatically extract features. However, the accuracy of image segmentation is affected not only by the network model, but also by the loss function, but the existing loss functions, except Binary Cross Entropy, are designed to deal with imbalanced dataset, no new research on improving Binary Cross Entropy for balanced dataset, and all loss function treat each pixel in isolation, without considering the spatial correlation between pixel and its neighbor pixels. To solve this problem, a new loss function, named NeighborLoss function, is proposed. Firstly, the deep learning network is used to get the prediction results of each pixel. According to whether the prediction results of the eight neighboring pixels of each pixel are consistent with the each pixel prediction, different weights are given to each pixel. Finally, the weighted average value of cross entropy of all pixels in the batch is taken as the final loss function value. We use the main deep learning semantic segmentation networks SegNet, PSPNet, UNET ++, MUNet with both NeighborLoss and cross entropy loss respectively to extract houses on the open data set named WHU dataset for remote sensing. The results show that compared with cross entropy loss functions, the MIoU, Precision, Recall, and Accuracy of NeighborLoss function are improved. From the predicted graph, the NeighborLoss function is more accurate to extract the edge of the house, especially in the corner of the house. NeighborLoss function is a more effective loss function for remote sensing image segmentation.",Entropy,Image segmentation,Semantics,Remote sensing,Deep learning,,,,,IEEE ACCESS,,Feature extraction,Correlation,Deep learning,loss function,remote sensing image,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_307,"Lopez, Josue","Santos, Stewart","Atzberger, Clement","Torres, Deni",Convolutional Neural Networks for Semantic Segmentation of Multispectral Remote Sensing Images,2018 IEEE 10TH LATIN-AMERICAN CONFERENCE ON COMMUNICATIONS (IEEE LATINCOM),2018,15,"The recent impulse in development of artificial intelligence (AI) methodologies has simplified the application of this in multiple research areas. This simplification was not favorable before, due to the limitations in dimensionality, processing time, computational resources, among others. Working with multispectral remote sensing (RS) images, in an artificial neural network (NN) was quite complex. Due the methods used required millions of processes that took a long time to be executed and produce competitive results compared with the state of the art (SoA). Deep learning (DL) strategies have been applied to alleviate these limitations and have greatly improved the use of neural networks. Therefore, this paper presents the analysis of DL-NNs to perform semantic segmentation of multispectral RS images. Images are captured by the constellation of satellites Sentinel-2 from the European Space Agency. The objective of this research is to classify each pixel of a scene into five categories: 1-vegetation, 2-soil, 3-water, 4-clouds and 5-cloud shadows. The selection of spectral bands for the formation of input datasets for segmentation of these classes is very important. The spectral signatures of each material aid to discern among several classes. Results presented in this work, show that the AI strategy proposed offer better accuracy segmentation than other methods of the SoA in competitive processing time.",Convolutional neural networks,multispectral images,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_308,"Lu, Xiao","Jiang, Zhiguo","Zhang, Haopeng",,Weakly Supervised Remote Sensing Image Semantic Segmentation With Pseudo-Label Noise Suppression,,2024,1,"Semantic segmentation of remote sensing images (RSIs) plays a crucial role in various applications, including urban planning and environmental monitoring. However, the high cost and complexity of obtaining detailed annotations for RSIs pose significant challenge. This issue necessitates the exploration of weakly supervised learning as an effective alternative, which utilizes more readily available, less granular forms of labeling. Yet, weakly supervised approaches face their own set of challenges, primarily due to scarcity of precise pixel-level labels which significantly hampers the model's ability to learn accurate representations. In this article, we introduce a weakly supervised semantic segmentation (WSSS) approach for RSIs that leverages self-supervised learning (SSL) and pseudo-label noise mitigation to address these challenges. Our method leverages a self-supervised encoder for providing similarity information, which enhances feature representation in RSIs and enables the generation of more accurate pseudo-labels, thus reducing the noise in the pseudo-labels. Furthermore, we propose a refined loss function that incorporates gradient clipping and label smoothing to mitigate the impact of noisy labels, thereby improving the robustness and accuracy of the segmentation results. Extensive experiments on the ISPRS Potsdam, ISPRS Vaihingen, and iSAID datasets demonstrate that our approach achieves state-of-the-art (SOTA) performance, closely matching that of fully supervised methods. Our method not only reduces the dependency on expensive pixel-level annotations but also showcases the potential of SSL in enhancing WSSS tasks.",Accuracy,Remote sensing,Semantics,Semantic segmentation,Task analysis,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Noise,Image-level label,pseudo-label noise suppression,remote sensing images (RSIs),self-supervised learning (SSL),weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,,,,,,,,,
Row_309,"Lu, Xiaoqiang","Jiao, Licheng","Liu, Fang","Yang, Shuyuan",Simple and Efficient: A Semisupervised Learning Framework for Remote Sensing Image Semantic Segmentation,,2022,19,"Semantic segmentation based on deep learning has achieved impressive results in recent years, but these results are supported by a large amount of labeled data, which requires intensive annotation at the pixel level, particularly for high-resolution remote sensing (RS) images. In this work, we propose a simple yet efficient semisupervised learning framework based on linear sampling (LS) self-training, named LSST, to improve the performance of RS image semantic segmentation. Specifically, the classical pseudolabeling-based self-training paradigm is enhanced by injecting strong data augmentations (SDAs) applicable to RS images, based on which a powerful baseline is constructed. Nevertheless, the problem of insufficient data training to generate pseudolabels with a high level of noise persists, and the noisy pseudolabels will continue to accumulate and impede model improvement during the retraining phase. Previous works commonly employ a predefined threshold to remove noise, but it will lead to overfitting the model to easily identified classes. To address it, a method using LS is presented for assigning thresholds to different classes in an adaptive manner, which provides noiseless regions for retraining. Experiments prove that the proposed pixelwise selection is more available for segmentation than image-level selection in RS images. Finally, LSST achieves state of the art on several datasets and different evaluation metrics.",Data augmentation,remote sensing (RS) images,self-training,semantic segmentation,semisupervised learning (SSL),"Liu, Xu","Feng, Zhixi","Li, Lingling","Chen, Puhua",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_310,"Zhang, Li","Tan, Zhenshan","Zhang, Guo","Zhang, Wen",Learn More and Learn Usefully: Truncation Compensation Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,2024,3,"Semantic segmentation of high-resolution remote-sensing images (HR-RSIs) focuses on classifying each pixel of input images. Recent methods have incorporated a downscaled global image as supplementary input to alleviate global context loss from cropping. Nonetheless, these methods encounter two key challenges: diminished detail in features due to downsampling of the global auxiliary image (GAI) and noise from the same image that reduces the network's discriminability of useful and useless information. To overcome these challenges, we propose a truncation compensation network (TCNet) for HR-RSI semantic segmentation. TCNet features three pivotal modules: the guidance feature extraction module (GFM), the related-category semantic enhancement module (RSEM), and the global-local contextual cross-fusion module (CFM). GFM focuses on compensating for truncated features in the local image and minimizing noise to emphasize learning of useful information. RSEM enhances discernment of global semantic information by predicting spatial positions of related categories and establishing spatial mappings for each. CFM facilitates local image semantic segmentation with extensive contextual information by transferring information from global to local feature maps. Extensive testing on the ISPRS, BLU, and GID datasets confirms the superior efficiency of TCNet over other approaches.",Semantic segmentation,Semantics,Remote sensing,Feature extraction,Aggregates,"Li, Zhijiang",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Transformers,Decoding,Context fusion,remote-sensing images,semantic enhancement,semantic segmentation,useful information,,,,,,,,,,,,,,,,,,,,,,,
Row_311,"Fan, Junyu","Li, Jinjiang","Hua, Zhen","Zhang, Fan",Elevation Information-Guided Multimodal Fusion Robust Framework for Remote Sensing Image Segmentation,,2024,1,"Currently, the task of remote sensing image segmentation still faces some challenges, such as variations in illumination, shadows, and occlusions present in remote sensing images. In addition, there may be similarities and confusions between different types of terrain features. In this letter, we aim to explore how to use information exchange between multiple modalities to reduce the impact of interfering factors. To fully exploit the complementary information between different modalities, we establish an information exchange mechanism between optical images (visible light + infrared) features and digital surface model (DSM) features. This allows them to interact and express themselves in a shared feature space, facilitating the acquisition of complementary information from different modalities. Furthermore, through a multimodal fusion encoder and decoder based on transformer design, the optical features and DSM features are integrated, enabling the learning of high-level semantic representations in different dimensions. Extensive subjective, objective comparative experiments, and ablation experiments are conducted on the ISPRS Vaihingen and Potsdam datasets to evaluate the proposed method. The mIoU on the Vaihingen and Potsdam datasets reached 85.06% and 87.6%, respectively, while the OA reached 92.01% and 91.92%, respectively. The source code will be available at https://github.com/JunyuFan/MIEFNet.",Multimodal,remote sensing,semantic segmentation,transformer,,"Zhang, Caiming",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_312,"Ding, Lei","Zhang, Jing","Bruzzone, Lorenzo",,Semantic Segmentation of Large-Size VHR Remote Sensing Images Using a Two-Stage Multiscale Training Architecture,,AUG 2020,106,"Very-high resolution (VHR) remote sensing images (RSIs) have significantly larger spatial size compared to typical natural images used in computer vision applications. Therefore, it is computationally unaffordable to train and test classifiers on these images at a full-size scale. Commonly used methodologies for semantic segmentation of RSIs perform training and prediction on cropped image patches. Thus, they have the limitation of failing to incorporate enough context information. In order to better exploit the correlations between ground objects, we propose a deep architecture with a two-stage multiscale training strategy that is tailored to the semantic segmentation of large-size VHR RSIs. In the first stage of the training strategy, a semantic embedding network is designed to learn high-level features from downscaled images covering a large area. In the second training stage, a local feature extraction network is designed to introduce low-level information from cropped image patches. The resulting training strategy is able to fuse complementary information learned from multiple levels to make predictions. Experimental results on two data sets show that it outperforms local-patch-based training models in terms of both accuracy and stability.",Convolutional neural network,deep learning,remote sensing,semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_313,"Wang, Xin","Jing, Shihan","Dai, Huifeng","Shi, Aiye",High-resolution remote sensing images semantic segmentation using improved UNet and SegNet,,MAY 2023,10,"Semantic segmentation for high-resolution (HR) remote sensing (RS) images under the condition of a small number of training samples with imprecise labels is a challenging task. In this paper, we propose a novel method based on improved UNet and SegNet. First, a batch normalization layer is introduced into the original UNet to accelerate the convergence speed and an ELU activation function is selected instead of ReLU to avoid the neuron death. Second, to enhance the conventional SegNet, the encoder is reconstructed and a skip connection is designed to reuse the deep features, which is also beneficial for the network convergence. Third, a joint model is constructed by combining the improved UNet and SegNet and meanwhile a voting strategy is proposed to compute the final results. Experiments on the real-world HR RS images verify the effectiveness and superiority of the proposed method.",Remote sensing,Semantic segmentation,Deep learning,UNet,SegNet,,,,,COMPUTERS & ELECTRICAL ENGINEERING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_314,"Chang, Zhanyuan","Xu, Mingyu","Wei, Yuwen","Lian, Jie",UNeXt: An Efficient Network for the Semantic Segmentation of High-Resolution Remote Sensing Images,,OCT 2024,0,"The application of deep neural networks for the semantic segmentation of remote sensing images is a significant research area within the field of the intelligent interpretation of remote sensing data. The semantic segmentation of remote sensing images holds great practical value in urban planning, disaster assessment, the estimation of carbon sinks, and other related fields. With the continuous advancement of remote sensing technology, the spatial resolution of remote sensing images is gradually increasing. This increase in resolution brings about challenges such as significant changes in the scale of ground objects, redundant information, and irregular shapes within remote sensing images. Current methods leverage Transformers to capture global long-range dependencies. However, the use of Transformers introduces higher computational complexity and is prone to losing local details. In this paper, we propose UNeXt (UNet+ConvNeXt+Transformer), a real-time semantic segmentation model tailored for high-resolution remote sensing images. To achieve efficient segmentation, UNeXt uses the lightweight ConvNeXt-T as the encoder and a lightweight decoder, Transnext, which combines a Transformer and CNN (Convolutional Neural Networks) to capture global information while avoiding the loss of local details. Furthermore, in order to more effectively utilize spatial and channel information, we propose a SCFB (SC Feature Fuse Block) to reduce computational complexity while enhancing the model's recognition of complex scenes. A series of ablation experiments and comprehensive comparative experiments demonstrate that our method not only runs faster than state-of-the-art (SOTA) lightweight models but also achieves higher accuracy. Specifically, our proposed UNeXt achieves 85.2% and 82.9% mIoUs on the Vaihingen and Gaofen5 (GID5) datasets, respectively, while maintaining 97 fps for 512 x 512 inputs on a single NVIDIA GTX 4090 GPU, outperforming other SOTA methods.",high-resolution remote sensing images,real-time semantic segmentation,convolutional attention,global-local context,transformer,"Zhang, Chongming","Li, Chuanjiang",,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_315,"Zhou, Shunping","Feng, Yuting","Li, Shengwen","Zheng, Daoyuan",DSM-Assisted Unsupervised Domain Adaptive Network for Semantic Segmentation of Remote Sensing Imagery,,2023,11,"The semantic segmentation of high-resolution remote sensing imagery (RSI) is an essential task for many applications. As a promising unsupervised learning method, unsupervised domain adaptation (UDA) methods remarkably contribute to the advancement of high-resolution RSI semantic segmentation. Previous methods focus on reducing the domain shift of orthophotos, suffering from some limitations because the available information in orthophotos is relatively homogeneous. This article proposes a framework to introduce digital surface model (DSM) data for the unsupervised semantic segmentation of RSI. The proposed method combines RSI with DSM through two modules, namely, multipath encoder (MPE) and multitask decoder (MTD), and aligns global data distribution in the source and target domains with a UDA module. A refined postfusion (RPF) module is proposed in the inference phase to exploit the height information fully for refining the segmentation results. Specifically, MPE is designed to utilize RSI and DSM to train the segmentation network jointly, which iteratively fuses RSI and DSM features at multiple levels to enhance their feature representations. MTD is designed to produce fusion prediction maps by filtering interference information of DSM and yielding accurate segmentation masks of DSM and RSI. Experimental results show that the proposed method substantially improves the semantic segmentation performance on high-resolution RSI and outperforms state-of-the-art methods. This article provides a methodological reference for fusing multimodal data in various RSI-based unsupervised tasks.",Semantic segmentation,Task analysis,Semantics,Remote sensing,Geology,"Fang, Fang","Liu, Yuanyuan","Wan, Bo",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Feature extraction,Data models,High-resolution remote sensing imagery (RSI),refined postfusion (RPF),semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,,,
Row_316,"Liu, Yuheng","Wang, Ye","Zhang, Yifan","Mei, Shaohui",Cross-City Semantic Segmentation (C2Seg) in Multimodal Remote Sensing: Outcome of the 2023 IEEE WHISPERS C2Seg Challenge,,2024,1,"Given the ever-growing availability of remote sensing data (e.g., Gaofen in China, Sentinel in the EU, and Landsat in the USA), multimodal remote sensing techniques have been garnering increasing attention and have made extraordinary progress in various Earth observation (EO)-related tasks. The data acquired by different platforms can provide diverse and complementary information. The joint exploitation of multimodal remote sensing has been proven effective in improving the existing methods of land-use/land-cover segmentation in urban environments. To boost technical breakthroughs and accelerate the development of EO applications across cities and regions, one important task is to build novel cross-city semantic segmentation models based on modern artificial intelligence technologies and emerging multimodal remote sensing data. This leads to the development of better semantic segmentation models with high transferability among different cities and regions. The Cross-City Semantic Segmentation contest is organized in conjunction with the 13th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS).",Artificial intelligence (AI),cross-city,deep learning,hyperspectral,land cover,"Zou, Jiaqi","Li, Zhuohong","Lu, Fangxiao","He, Wei",IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Zhang, Hongyan",multimodal benchmark datasets,remote sensing,semantic segmentation,,,,,,,"Zhao, Huilin","Chen, Chuan","Xia, Cong","Li, Hao","Vivone, Gemine","Haensch, Ronny","Taskin, Gulsen","Yao, Jing","Qin, A. K.","Zhang, Bing","Chanussot, Jocelyn","Hong, Danfeng",,,,,,,,,
Row_317,"Lu, Junzhe","He, Guangjun","Dou, Hongkun","Gao, Qing",ScoreSeg: Leveraging Score-Based Generative Model for Self-Supervised Semantic Segmentation of Remote Sensing,,2023,3,"The performance of semantic segmentation of remote sensing images (RSIs) heavily depends on the number of pixel-level annotations. In practice, the accumulation of pixel-level annotations for large RSIs is quite expensive or even impossible under certain scenarios. Here, we try to solve this data-intensive problem from the novel aspect of score-based self-supervise learning (SSL) and introduce a robust RSI semantic segmentation model called ScoreSeg. Unlike traditional pixel-level SSL paradigms, the generative SSL mechanism in ScoreSeg is simple in loss design and stable in pretraining, granting it an indispensable ability in dense feature learning from very large RSIs. In the model implementation, ScoreSeg first extracts pixelwise representations of RSIs by pretraining a time-dependent score-based model on abundant off-the-shelf unlabeled RSIs. Then, to address the sparse feature problem in RSIs, the collected features from different timesteps and resolutions are aggregated together forming a rich feature map for downstream semantic segmentation. Experimental results on three datasets show that our proposed ScoreSeg outperforms state-of-the-art (SOTA) SSL methods and alternative pretraining models on ImageNet by nontrivial margins, especially with very limited annotations.",Remote sensing,score-based generative model,self-supervised learning,semantic segmentation,,"Fang, Leyuan","Deng, Yue",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_318,"Zhang, Hua","Jiang, Zhengang","Zheng, Guoxun","Yao, Xuekun",Semantic Segmentation of High-Resolution Remote Sensing Images with Improved U-Net Based on Transfer Learning,,NOV 14 2023,5,"Semantic segmentation of high-resolution remote sensing images has emerged as one of the foci of research in the remote sensing field, which can accurately identify objects on the ground and determine their localization. In contrast, the traditional deep learning-based semantic segmentation, on the other hand, requires a large amount of annotated data, which is unsuitable for high-resolution remote sensing tasks with limited resources. It is therefore important to build a semantic segmentation method for high-resolution remote sensing images. In this paper, it is proposed an improved U-Net model based on transfer learning to solve the semantic segmentation problem of high-resolution remote sensing images. The model is based on the symmetric encoder-decoder structure of U-Net. For the encoder, transfer learning is applied and VGG16 is used as the backbone of the feature extraction network, and in the decoder, after upsampling using bilinear interpolation, it is performed multiscale fusion with the feature maps of the corresponding layers of the encoder in turn and is finally obtained the predicted value of each pixel to achieve precise localization. To verify the efficacy of the proposed network, experiments are performed on the ISPRS Vaihingen dataset. The experiments show that the applied method has achieved high-quality semantic segmentation results on the high-resolution remote sensing dataset, and the MIoU is 1.70%, 2.20%, and 2.33% higher on the training, validation, and test sets, respectively, and the IoU is 4.26%, 6.89%, and 5.44% higher for the automotive category compared to the traditional U-Net.",Deep learning,Convolutional neural network,Semantic segmentation,Transfer learning,High-resolution remote sensing images,,,,,INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_319,"Li, Zhenshi","Zhang, Xueliang","Xiao, Pengfeng","Zheng, Zixian",On the Effectiveness of Weakly Supervised Semantic Segmentation for Building Extraction From High-Resolution Remote Sensing Imagery,,2021,48,"A critical obstacle to achieve semantic segmentation of remote sensing images by the deep convolutional neural network is the requirement of huge pixel-level labels. Taking building extraction as an example, this study focuses on how to effectively apply weakly supervised semantic segmentation (WSSS) to high-resolution remote sensing (HR) images with image-level labels, which is a prominent solution for the huge labeling challenge. The widely used two-step WSSS framework is adopted, in which the pseudo-masks are first produced from image-level labels and followed by a segmentation network trained by the pseudo-masks. In addition, the fully connected conditional random field (CRF) is utilized to explore spatial context in both training and prediction stages. Detailed analyzes are implemented on applying WSSS on HR images in terms of producing pseudo-masks, training segmentation network, and optimizing predictions. We show that the tradeoff between precision and recall of pseudo-masks, as well as the boundary accuracy and the background, needs to be carefully considered. The benefits of the segmentation network in the two-step framework are demonstrated in comparison to using classification network only for WSSS, and the effects of CRF-loss are identified to be powerful for improving the segmentation network while it is not appropriate for dense buildings. An overlapping strategy and CRF postprocessing are further demonstrated to be effective for optimizing the segmentation results during inferencing. Through deliberate settings, we can generate results comparable to fully supervised on the ISPRS Potsdam and Vaihingen dataset, which is meaningful for promoting WSSS applications for extracting geographic information from HR images.",Image segmentation,Training,Buildings,Remote sensing,Semantics,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Feature extraction,Data mining,Building extraction,fully convolutional network,high-resolution remote sensing imagery,weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,,,,,,,,,,
Row_320,"Gao, Zhi","Sun, Wenbo","Lu, Yao","Zhang, Yichen",Joint Learning of Semantic Segmentation and Height Estimation for Remote Sensing Image Leveraging Contrastive Learning,,2023,9,"Semantic segmentation (SS) and height estimation (HE) are two critical tasks in remote sensing scene understanding that are highly correlated with each other. To address both the tasks simultaneously, it is natural to consider designing a unified deep learning model that aims to improve performance by jointly learning complementary information among the associated tasks. In this article, we learn the two tasks jointly under a deep multitask learning (MTL) framework and propose two novel objective functions, called cross-task contrastive (CTC) loss and cross-pixel contrastive (CPC) loss, respectively, to enhance MTL performance through contrastive learning. Specifically, the CTC loss is designed to maximize the mutual information of different task features and enforce the model to learn the consistency between SS and height estimation. In addition, our method goes beyond previous approaches that only apply contrastive learning at the instance level. Instead, we design a pixelwise contrastive loss function that pulls together pixel embeddings belonging to the same semantic class, while pushing apart pixel embeddings from different semantic classes. Furthermore, we find that this semantic-guided contrastive loss simultaneously improves the performance of the HE task. Our proposed approach is simple and effective and does not introduce any additional overhead to the model during the testing phase. We extensively evaluate our method on the Vaihingen and Potsdam datasets, and the experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods in both HE and SS.",Contrastive learning,height estimation,multi-task learning (MTL),remote sensing,semantic segmentation (SS),"Song, Weiwei","Zhang, Yongjun","Zhai, Ruifang",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_321,"Tan, Xiaowei","Xiao, Zhifeng","Zhang, Yanru","Wang, Zhenjiang",Context-Driven Feature-Focusing Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,MAR 2023,0,"High-resolution remote sensing images (HRRSIs) cover a broad range of geographic regions and contain a wide variety of artificial objects and natural elements at various scales that comprise different image contexts. In semantic segmentation tasks based on deep convolutional neural networks (DCNNs), different resolution features are not equally effective for extracting ground objects with different scales. In this article, we propose a novel context-driven feature-focusing network (CFFNet) aimed at focusing on the multi-scale ground object in fused features of different resolutions. The CFFNet consists of three components: a depth-residual encoder, a context-driven feature-focusing (CFF) decoder, and a classifier. First, features with different resolutions are extracted using the depth-residual encoder to construct a feature pyramid. The multi-scale information in the fused features is then extracted using the feature-focusing (FF) module in the CFF decoder, followed by computing the focus weights of different scale features adaptively using the context-focusing (CF) module to obtain the weighted multi-scale fused feature representation. Finally, the final results are obtained using the classifier. The experiments are conducted on the public LoveDA and GID datasets. Quantitative and qualitative analyses of state-of-the-art (SOTA) segmentation benchmarks demonstrate the rationality and effectiveness of the proposed approach.",deep learning,feature-focusing,semantic segmentation,remote sensing,attention,"Qi, Xiaole","Li, Deren",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_322,"Hu, Lei","Zhou, Xun","Ruan, Jiachen","Li, Supeng",ASPP+-LANet: A Multi-Scale Context Extraction Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,MAR 2024,3,"Semantic segmentation of remote sensing (RS) images is a pivotal branch in the realm of RS image processing, which plays a significant role in urban planning, building extraction, vegetation extraction, etc. With the continuous advancement of remote sensing technology, the spatial resolution of remote sensing images is progressively improving. This escalation in resolution gives rise to challenges like imbalanced class distributions among ground objects in RS images, the significant variations of ground object scales, as well as the presence of redundant information and noise interference. In this paper, we propose a multi-scale context extraction network, ASPP+-LANet, based on the LANet for semantic segmentation of high-resolution RS images. Firstly, we design an ASPP+ module, expanding upon the ASPP module by incorporating an additional feature extraction channel, redesigning the dilation rates, and introducing the Coordinate Attention (CA) mechanism so that it can effectively improve the segmentation performance of ground object targets at different scales. Secondly, we introduce the Funnel ReLU (FReLU) activation function for enhancing the segmentation effect of slender ground object targets and refining the segmentation edges. The experimental results show that our network model demonstrates superior segmentation performance on both Potsdam and Vaihingen datasets, outperforming other state-of-the-art (SOTA) methods.",high-resolution remote sensing images,semantic segmentation,ASPP module,local attention network model,activation function,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_323,"Chen, Chao","Qian, Yurong","Liu, Hui","Yang, Guangqi",CLANET: a cross-linear attention network for semantic segmentation of urban scenes remote sensing images,,DEC 2 2023,0,"Semantic segmentation of high-resolution remote sensing images is important in land cover classification, road extraction, building extraction, water extraction, etc. However, high-resolution remote-sensing images have a lot of details. Due to the fixed receptive field of convolution blocks, it is impossible to model the correlation of global features. In addition, complex fusion methods cannot integrate spatial and global context information. In order to solve these problems, this paper proposes a cross-linear attention network (CLANet) to capture spatial and context information in images. The structure consists of a spatial branch and a context branch. The spatial branch is constructed by stacked convolution to better capture spatial information. The context branch models the global information based on the transformer deformation module. In addition, to effectively fuse spatial and context information, this paper also designs a feature fusion module (FFM), which uses a cross-linear attention mechanism for feature aggregation. Finally, this paper conducts many experiments on the ISPRS Vaihingen and the ISPRS Potsdam datasets. Among them, 82.28% of mIoU achieves on the ISPRS Vaihingen dataset. The experimental results show that CLANet has better performance and effect than the methods in recent years.",Remote sensing,semantic segmentation,cross attention,convolutional neural network,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_324,"He, Shumeng","Yang, Houqun","Zhang, Xiaoying","Li, Xuanyu",MFTransNet: A Multi-Modal Fusion with CNN-Transformer Network for Semantic Segmentation of HSR Remote Sensing Images,,FEB 2023,9,"Due to the inherent inter-class similarity and class imbalance of remote sensing images, it is difficult to obtain effective results in single-source semantic segmentation. We consider applying multi-modal data to the task of the semantic segmentation of HSR (high spatial resolution) remote sensing images, and obtain richer semantic information by data fusion to improve the accuracy and efficiency of segmentation. However, it is still a great challenge to discover how to achieve efficient and useful information complementarity based on multi-modal remote sensing image semantic segmentation, so we have to seriously examine the numerous models. Transformer has made remarkable progress in decreasing model complexity and improving scalability and training efficiency in computer vision tasks. Therefore, we introduce Transformer into multi-modal semantic segmentation. In order to cope with the issue that the Transformer model requires a large amount of computing resources, we propose a model, MFTransNet, which combines a CNN (convolutional neural network) and Transformer to realize a lightweight multi-modal semantic segmentation structure. To do this, a small convolutional network is first used for performing preliminary feature extraction. Subsequently, these features are sent to the multi-head feature fusion module to achieve adaptive feature fusion. Finally, the features of different scales are integrated together through a multi-scale decoder. The experimental results demonstrate that MFTransNet achieves the best balance among segmentation accuracy, memory-usage efficiency and inference speed.",semantic segmentation,high spatial resolution remote sensing images,transformer,multi-modal,,,,,,MATHEMATICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_325,"Chen, Junsong","Yi, Jizheng","Chen, Aibin","Lin, Hui",SRCBTFusion-Net: An Efficient Fusion Architecture via Stacked Residual Convolution Blocks and Transformer for Remote Sensing Image Semantic Segmentation,,2023,4,"Convolutional neural network (CNN) and transformer-based self-attention models have their advantages in extracting local information and global semantic information, and it is a trend to design a model combining stacked residual convolution blocks (SRCBs) and transformer. How to efficiently integrate the two mechanisms to improve the segmentation effect of remote sensing (RS) images is an urgent problem to be solved. An efficient fusion via SRCB and transformer (SRCBTFusion-Net) is proposed as a new semantic segmentation architecture for RS images. The SRCBTFusion-Net adopts an encoder-decoder structure and the Transformer is embedded into SRCB to form a double coding structure, then the coding features are upsampled and fused with multiscale features of SRCB to form a decoding structure. First, a semantic information enhancement module (SIEM) is proposed to get global clues for enhancing deep semantic information. Subsequently, the relationship guidance module (RGM) is incorporated to reencode the decoder's upsampled feature maps, enhancing the edge segmentation performance. Second, a multipath atrous self-attention module (MASM) is developed to enhance the effective selection and weighting of low-level features, effectively reducing the potential confusion introduced by the skip connections between low- and high-level features. Finally, a multiscale feature aggregation module (MFAM) is developed to enhance the extraction of semantic and contextual information, thus alleviating the loss of image feature information and improving the ability to identify similar categories. The proposed SRCBTFusion-Net's performance on the Vaihingen and Potsdam datasets is superior to the state-of-the-art methods. The code will be freely available at https://github.com/js257/SRCBTFusion-Net.",Multiscale feature,remote sensing (RS),semantic segmentation,transformer,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_326,"Dong, Zhe","Gao, Guoming","Liu, Tianzhu","Gu, Yanfeng",Distilling Segmenters From CNNs and Transformers for Remote Sensing Images' Semantic Segmentation,,2023,16,"Semantic segmentation is a crucial task in remote sensing and has been predominantly performed using convolutional neural networks (CNNs) for the past decade. Recently, transformers with self-attention mechanisms have demonstrated superior performance compared with CNNs. However, due to the locality of CNN and the high computational complexity and massive data resource requirements of transformer, neither of them can be well applied in resource-constrained practical remote sensing scenarios. Motivated by the limitations of using either CNNs or transformers alone in the task of semantic segmentation of remote sensing images, a novel cross-model knowledge distillation (KD) framework, named distilling segmenters from CNNs and transformers (DSCTs), is proposed in this article to harness the complementary advantages of both the models. The framework uses a channel-weighted attention-guided feature distillation (CAFD) module to condense the feature from the teacher model and enhance the student model's focus on the teacher-focused regions. In addition, a target-nontarget KD (TNKD) module is proposed that decouples logit distillation into target and nontarget KD to guide the student model in learning the underlying representations and decision boundaries from the teacher model. By learning the complementary knowledge from the teacher, our proposed DSCT framework improves the student's segmentation performance without adding trainable parameters. Experiments on four available remote sensing datasets (ISPRS Potsdam, Vaihingen, GID, and LoveDA) indicate that the proposed DSCT outperforms the state-of-the-art KD methods and demonstrates its effectiveness and robustness.",Convolutional neural networks (CNNs),knowl-edge distillation (KD),remote sensing images,semantic segmen-tation,transformer,"Zhang, Xiangrong",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_327,"Chen, Xiaoshu","Pan, Shaoming","Chong, Yanwen",,Unsupervised Domain Adaptation for Remote Sensing Image Semantic Segmentation Using Region and Category Adaptive Domain Discriminator,,2022,23,"By reason of factors such as terrains, weather conditions, sensor imaging methods, and cultural and economic development, there is a large shift between the remote sensing imagery collected from different geographic locations and different sensors, which makes the state-of-the-art semantic segmentation models trained on source domain (an image set gathered from specific geographic locations and sensors) difficult to generalize to target domain (another image set collected from other geographic locations and sensors). Currently, unsupervised domain adaptation (UDA) using adversarial training, whose purpose is to align the marginal distribution in the output space between the source and target domains, is the most explored and practical approach to address this issue. However, this global alignment approach does not take into account diversities of different regions in a specific image nor the category-level distribution, which leads to the consequence that some regions and categories which are already well aligned between the source and target domains may be incorrectly remapped. Therefore, we propose a region and category adaptive domain discriminator (RCA-DD), aiming to emphasize the differences in regions and categories during the process of alignment. Specifically, on the one hand, we propose an entropy-based regional attention module (ERAM) in domain discriminator to emphasize the importance of difficult-to-align regions. On the other hand, we propose a class-clear module (CCM) to update only the distribution of existing categories in one iteration without affecting all categories. Finally, a lot of experiments are introduced to indicate that the proposed method can obtain better results when compared with other state-of-the-art UDA methods using adversarial training.",Image segmentation,Semantics,Remote sensing,Training,Adaptation models,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Entropy,Predictive models,Remote sensing image processing,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,,,,
Row_328,"Bousbih, Safa","Chan-Hon-Tong, Adrien","Lenczner, Gaston",,WHAT COULD WE LEARN FROM MANY DATASETS IN REMOTE SENSING ROOF SEMANTIC SEGMENTATION?,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,1,"Deep learning models trained for roof semantic segmentation from remote sensing images may suffer from low performance when applied in new cities as roof appearance may be different depending on the geographic areas. However, we claim in this paper that models learned on many different cities have better performances on a new city than a model learned on few data from this targeted city. This result may interest the community for industrial purpose: annotating few tiles of an area for simple supervised training is worthless in our experiments.",semantic segmentation,geographic robustness,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_329,"Wu, Zhipeng","Liu, Chang","Song, Bingze","Pei, Huaxin",Diff-HRNet: A Diffusion Model-Based High-Resolution Network for Remote Sensing Semantic Segmentation,,2025,0,"The semantic segmentation methods based on deep neural networks predominantly employ supervised learning, relying heavily on the quantity and quality of annotated samples. Due to the complexity of high-resolution remote sensing imagery, obtaining sufficient and precise pixel-level labeled data is highly challenging. This letter introduces a novel self-supervised learning method using a pretrained denoising diffusion probabilistic model (DDPM) to leverage semantic information from large-scale unlabeled remote sensing imageries. Building on this, a multistage fusion scheme between pretrained features and high-resolution features is proposed, enabling the network to learn more effective strategies to leverage prior information provided by the pretrained model while preserving the rich semantic details of high-resolution images. Experimental results on two remote sensing semantic segmentation datasets show that the proposed Diff-HRNet outperforms all compared methods, demonstrating the potential of pretrained diffusion models in extracting crucial feature representations for semantic segmentation tasks.",Feature extraction,Remote sensing,Semantics,Spatial resolution,Diffusion models,"Li, Pinjie","Chen, Mengshuo",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Noise reduction,Noise measurement,Convolution,Supervised learning,Denoising diffusion probabilistic model (DDPM),self-supervised learning,semantic segmentation,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,
Row_330,"Zhang, Shijie","Zhang, Bin","Wu, Yuntao","Zhou, Huabing",SegCLIP: Multimodal Visual-Language and Prompt Learning for High-Resolution Remote Sensing Semantic Segmentation,,2024,0,"Remote sensing semantic segmentation is considered a key step in the intelligent interpretation of high-resolution remote sensing (HRRS) images, with widespread applications in fields such as hazard assessment, environmental monitoring, and urban planning. Recently, numerous deep learning-based semantic segmentation methods have emerged, achieving significant breakthroughs. However, the majority of current research still concentrates on representation learning in the visual feature space, with the potential of multimodal data sources yet to be fully explored. In recent years, the foundational visual language model, namely contrastive language-image pretraining (CLIP), has established a new paradigm in the visual field, demonstrating excellent generalization capabilities and deep semantic understanding across a variety of tasks. Inspired by prompt learning, we propose a prompting approach based on linguistic descriptions to enable CLIP to generate semantically distinct contextual information for remote sensing images. We introduce the SegCLIP network architecture, a novel framework specifically designed for semantic segmentation of HRRS images. Specifically, we have adapted CLIP to extract text information, thereby guiding the visual model in distinguishing among classes. Additionally, we have designed a cross-modal feature fusion (CFF) module that integrates linguistic and visual semantic features, ensuring semantic consistency across modalities. Finally, we have fully exploited the potential of text data and have used additional real text to refine ambiguous query features. Experimental evaluations confirm that the method exhibits superior performance on the LoveDA, iSAID, and UAVid public semantic segmentation datasets.",Semantic segmentation,Remote sensing,Semantics,Visualization,Transformers,"Jiang, Junjun","Ma, Jiayi",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Linguistics,Feature extraction,Sensors,Accuracy,Laser radar,Attention mechanism,contrastive language-image pretraining (CLIP),prompt learning,remote sensing,,,,,,,,,,,,,semantic segmentation,,,,,,,,
Row_331,"Liu, Kuan-Hsien","Lin, Bo-Yen",,,MSCSA-Net: Multi-Scale Channel Spatial Attention Network for Semantic Segmentation of Remote Sensing Images,,SEP 2023,13,"Although deep learning-based methods for semantic segmentation have achieved prominent performance in the general image domain, semantic segmentation for high-resolution remote sensing images remains highly challenging. One challenge is the large image size. High-resolution remote sensing images can have very high spatial resolution, resulting in images with hundreds of millions of pixels. This makes it difficult for deep learning models to process the images efficiently, as they typically require large amounts of memory and computational resources. Another challenge is the complexity of the objects and scenes in the images. High-resolution remote sensing images often contain a wide variety of objects, such as buildings, roads, trees, and water bodies, with complex shapes and textures. This requires deep learning models to be able to capture a wide range of features and patterns to segment the objects accurately. Moreover, remote sensing images can suffer from various types of noise and distortions, such as atmospheric effects, shadows, and sensor noises, which can also increase difficulty in segmentation tasks. To deal with the aforementioned challenges, we propose a new, mixed deep learning model for semantic segmentation on high-resolution remote sensing images. Our proposed model adopts our newly designed local channel spatial attention, multi-scale attention, and 16-piece local channel spatial attention to effectively extract informative multi-scale features and improve object boundary discrimination. Experimental results with two public benchmark datasets show that our model can indeed improve overall accuracy and compete with several state-of-the-art methods.",channel spatial attention,encoder-decoder,multi-scale attention,remote sensing image,semantic segmentation,,,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_332,"Chen, Yan","Zhang, Qianchuan","Wang, Xiaofeng","Dong, Quan",AANet: Adaptive Attention Networks for Semantic Segmentation of High-Resolution Remote Sensing Imagery,,2024,0,"Contextual information can effectively aid deep-learning models in extracting interclass and intraclass difference features in remote sensing images. This article presents a novel approach called the adaptive attention network (AANet) for semantic segmentation in high-resolution remote sensing images. The proposed AANet aims to enhance the segmentation performance while minimizing the network's computational and parametric aspects. Furthermore, the AANet is designed to facilitate real-time segmentation. The AANet involves the construction of three distinct modules, namely the multiscale channel attention module (MCAM), the multidimensional spatial attention module (MSAM), and the contextual information adaptive fusion module (CIAFM). MCAM enhances a multiscale approach to effectively capture contextual information from neighboring channels and category information. MSAM is designed to extract and combine detailed information from each dimension of the spatial domain. CIAFM focuses on the complementary nature of channel and spatial context information and the correlation between pixels and categories. The methodology employed in this article involved conducting experiments on the ISPRS Vaihingen, ISPRS Potsdam, and multiobject coastal supervision semantic segmentation dataset (MO-CSSSD) datasets alongside a comparative analysis with conventional semantic segmentation models. The results of the article indicate that our approach demonstrates exceptional performance on the ISPRS Vaihingen dataset, ISPRS Potsdam dataset, and MO-CSSSD dataset, achieving mean intersection over union scores of 83.17%, 85.67%, and 89.68%, respectively.",Feature extraction,Data mining,Remote sensing,Semantic segmentation,Convolution,"Kang, Menglei","Jiang, Wenxiang","Wang, Mengyuan","Xu, Lixiang",IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Zhang, Chen",Semantics,Attention mechanisms,Adaptive attention,contextual information,high-resolution remote sensing imagery,multidimensional spatial attention,multiscale channel attention,,,,,,,,,,,,,,,,,,,,,,,
Row_333,"Zhang, Cheng","Jiang, Wanshou","Zhang, Yuan","Wang, Wei",Transformer and CNN Hybrid Deep Neural Network for Semantic Segmentation of Very-High-Resolution Remote Sensing Imagery,,2022,178,"This article presents a transformer and convolutional neural network (CNN) hybrid deep neural network for semantic segmentation of very high resolution (VHR) remote sensing imagery. The model follows an encoder-decoder structure. The encoder module uses a new universal backbone Swin transformer to extract features to achieve better long-range spatial dependencies modeling. The decoder module draws on some effective blocks and successful strategies of CNN-based models in remote sensing image segmentation. In the middle of the framework, an atrous spatial pyramid pooling block based on depthwise separable convolution (SASPP) is applied to obtain a multiscale context. A U-shaped decoder is used to gradually restore the size of the feature maps. Three skip connections are built between the encoder and decoder feature maps of the same size to maintain the transmission of local details and enhance the communication of multiscale features. A squeeze-and-excitation (SE) channel attention block is added before segmentation for feature augmentation. An auxiliary boundary detection branch is combined to provide edge constraints for semantic segmentation. Extensive ablation experiments were conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam benchmarks to test the effectiveness of multiple components of the network. At the same time, the proposed method is compared with the current state-of-the-art methods on the two benchmarks. The proposed hybrid network achieved the second highest overall accuracy (OA) on both the Potsdam and Vaihingen benchmarks (code and models are available at https://github.com/zq7734509/mmsegmentation- multilayer).",Transformers,Semantics,Image segmentation,Feature extraction,Remote sensing,"Zhao, Qing","Wang, Chenjie",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Decoding,Convolutional neural networks,Boundary detection,semantic segmentation,squeeze-and-excitation (SE) block,Swin transformer,very high resolution (VHR) remote sensing imagery,,,,,,,,,,,,,,,,,,,,,,,
Row_334,"Sun, Xian","Shi, Aijun","Huang, Hai","Mayer, Helmut",BAS4Net: Boundary-Aware Semi-Supervised Semantic Segmentation Network for Very High Resolution Remote Sensing Images,,2020,91,"Semantic segmentation is a fundamental task in remote sensing image understanding. Recently, Deep Convolutional Neural Networks (DCNNs) have considerably improved the performance of the semantic segmentation of natural scenes. However, it is still challenging for very high-resolution remote sensing images. Due to the large and complex scenes as well as the influence of illumination and imaging angle, it is particularly difficult for the existing methods to accurately obtain the category of pixels at object boundaries-the so-called boundary blur. We propose a framework called Boundary-Aware Semi-Supervised Semantic Segmentation Network (BAS(4)Net), which obtains more accurate segmentation results without additional annotation workload, especially at the object boundaries. The Channel-weighted Multi-scale Feature (CMF) module balances semantic and spatial information, and the Boundary Attention Module (BAM) weights the features with rich semantic boundary information to alleviate the boundary blur. Additionally, to decrease the amount of difficult and tedious manual labeling of remote sensing images, a discriminator network infers pseudolabels from unlabeled images to assist semisupervised learning, and further improves the performance of the segmentation network. To validate the effectiveness of the proposed framework, extensive experiments have been performed on both the ISPRS Vaihingen dataset, and the novel remote sensing dataset AIR-SEG with more categories, and complex boundaries. The results demonstrate a significant improvement of accuracy especially on boundaries and for small objects.",Hospitals,Collaboration,Lenses,Cybernetics,Systems thinking,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Economics,Boundary-aware,fully convolutional networks (FCNs),remote sensing images,semantic segmentation,semisupervised learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_335,"Lang, FengKai","Zhang, Ming","Zhao, JinQi","Zheng, NanShan",Semantic segmentation for multisource remote sensing images incorporating feature slice reconstruction and attention upsampling,,APR 17 2024,0,"Multisource remote sensing images have rich features and high interpretability and are widely employed in many applications. However, highly unbalanced category distributions and complex backgrounds have created some difficulties in the application of remote sensing image semantic segmentation tasks, such as low accuracy of small target segmentation and inaccurate edge extraction. To solve these problems, in this paper, a feature map segmentation reconstruction module and an attention upsampling module are proposed. In the encoder part, the input feature map is equally segmented, and the segmented feature map is enlarged to effectively improve the small target feature information expression ability in the model. In the decoder part, the key segmentation and location information of shallow features are obtained using the global view. The deep semantic information and shallow spatial location information are fully combined to achieve a more refined upsampling operation. In addition, the attention mechanism of the spatial and channel squeeze and excitation block (scSE) is applied to pay more attention to important features and to suppress irrelevant background and redundant information. To verify the effectiveness of the proposed method, the WHU-OPT-SAR dataset and six state-of-the-art algorithms are utilized in comparative experiments. The experimental results show that our model has demonstrated the best performance and low computational complexity. With only approximately half the floating-point operation count and the number of model parameters of the MCANet model, which is specially designed for the dataset, our model surpasses MCANet by 1.52% and 1.53% in terms of mean intersection over union (mIoU) and F1 score, respectively. In particular, for small object regions such as roads and other categories, compared to the baseline model, the IoU and F1 score of our model are improved by 5.27% and 3.99% and by 5.68% and 5.65%, respectively. These results demonstrate the superior performance of our model in terms of accuracy and efficiency.",Remote sensing images,feature slice reconstruction,attention upsampling,attention mechanism,semantic segmentation,"Shi, Hongtao",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_336,"Cui, Binge","Chen, Xin","Lu, Yan",,Semantic Segmentation of Remote Sensing Images Using Transfer Learning and Deep Convolutional Neural Network With Dense Connection,,2020,56,"Semantic segmentation is an important approach in remote sensing image analysis. However, when segmenting multiobject from remote sensing images with insufficient labeled data and imbalanced data classes, the performances of the current semantic segmentation models were often unsatisfactory. In this paper, we try to solve this problem with transfer learning and a novel deep convolutional neural network with dense connection. We designed a UNet-based deep convolutional neural network, which is called TL-DenseUNet, for the semantic segmentation of remote sensing images. The proposed TL-DenseUNet contains two subnetworks. Among them, the encoder subnetwork uses a transferring DenseNet pretrained on three-band ImageNet images to extract multilevel semantic features, and the decoder subnetwork adopts dense connection to fuse the multiscale information in each layer, which can strengthen the expressive capability of the features. We carried out comprehensive experiments on remote sensing image datasets with 11 classes of ground objects. The experimental results demonstrate that both transfer learning and dense connection are effective for the multiobject semantic segmentation of remote sensing images with insufficient labeled data and imbalanced data classes. Compared with several other state-of-the-art models, the kappa coefficient of TL-DenseUNet is improved by more than 0.0752. TL-DenseUNet achieves better performance and more accurate segmentation results than the state-of-the-art models.",Dense connection,transfer learning,remote sensing image,multiscale feature fusion,semantic segmentation,,,,,IEEE ACCESS,,UNet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_337,"Lv, Liang","Guo, Yiyou","Bao, Tengfei","Fu, Chenqin",MFALNet: A Multiscale Feature Aggregation Lightweight Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,DEC 2021,12,"Semantic segmentation labels each pixel in high-resolution remote sensing (HRRS) images with a category. To tackle with the large size and complexity of HRRS images, this letter presents a novel multiscale feature aggregation lightweight network (MFALNet) for semantic segmentation. Unlike standard convolution, asymmetric depth-wise separable convolution residual (ADCR) unit is used to reduce the parameter size of the network and makes the optimized structure deeper but lightweight and less complex. The proposed network is an encoder-decoder structure, where multiscale feature aggregation is implemented in both the encoder and the decoder. The spatial self-attention block helps to capture long-range contextual information, and the gated convolution modules are further used for refining features when aggerating high- and low-level feature maps in the decoder. The proposed MFALNet has evaluated on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam 2-D semantic labeling contest open benchmark data set, and the experimental results prove that the scheme can obtain a better tradeoff between segmentation accuracy and computational efficiency compared with the state-of-the-art semantic segmentation models.",Semantics,Convolution,Image segmentation,Kernel,Decoding,"Huo, Hong","Fang, Tao",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Remote sensing,Feature extraction,Aggregate,gated convolution (GC),lightweight network,remote sensing,semantic segmentation,spatial attention,,,,,,,,,,,,,,,,,,,,,,
Row_338,"Liu, Wenjie","Zhang, Yongjun","Yan, Jun","Zou, Yongjie",Semantic Segmentation Network of Remote Sensing Images With Dynamic Loss Fusion Strategy,,2021,4,"The remote sensing (RS) images are widely used in various industries, among which semantic segmentation of RS images is a common research direction. At the same time, because of the complexity of target information and the high similarity of features between the classes, this task is very challenging. In recent years, semantic segmentation algorithms of RS images have emerged in an endless stream, but most of them are improved around the scale features of the target, and the accuracy has great room for improvement. In this case, we propose a semantic segmentation framework for RS images with dynamic perceptual loss. The framework is improved based on the InceptionV-4 network to form a network that includes contextual semantic fusion and dual-channel atrous spatial pyramid pooling (ASPP). The semantic segmentation network is an encoder-decoder structure. In addition, we design a dynamic perceptual loss module and a dynamic loss fusion strategy by further observing the loss changes of the network, so as to better improve the classified details. Finally, experiment on the ISPRS 2D Semantic Labeling Contest Vaihingen Dataset and Massachusetts Building Dataset. Compared with some segmentation networks, our model has excellent performance.",Image segmentation,Semantics,Convolution,Feature extraction,Task analysis,"Cui, Zhongwei",,,,IEEE ACCESS,,Deep learning,Training,Remote sensing,semantic segmentation,perceptual loss,loss fusion,,,,,,,,,,,,,,,,,,,,,,,,
Row_339,"Zheng, Xiaoxiong","Chen, Tao",,,SEGMENTATION OF HIGH SPATIAL RESOLUTION REMOTE SENSING IMAGE BASED ON U-NET CONVOLUTIONAL NETWORKS,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,10,"With the rapid development of deep learning in recent years, the field of remote sensing image processing has gradually started to use some deep learning algorithms to achieve intelligent and fast processing of images, and the results have improved to a certain extent compared to traditional methods. The U-Net convolutional neural network was proposed used in medical image segmentation in 2015. Based on the previous work, we transferred the U-Net to remote sensing image segmentation to realize the pixel level semantic segmentation of remote sensing image end-to-end. Through UNet training and learning on GF-2 remote sensing image, the overall accuracy of training sets is 93.83%, while the overall accuracy of the test data is 82.27%, the kappa coefficient is 0.7721, and the Mean intersection Over Union (MiOU) is 0.6405. The results showed that the experiment has high segmentation accuracy and generalization ability.",High spatial resolution remote sensing image,Semantic segmentation,U-Net,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_340,"Barbato, Mirko Paolo","Piccoli, Flavio","Napoletano, Paolo",,Ticino: A multi-modal remote sensing dataset for semantic segmentation,,SEP 1 2024,2,"Multi-modal remote sensing (RS) involves the fusion of data from multiple sensors, such as RGB, Multispectral, Hyperspectral, Light Detection and Ranging, Synthetic Aperture Radar, etc., each capturing unique information across different regions of the electromagnetic spectrum. The fusion of different modalities can provide complementary information, allowing for a comprehensive understanding of the Earth's surface. Multi-modal RS image segmentation leverages various RS modalities to achieve pixel-level semantics classification. While deep learning has demonstrated promise in this domain, the limited availability of labeled multi-modal data poses a constraint on leveraging data-intensive techniques like deep learning to their full potential. To address this gap, we present Ticino, a novel multi-modal remote sensing dataset tailored for semantic segmentation. Ticino includes five modalities, including RGB, Digital Terrain Model, Panchromatic, and Hyperspectral images within the visual-near and short-wave infrared spectrum. Specifically annotated for Land Cover and Soil Agricultural Use, the dataset serves as a valuable resource for researchers in the field. Additionally, we conduct a comparative analysis, comparing single-modality with multi-modality deep learning techniques and evaluating the effectiveness of early fusion versus middle fusion approaches. This work aims to facilitate future research efforts in the domain by providing a robust benchmark dataset and insights into the effectiveness of various segmentation approaches.",Multi -modal remote sensing,Image semantic segmentation,Deep learning,Multi -modal dataset,Data fusion,,,,,EXPERT SYSTEMS WITH APPLICATIONS,,Hyperspectral imaging,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_341,"Wang, Chendan","Chen, Bowen","Zou, Zhengxia","Shi, Zhenwei",Remote Sensing Image Synthesis via Semantic Embedding Generative Adversarial Networks,,2023,5,"Generating photo-realistic remote sensing images conditioned on semantic masks has many practical applications like image editing, detecting deep fake geography, and data augmentation. Although previous methods achieved high-quality synthesis results for natural images like faces and everyday objects, they still underperform in remote sensing scenarios in terms of both visual fidelity and diversity. The high data imbalance and high semantic similarity of remote-sensing object categories make the semantic synthesis of remote sensing images more challenging than natural images. To tackle these challenges, we propose a novel method named conducted semantic embedding GAN (CSEBGAN) for semantic-controllable remote sensing image synthesis. The proposed method decouples different semantic classes into independent semantic embeddings, which explores the regularities between classes to improve visual fidelity and naturally supports semantic-level. We further introduce a novel tripartite cooperation adversarial training scheme that involves a conductor network to provide fine-grained semantic feedback for the generator. We also show that the proposed semantic image synthesis method can be utilized as an effective data augmentation approach on improving the performance of the downstream remote sensing image segmentation tasks. Extensive experiments show the superiority of our method compared with the state-of-the-art image synthesis methods.",Semantics,Remote sensing,Image synthesis,Generators,Training,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Image segmentation,Visualization,Generative adversarial networks,image segmentation,remote sensing images,semantic image synthesis,,,,,,,,,,,,,,,,,,,,,,,,
Row_342,"Liu, Siyu","He, Changtao","Bai, Haiwei","Zhang, Yijie",LIGHT-WEIGHT ATTENTION SEMANTIC SEGMENTATION NETWORK FOR HIGH-RESOLUTION REMOTE SENSING IMAGES,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,12,"Semantic segmentation of high-resolution remote sensing (HRRS) images becomes more and more important at present. Popular approaches use deep learning to solve this task, which depends on a large amount of labeled data and powerful computing resources. When computing resources or the labeled data are insufficient, their performance will be severely degraded. To deal with this problem, we proposed a light-weight network with attention modules for semantic segmentation of HRRS images. The depth and width of the network are designed, which has a small number of parameters to ensure the efficiency of training. The network adopts an encoder-decoder architecture. The feature maps of different scales from the encoder are concatenated together after resizing to carry out multi-scale feature fusion. To capture the global semantic information from the context, the attention mechanism is employed in the decoder. With one GTX2080Ti GPU and only 15 MB parameters the model owns, our light-weight network has quality results evaluated on ISPRS Vaihingen Dataset with fewer parameters compared to other popular approaches.",Light-weight network,attention mechanism,semantic segmentation,,,"Cheng, Jian",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_343,"Milosavljevic, Aleksandar",,,,Automated Processing of Remote Sensing Imagery Using Deep Semantic Segmentation: A Building Footprint Extraction Case,,AUG 2020,17,"The proliferation of high-resolution remote sensing sensors and platforms imposes the need for effective analyses and automated processing of high volumes of aerial imagery. The recent advance of artificial intelligence (AI) in the form of deep learning (DL) and convolutional neural networks (CNN) showed remarkable results in several image-related tasks, and naturally, gain the focus of the remote sensing community. In this paper, we focus on specifying the processing pipeline that relies on existing state-of-the-art DL segmentation models to automate building footprint extraction. The proposed pipeline is organized in three stages: image preparation, model implementation and training, and predictions fusion. For the first and third stages, we introduced several techniques that leverage remote sensing imagery specifics, while for the selection of the segmentation model, we relied on empirical examination. In the paper, we presented and discussed several experiments that we conducted on Inria Aerial Image Labeling Dataset. Our findings confirmed that automatic processing of remote sensing imagery using DL semantic segmentation is both possible and can provide applicable results. The proposed pipeline can be potentially transferred to any other remote sensing imagery segmentation task if the corresponding dataset is available.",remote sensing imagery,deep learning,semantic segmentation,building extraction,convolutional neural networks,,,,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_344,"Guo, Xuejun","Chen, Zehua","Wang, Chengyi",,Fully convolutional DenseNet with adversarial training for semantic segmentation of high-resolution remote sensing images,,MAR 17 2021,9,"Semantic segmentation is an important and foundational task in the application of high-resolution remote sensing images (HRRSIs). However, HRRSIs feature large differences within categories and minor variances across categories, posing a significant challenge to the high-accuracy semantic segmentation of HRRSIs. To address this issue and obtain powerful feature expressiveness, a deep conditional generative adversarial network (DCGAN), integrating fully convolutional DenseNet (FC-DenseNet) and Pix2pix, is proposed. The DCGAN is composed of a generator-discriminator pair, which is built on a modified downsampling unit of FC-DenseNet. The proposed method possesses strong feature expression ability because of its skip connections, the very deep network structure and multiscale supervision introduced by FC-DenseNet, and the supervision from the discriminator. Experiments on a Deep Globe Land Cover dataset demonstrate the feasibility and effectiveness of this approach for the semantic segmentation of HRRSIs. The results also reveal that our method can mitigate the influence of class imbalance. Our approach for precise semantic segmentation can effectively facilitate the application of HRRSIs. (C) 2021 Society of Photo-Optical Instrumentation Engineers (SPIE)",semantic segmentation,generative adversarial network,fully convolutional neural network,high resolution remote sensing,DenseNets,,,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_345,"Cai, Yuxiang","Yang, Yingchun","Shang, Yongheng","Shen, Zhengwei",DASRSNet: Multitask Domain Adaptation for Super-Resolution-Aided Semantic Segmentation of Remote Sensing Images,,2023,7,"Unsupervised domain adaptation (UDA) has become an important technique for cross-domain semantic segmentation (SS) in the remote sensing community and obtained remarkable results. However, when transferring from high-resolution (HR) remote sensing images to low-resolution (LR) images, the existing UDA frameworks always fail to segment the LR target images, especially for small objects (e.g., cars), due to the severe spatial resolution shift problem. In this article, to improve the segmentation ability of UDA models for LR target images and small objects, we propose a novel multitask domain adaptation network (DASRSNet) for SS of remote sensing images with the aid of super-resolution (SR). The proposed DASRSNet contains domain adaptation for SS (DASS) branch, domain adaptation for SR (DASR) branch, and feature affinity (FA) module. Specifically, the DASS and DASR branches share the same encoder to extract the domain-invariant features for the target and source domains, and these two branches utilize different decoders and discriminators to conduct cross-domain SS task and SR task, which align the domain shift in output space and image space, respectively. Finally, the FA module, which involves the proposed FA loss, is applied to enhance the affinity of SS features and SR features for both source and target domains. The experimental results on the cross-city aerial datasets demonstrate the effectiveness and superiority of our DASRSNet against the recent UDA models.",Remote sensing,Spatial resolution,Feature extraction,Adaptation models,Task analysis,"Yin, Jianwei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantic segmentation,Multitasking,Adversarial learning,multitask learning (MTL),remote sensing images,semantic segmentation (SS),super-resolution (SR),unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,
Row_346,"Ma, Xianping","Xu, Xichen","Zhang, Xiaokang","Pun, Man-On",Adjacent-Scale Multimodal Fusion Networks for Semantic Segmentation of Remote Sensing Data,,2024,0,"Semantic segmentation is a fundamental task in remote sensing image analysis. The accurate delineation of objects within such imagery serves as the cornerstone for a wide range of applications. To address this issue, edge detection, cross-modal data, large intraclass variability, and limited interclass variance must be considered. Traditional convolutional-neural-network-based models are notably constrained by their local receptive fields, Nowadays, transformer-based methods show great potential to learn features globally, while they ignore positional cues easily and are still unable to cope with multimodal data. Therefore, this work proposes an adjacent-scale multimodal fusion network (ASMFNet) for semantic segmentation of remote sensing data. ASMFNet stands out not only for its innovative interaction mechanism across adjacent-scale features, effectively capturing contextual cues while maintaining low computational complexity but also for its remarkable cross-modal capability. It seamlessly integrates different modalities, enriching feature representation. Its hierarchical scale attention (HSA) module bolsters the association between ground objects and their surrounding scenes through learning discriminative features at higher level abstractions, thereby linking the broad structural information. Adaptive modality fusion module is equipped by HSA with valuable insights into the interrelationships between cross-model data, and it assigns spatial weights at the pixel level and seamlessly integrates them into channel features to enhance fusion representation through an evaluation of modality importance via feature concatenation and filtering. Extensive experiments on representative remote sensing semantic segmentation datasets, including the ISPRS Vaihingen and Potsdam datasets, confirm the impressive performance of the proposed ASMFNet.",Transformers,Remote sensing,Semantics,Semantic segmentation,Feature extraction,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Decoding,Data models,Convolutional neural networks,Stacking,Optical sensors,Adjacent-scale,multimodal fusion,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_347,"Bai, Lin","Lin, Xiangyuan","Ye, Zhen","Xue, Dongling",MsanlfNet: Semantic Segmentation Network With Multiscale Attention and Nonlocal Filters for High-Resolution Remote Sensing Images,,2022,15,"With the development of deep learning, remote sensing image (RSI) semantic segmentation has produced significant advances. The majority of existing methods use fully convolutional network (FCN) that lacks fine-grained multiscale representation and fails to extract global context information. Thus, we improve FCN by adding two modules-multiscale attention (MSA) and nonlocal filter (NLF). The MSA module enhances the network's fine-grained multiscale representation capability and allows modeling the interdependencies of feature maps among different channels. The NLF module can capture global context information by sequential using fast Fourier transform (FFT), parameter learnable filters, and inverse FFT. By using MSA module for encoder and NLF module for decoder in the FCN framework, MSA and NLF network (MsanlfNet) can obtain both fine-grained multiscale spatial feature and global context information, thus achieving a balance between performance and computational effort. Experimental results on the remote sensing semantic segmentation public datasets demonstrate that our method can achieve better performance. The code is available at https://github.com/xyuanLin/MsanlfNet.",Feature extraction,Semantics,Convolution,Remote sensing,Image segmentation,"Yao, Cheng","Hui, Meng",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Frequency-domain analysis,Decoding,Multiscale attention (MSA),nonlocal filters (NLFs),remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_348,"Sun, Li","Zou, Huanxin","Wei, Juan","Cao, Xu",Semantic Segmentation of High-Resolution Remote Sensing Images Based on Sparse Self-Attention and Feature Alignment,,MAR 2023,4,"Semantic segmentation of high-resolution remote sensing images (HRSI) is significant, yet challenging. Recently, several research works have utilized the self-attention operation to capture global dependencies. HRSI have complex scenes and rich details, and the implementation of self-attention on a whole image will introduce redundant information and interfere with semantic segmentation. The detail recovery of HRSI is another challenging aspect of semantic segmentation. Several networks use up-sampling, skip-connections, parallel structure, and enhanced edge features to obtain more precise results. However, the above methods ignore the misalignment of features with different resolutions, which affects the accuracy of the segmentation results. To resolve these problems, this paper proposes a semantic segmentation network based on sparse self-attention and feature alignment (SAANet). Specifically, the sparse position self-attention module (SPAM) divides, rearranges, and resorts the feature maps in the position dimension and performs position attention operations (PAM) in rearranged and restored sub-regions, respectively. Meanwhile, the proposed sparse channel self-attention module (SCAM) groups, rearranges, and resorts the feature maps in the channel dimension and performs channel attention operations (CAM) in the rearranged and restored sub-channels, respectively. SPAM and SCAM effectively model long-range context information and interdependencies between channels, while reducing the introduction of redundant information. Finally, the feature alignment module (FAM) utilizes convolutions to obtain a learnable offset map and aligns feature maps with different resolutions, helping to recover details and refine feature representations. Extensive experiments conducted on the ISPRS Vaihingen, Potsdam, and LoveDA datasets demonstrate that the proposed method precedes general semantic segmentation- and self-attention-based networks.",semantic segmentation,high-resolution remote sensing,self-attention,context modeling,feature alignment,"He, Shitian","Li, Meilin","Liu, Shuo",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_349,"Xu, Yizhe","Jiang, Jie",,,High-Resolution Boundary-Constrained and Context-Enhanced Network for Remote Sensing Image Segmentation,,APR 2022,7,"The technology of remote sensing image segmentation has made great progress in recent years. However, there are still several challenges which need to be addressed (e.g., ground objects blocked by shadows, higher intra-class variance and lower inter-class variance). In this paper, we propose a novel high-resolution boundary-constrained and context-enhanced network (HBCNet), which combines boundary information to supervise network training and utilizes the semantic information of categories with the regional feature presentations to improve final segmentation accuracy. On the one hand, we design the boundary-constrained module (BCM) and form the parallel boundary segmentation branch, which outputs the boundary segmentation results and supervises the network training simultaneously. On the other hand, we also devise a context-enhanced module (CEM), which integrates the self-attention mechanism to advance the semantic correlation between pixels of the same category. The two modules are independent and can be directly embedded in the main segmentation network to promote performance. Extensive experiments were conducted using the ISPRS Vahingen and Potsdam benchmarks. The mean F1 score (m-F1) of our model reached 91.32% and 93.38%, respectively, which exceeds most existing CNN-based models and represents state-of-the-art results.",remote sensing image,semantic segmentation,attention mechanism,boundary information,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_350,"Xin, Yi","Fan, Zide","Qi, Xiyu","Geng, Ying",Enhancing Semi-Supervised Semantic Segmentation of Remote Sensing Images via Feature Perturbation-Based Consistency Regularization Methods,,FEB 2024,4,"In the field of remote sensing technology, the semantic segmentation of remote sensing images carries substantial importance. The creation of high-quality models for this task calls for an extensive collection of image data. However, the manual annotation of these images can be both time-consuming and labor-intensive. This has catalyzed the advent of semi-supervised semantic segmentation methodologies. Yet, the complexities inherent within the foreground categories of these remote sensing images present challenges in preserving prediction consistency. Moreover, remote sensing images possess more complex features, and different categories are confused within the feature space, making optimization based on the feature space challenging. To enhance model consistency and to optimize feature-based class categorization, this paper introduces a novel semi-supervised semantic segmentation framework based on Mean Teacher (MT). Unlike the conventional Mean Teacher that only introduces perturbations at the image level, we incorporate perturbations at the feature level. Simultaneously, to maintain consistency after feature perturbation, we employ contrastive learning for feature-level learning. In response to the complex feature space of remote sensing images, we utilize entropy threshold to assist contrastive learning, selecting feature key-values more precisely, thereby enhancing the accuracy of segmentation. Extensive experimental results on the ISPRS Potsdam dataset and the challenging iSAID dataset substantiate the superior performance of our proposed methodology.",remote sensing,semantic segmentation,semi-supervised learning,consistency regularization,feature perturbation,"Li, Xinming",,,,SENSORS,,contrastive learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_351,"Sui, Baikai","Cao, Yungang","Cheng, Haibo","Zhang, Shuang",Detail-Optimized Super-Resolution Reconstruction-Based Multistage Training Strategy for Remote Sensing Semantic Segmentation,,2024,0,"Low resolution is a major factor that negatively impacts the accuracy of remote sensing (RS) interpretation. High-quality super-resolution reconstruction (SRR) can help alleviate this problem. In this study, we present a multistage semantic segmentation training strategy called SRSSTS, which is based on detail-optimized SRR. SRSSTS addresses the challenges of poor reconstruction quality and low semantic segmentation (SS) accuracy of RS images. Our approach involves constructing a generative adversarial network (GAN)-based perceptual-loss-dominated SRR network, which generates texture-realistic, high-quality high-resolution images from low-resolution RS images. We then input the generated images into an SS network in steps and propose a three-stage joint loss to generate segmentation results at different resolutions. SRSSTS is straightforward and efficient, and it can be applied to any SS backbone networks. Additionally, this strategy does not increase computational resources compared to other SR-based segmentation methods, since the SRR network is trained separately. We evaluated the effectiveness of our designed SRR network and SRSSTS on five RS datasets (ISPRS Potsdam and Vaihingen, WHDLD, LoveDA Rural and Urban) with different resolutions, achieving excellent performance compared to an SS model for a single task and other SR-based segmentation methods. Moreover, we observed that the learned perceptual image patch similarity (LPIPS) of the super-resolution (SR) reconstructed images, i.e., the stronger the perception ability, the more helpful it is for the SS task.",remote sensing (RS),semantic segmentation (SS),super resolution (SR),training strategy (TS),,"Zhu, Jun","Xie, Yakun",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_352,"Song, Wanying","Zhou, Xinwei","Zhang, Shiru","Wu, Yan",GLF-Net: A Semantic Segmentation Model Fusing Global and Local Features for High-Resolution Remote Sensing Images,,OCT 2023,3,"Semantic segmentation of high-resolution remote sensing images holds paramount importance in the field of remote sensing. To better excavate and fully fuse the features in high-resolution remote sensing images, this paper introduces a novel Global and Local Feature Fusion Network, abbreviated as GLF-Net, by incorporating the extensive contextual information and refined fine-grained features. The proposed GLF-Net, devised as an encoder-decoder network, employs the powerful ResNet50 as its baseline model. It incorporates two pivotal components within the encoder phase: a Covariance Attention Module (CAM) and a Local Fine-Grained Extraction Module (LFM). And an additional wavelet self-attention module (WST) is integrated into the decoder stage. The CAM effectively extracts the features of different scales from various stages of the ResNet and then encodes them with graph convolutions. In this way, the proposed GLF-Net model can well capture the global contextual information with both universality and consistency. Additionally, the local feature extraction module refines the feature map by encoding the semantic and spatial information, thereby capturing the local fine-grained features in images. Furthermore, the WST maximizes the synergy between the high-frequency and the low-frequency information, facilitating the fusion of global and local features for better performance in semantic segmentation. The effectiveness of the proposed GLF-Net model is validated through experiments conducted on the ISPRS Potsdam and Vaihingen datasets. The results verify that it can greatly improve segmentation accuracy.",high-resolution remote sensing,semantic segmentation,global context information,fine-grained feature,feature fusion,"Zhang, Peng",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_353,"Chen, Bingyu","Xia, Min","Qian, Ming","Huang, Junqing",MANet: a multi-level aggregation network for semantic segmentation of high-resolution remote sensing images,,AUG 18 2022,65,"With the continuous improvement of the segmentation effect for natural datasets, some studies have gradually been applied to high-resolution remote sensing images (HRRSIs). Due to a large amount of ground object information contained, even objects of the same type present the diversity and complexity of features in different periods or locations. The existing algorithms applied to semantic segmentation of remote sensing images are limited by the short-range context, and the high-resolution details, especially the edges, couldnot be fully recovered. Aiming at the problem, a multi-level aggregation network (MANet) is proposed. Firstly, the proposed global dependency module extracts deep global features by learning the interrelationships of all positions in the context, and filters redundant channel information as well. Secondly, MANet we proposed extends Multi-level Feature Aggregation Network by adding a simple and effective two-path feature refining module before each up-sample module to optimize the segmentation results. The two-path feature refining module uses two independent branches to obtain the features with different depths, which enriches the hierarchical structure of the network. Besides, it is combined with the subsequent up-sample module to effectively enhance MANet's ability to recover detailed information of HRRSI. Experimental results show that the methods proposed in the paper achieve competitive performance.",High-Resolution,remote sensing images,Mosaic data enhancement,semantic segmentation,deep learning,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_354,"Chen, Yan","Jiang, Wenxiang","Wang, Mengyuan","Kang, Menglei",LightFGCNet: A Lightweight and Focusing on Global Context Information Semantic Segmentation Network for Remote Sensing Imagery,,DEC 2022,4,"Convolutional neural networks have attracted much attention for their use in the semantic segmentation of remote sensing imagery. The effectiveness of semantic segmentation of remote sensing images is significantly influenced by contextual information extraction. The traditional convolutional neural network is constrained by the size of the convolution kernel and mainly concentrates on local contextual information. We suggest a new lightweight global context semantic segmentation network, LightFGCNet, to fully utilize the global context data and to further reduce the model parameters. It uses an encoder-decoder architecture and gradually combines feature information from adjacent encoder blocks during the decoding upsampling stage, allowing the network to better extract global context information. Considering that the frequent merging of feature information produces a significant quantity of redundant noise, we build a unique and lightweight parallel channel spatial attention module (PCSAM) for a few critical contextual features. Additionally, we design a multi-scale fusion module (MSFM) to acquire multi-scale feature target information. We conduct comprehensive experiments on the two well-known datasets ISPRS Vaihingen and WHU Building. The findings demonstrate that our suggested strategy can efficiently decrease the number of parameters. Separately, the number of parameters and FLOPs are 3.12 M and 23.5 G, respectively, and the mIoU and IoU of our model on the two datasets are 70.45% and 89.87%, respectively, which is significantly better than what the conventional convolutional neural networks for semantic segmentation can deliver.",remote sensing imagery,semantic segmentation,attention mechanism,global contextual information,multi-scale fusion,"Weise, Thomas","Wang, Xiaofeng","Tan, Ming","Xu, Lixiang",REMOTE SENSING,"Li, Xinlu",lightweight model,,,,,,,,,"Zhang, Chen",,,,,,,,,,,,,,,,,,,,
Row_355,"Guo, Shichen","Jin, Qizhao","Wang, Hongzhen","Wang, Xuezhi",Learnable Gated Convolutional Neural Network for Semantic Segmentation in Remote-Sensing Images,,AUG 2019,29,"Semantic segmentation in high-resolution remote-sensing (RS) images is a fundamental task for RS-based urban understanding and planning. However, various types of artificial objects in urban areas make this task quite challenging. Recently, the use of Deep Convolutional Neural Networks (DCNNs) with multiscale information fusion has demonstrated great potential in enhancing performance. Technically, however, existing fusions are usually implemented by summing or concatenating feature maps in a straightforward way. Seldom do works consider the spatial importance for global-to-local context-information aggregation. This paper proposes a Learnable-Gated CNN (L-GCNN) to address this issue. Methodologically, the Taylor expression of the information-entropy function is first parameterized to design the gate function, which is employed to generate pixelwise weights for coarse-to-fine refinement in the L-GCNN. Accordingly, a Parameterized Gate Module (PGM) was designed to achieve this goal. Then, the single PGM and its densely connected extension were embedded into different levels of the encoder in the L-GCNN to help identify the discriminative feature maps at different scales. With the above designs, the L-GCNN is finally organized as a self-cascaded end-to-end architecture that is able to sequentially aggregate context information for fine segmentation. The proposed model was evaluated on two public challenging benchmarks, the ISPRS 2Dsemantic segmentation challenge Potsdam dataset and the Massachusetts building dataset. The experiment results demonstrate that the proposed method exhibited significant improvement compared with several related segmentation networks, including the FCN, SegNet, RefineNet, PSPNet, DeepLab and GSN.For example, on the Potsdam dataset, our method achieved a 93.65% F1 score and 88.06% IoU score for the segmentation of tiny cars in high-resolution RS images. As a conclusion, the proposed model showed potential for object segmentation from the RS images of buildings, impervious surfaces, low vegetation, trees and cars in urban settings, which largely varies in size and have confusing appearances.",semantic segmentation,CNN,deep learning,remote sensing,gate function,"Wang, Yangang","Xiang, Shiming",,,REMOTE SENSING,,multiscale feature fusion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_356,"Li, Zhenshi","Zhang, Xueliang","Xiao, Pengfeng",,One Model Is Enough: Toward Multiclass Weakly Supervised Remote Sensing Image Semantic Segmentation,,2023,25,"Semantic segmentation of remote sensing images (RSIs) is effective for large-scale land cover mapping, which heavily relies on a large amount of training data with laborious pixel-level labeling. Due to the easy availability of image-level labels, weakly supervised semantic segmentation (WSSS) based on them has attracted intensive attention. However, existing image-level WSSS methods for RSIs mainly focus on binary segmentation, which are difficult to apply to multiclass scenarios. This study proposes a comprehensive framework for image-level multiclass WSSS of RSIs, consisting of appropriate image-level label generation, high-quality pixel-level pseudo mask generation, and segmentation network iterative training. Specifically, a training sample filtering method, as well as a dataset co-occurrence evaluation metric, is proposed to demonstrate proper image-level training samples. Leveraging multiclass class activation maps (CAMs), an uncertainty-driven pixel-level weighted mask is proposed to relieve the overfitting of labeling noise in pseudo masks when training the segmentation network. Extensive experiments demonstrate that the proposed framework can achieve high-quality multiclass WSSS performance with image-level labels, which can attain 94.23% and 90.77% of the mean intersection over union (mIoU) from pixel-level labels for the ISPRS Potsdam and Vaihingen datasets, respectively. Beyond that, for the DeepGlobe dataset with more complex landscapes, the WSSS framework can achieve an accuracy close to 99% of the fully supervised case. In addition, we further demonstrate that compared to adopting multiple binary WSSS models, directly training a multiclass WSSS model can achieve better results, which can provide new thoughts to achieve WSSS of RSIs for multiclass application scenarios. Our code is publically available at https://github.com/NJU-LHRS/OME.",Class activation map (CAM),image-level label,multiclass,pixel-level uncertainty,remote sensing image (RSI),,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_357,"Zheng, Chen","Zhang, Yun","Wang, Leiguang",,Semantic Segmentation of Remote Sensing Imagery Using an Object-Based Markov Random Field Model With Auxiliary Label Fields,,MAY 2017,37,"The Markov random field (MRF) model has attracted great attention in the field of image segmentation. However, most MRF-based methods fail to resolve segmentation misclassification problems for high spatial resolution remote sensing images due to insufficiently using the hierarchical semantic information. In order to solve such a problem, this paper proposes an object-based MRF model with auxiliary label fields that can capture more macro and detailed information and apply it to the semantic segmentation of high spatial resolution remote sensing images. Specifically, apart from the label field, two auxiliary label fields are first introduced into the proposed model for interpreting remote sensing images from different perspectives, which are implemented by setting a different number of auxiliary classes. Then, the multilevel logistic model is used to describe the interactions within each label field, and a conditional probability distribution is developed to model the interactions between label fields. A net context structure is established among them to model the interactions of classes within and between label fields. A principled probabilistic inference is suggested to solve the proposed model by iteratively renewing the label field and auxiliary label fields, in which different information of auxiliary label fields can be integrated into the label field during iterations. Experiments on different remote sensing images demonstrate that our model produces more accurate segmentation than the state-of-the-art MRF-based methods. If some prior information is added, the proposed model can produce accurate results even in complex areas.",Auxiliary label field,object-based Markov random field,remote sensing image,semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_358,"Zhang, Zhen","Huang, Jue","Jiang, Tao","Sui, Baikai",Semantic segmentation of very high-resolution remote sensing image based on multiple band combinations and patchwise scene analysis,,JAN 6 2020,19,"Large intraclass variance and low interclass variance are among the most challenging problems in very high-resolution (VHR) image classification. Semantic segmentation constructed in a deep convolution neural network is used as a classification algorithm conducted via end-to-end training, which combines spectral-spatial features and context information. However, large-scale remote sensing images cannot be directly processed because they are limited by GPU memory and segmentation algorithm. At the same time, classification using single band combinations is also unsatisfactory due to the extraordinary complex features of VHR images. Therefore, a method is proposed based on multiple band combinations and patchwise scene analysis. A complex remote sensing image can be considered as into a combination of simple scenes from multiple patchwise images. And optimal band combinations of each patchwise image are selected according to their scene. The segmentation results of each patchwise image are merged to get the desired results according to geographical coordinates. Our method is validated on the ISPRS 2-D Semantic Labeling dataset of Potsdam, on which results competitive with the state-of-the-art are obtained. The proposed scheme has strong universality and can be used for large-scale high-resolution remote sensing image classification. (C) 2020 Society of Photo-Optical Instrumentation Engineers (SPIE)",very high-resolution remote sensing,semantic segmentation,multiple band combination,scene analysis,,"Pan, Xinliang",,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_359,"Peng, Cheng","Li, Yangyang","Jiao, Licheng","Chen, Yanqiao",Densely Based Multi-Scale and Multi-Modal Fully Convolutional Networks for High-Resolution Remote-Sensing Image Semantic Segmentation,,AUG 2019,96,"Automatic and accurate semantic segmentation from high-resolution remote-sensing images plays an important role in the field of aerial images analysis. The task of dense semantic segmentation requires that semantic labels be assigned to each pixel in the image. Recently, convolutional neural networks (CNNs) have proven to be powerful tools for image classification, and they have been adopted in the remote-sensing community. But many limitations still exist when modern CNN architectures are directly applied to remote-sensing images, such as gradient explosion when the depth of the network increases, over-fitting with limited labeled remote-sensing data, and special differences between remote-sensing images and natural images. In this paper, we present a novel architecture that combines the thought of dense connection and fully convolutional networks, referred as DFCN, to automatically provide fine-grained semantic segmentation maps. In addition, we improve DFCN with multi-scale filters to widen the network and to increase the richness and diversity of extracted information, making the network more powerful and expressive than the naive convolution layer. Furthermore, we investigate a multi-modal network that incorporates digital surface models (DSMs) into a DFCN structure, and then we propose dual-path densely convolutional networks where the encoder consists of two paths that, respectively, extract features from spectral data and DSMs data and then fuse them. Finally, through conducting comprehensive experimental evaluations on two remote sensing benchmark datasets, we test our proposed models and compare them with other deep networks. The results demonstrate the effectiveness of proposed approaches; they can achieve competitive performance compared with the current state-of-the-art methods.",Dense connection,fully convolutional networks (FCN),high-resolution remote-sensing image,semantic segmentation,,"Shang, Ronghua",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_360,"Wang, Jiaxin","Ding, Chris H. Q.","Chen, Sibao","He, Chenggang",Semi-Supervised Remote Sensing Image Semantic Segmentation via Consistency Regularization and Average Update of Pseudo-Label,,NOV 2020,57,"Image segmentation has made great progress in recent years, but the annotation required for image segmentation is usually expensive, especially for remote sensing images. To solve this problem, we explore semi-supervised learning methods and appropriately utilize a large amount of unlabeled data to improve the performance of remote sensing image segmentation. This paper proposes a method for remote sensing image segmentation based on semi-supervised learning. We first design a Consistency Regularization (CR) training method for semi-supervised training, then employ the new learned model for Average Update of Pseudo-label (AUP), and finally combine pseudo labels and strong labels to train semantic segmentation network. We demonstrate the effectiveness of the proposed method on three remote sensing datasets, achieving better performance without more labeled data. Extensive experiments show that our semi-supervised method can learn the latent information from the unlabeled data to improve the segmentation performance.",semi-supervised learning,remote sensing image segmentation,consistency training,pseudo label,,"Luo, Bin",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_361,"Li, Yunbo","Yi, Zhiyu","Wang, Yuebin","Zhang, Liqiang",Adaptive Context Transformer for Semisupervised Remote Sensing Image Segmentation,,2023,2,"Current deep learning methods for semantic seg-mentation in remote sensing heavily depend on a substantial amount of labeled data. However, obtaining pixel-level labeled data in this field is both time-consuming and laborious. To address this challenge, semi supervised learning (SSL) method shave been introduced. Pseudo supervision is one of the most effective methods, which can be adopted to enhance the performance of semsupervised semantic segmentation of remotesensing images. But incorrect pseudolabels can cause substantialdistortions to the segmentation model in SSL. Moreover, it isdifficult for conventional semantic segmentation methods to dealwith global-local features of the remote sensing image withoutadaptive context feature. In this article, we propose a novel learn-ing approach based on an adaptive context transformer (ACT)and pseudolabeling, called ACT for semisupervised (ACTSS)remote sensing image segmentation. We propose an adaptivecontext attention model with adjustable sliding windows. A smallwindow is used to capture Query (Q) for local feature, andbigger windows are used to capture Key (K) and Value(V)forglobal feature. Then, we combine them and get the global-local features. And we propose a point-line-plane (PLP) pseudolabelfilter mechanism based on clustering and boundary extraction, which can filter unreliable pseudolabels from three angles: point,line, and plane. To validate the effectiveness of the model, we carried out extensive experiments on the LOVEDA, Potsdam, and Vaihingen datasets and compared ACTSS with other methods. These experiments demonstrate that ACTSS achieves state-of-the-art performance for semisupervised semantic segmentation on all tested datasets.",Adaptive context transformer (ACT),pseudolabel filter,semantic segmentation,semisupervised learning (SSL),,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_362,"Su, Hao","Wei, Shunjun","Liu, Shan","Liang, Jiadian",HQ-ISNet: High-Quality Instance Segmentation for Remote Sensing Imagery,,MAR 2020,76,"Instance segmentation in high-resolution (HR) remote sensing imagery is one of the most challenging tasks and is more difficult than object detection and semantic segmentation tasks. It aims to predict class labels and pixel-wise instance masks to locate instances in an image. However, there are rare methods currently suitable for instance segmentation in the HR remote sensing images. Meanwhile, it is more difficult to implement instance segmentation due to the complex background of remote sensing images. In this article, a novel instance segmentation approach of HR remote sensing imagery based on Cascade Mask R-CNN is proposed, which is called a high-quality instance segmentation network (HQ-ISNet). In this scheme, the HQ-ISNet exploits a HR feature pyramid network (HRFPN) to fully utilize multi-level feature maps and maintain HR feature maps for remote sensing images' instance segmentation. Next, to refine mask information flow between mask branches, the instance segmentation network version 2 (ISNetV2) is proposed to promote further improvements in mask prediction accuracy. Then, we construct a new, more challenging dataset based on the synthetic aperture radar (SAR) ship detection dataset (SSDD) and the Northwestern Polytechnical University very-high-resolution 10-class geospatial object detection dataset (NWPU VHR-10) for remote sensing images instance segmentation which can be used as a benchmark for evaluating instance segmentation algorithms in the high-resolution remote sensing images. Finally, extensive experimental analyses and comparisons on the SSDD and the NWPU VHR-10 dataset show that (1) the HRFPN makes the predicted instance masks more accurate, which can effectively enhance the instance segmentation performance of the high-resolution remote sensing imagery; (2) the ISNetV2 is effective and promotes further improvements in mask prediction accuracy; (3) our proposed framework HQ-ISNet is effective and more accurate for instance segmentation in the remote sensing imagery than the existing algorithms.",instance segmentation,HRFPN,ISNetV2,SSDD,NWPU VHR-10,"Wang, Chen","Shi, Jun","Zhang, Xiaoling",,REMOTE SENSING,,remote sensing images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_363,"Zheng, Zhiyu","Lv, Liang","Zhang, Lefei",,Enhancing the Semi-Supervised Semantic Segmentation With Prototype-Based Supervision for Remote Sensing Images,,2024,0,"While image semantic segmentation is a fundamental and well-studied task in remote sensing (RS) society, it usually depends on large amounts of pixel-level annotations. RS image semi-supervised semantic segmentation (RSIS4) tries to improve performance by exploring the unlabeled data, thus significantly reducing the label costs. The core idea of RSIS4 is to transfer the prior information from the labeled to unlabeled pixels, which is commonly achieved by considering the confident part of the softmax prediction as pseudolabels for further supervised learning. However, such pixel-level instruction could inevitably involve uncertainty (e.g., noise and error) due to the extremely limited annotated data at the initialization. To address this issue, in this letter, we employ the prototypes, which contain inbuilt resistance to potentially inaccurate pixels, to bring substantial supervision directly from the embedded feature space. Specifically, we project deep features into the embedding space to generate prototypes, each of which can be regarded as the category-level feature representation of a certain semantic category. These prototypes are then used to perform the pixelwise classification, with the advantage of capturing the global similarity throughout the whole pixels within the category. Moreover, to ensure accurate prototypes, we further introduce pixel-prototype contrast to better explore the discriminative category-level feature embedding. By integrating the guidance from the above pixel-level and category-level feature representations, the proposed algorithm obtains high-quality pseudolabels and extracts effective features. Extensive experiments on four RS image segmentation datasets have demonstrated the effectiveness of the proposed method. The code is available at https://github.com/Duckyee728/PCSSS.git.",Prototype learning,remote sensing (RS) images,semi-supervised semantic segmentation (S4),Prototype learning,remote sensing (RS) images,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,semi-supervised semantic segmentation (S4),,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_364,"Wu, Linshan","Fang, Leyuan","Yue, Jun","Zhang, Bob",Deep Bilateral Filtering Network for Point-Supervised Semantic Segmentation in Remote Sensing Images,,2022,43,"Semantic segmentation methods based on deep neural networks have achieved great success in recent years. However, training such deep neural networks relies heavily on a large number of images with accurate pixel-level labels, which requires a huge amount of human effort, especially for large-scale remote sensing images. In this paper, we propose a point-based weakly supervised learning framework called the deep bilateral filtering network (DBFNet) for the semantic segmentation of remote sensing images. Compared with pixel-level labels, point annotations are usually sparse and cannot reveal the complete structure of the objects; they also lack boundary information, thus resulting in incomplete prediction within the object and the loss of object boundaries. To address these problems, we incorporate the bilateral filtering technique into deeply learned representations in two respects. First, since a target object contains smooth regions that always belong to the same category, we perform deep bilateral filtering (DBF) to filter the deep features by a nonlinear combination of nearby feature values, which encourages the nearby and similar features to become closer, thus achieving a consistent prediction in the smooth region. In addition, the DBF can distinguish the boundary by enlarging the distance between the features on different sides of the edge, thus preserving the boundary information well. Experimental results on two widely used datasets, the ISPRS 2-D semantic labeling Potsdam and Vaihingen datasets, demonstrate that our proposed DBFNet can achieve a highly competitive performance compared with state-of-the-art fully-supervised methods. Code is available at https://github.com/Luffy03/DBFNet.",Bilateral filtering,point annotations,remote sensing,semantic segmentation,weakly-supervised learning,"Ghamisi, Pedram","He, Min",,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_365,"Sun, Le","Cheng, Shiwei","Zheng, Yuhui","Wu, Zebin",SPANet: Successive Pooling Attention Network for Semantic Segmentation of Remote Sensing Images,,2022,54,"In the convolutional neural network, the precise segmentation of small-scale objects and object boundaries in remote sensing images is a great challenge. As the model gets deeper, low-level features with geometric information and high-level features with semantic information cannot be obtained simultaneously. To alleviate this problem, a successive pooling attention network (SPANet) was proposed. The SPANet mainly consists of ResNet50 as the backbone, successive pooling attention module (SPAM), and feature fusion module (FFM). Specifically, the SPANet uses two parallel branches to extract high-level features by ResNet50 and low-level features by the first 11 layers of ResNet50. Then, both the high- and low-level features are fed to the SPAM, which is mainly composed of a successive pooling operator and a self-attention submodule, for further extracting deeper multiscale and salient features. In addition, the low- and high-level features after the SPAM are fused by the FFM to achieve the complementarity of spatial and geometric information. This fusion module alleviates the problem of the accurate segmentation of object edges. Finally, the high-level features and enhanced low-level features of the two branches are fused to obtain the final prediction results. Experiments show that the proposed SPANet achieves a good segmentation effect compared with other models on two remotely sensed datasets.",Feature extraction,Semantics,Image segmentation,Remote sensing,Data mining,"Zhang, Jianwei",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Context modeling,Decoding,Attention mechanism,convolutional neural network,remote sensing images,semantic segmentation,successive pooling,,,,,,,,,,,,,,,,,,,,,,,
Row_366,"Zhang, Jinglin","Li, Yuxia","Zhang, Bowei","He, Lei",CD-MQANet: Enhancing Multi-Objective Semantic Segmentation of Remote Sensing Images through Channel Creation and Dual-Path Encoding,,SEP 2023,0,"As a crucial computer vision task, multi-objective semantic segmentation has attracted widespread attention and research in the field of remote sensing image analysis. This technology has important application value in fields such as land resource surveys, global change monitoring, urban planning, and environmental monitoring. However, multi-target semantic segmentation of remote sensing images faces challenges such as complex surface features, complex spectral features, and a wide spatial range, resulting in differences in spatial and spectral dimensions among target features. To fully exploit and utilize spectral feature information, focusing on the information contained in spatial and spectral dimensions of multi-spectral images, and integrating external information, this paper constructs the CD-MQANet network structure, where C represents the Channel Creator module and D represents the Dual-Path Encoder. The Channel Creator module (CCM) mainly includes two parts: a generator block and a spectral attention module. The generator block aims to generate spectral channels that can expand different ground target types, while the spectral attention module can enhance useful spectral information. Dual-Path Encoders include channel encoders and spatial encoders, intended to fully utilize spectrally enhanced images while maintaining the spatial information of the original feature map. The decoder of CD-MQANet is a multitasking decoder composed of four types of attention, enhancing decoding capabilities. The loss function used in the CD-MQANet consists of three parts, which are generated by the intermediate results of the CCM, the intermediate results of the decoder, and the final segmentation results and label calculation. We performed experiments on the Potsdam dataset and the Vaihingen dataset. Compared to the baseline MQANet model, the CD-MQANet network improved mean F1 and OA by 2.03% and 2.49%, respectively, on the Potsdam dataset, and improved mean F1 and OA by 1.42% and 1.25%, respectively, on the Vaihingen dataset. The effectiveness of CD-MQANet was also proven by comparative experiments with other studies. We also conducted a thermographic analysis of the attention mechanism used in CD-MQANet and analyzed the intermediate results generated by CCM and LAM. Both modules generated intermediate results that had a significant positive impact on segmentation.",deep learning,remote sensing,semantic segmentation,attention mechanism,multispectral remote sensing data,"He, Yuan","Deng, Wantao","Si, Yu","Tong, Zhonggui",REMOTE SENSING,"Gong, Yushu",,,,,,,,,,"Liao, Kunwei",,,,,,,,,,,,,,,,,,,,
Row_367,"Yang, Liangzhe","Chen, Hao","Yang, Anran","Li, Jun",EasySeg: An Error-Aware Domain Adaptation Framework for Remote Sensing Imagery Semantic Segmentation via Interactive Learning and Active Learning,,2024,2,"Semantic segmentation of remote sensing images has attracted much attention for its wide applications. While deep learning models have shown impressive performance in this task, challenges arise when applying them to data from other domains without fine-tuning, due to domain gaps. Domain adaptation (DA) has emerged as a solution to bridge this gap. Existing works mainly focus on unsupervised DA (UDA), which lags far behind fully supervised models. However, active DA (ADA) methods focused on natural images face challenges when applied to remote sensing images due to pronounced domain gaps and error unawareness problems. In this work, we propose a novel error-aware DA framework for remote sensing imagery semantic segmentation, called EasySeg, via interactive learning and active learning. First, we introduce a point-level labeling strategy, named ""See-First-Ask-Later"" (SFAL), combining both interactive and active learning manners, allowing obvious errors and information-rich pixels to be annotated easily and efficiently. Then, we introduce an interactive semantic segmentation network (ISS-Net), which can perform automatic semantic segmentation and interactive refinement. Based on the acquired target point-level labels, ISS-Net generates dense and accurate pseudo-labels to enhance DA performance through retraining with consistency regularization. Comprehensive experiments on two tasks demonstrate that our method outperforms the state-of-the-art ADA methods in terms of overall accuracy (OA), mean accuracy (MA), $F1$ score, and mean intersection of union (mIoU) with lower labeling costs, even surpassing some fully supervised models. The source code of EasySeg is freely available at https://github.com/Yangliangzhe/EasySeg.",Active learning,domain adaptation (DA),interactive learning,remote sensing images,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_368,"Li, Yuxia","Si, Yu","Tong, Zhonggui","He, Lei",MQANet: Multi-Task Quadruple Attention Network of Multi-Object Semantic Segmentation from Remote Sensing Images,,DEC 2022,9,"Multi-object semantic segmentation from remote sensing images has gained significant attention in land resource surveying, global change monitoring, and disaster detection. Compared to other application scenarios, the objects in the remote sensing field are larger and have a wider range of distribution. In addition, some similar targets, such as roads and concrete-roofed buildings, are easily misjudged. However, existing convolutional neural networks operate only in the local receptive field, and this limits their capacity to represent the potential association between different objects and surrounding features. This paper develops a Multi-task Quadruple Attention Network (MQANet) to address the above-mentioned issues and increase segmentation accuracy. The MQANet contains four attention modules: position attention module (PAM), channel attention module (CAM), label attention module (LAM), and edge attention module (EAM). The quadruple attention modules obtain global features by expanding the receptive fields of the network and introducing spatial context information in the label. Then, a multi-tasking mechanism which splits a multi-category segmentation task into several binary-classification segmentation tasks is introduced to improve the ability to identify similar objects. The proposed MQANet network was applied to the Potsdam dataset, the Vaihingen dataset and self-annotated images from Chongzhou and Wuzhen (CZ-WZ), representative cities in China. Our MQANet performs better over the baseline net by a large margin of +6.33 OA and +7.05 Mean F1-score on the Vaihingen dataset, +3.57 OA and +2.83 Mean F1-score on the Potsdam dataset, and +3.88 OA and +8.65 Mean F1-score on the self-annotated dataset (CZ-WZ dataset). In addition, each image execution time of the MQANet model is reduced 66.6 ms compared to UNet. Moreover, the effectiveness of MQANet was also proven by comparative experiments with other studies.",deep learning,remote sensing,semantic segmentation,multi-task learning,attention mechanism,"Zhang, Jinglin","Luo, Shiyu","Gong, Yushu",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_369,"Li, Xin","Xu, Feng","Yong, Xi","Chen, Deqing",SSCNet: A Spectrum-Space Collaborative Network for Semantic Segmentation of Remote Sensing Images,,DEC 2023,21,"Semantic segmentation plays a pivotal role in the intelligent interpretation of remote sensing images (RSIs). However, conventional methods predominantly focus on learning representations within the spatial domain, often resulting in suboptimal discriminative capabilities. Given the intrinsic spectral characteristics of RSIs, it becomes imperative to enhance the discriminative potential of these representations by integrating spectral context alongside spatial information. In this paper, we introduce the spectrum-space collaborative network (SSCNet), which is designed to capture both spectral and spatial dependencies, thereby elevating the quality of semantic segmentation in RSIs. Our innovative approach features a joint spectral-spatial attention module (JSSA) that concurrently employs spectral attention (SpeA) and spatial attention (SpaA). Instead of feature-level aggregation, we propose the fusion of attention maps to gather spectral and spatial contexts from their respective branches. Within SpeA, we calculate the position-wise spectral similarity using the complex spectral Euclidean distance (CSED) of the real and imaginary components of projected feature maps in the frequency domain. To comprehensively calculate both spectral and spatial losses, we introduce edge loss, Dice loss, and cross-entropy loss, subsequently merging them with appropriate weighting. Extensive experiments on the ISPRS Potsdam and LoveDA datasets underscore SSCNet's superior performance compared with several state-of-the-art methods. Furthermore, an ablation study confirms the efficacy of SpeA.",semantic segmentation,remote sensing images,spectral attention,spectral and spatial contexts,loss function,"Xia, Runliang","Ye, Baoliu","Gao, Hongmin","Chen, Ziqi",REMOTE SENSING,"Lyu, Xin",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_370,"He, Yongjun","Wang, Jinfei","Liao, Chunhua","Shan, Bo",ClassHyPer: ClassMix-Based Hybrid Perturbations for Deep Semi-Supervised Semantic Segmentation of Remote Sensing Imagery,,FEB 2022,28,"Inspired by the tremendous success of deep learning (DL) and the increased availability of remote sensing data, DL-based image semantic segmentation has attracted growing interest in the remote sensing community. The ideal scenario of DL application requires a vast number of annotation data with the same feature distribution as the area of interest. However, obtaining such enormous training sets that suit the data distribution of the target area is highly time-consuming and costly. Consistency-regularization-based semi-supervised learning (SSL) methods have gained growing popularity thanks to their ease of implementation and remarkable performance. However, there have been limited applications of SSL in remote sensing. This study comprehensively analyzed several advanced SSL methods based on consistency regularization from the perspective of data- and model-level perturbation. Then, an end-to-end SSL approach based on a hybrid perturbation paradigm was introduced to improve the DL model's performance with a limited number of labels. The proposed method integrates the semantic boundary information to generate more meaningful mixing images when performing data-level perturbation. Additionally, by using implicit pseudo-supervision based on model-level perturbation, it eliminates the need to set extra threshold parameters in training. Furthermore, it can be flexibly paired with the DL model in an end-to-end manner, as opposed to the separated training stages used in the traditional pseudo-labeling. Experimental results for five remote sensing benchmark datasets in the application of segmentation of roads, buildings, and land cover demonstrated the effectiveness and robustness of the proposed approach. It is particularly encouraging that the ratio of accuracy obtained using the proposed method with 5% labels to that using the purely supervised method with 100% labels was more than 89% on all benchmark datasets.",deep learning,remote sensing semantic segmentation,transfer learning,semi-supervised learning,consistency regularization,"Zhou, Xin",,,,REMOTE SENSING,,hybrid perturbation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_371,"Amine, Hadir","Olga, Assainova","Mohamed, Adjou","Gaetan, Palka",Bridging the gap: deep learning in the semantic segmentation of remote sensing data,PATTERN RECOGNITION AND PREDICTION XXXV,2024,0,"Semantic segmentation has crucial importance in various domains due to its ability to recognize and categorize objects within an image at a pixel level. This task enables a wide range of applications, such as autonomous vehicles, environmental monitoring, and remote sensing (RS). In RS, semantic segmentation plays a crucial role, acting as the basis for applications including land cover classification. Following the success of deep learning (DL) methods in computer vision, our paper addresses the intersection between DL and RS imagery. We focus on improving the efficiency of some baseline and backbone models to ensure their adaptability to the challenges posed by RS imagery. Therefore, we evaluate state-of-the-art models on two datasets and investigate their ability to accurately segment objects in RS imagery. Our research aims to open the way for more accurate and reliable semantic segmentation methods in geospatial analysis.",Remote sensing images,deep learning,semantic segmentation,land cover or crop type classification,,"Marwa, Elbouz","Ayman, Al Falou",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_372,"Cai, Jian","Ma, Lei","Li, Feimo","Yang, Yiping",JOINT FEATURE NETWORK FOR BRIDGE SEGMENTATION IN REMOTE SENSING IMAGES,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2018,1,"This paper proposes a novel convolutional neural network architecture for semantic segmentation of bridges with various scales in optical remote sensing images. In the context of RSI analysis on objects with irregular shapes, it is necessary to get dense, pixelwise classification maps. To address the issue, a new network architecture for producing refined shapes is required instead of image categorization labels. In our end-to-end framework, a ResNet is used as a backbone model to extract semantic features, then a cascaded top-down path is added to fuse these features as different scales. Joint features are obtained by stacking different layers of feature maps. Experiments show our proposed architecture has the ability to combine rich multi-scale contextual information to produce semantic segmentation maps with high accuracy.",convolutional neural networks (CNNs),semantic segmentation,pixelwise classification,remote sensing images (RSIs),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_373,"Chen, Jie","Zhu, Jingru","Guo, Ya","Sun, Geng",Unsupervised Domain Adaptation for Semantic Segmentation of High-Resolution Remote Sensing Imagery Driven by Category-Certainty Attention,,2022,48,"Semantic segmentation is an important task of analysis and understanding of high-resolution remote sensing images (HRSIs). The deep convolutional neural network (DCNN)-based model shows their excellent performance in remote sensing image semantic segmentation. Most of the existing HRSI semantic segmentation methods are only designed for a very limited data domain, that is, the training and test images are from the same dataset. The accuracy drops sharply once a model trained on a certain dataset is used for cross-domain prediction due to the difference in feature distribution of the dataset. To this end, this article proposes an unsupervised domain adaptation framework based on adversarial learning for HRSI semantic segmentation. This framework uses high-level feature alignment to narrow the difference between the source and target domains at the semantic level. It uses the category-certainty attention module to reduce the attention of the classifier on category-level aligned features and increase the attention on category-level unaligned features. Experimental results show that the proposed method performs favorably against the state-of-the-art methods in cross-domain segmentation.",Semantics,Image segmentation,Feature extraction,Adaptation models,Task analysis,"Zhang, Yi","Deng, Min",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Remote sensing,Training,Category-certainty attention,domain adaptation,generative adversarial networks,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_374,"Chen, Yuncheng","Wang, Leiguang","Li, Jingying","Zheng, Chen",Hierarchical Self-Learning Knowledge Inference Based on Markov Random Field for Semantic Segmentation of Remote Sensing Images,,2024,0,"Semantic segmentation is one of the most important tasks in the field of remote sensing. As the spatial resolution increases, the remote sensing images can capture more detailed information and make hierarchical semantic interpretation possible. However, hierarchical semantic segmentation encounters high heterogeneity not only within the intra-layer classes but also among inter-layer classes. It brings challenges to semantic segmentation methods such as the convolutional neural network (CNN). In this article, a hierarchical self-learning knowledge inference model (HSKIM) based on the Markov random field (MRF) model is proposed for hierarchical semantic segmentation of remote sensing images. The HSKIM model introduces a new framework that integrates the advantages of CNN-based data feature learning and MRF-based hierarchical semantic inference. It contains three modules: data learning module ( $\boldsymbol {D}$ ), inference units generation module ( $\boldsymbol {I}$ ), and self-learning knowledge inference module ( $\boldsymbol {S}$ ). The module $\boldsymbol {D}$ uses CNN to learn specific data features layer by layer and extract preliminary geographical objects as the initial results. The module I refines the geographical objects using a novel boundary-preservation trick to generate more accurate inference units with clear geographical meaning. The module S introduces a hierarchical object-based MRF model to implement semantic inference among intra-layer and inter-layer inference units, guided by the spatial interactions and geographical criteria. This module can self-learn and update the relationship between classes iteratively and provide the final result. Experiments on the GID dataset with hierarchical classes, alongside 12 state-of-the-art CNN-based methods, validate the effectiveness and robustness of the proposed HSKIM model. The code of this article is available at https://github.com/iichengzi/HSKIM.",Semantics,Semantic segmentation,Remote sensing,Feature extraction,Spatial resolution,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Data mining,Convolutional neural networks,Convolutional neural network (CNN),hierarchical semantic inference,hierarchical semantic segmentation,Markov random field model (MRF),remote sensing,,,,,,,,,,,,,,,,,,,,,,,
Row_375,"Feng, Jiangfan","Zheng, Wei","Gu, Zhujun","Guo, Dongen",A position-aware attention network with progressive detailing for land use semantic segmentation of Remote Sensing images,,NOV 2 2023,1,"Deep learning has achieved remarkable success in the semantic segmentation of remote sensing images (RSIs).In the domain of semantic segmentation, where classification and localization tasks need to be performed simultaneously, it is crucial to consider both global and local spatial relationships in RSIs. This is especially important for the recognition of ground objects that have a slim and elongated appearance. However, existing methods for land use semantic segmentation lack an effective mechanism to coordinate and address these two aspects, resulting in limitations on the recognition of slim targets and the continuity of land object identification. Here, a specific attention-based network called PaANet is developed for semantic segmentation. Our proposed framework builds upon the Swin transformer by incorporating two key modules: the position-aware attention (PaA) module and the pyramid pooling expectation-maximization (PPEM) module. These modules provide significant improvements in recognition accuracy and the continuity of ground object recognition while preserving structural classification details. Furthermore, we propose a multiresolution data augmentation method that utilizes scale-related information to guide the encoder. This approach leads to improved performance and generalization ability for the model. In experiments, the mIoU of our approach for the BLU and GID datasets is 2.37% and 3.94% higher than that of the baseline model, respectively. Our results also show significant superior to those of other methods regarding the continuity of ground object recognition.",Semantic Segmentation,Remote Sensing Images,Attention mechanism,Land-use Classification,,"Qin, Rui",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_376,"Zhang, Lili","Lu, Yushi","Shi, Rui","Shen, Yipin",Towards interpretability lightweight semantic segmentation model for waterbody extraction in large-scale high resolution remote sensing images,,APR 17 2024,2,"In recent years, high resolution (HR) remote sensing images have brought significant changes to land cover monitoring. Monitoring water resources, a vital and scarce commodity for human survival, is a crucial aspect of land cover monitoring and forms the foundation for water resource allocation in regions facing shortages. Hence, a universal method is necessary to address this problem and handle large-scale water resources monitoring. We explore the network structure via improving the interpretability to reduce the network layers and propose a lightweight pixel-wise semantic segmentation model, which achieves waterbody extraction from GF-1 remote sensing images with high accuracy and high speed. The proposed model structure addresses both semantic segmentation for large and small waterbodies, extracting fine-grained edge and contour information from high-resolution feature maps. Additionally, it efficiently extracts multi-scale high-level semantic information using residual convolutional blocks and dilated convolutional blocks. The multi-scale feature maps are fused, and binary classification is predicted through Support Vector Machines (SVM). Furthermore, the paper introduces a model training method with an adaptive learning rate, reducing the overall training time. To validate the model's performance, remote sensing images from GF-1 are utilized to construct a dataset. Experimental results, compared with five models, demonstrate that the proposed method achieves the highest accuracy and least training time.",High resolution (HR) remote sensing images,lightweight semantic segmentation,pixel-wise waterbody extraction,,,"Shao, Yehong",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_377,"Liang, Zhengyin","Wang, Xili",,,SEMANTIC SEGMENTATION NETWORK WITH BAND-LOCATION ADAPTIVE SELECTION MECHANISM FOR MULTISPECTRAL REMOTE SENSING IMAGES,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,1,"How to acquire effective information from multispectral remote sensing images is a challenging task in semantic segmentation of remote sensing images. In this paper, an end-to-end semantic segmentation network (BLASeNet) is proposed. The model adopts an encoder-decoder structure. In the encoder phase, to exploit the band correlation of multispectral remote sensing images, we propose an effective 3D Residual block to encode the spectral-spatial features of images. In order to extract more discriminative features from multispectral images, a band-location adaptive selection mechanism is proposed to adaptively learn the weights of different bands and different spatial locations within a single band, enhancing the expression of features. In the decoder phase, we introduce two trainable parameter matrices W-alpha and W-beta in the skip connections, adaptively adjusting the fusion ratio of low-level detail features and high-level semantic features by network learning, improving the image segmentation accuracy. In addition, we extend the channel attention to 3D data, further refining the fused feature maps. Experimental results on ISPRS Potsdam and Qinghai datasets demonstrate the effectiveness of BLASeNet.",Multispectral images,semantic segmentation,3D convolution,band-location adaptive selection mechanism,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_378,"Wang, Lijun","Li, Bicao","Wang, Bei","Li, Chunlei",PSR-Net: A Dual-Branch Pyramid Semantic Reasoning Network for Segmentation of Remote Sensing Images,,2023,0,"The long-range context information in the semantic segmentation network for remote sensing images (RSIs) plays an important role in the improvement of segmentation performance. However, in large RSIs, the interaction between local information and global information is limited. In order to solve the problem, we propose a dual-branch pyramid semantic reasoning segmentation network. Our dual-branch network consists of a global and local branch. The traditional CNN network is employed on the global branch, and a lightweight multi-scale hierarchical feature aggregation (MHFA) module is introduced into the local branch. In addition, the Feature Semantic Reasoning (FSR) module is proposed to enhance the valuable features and weaken the useless features to improve the semantic representation of RSIs, and then the double branch transformer is embedded. The ablation experiment on the Beijing Land-Use (BLU) dataset illustrates the effectiveness of the added modules, and the results presented by comparison with other traditional networks also confirm the superiority of our proposed network. The proposed network can achieve better segmentation accuracy on large-scale RSI datasets.",Remote Sensing Images,Semantic Segmentation,Multi-scale Hierarchical Feature Aggregation,Feature Semantic Reasoning,,"Huang, Jie","Song, Mengxing",,,"ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2023, PT II",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_379,"Ma, Nan","Sun, Lin","Zhou, Chenghu","He, Yawen",CLOUD DETECTION FOR REMOTE SENSING IMAGES BASED ON DIFFERENCE FEATURES AND SEMANTIC SEGMENTATION NETWORK,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,0,"High-precision cloud detection for remote sensing image is of great significance for applications in agriculture, environment, meteorology and other fields. Cloud detection in bright surface environments and thin clouds identification have always been a challenge in cloud detection research. Aiming at this problem, a cloud detection method for remote sensing images based on difference features and semantic segmentation network is proposed in this paper. Cloudy and cloudless images of the same area are used to obtain difference features, and cloudless images are used as the surface reference. The multi-band cloud detection network based on U-network (U-net) fully learns the difference feature between clouds and the surface, and combines the feature information of the shallow and deep layers to accurately identify the cloud and the surface. Experiments were conducted on the Landsat 8 validation dataset. The results show that the proposed method achieves good performance. It improves the detection accuracy and reduces the misclassification over the bright surface underlying compared with the method based on top of atmosphere reflectance (TOA).",Difference feature,segmentation networks,cloud detection,remote sensing image,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_380,"Pang, Shiyan","Shi, Yepeng","Hu, Hanchun","Ye, Lizhi",PTRSegNet: A Patch-to-Region BottomUp Pyramid Framework for the Semantic Segmentation of Large-Format Remote Sensing Images,,2024,1,"Semantic segmentation is a basic task in the interpretation of remote sensing images. Mainstream deep-learning-based semantic segmentation algorithms typically process images with small sizes. However, remote sensing images typically involve large areas with buildings and water, which have weak textures. Because of the limited range of receptive fields, the semantic segmentation of such areas from small images may lead to problems, such as loss of spatial features and inaccurate boundary extraction. To address these problems, this article devises a patch-to-region framework for the semantic segmentation of large-format remote sensing images. This framework has a bottom-up pyramid structure, where the bottom layer is a small image patch, referred to as a ""patch,"" and the upper layer is a large image region, referred to as a ""region."" The classical semantic segmentation network is first used to process small image patches to obtain pixel-by-pixel semantic features. Then, the pixel-by-pixel semantic features are sparsely reduced into patch-level semantic feature vectors, and the semantic feature vectors of the entire image region are processed through the contextual information extractor to extract the global semantic feature vectors. Subsequently, an information aggregation module is used to integrate the global semantic feature vectors and semantic features to obtain new semantic features with both global and local information. Finally, a lightweight decoding module is used to process the new semantic features to obtain the final semantic segmentation result. The developed framework is evaluated over three public datasets. The results of extensive experiments show that the framework can achieve more accurate and reliable semantic segmentation of high-resolution remote sensing images than state-of-the-art semantic segmentation algorithms. Moreover, ablation studies are performed to verify the effectiveness of each module of the framework.",Feature extraction,Semantics,Transformers,Semantic segmentation,Remote sensing,"Chen, Jia",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Data mining,Convolution,Building extraction,high-resolution remote sensing images,large-format image processing,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_381,"Zhang, Ronghuan","Zhao, Jing","Li, Ming","Zou, Qingzhi",FEST: Feature Enhancement Swin Transformer for Remote Sensing Image Semantic Segmentation,"PROCEEDINGS OF THE 2024 27 TH INTERNATIONAL CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK IN DESIGN, CSCWD 2024",2024,0,"The global context is crucial for the precise segmentation of remote sensing images. However, the large volumes and high spatial resolutions of remote sensing images make efficient analysis of the entire scene challenging for most convolutional neural network (CNN)-based methods. To address this issue, we propose to design an innovative framework for semantic segmentation of remote sensing images called Feature Enhancement Swin Transformer (FEST). Firstly, we utilize the Swin Transformer as the encoder and incorporates a Global Information Enhancement Model (GIEM) within each Swin Transformer block to reduce information loss and enable encoding of more accurate spatial information. Secondly, we introduce an enhanced decoding structure called Enhanced Feature Fusion Module (EFFM) with added enhanced channel and spatial attention modules to retain localized information while obtaining extensive contextual information. Finally, for loss calculation, we utilize the dice and cross-entropy loss to jointly supervise the model, aiming to achieve a competitive performance. We comprehensively evaluated FEST on the ISPRS-Vaihingen and Potsdam datasets. The results indicate that our approach has achieved significant improvements in semantic segmentation tasks compared to existing methods.",global information,semantic segmentation,Swin Transformer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_382,"Yang, Ruiqi","Dai, Qinlin","Cheng, Haiyan","Zhang, Yue",IMPROVING SEMANTIC SEGMENTATION PERFORMANCE BY JOINTLY USING HIGH RESOLUTION REMOTE SENSING IMAGE AND NDSM,"XXIV ISPRS CONGRESS: IMAGING TODAY, FORESEEING TOMORROW, COMMISSION III",2022,2,"Semantic segmentation algorithms based on full convolutional neural network have greatly improved segmentation accuracy of highresolution remote sensing (RS) images. However, the interpretation of RS images from single sensor is still challenging due to the variety and complexity of land objects, the extreme imbalance distributions of land objects on size and numbers. In contrast, multiple sensors can provide complementary information on the land classes, and thus benefit the interpretation. In this context, this research explores the joint use of RGB optical bands and normalized DSM (nDSM) to analyze an urban scene. The method firstly concatenated three channels RGB image and one channel nDSM band into a four-channel image. Thereafter, ResNet-101 network with fine adjustment were utilized as the backbone network to retain multiple feature information by residual blocks. Then the augmented RGB and nDSM images were used to training the network. The established model was evaluated on the Postdam test set. Results show that the proposed method achieves 86.85% on Overall Accuracy (OA), 77.42% Mean Intersection Over Union (MIOU), which is 6.88% and 11.39% higher than the result achieved by single RGB images. Especially, small targets, such as car and tree, are higher. The experimental results show that the simple structure adjustment of ResNet-101 network can achieve good segmentation performance on RS images (especially small targets) after the combination of twice augmented RGB channels and nDSM channels respectively. In addition, with the addition of nDSM, the accuracy of buildings and trees with height information has been improved.",Semantic Segmentation,Deep Learning,nDSM,ResNet,Resolution Remote Sensing,"Chen, Nan","Wang, Leiguang",,,,,Augmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_383,"Lv, Xiaowei","Wang, Rui","Zheng, Chengyu","Yang, Qicheng",Multi-representation decoupled joint network for semantic segmentation of remote sensing images,,FEB 2024,0,"In recent years, semantic segmentation has become an important means of processing remote sensing images, and it is widely used in various fields such as natural disaster detection, environmental protection, and land resource management. In response to this, the mainstream method of the deep convolutional network is constantly innovating and iterating. However, previous methods usually do not fully exploit the information associations between different representations, and the information of low-level representations is usually not well applied. In response to this, we propose a multi-representation decoupled joint network (MDJN) based on a three-branch architecture to improve the performance of semantic segmentation on remote sensing images, which utilizes multi-representation decoupling (MRD) to decouple the original single-branch network into the main branch, body branch and edge branch to enhance information fusion for different representations. Specifically, based on representation learning, we first propose a cross-representation graph convolution (CGC) module to fully mine and learn the context information between different representations with the help of graph convolutional networks (GCN). Secondly, we propose a new three-branch information interaction (TII) module to perform three-way interaction for the information of the three branches, so that the intra-class consistency and inter-class expressivity between different representations can fully play a role. The mean intersection over union (mIoU) of MDJN reaches 78.19% and 81.26% respectively on on both International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam datasets.",Graph convolution,Multi-representation,Decoupling module,Three-way information interaction,Remote sensing image,"Wang, Zhaoxin","Nie, Jie",,,MULTIMEDIA TOOLS AND APPLICATIONS,,Semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_384,"Xu, Penglei","Tang, Hong","Ge, Jiayi","Feng, Lin",ESPC_NASUnet: An End-to-End Super-Resolution Semantic Segmentation Network for Mapping Buildings From Remote Sensing Images,,2021,24,"Higher resolution building mapping from lower resolution remote sensing images is in great demand due to the lack of higher resolution data access, especially in the context of disaster assessment. High resolution building layout map is crucial for emergency rescue after the disaster. The emergency response time would be reduced if detailed building footprints were delineated from more easily available low-resolution data. To achieve this goal, we propose a super-resolution semantic segmentation network calledESPC_NASUnet, which consists of a feature super-resolution module and a semantic segmentation module. To the best of authors' knowledge, this is the first work to systematically explore a deep learning-based approach to generate semantic maps with higher spatial resolution fromlower spatial resolution remote sensing images in an end-to-end fashion. The experimental results for two datasets suggest that the proposed network is the best among four different end-to-end architectures in terms of both pixel-level metrics and object-level metrics. In terms of pixel-level F1-score, the improvements are greater than 0.068 and 0.055. Regarding the object-levelF1-score, the disparities between ESPC_NASUnet and other end-to-end methods are more than 0.083 and 0.161 in the two datasets, respectively. Compared with stage-wise methods, our end-to-end network is less impacted by low-resolution input images. Finally, the proposed network produces building semantic maps comparable to those generated by semantic segmentation networks trained with high-resolution images and the ground truth utilizing the two datasets.",Buildings,Semantics,Remote sensing,Spatial resolution,Image segmentation,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Convolution,Superresolution,Building extraction,end-to-end network,remote sensing,super-resolution semantic segmentation (SRSS),,,,,,,,,,,,,,,,,,,,,,,,
Row_385,"Li, Haifeng","Qiu, Kaijian","Chen, Li","Mei, Xiaoming",SCAttNet: Semantic Segmentation Network With Spatial and Channel Attention Mechanism for High-Resolution Remote Sensing Images,,MAY 2021,163,"High-resolution remote sensing images (HRRSIs) contain substantial ground object information, such as texture, shape, and spatial location. Semantic segmentation, which is an important task for element extraction, has been widely used in processing mass HRRSIs. However, HRRSIs often exhibit large intraclass variance and small interclass variance due to the diversity and complexity of ground objects, thereby bringing great challenges to a semantic segmentation task. In this letter, we propose a new end-to-end semantic segmentation network, which integrates lightweight spatial and channel attention modules that can refine features adaptively. We compare our method with several classic methods on the ISPRS Vaihingen and Potsdam data sets. Experimental results show that our method can achieve better semantic segmentation results. The source codes are available at https://github.com/lehaifeng/SCAttNet.",Semantics,Image segmentation,Feature extraction,Remote sensing,Task analysis,"Hong, Liang","Tao, Chao",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Computational modeling,Training,Attention module,convolutional neural network,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_386,"Zhu, Enze","Chen, Zhan","Wang, Dingkai","Shi, Hanru",UNetMamba: An Efficient UNet-Like Mamba for Semantic Segmentation of High-Resolution Remote Sensing Images,,2025,0,"Semantic segmentation of high-resolution remote sensing images is vital in downstream applications such as land-cover mapping, urban planning, and disaster assessment. Existing Transformer-based methods suffer from the constraint between accuracy and efficiency, while the recently proposed Mamba is renowned for being efficient. Therefore, to overcome the dilemma, we propose UNetMamba, a UNet-like semantic segmentation model based on Mamba. It incorporates a Mamba segmentation decoder (MSD) that can efficiently decode the complex information within high-resolution images, and a local supervision module (LSM), which is train-only but can significantly enhance the perception of local contents. Extensive experiments demonstrate that UNetMamba outperforms the state-of-the-art (SOTA) methods with mIoU increased by 0.87% on LoveDA and 0.39% on ISPRS Vaihingen while achieving high efficiency through the lightweight design, less memory footprint, and reduced computational cost. The source code is available at https://github.com/EnzeZhu2001/UNetMamba.",Semantic segmentation,Decoding,Remote sensing,Semantics,Transformers,"Liu, Xiaoxuan","Wang, Lei",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Accuracy,Visualization,Buildings,Training,Sensors,Mamba,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,
Row_387,"Zheng, Chen","Wang, Leiguang","Chen, Xiaohui",,A Hybrid Markov Random Field Model With Multi-Granularity Information for Semantic Segmentation of Remote Sensing Imagery,,AUG 2019,14,"High-spatial-resolution (HSR) remote sensing images usually contain rich hierarchical semantic information. However, many methods fail to solve the segmentation misclassification problems for HSR images due to just considering one layer of granularity information, such as the pixel granularity layer or the object granularity layer. This paper presents a hybrid Markov random field model for the semantic segmentation of HSR images by paying closer attention to capture the multi-granularity information. In this model, a probability graph with a multilayer structure is first built to represent different granularities information. Then, a hybrid label field is developed to model the multi-granularity classes with the form of a vector, and a new joint distribution is designed to capture the isotropic spatial interactions within each layer of the hybrid label field and the anisotropic spatial interactions between different layers. A generative probabilistic inference is proposed to realize the synergy between the multi-granularity information and the hybrid label interactions by iteratively updating the likelihood function and the joint probability of the hybrid label. The final semantic segmentation result can be achieved when the probabilistic inference converges. Experimental results over different HSR remote sensing images show that the proposed method can achieve more accurate segmentation than other state-of-the-art methods.",Markov random field (MRF),multi-granularity information,remote sensing image,semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_388,"Xiu, Xiaochen","Ma, Xianping","Pun, Man-On","Liu, Ming",AMBNet: Adaptive Multi-feature Balanced Network for Multimodal Remote Sensing Semantic Segmentation,,2024,0,"This work proposes an Adaptive Multi-feature Balanced network (AMBNet) for semantic segmentation in complex urban remote sensing scenarios. To fully exploit optical images and Digital Surface Models (DSM) data obtained from remote sensing sensors, a Depth Feature Extraction and Balancer (DFEB) module is devised to estimate and balance the depth information of all pixels by capturing detailed structural compositions of the ground surface. After that, a Parallel Multi-Stage Segmentator (PMSS) comprised of a dual-branch Encoder and Decoder with skip connections is constructed to perform effective segmentation by exploiting the balanced DSM (BDSM) and optical information. As a result, the proposed AMBNet can make effective use of optical images to complete depth information, so as to achieve multimodal information-assisted semantic segmentation for complex remote sensing scenes. Comprehensive experiments performed on the ISPRS Vaihingen and Potsdam remote sensing datasets confirm the segmentation performance of the proposed method.",Height estimation,Feature fusion,Multimodal semantic segmentation,,,,,,,APSIPA TRANSACTIONS ON SIGNAL AND INFORMATION PROCESSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_389,"Ma, Xianping","Wu, Qianqian","Zhao, Xingyu","Zhang, Xiaokang",SAM-Assisted Remote Sensing Imagery Semantic Segmentation With Object and Boundary Constraints,,2024,1,"Semantic segmentation of remote sensing imagery plays a pivotal role in extracting precise information for diverse downstream applications. Recent development of the segment anything model (SAM), an advanced general-purpose segmentation model, has revolutionized this field, presenting new avenues for accurate and efficient segmentation. However, SAM is limited to generating segmentation results without class information. Meanwhile, the segmentation map predicted by current methods generally exhibits excessive fragmentation and inaccuracy of boundary. This article introduces a streamlined framework designed to leverage the raw output of SAM by exploiting two novel concepts called SAM-generated object (SGO) and SAM-generated boundary (SGB). More specifically, we propose a novel object consistency loss and further introduce a boundary preservation loss in this work. Considering the content characteristics of SGO, we introduce the concept of object consistency to leverage segmented regions lacking semantic information. By imposing constraints on the consistency of predicted values within objects, the object consistency loss aims to enhance semantic segmentation performance. Furthermore, the boundary preservation loss capitalizes on the distinctive features of SGB by directing the model's attention to the boundary information of the object. Experimental results on two well-known datasets, ISPRS Vaihingen and LoveDA Urban, demonstrate the effectiveness and broad applicability of the proposed method. The source code for this work is accessible at https://github.com/sstary/ SSRS.",Semantic segmentation,Remote sensing,Task analysis,Decoding,Transformers,"Pun, Man-On","Huang, Bo",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantics,Computational modeling,Boundary preservation loss,object consistency loss,remote sensing,segment anything model (SAM),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_390,"Zhang, Zhili","Hu, Xiangyun","Yang, Bingnan","Deng, Kai",Enhanced semantic-positional feature fusion network via diverse pre-trained encoders for remote sensing image water-body segmentation,,OCT 2024,0,"In the era of increasingly advanced Earth Observation (EO) technologies, extracting pertinent information (such as water-bodies) from the Earth's surface has become a crucial task. Deep Learning, especially via pre-trained models, currently offers a highly promising approach for the semantic segmentation of Remote Sensing Imagery (RSI). However, effectively adapting these pre-trained models to RSI tasks remains challenging. Typically, these models undergo fine-tuning for specialized tasks, involving modifications to their parameters or structure of the original architecture, which may impact their inherent generalization capabilities. Furthermore, robust pre-trained models on nature images are not specifically designed for RSI, presenting challenges in their direct application to RSI tasks. To alleviate these problems, our study introduces a light-weight Enhanced Semantic-positional Feature Fusion Network (ESFFNet), leveraging diverse pre-trained image encoders alongside extensive EO data. The proposed method begins by leveraging pre-trained encoders, specifically Vision Transformer (ViT)-based and Convolutional Neural Network (CNN)-based models, to extract deep semantic and precise positional features respectively, without additional training. Following this, we introduce the Enhanced Semantic-positional Feature Fusion Module (ESFFM). This module adeptly merges semantic features derived from the ViT-based encoder with spatial features extracted from the CNN-based encoder. Such integration is realized via multi-scale feature fusion, local and long-distance feature integration, and dense connectivity strategies, leading to a robust feature representation. Finally, the Primary Segmentation-guided Fine Extraction Module (PSFEM) further bolsters the precision of remote sensing image segmentation. Collectively, these two modules constitute our light-weight decoder, with a parameter size of less than 4 M. Our approach is evaluated on two distinct water-body datasets, indicating superiority over other leading segmentation techniques. In addition, our method also demonstrates exemplary efficacy in diverse remote sensing segmentation tasks, such as building extraction and land cover classification. The source codes will be available at https://github.com/zhilyzhang/ESFFNet.",Pre-trained image encoders,extensive remote sensing imagery,feature fusion,deep learning,semantic segmentation,"Zhang, Mi","Zhu, Dehui",,,GEO-SPATIAL INFORMATION SCIENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_391,"Wu, Boyang","Cui, Jianyong","Cui, Wenkai","Yuan, Yirong",Fast Semantic Segmentation of Remote Sensing Images Using a Network That Integrates Global and Local Information,,JUN 3 2023,1,"Efficient processing of ultra-high-resolution images is increasingly sought after with the continuous advancement of photography and sensor technology. However, the semantic segmentation of remote sensing images lacks a satisfactory solution to optimize GPU memory utilization and the feature extraction speed. To tackle this challenge, Chen et al. introduced GLNet, a network designed to strike a better balance between GPU memory usage and segmentation accuracy when processing high-resolution images. Building upon GLNet and PFNet, our proposed method, Fast-GLNet, further enhances the feature fusion and segmentation processes. It incorporates the double feature pyramid aggregation (DFPA) module and IFS module for local and global branches, respectively, resulting in superior feature maps and optimized segmentation speed. Extensive experimentation demonstrates that Fast-GLNet achieves faster semantic segmentation while maintaining segmentation quality. Additionally, it effectively optimizes GPU memory utilization. For example, compared to GLNet, Fast-GLNet's mIoU on the Deepglobe dataset increased from 71.6% to 72.1%, and GPU memory usage decreased from 1865 MB to 1639 MB. Notably, Fast-GLNet surpasses existing general-purpose methods, offering a superior trade-off between speed and accuracy in semantic segmentation.",remote sensing images,semantic segmentation,global and local information,,,"Ren, Xiancong",,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_392,"Zheng, Aihua","He, Jinbo","Wang, Ming","Li, Chenglong",Category-Wise Fusion and Enhancement Learning for Multimodal Remote Sensing Image Semantic Segmentation,,2022,9,"This article presents a simple yet effective method called Category-wise Fusion and Enhancement learning (CaFE), which leverages the category priors to achieve effective feature fusion and imbalance learning, for multimodal remote sensing image semantic segmentation. In particular, we disentangle the feature fusion process via the categories to achieve the category-wise fusion based on the fact that the feature fusion in the same category regions tends to have similar characteristics. The disentangled fusion would also increase the fusion capacity with a small number of parameters while reducing the dependence on large-scale training data. For the sample imbalance problem, we design a simple yet effective category-wise enhancement learning scheme. In particular, we assign the weight for each category region based on the proportion of samples in this region over the whole image. By this way, the learning algorithm would focus more on the regions with smaller proportion. Note that both category-wise feature fusion and imbalance learning are only performed in the training stage, and the segmentation efficiency is thus not affected. Experimental results on two benchmark datasets demonstrate the effectiveness of our CaFE against other state-of-the-art methods.",Category-wise enhancement learning (CEL),category-wise fusion,imbalance learning,multimodal remote sensing,semantic segmentation,"Luo, Bin",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_393,"Li, Jinsong","Zhang, Shujun","Han, Qi","Sun, Yuanyuan",CSRL-Net: contextual self-rasterization learning network with joint weight loss for remote sensing image semantic segmentation,,DEC 2 2023,1,"Semantic segmentation plays a vital role in the intelligent comprehension of remote sensing images (RSIs). However, research on semantic segmentation of RSIs still faces the following challenges: 1) The complexity of ground object structures, including variations in scale and shading environments, poses difficulties for current methods in capturing global context. 2) In long-tail distributed remote sensing data, the scarcity of tail classes makes it difficult for their features to be effectively learned, as features of head classes often overshadow them. To address these issues, we propose a contextual self-rasterization learning network (CSRL-Net) for the semantic segmentation of RSIs. Our approach comprises the following two key components. Firstly, a grid context perception mechanism is developed to collaboratively establish context dependencies within and among multi-scale grids, capturing long-range spatial correlations. Secondly, a joint weight loss function is designed to convert the prior knowledge into weight factors. This loss function combines re-weighting and logit adjustment, giving more attention to tail classes and integrally balancing learning bias. To evaluate the effectiveness of our proposed method, we apply it to the Potsdam, Vaihingen, and GID datasets and compare its performance with current advanced models. Experimental results demonstrate that our method achieves excellent performance in terms of MIoU, mean F1 and OA, with improvements ranging from 0.364% to 1.764% compared to the 17 comparison models. Notably, the proposed joint weight loss significantly improves IoU and F1 for tail classes, resulting in increases of 2.909% (IoU) and 2.085% (F1) on the Vaihingen dataset and 4.697% (IoU) and 4.043% (F1) on the GID dataset.",Semantic segmentation,remote sensing images,grid context perception,joint weight loss,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_394,"Feng, Dongdong","Zhang, Zhihua","Yan, Kun",,A Semantic Segmentation Method for Remote Sensing Images Based on the Swin Transformer Fusion Gabor Filter,,2022,16,"Semantic segmentation of remote sensing images is increasingly important in urban planning, autonomous driving, disaster monitoring, and land cover classification. With the development of high-resolution remote sensing satellite technology, multilevel, large-scale, and high-precision segmentation has become the focus of current research. High-resolution remote sensing images have high intraclass diversity and low interclass separability, which pose challenges to the precision of the detailed representation of multiscale information. In this paper, a semantic segmentation method for remote sensing images based on Swin Transformer fusion with a Gabor filter is proposed. First, a Swin Transformer is used as the backbone network to extract image information at different levels. Then, the texture and edge features of the input image are extracted with a Gabor filter, and the multilevel features are merged by introducing a feature aggregation module (FAM) and an attentional embedding module (AEM). Finally, the segmentation result is optimized with the fully connected conditional random field (FC-CRF). Our proposed method, called Swin-S-GF, its mean Intersection over Union (mIoU) scored 80.14%, 66.50%, and 70.61% on the large-scale classification set, the fine land-cover classification set, and the ""AI + Remote Sensing imaging dataset"" (AI+RS dataset), respectively. Compared with DeepLabV3, mIoU increased by 0.67%, 3.43%, and 3.80%, respectively. Therefore, we believe that this model provides a good tool for the semantic segmentation of high-precision remote sensing images.",Image segmentation,Feature extraction,Transformers,Remote sensing,Convolution,,,,,IEEE ACCESS,,Semantics,Image edge detection,FAM,Gabor filter,remote sensing,semantic segmentation,Swin transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_395,"Liu, Jiamin","Wang, Ziyi","Luo, Fulin","Guo, Tan",ESMS-Net: Enhancing Semantic-Mask Segmentation Network With Pyramid Atrousformer for Remote Sensing Image,,2024,0,"Transformers has gained widespread adoption in remote sensing image (RSI) segmentation. However, RSI has densely overlapping terrain and significant shadow, making it challenging to segment the blended boundaries of terrains that are the hard classes. Currently, most transformer-based methods construct the self-attention with a sliding window, which influences the feature receptive fields to conquer the intersecting and overlapping objects. Additionally, they often rarely focus specifically on the representation of these hard segmentation objects. To overcome these challenges, we propose a novel Enhancing Semantic Mask Segmentation Network (ESMS-Net) framework including a local-global joint encoder, an auxiliary enhanced encoder, and a multiscale dense decoder. In the local-global joint encoder, we construct a Pyramid Pooling AtrousFormer (PPAFormer) that performs the self-attention with a pyramid-structured atrous sliding window, which enhances the range of receptive fields and the global representation performance. Meanwhile, we construct the dual-feature fusion module (DFFM) and multilevel feature weighted fusion (MFWF) in the multiscale dense decoder to reduce information loss and facilitate the interaction of deep semantic information. For the auxiliary enhanced encoder, we develop a semantic mask based on the predicted results to maintain the hard segmentation classes, and then use the same structure as the first two stages of the local-global joint encoder to learn the hard regions again. Extensive experiments demonstrate the proposed ESMS-Net can achieve significant improvements for segmentation performance compared with the state-of-the-art methods on the ISPRS-Vaihingen and Potsdam datasets.",Feature extraction,Transformers,Semantics,Decoding,Semantic segmentation,"Yang, Feng","Gao, Xinbo",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Remote sensing,Fuses,Data mining,Convolutional codes,Computer vision,Pyramid atrous structure,remote sensing image (RSI),semantic mask,semantic segmentation,,,,,,,,,,,,,transformer,,,,,,,,
Row_396,"Bello, Inuwa Mamuda","Zhang, Ke","Wang, Jingyu","Aslam, Muhammad Azeem",A Multiscale Segmentation Framework for Uncompleted Building Footprint Extraction from Remote Sensing Imagery,,2021,1,"Building extraction from aerial and satellite images has been playing a significant role in urban development. The deep neural networks' automatic feature extraction capability provides the ease to infer building footprint from remote sensing imagery with greater accuracy. However, designing a classifier that can infer salient features such as the building category remains a challenging task This article proposes a parameter- efficient, multiscale segmentation network for uncompleted building structure extraction. The proposed network was designed based on the architectural framework of the inception module that allows feature learning at multiscale level. Our proposed framework consists of three types of modules known as the subnets that form the encoder, the decoder, and the bottleneck of the network that allow multiscale semantic learning for segmentation application. The experimental result indicates that our proposed network required less training time to attain the best accuracy than state-of-the-art networks. We also present an approach to determine the precise geographical coordinates of the uncompleted building segment's using the georeferencing technique.",Remote sensing,image segmentation,neural networks,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_397,"Wang, Kaiyue","Fan, Xiaoye","Wang, Qi",,FPB-UNet plus plus : Semantic Segmentation for Remote Sensing Images of reservoir area via Improved UNet plus plus with FPN,"6TH INTERNATIONAL CONFERENCE ON INNOVATION IN ARTIFICIAL INTELLIGENCE, ICIAI2022",2022,0,"In order to improve the accuracy of semantic segmentation of remote sensing images in the reservoir area, this paper improves UNet ++, and proposes a UNet ++ semantic segmentation network model fused with feature pyramid network, called FPB-UNet ++. First, in order to fully extract the semantic information of different scales and enhance the recovery ability of the spatial information of remote sensing images, this paper uses the improved feature pyramid structure as the basic unit of the UNet ++ coding structure. Then, the pooling of position information will be lost between each coding unit To remove the layer, use convolution instead. Finally, in order to make full use of multi-scale feature information in the multi-sided output part, all the side output feature maps are stitched and fused in the channel dimension. Through experiments on the open and self-built remote sensing image semantic segmentation data set of Xiaolangdi Reservoir area, the results show that the network model has a good segmentation effect on feature information.",Remote Sensing Image,Semantic Segmentation,UNet plus,Deep Neural Network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_398,"Sun, Long","Li, Lingling","Shao, Yilin","Jiao, Licheng",Which Target to Focus on: Class-Perception for Semantic Segmentation of Remote Sensing,,2023,9,"Deep-learning-based (DL) methods have dominated the task of semantic segmentation of remote sensing images. However, the sizes of different objects vary widely, and there is a great deal of label noise due to the inevitable shadows. Therefore, there is an urgent need for a method that can precisely handle complex ground data. In this article, we propose an interclass enhanced network (ICEN) for representing features of varying sizes. It comprises two branches: sparse representation network (SPN) and feature extraction network (FEN). Then, a class-perception block (CPB) is inserted between the two branches to instruct the SPN's low-level semantic features to be merged into the deeper network. Such a block can reduce label noise in remote sensing image segmentation. In addition, the proposed EIRI provides a more precise classification process for target edges containing many misclassified points without requiring excessive computational overhead. The experimental results of our proposed class-perception network (C-PNet) achieve competitive performance on the Vaihingen, Potsdam, LoveDA, and UAVid datasets.",Feature extraction,Remote sensing,Semantics,Deep learning,Computer architecture,"Liu, Xu","Chen, Puhua","Liu, Fang","Yang, Shuyuan",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Hou, Biao",Semantic segmentation,Neural networks,Class-perception,computational overhead,label noise,multiresolution,remote sensing,sparse representation,,,,,,,,,,,,,,,,,,,,,,
Row_399,"Shi, Wenxu","Meng, Qingyan","Zhang, Linlin","Zhao, Maofan",DSANet: A Deep Supervision-Based Simple Attention Network for Efficient Semantic Segmentation in Remote Sensing Imagery,,NOV 2022,11,"Semantic segmentation for remote sensing images (RSIs) plays an important role in many applications, such as urban planning, environmental protection, agricultural valuation, and military reconnaissance. With the boom in remote sensing technology, numerous RSIs are generated; this is difficult for current complex networks to handle. Efficient networks are the key to solving this challenge. Many previous works aimed at designing lightweight networks or utilizing pruning and knowledge distillation methods to obtain efficient networks, but these methods inevitably reduce the ability of the resulting models to characterize spatial and semantic features. We propose an effective deep supervision-based simple attention network (DSANet) with spatial and semantic enhancement losses to handle these problems. In the network, (1) a lightweight architecture is used as the backbone; (2) deep supervision modules with improved multiscale spatial detail (MSD) and hierarchical semantic enhancement (HSE) losses synergistically strengthen the obtained feature representations; and (3) a simple embedding attention module (EAM) with linear complexity performs long-range relationship modeling. Experiments conducted on two public RSI datasets (the ISPRS Potsdam dataset and Vaihingen dataset) exhibit the substantial advantages of the proposed approach. Our method achieves 79.19% mean intersection over union (mIoU) on the ISPRS Potsdam test set and 72.26% mIoU on the Vaihingen test set with speeds of 470.07 FPS on 512 x 512 images and 5.46 FPS on 6000 x 6000 images using an RTX 3090 GPU.",convolutional neural network (CNN),deep supervision,lightweight model,remote sensing,semantic segmentation,"Su, Chen","Jancso, Tamas",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_400,"Liu, Tao","Cheng, Shuli","Yuan, Jian",,Category-Based Interactive Attention and Perception Fusion Network for Semantic Segmentation of Remote Sensing Images,,OCT 2024,0,"With the development of CNNs and the application of transformers, the segmentation performance of high-resolution remote sensing image semantic segmentation models has been significantly improved. However, the issue of category imbalance in remote sensing images often leads to the model's segmentation ability being biased towards categories with more samples, resulting in suboptimal performance for categories with fewer samples. To make the network's learning and representation capabilities more balanced across different classes, in this paper we propose a category-based interactive attention and perception fusion network (CIAPNet), where the network divides the feature space by category to ensure the fairness of learning and representation for each category. Specifically, the category grouping attention (CGA) module utilizes self-attention to reconstruct the features of each category in a grouped manner, and optimize the foreground-background relationship and its feature representation for each category through the interactive foreground-background relationship optimization (IFBRO) module therein. Additionally, we introduce a detail-aware fusion (DAF) module, which uses shallow detail features to complete the semantic information of deep features. Finally, a multi-scale representation (MSR) module is deployed for each class in the CGA and DAF modules to enhance the description capability of different scale information for each category. Our proposed CIAPNet achieves mIoUs of 54.44%, 85.71%, and 87.88% on the LoveDA urban-rural dataset, and the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam urban datasets, respectively. Compared with current popular methods, our network not only achieves excellent performance but also demonstrates outstanding class balance.",remote images,semantic segmentation,detail-aware fusion,category grouping attention,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_401,"Zhao, Boyu","Zhang, Mengmeng","Wang, Jianbu","Song, Xiukai",Multiple Attention Network for Spartina alterniflora Segmentation Using Multitemporal Remote Sensing Images,,2023,11,"The semantic segmentation of multitemporal remote sensing images to construct wetland land surface coverage is the basis for the perception and dynamic modeling of geographic scenes. However, the segmentation of Spartina alterniflora in remote sensing images on wetlands faces the problems such as a low level of cooperative interpretation in multitemporal images and high fragmentation in the distribution of S. alterniflora. To solve the issues, a multiple attention network (MARNet) based on transfer learning is proposed. The method is designed with a plug-and-play attention module to enhance the learning of vegetation features and improve the network's ability to focus on small areas of S. alterniflora. At the same time, MARNet designs the transfer learning architecture from both interdomain alignment and intradomain adaptation perspectives, aligning the statistical distribution by using the maximum mean difference (MMD) between the source and target domains (TDs), and entropy minimization within the domain of the TD to enhance the high confidence prediction of this domain. In addition, since the samples have a serious imbalance problem, redundant cutting and splicing steps are employed for the prediction results to prevent the poor edge prediction of some image blocks. Experimental results on three cross-year RSI datasets demonstrate that the proposed MARNet performs significantly better than other networks and is able to extract S. alterniflora in wetlands more accurately.",Remote sensing,Feature extraction,Wetlands,Kernel,Semantic segmentation,"Gui, Yuanyuan","Zhang, Yuxiang","Li, Wei",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Transfer learning,Semantics,Attention block,deep learning,multitemporal remote sensing images,Spartina alterniflora segmentation,transfer learning,,,,,,,,,,,,,,,,,,,,,,,
Row_402,"Nuradili, Pakezhamu","Zhou, Ji","Zhou, Xiangbing","Ma, Jin",UAV Remote-Sensing Image Semantic Segmentation Strategy Based on Thermal Infrared and Multispectral Image Features,,SEP 2023,4,"The availability of high-resolution imagery resources for semantic segmentation research has expanded significantly due to the rapid development of remote-sensing technology utilizing unmanned aerial vehicles (UAVs). These images provide researchers with a more accurate view of the region of interest and allow for more detailed analysis and interpretation of the images. However, semantic segmentation based on UAV remote-sensing imagery still faces new challenges in deriving ground objects. In contrast to the commonly used multispectral (MS) imagery, thermal infrared (TIR) imagery can record the emission of ground objects, making the temperature characteristics of TIR imagery and the color characteristics of MS imagery complementary. These two approaches can be used synergistically to provide more comprehensive image information. On this basis, we propose a strategy for semantic segmentation of UAV images by utilizing both TIR and MS image features. The approach combines principal component analysis (PCA) transformation with a deep learning semantic segmentation network, namely, Deeplv3. The effectiveness of the proposed strategy is evaluated by comparing it with both traditional supervised classification algorithms and deep learning algorithms. According to the results, the proposed strategy exhibits greater robustness, achieving a mean pixel accuracy (MPA) of 92.8% and a mean intersection over union (MIOU) of 73.5%. These results outperform several classical deep learning semantic segmentation algorithms that were also evaluated. The proposed strategy would be beneficial to promote the development of semantic segmentation technology for UAV remote-sensing images.",Semantic segmentation,Remote sensing,Feature extraction,Deep learning,Classification algorithms,"Wang, Ziwei","Meng, Lingxuan","Tang, Wenbin","Meng, Yizhen",ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,Autonomous aerial vehicles,Semantics,image semantic segmentation,multispectral (MS) image,principal component analysis (PCA) image fusion,thermal infrared (TIR) image,unmanned aerial vehicle (UAV) remote sensing,,,,,,,,,,,,,,,,,,,,,,,
Row_403,"Liu, Jiang","Cheng, Shuli","Du, Anyu",,Multi-View Feature Fusion and Rich Information Refinement Network for Semantic Segmentation of Remote Sensing Images,,SEP 2024,0,"Semantic segmentation is currently a hot topic in remote sensing image processing. There are extensive applications in land planning and surveying. Many current studies combine Convolutional Neural Networks (CNNs), which extract local information, with Transformers, which capture global information, to obtain richer information. However, the fused feature information is not sufficiently enriched and it often lacks detailed refinement. To address this issue, we propose a novel method called the Multi-View Feature Fusion and Rich Information Refinement Network (MFRNet). Our model is equipped with the Multi-View Feature Fusion Block (MAFF) to merge various types of information, including local, non-local, channel, and positional information. Within MAFF, we introduce two innovative methods. The Sliding Heterogeneous Multi-Head Attention (SHMA) extracts local, non-local, and positional information using a sliding window, while the Multi-Scale Hierarchical Compressed Channel Attention (MSCA) leverages bar-shaped pooling kernels and stepwise compression to obtain reliable channel information. Additionally, we introduce the Efficient Feature Refinement Module (EFRM), which enhances segmentation accuracy by interacting the results of the Long-Range Information Perception Branch and the Local Semantic Information Perception Branch. We evaluate our model on the ISPRS Vaihingen and Potsdam datasets. We conducted extensive comparison experiments with state-of-the-art models and verified that MFRNet outperforms other models.",semantic segmentation,feature refinement,remote sensing,multi-view feature fusion,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_404,"Sun, Shihao","Yang, Lei","Liu, Wenjie","Li, Ruirui",Feature Fusion through Multitask CNN for Large-scale Remote Sensing Image Segmentation,2018 10TH IAPR WORKSHOP ON PATTERN RECOGNITION IN REMOTE SENSING (PRRS),2018,1,"In recent years, Fully Convolutional Networks (FCN) has been widely used in various semantic segmentation tasks, including multi-modal remote sensing imagery. How to fuse multi-modal data to improve the segmentation performance has always been a research hotspot. In this paper, a novel end-to-end fully convolutional neural network is proposed for semantic segmentation of natural color, infrared imagery and Digital Surface Models (DSM). It is based on a modified DeepUNet and perform the segmentation in a multi-task way. The channels are clustered into groups and processed on different task pipelines. After a series of segmentation and fusion, their shared features and private features are successfully merged together. Experiment results show that the feature fusion network is efficient. And our approach achieves good performance in ISPRS Semantic Labeling Contest (2D).",remote sensing,feature fusion,DeepUNet,ISPRS,semantic segmentation,,,,,,,pixel-wise classification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_405,"Zheng, Jianwei","Shao, Anhao","Yan, Yidong","Wu, Jie",Remote Sensing Semantic Segmentation via Boundary Supervision-Aided Multiscale Channelwise Cross Attention Network,,2023,16,"High spatial resolution (HSR) remote sensing (RS) images inevitably pose the challenge of multiscale transformation, as small objects, such as cars and helicopters (HCs), may occupy only a few pixel points. This incurs a significant hurdle for global context modeling, particularly in backbone networks with large downsampling coefficients. Simple summation or concatenation techniques, such as skip connections, fail to address semantic gaps and even impose negative impacts on multiscale feature fusion. Meanwhile, due to the complexity of foreground objects, the boundary details of HSR RS images are easy to lose in sampling operations. To overcome these challenges, we propose a multiscale channelwise cross attention network (MCCANet) assisted by boundary supervision (BS). Technically, MCCA captures the channel attention (CA) with various scales, which allows dynamic and adaptive feature fusion in a contextual scale-aware manner and focuses on both large and small objects distributed throughout the inputs. Besides, a channel and context strainer (CCS) module is proposed and embedded in MCCA, filtering channels and contexts for the mitigation of intraclass differences. In addition, we apply a BS module to recover boundary contour, avoiding the blurring effect during the construction of contextual information. The refined boundary allows for the effective recognition of surrounding pixels, ensuring a better segmentation performance. Extensive experiments on the instance segmentation in aerial images dataset (iSAID), International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam, and land-cover domain adaptive (LoveDA) datasets demonstrate that our proposed MCCANet achieves a good balance of high accuracy and efficiency. Code will be available at: https://github.com/ZhengJianwei2/MCCANet.",Attention module,boundary supervision (BS),convolutional neural network (CNN),remote sensing (RS),semantic segmentation,"Zhang, Meiyu",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_406,"Hou, Yandong","Wu, Zhengbo","Ren, Xinghua","Liu, Kaiwen",BFFNet: a bidirectional feature fusion network for semantic segmentation of remote sensing objects,,FEB 29 2024,8,"PurposeHigh-resolution remote sensing images possess a wealth of semantic information. However, these images often contain objects of different sizes and distributions, which make the semantic segmentation task challenging. In this paper, a bidirectional feature fusion network (BFFNet) is designed to address this challenge, which aims at increasing the accurate recognition of surface objects in order to effectively classify special features.Design/methodology/approachThere are two main crucial elements in BFFNet. Firstly, the mean-weighted module (MWM) is used to obtain the key features in the main network. Secondly, the proposed polarization enhanced branch network performs feature extraction simultaneously with the main network to obtain different feature information. The authors then fuse these two features in both directions while applying a cross-entropy loss function to monitor the network training process. Finally, BFFNet is validated on two publicly available datasets, Potsdam and Vaihingen.FindingsIn this paper, a quantitative analysis method is used to illustrate that the proposed network achieves superior performance of 2-6%, respectively, compared to other mainstream segmentation networks from experimental results on two datasets. Complete ablation experiments are also conducted to demonstrate the effectiveness of the elements in the network. In summary, BFFNet has proven to be effective in achieving accurate identification of small objects and in reducing the effect of shadows on the segmentation process.Originality/valueThe originality of the paper is the proposal of a BFFNet based on multi-scale and multi-attention strategies to improve the ability to accurately segment high-resolution and complex remote sensing images, especially for small objects and shadow-obscured objects.",Remote sensing images,Semantic segmentation,Deep learning,Branch network,Multi-scale fusion,"Chen, Zhengquan",,,,INTERNATIONAL JOURNAL OF INTELLIGENT COMPUTING AND CYBERNETICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_407,"Guo, Zhiling","Wu, Guangming","Song, Xiaoya","Yuan, Wei",Super-Resolution Integrated Building Semantic Segmentation for Multi-Source Remote Sensing Imagery,,2019,39,"Multi-source remote sensing imagery has become widely accessible owing to the development of data acquisition systems. In this paper, we address the challenging task of the semantic segmentation of buildings via multi-source remote sensing imagery with different spatial resolutions. Unlike previous works that mainly focused on optimizing the segmentation model, which did not enable the severe problems caused by the unaligned resolution between the training and testing data to be fundamentally solved, we propose to integrate SR techniques with the existing framework to enhance the segmentation performance. The feasibility of the proposed method was evaluated by utilizing representative multi-source study materials: high-resolution (HR) aerial and low-resolution (LR) panchromatic satellite imagery as the training and testing data, respectively. Instead of directly conducting building segmentation from the LR imagery by using the model trained using the HR imagery, the deep learning-based super-resolution (SR) model was first adopted to super-resolved LR imagery into SR space, which could mitigate the influence of the difference in resolution between the training and testing data. The experimental results obtained from the test area in Tokyo, Japan, demonstrate that the proposed SR-integrated method significantly outperforms that without SR, improving the Jaccard index and kappa by approximately 19.01% and 19.10%, respectively. The results confirmed that the proposed method is a viable tool for building semantic segmentation, especially when the resolution is unaligned.",Building segmentation,deep learning,remote sensing,super-resolution,,"Chen, Qi","Zhang, Haoran","Shi, Xiaodan","Xu, Mingzhou",IEEE ACCESS,"Xu, Yongwei",,,,,,,,,,"Shibasaki, Ryosuke","Shao, Xiaowei",,,,,,,,,,,,,,,,,,,
Row_408,"Wang, Hengyou","Li, Xiao","Huo, Lianzhi","Hu, Changmiao",Global and edge enhanced transformer for semantic segmentation of remote sensing,,APR 2024,1,"Global context information and edge information are the keys to remote sensing (RS) image semantic segmentation. However, the existing methods have limited ability to obtain global and edge information, and category edge blurring and efficiency problems in small-scale object recognition in remote sensing image semantic segmentation tasks. In this work, we propose a global and edge enhanced Transformer (GE-Swin) for the semantic segmentation of remote sensing images. To improve the sensitivity to edge information, we design dual decoders based on the parallel model. One is the main decoder, which extracts multi-level semantic information from multi-scale features. The other is an auxiliary decoder related to low-layer features with low resolution. Thus, the auxiliary decoder has better sensitivity to edge information. Then, the feature fusion module (FFM) is designed between the encoder and decoder to fuse the multilevel features, enhancing the model's ability to obtain global features. Finally, to verify the performance of the proposed approach, we perform extensive experiments with the ISPRS and LoveDA datasets. The experimental results illustrate that the proposed model achieves superior performance compared to state-of-the-art methods.",Semantic segmentation,Feature fusion module,Auxiliary decoder,Transformer,,,,,,APPLIED INTELLIGENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_409,"Zhang, Jing","Li, Bin","Li, Jun",,A Novel Semantic Segmentation Method for Remote Sensing Images Through Adaptive Scale-Based Convolution Neural Network,,2024,0,"In semantic segmentation tasks for remote sensing images, effective feature extraction acts as the most important foundation. As a result, this paper proposes a novel semantic segmentation method for remote sensing images through adaptive scale-based convolution neural network. Firstly, it uses an encoder to extract features from remote sensing images, and utilizes attention mechanisms to control the information flow in the neural network. This is expected to reduce the impact between different scales. Then, a semantic segmentation for updating scale weights is established, and a scale-adaptive convolutional neural network is constructed. The upsampling unit is improved to increase the resolution to original image level, in order to better identify smaller targets. For large-sized problems, pyramid-like processing is used to segment images from multiple scales, and results are finally merged. Besides, we also make some experiments on ISPRS Potsdam dataset, UC Merced dataset, and DeepGlobe dataset, in order to make performance evaluation. The research shows that the maximum pixel accuracy of the proposed remote sensing semantic segmentation method is increased to 86.18%, the average value of the semantic segmentation task is up to 63.72, and the fastest running speed is up to 9.16FPS. In other words, the method proposed in this study has more accurate processing results and better stability.",Remote sensing,Semantic segmentation,Feature extraction,Kernel,Convolutional neural networks,,,,,IEEE ACCESS,,Accuracy,Adaptive scale,convolutional neural network,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_410,"Ju, Haoran","Bi, Fukun","Bian, Mingming","Shi, Yinni",Multiscale feature fusion network for automatic port segmentation from remote sensing images,,OCT 2022,1,"In recent years, remote sensing image observation technology has developed rapidly. Extracting coastlines from remote sensing images has become an indispensable means of port area measurement. Port segmentation from remote sensing images is an important method of coastline extraction and measurement. The remote sensing images used are panchromatic remote sensing images. Due to complex remote sensing image scenes and the large difference in feature information at different scales, traditional segmentation methods cannot perform effective extraction, and it is difficult to accurately segment the coastline in remote sensing images. We propose a multiscale feature fusion network for automatic port segmentation from remote sensing images. First, to reduce the redundant parameters and complex operation problems in traditional convolutional neural networks, we propose using MobileNetv2 as the base network for feature extraction to achieve a lightweight model. Then aiming at the feature differences of remote sensing images at different scales, we present atrous convolution as a convolution method for the entire network and combine a multiscale feature fusion method to extract the features of remote sensing images and improve the feature extraction ability. To reduce the problem of ships calling at the port being easily mistaken for port area and causing false segmentation, we propose a method of eliminating the ship area to reduce the interference. Finally, the comprehensive evaluation of a large number of Google port remote sensing data shows that compared with existing methods, the proposed method has the characteristics of being lightweight and having high precision. (c) 2022 Society of Photo-Optical Instrumentation Engineers (SPIE)",remote sensing image segmentation,portsegmentation,multiscale features,semantic,segmentation,,,,,JOURNAL OF APPLIED REMOTE SENSING,,weighted loss,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_411,"Cai, Yuxiang","Yang, Yingchun","Zheng, Qiyi","Shen, Zhengwei",BiFDANet: Unsupervised Bidirectional Domain Adaptation for Semantic Segmentation of Remote Sensing Images,,JAN 2022,19,"When segmenting massive amounts of remote sensing images collected from different satellites or geographic locations (cities), the pre-trained deep learning models cannot always output satisfactory predictions. To deal with this issue, domain adaptation has been widely utilized to enhance the generalization abilities of the segmentation models. Most of the existing domain adaptation methods, which based on image-to-image translation, firstly transfer the source images to the pseudo-target images, adapt the classifier from the source domain to the target domain. However, these unidirectional methods suffer from the following two limitations: (1) they do not consider the inverse procedure and they cannot fully take advantage of the information from the other domain, which is also beneficial, as confirmed by our experiments; (2) these methods may fail in the cases where transferring the source images to the pseudo-target images is difficult. In this paper, in order to solve these problems, we propose a novel framework BiFDANet for unsupervised bidirectional domain adaptation in the semantic segmentation of remote sensing images. It optimizes the segmentation models in two opposite directions. In the source-to-target direction, BiFDANet learns to transfer the source images to the pseudo-target images and adapts the classifier to the target domain. In the opposite direction, BiFDANet transfers the target images to the pseudo-source images and optimizes the source classifier. At test stage, we make the best of the source classifier and the target classifier, which complement each other with a simple linear combination method, further improving the performance of our BiFDANet. Furthermore, we propose a new bidirectional semantic consistency loss for our BiFDANet to maintain the semantic consistency during the bidirectional image-to-image translation process. The experiments on two datasets including satellite images and aerial images demonstrate the superiority of our method against existing unidirectional methods.",unsupervised domain adaptation,bidirectional domain adaptation,convolutional neural networks (CNNs),image-to-image translation,generative adversarial networks (GANs),"Shang, Yongheng","Yin, Jianwei","Shi, Zhongtian",,REMOTE SENSING,,remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_412,"Zhang, Tianyang","Zhang, Xiangrong","Zhu, Peng","Tang, Xu",Semantic Attention and Scale Complementary Network for Instance Segmentation in Remote Sensing Images,,OCT 2022,31,"In this article, we focus on the challenging multicategory instance segmentation problem in remote sensing images (RSIs), which aims at predicting the categories of all instances and localizing them with pixel-level masks. Although many landmark frameworks have demonstrated promising performance in instance segmentation, the complexity in the background and scale variability instances still remain challenging, for instance, segmentation of RSIs. To address the above problems, we propose an end-to-end multicategory instance segmentation model, namely, the semantic attention (SEA) and scale complementary network, which mainly consists of a SEA module and a scale complementary mask branch (SCMB). The SEA module contains a simple fully convolutional semantic segmentation branch with extra supervision to strengthen the activation of interest instances on the feature map and reduce the background noise's interference. To handle the undersegmentation of geospatial instances with large varying scales, we design the SCMB that extends the original single mask branch to trident mask branches and introduces complementary mask supervision at different scales to sufficiently leverage the multiscale information. We conduct comprehensive experiments to evaluate the effectiveness of our proposed method on the iSAID dataset and the NWPU Instance Segmentation dataset and achieve promising performance.",Semantics,Image segmentation,Remote sensing,Feature extraction,Proposals,"Li, Chen","Jiao, Licheng","Zhou, Huiyu",,IEEE TRANSACTIONS ON CYBERNETICS,,Marine vehicles,Task analysis,Instance segmentation,remote sensing images (RSIs),scale complementarity,semantic attention (SEA),,,,,,,,,,,,,,,,,,,,,,,,
Row_413,"Huynh-The, Thien","Truong, Son Ngoc","Nguyen, Gia-Vuong",,HBSeNet: A Hybrid Bilateral Network for Accurate Semantic Segmentation of Remote Sensing Images,,2024,0,"Semantic segmentation of aerial and satellite images plays a crucial role in a wide range of applications and services, catering to the increasing needs of environmental resource management, urban planning, and traffic safety. Many efficient semantic segmentation methods have been proposed by exploiting deep learning techniques with convolution neural networks (CNNs) architectures and self-attention mechanisms to achieve superior accuracy if compared with conventional machine learning-based approaches. In this article, we introduce a hybrid bilateral segmentation network (HBSeNet), a novel semantic segmentation architecture. Inspired by the success of dual-path network models that replace conventional single-branch encoder-decoder architectures, we construct a model with the core idea of combining the context path and the spatial path to optimize both the accuracy and the complexity of deep learning model in the field of remote sensing image segmentation. Moreover, HBSeNet innovates with auxiliary modules designed to enhance its performance, such as sequential atrous convolution, information synthesis module, and bridge for efficient multiscale feature extraction, fusion, and integration. In simulations, our model achieves a global accuracy of 92.04%, a mean intersection-over-union of 83.57%, and a mean boundary-F1-score of 90.23% when evaluated on the ISPRS Potsdam dataset, surpassing the state-of-the-art segmentation models, such as DeepLabV3+, SwinCNN, and ST-UNet.",Semantic segmentation,Computer architecture,Accuracy,Feature extraction,Convolution,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Remote sensing,Context modeling,Artificial intelligence,bilateral network,image segmentation,object recognition,remote sensing,,,,,,,,,,,,,,,,,,,,,,,
Row_414,"Gu, Xingjian","Li, Sizhe","Ren, Shougang","Zheng, Hengbiao",Adaptive enhanced swin transformer with U-net for remote sensing image segmentation*,,SEP 2022,21,"Semantic segmentation of remote sensing images often faces complex situations, such as variable scale objects, large intra-class differences, and imbalanced distribution among classes. Convolutional Neural Network (CNN) based models have been widely used in remote sensing image segmentation tasks for its powerful feature extraction capability. Due to intrinsic locality of CNN architectures, it is difficult to understand the long-range dependencies among image patches. Recently, the transformer leverages long-range dependencies and performs well in computer vision tasks. To take advantages of both CNN and Transformer, a novel Adaptive Enhanced Swin Transformer with U-Net (AESwin-UNet) is proposed for remote sensing segmentation. AESwinUNet uses a hybrid Transformer-based U-type Encoder-Decoder architecture with skip connections to extract local and global semantic features. Specifically, the Enhanced Swin Transformer (E-Swin Transformer) contains Enhanced Multi-head Self-Attention and Deformable Adaptive Patch Merging layer in encoder. A symmetric cascaded decoder is designed for up-sampling to obtain higher resolution feature maps. Experiments on two public benchmark datasets, WHDLD and LoveDA, demonstrate that the proposed AESwin-UNet performs well in semantic segmentation.",Remote sensing,Semantic segmentation,Unet,Transformer,CNN,"Fan, Chengcheng","Xu, Huanliang",,,COMPUTERS & ELECTRICAL ENGINEERING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_415,"Zhong, Hai-Feng","Sun, Qing","Sun, Hong-Mei","Jia, Rui-Sheng",NT-Net: A Semantic Segmentation Network for Extracting Lake Water Bodies From Optical Remote Sensing Images Based on Transformer,,2022,34,"The automatic extraction of lake water is one of the research hotspots in the field of remote sensing image processing. Due to the small interclass variance between lakes and other ground objects, and the complex texture characteristics of lake boundaries, existing methods often have problems such as over-segmentation and inaccurate boundary segmentation when segmenting lake water bodies. To alleviate these problems, this article designs an end-to-end semantic segmentation network [noise-canceling transformer network (NT-Net)] for the automatic extraction of lake water bodies from remote sensing images. Aiming at the problem of over-segmentation caused by nonlake objects, an interference attenuation module is designed in the network. This module can model the key features that are distinguishable and suitable for segmenting lake water by analyzing the difference in feature representation between lakes and other ground objects, thus suppressing the feature representation of nonlake objects. To more accurately segment the lake boundary, a multilevel transformer module is designed. This module can capture the context association of boundary information and enhance the feature representation of boundary information by using the self-attention mechanism. The comparative experimental results show that, compared with the current mainstream semantic segmentation networks, the method in this article has advantages in extracting lake water bodies comprehensively and coherently.",Lakes,Image segmentation,Transformers,Feature extraction,Remote sensing,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantics,Data mining,Convolutional neural network (CNN),lake segmentation,optical remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_416,"Wang, Zhimin","Wang, Jiasheng","Yang, Kun","Wang, Limeng",Semantic segmentation of high-resolution remote sensing images based on a class feature attention mechanism fused with Deeplabv3+,,JAN 2022,75,"Aiming at solving the problems of inaccurate segmentation of edge targets, inconsistent segmentation of different types of targets, and slow prediction efficiency on semantic segmentation of high-resolution remote sensing images by classical semantic segmentation network, this study proposed a class feature attention mechanism fused with an improved Deeplabv3+ network called CFAMNet for semantic segmentation of common features in remote sensing images. First, the correlation between classes is enhanced using the class feature attention module to extract and process different categories of semantic information better. Second, the multi-parallel atrous spatial pyramid pooling structure is used to enhance the correlation between spaces, to extract the context information of different scales of an image better. Finally, the encoder-decoder structure is used to refine the segmentation results. The segmentation effect of the proposed network is verified by experiments on the public data set GaoFen image dataset (GID). The experimental results show that the CFAMNet can achieve the mean intersection over union (MIOU) and overall accuracy (OA) of 77.22% and 85.01%, respectively, on the GID, thus surpassing the current mainstream semantic segmentation networks.",Remote sensing,Deep learning,Convolution neural network,Semantic segmentation,Attention mechanism,"Su, Fanjie","Chen, Xinya",,,COMPUTERS & GEOSCIENCES,,Deeplabv3+,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_417,"Chen, Guanzhou","Zhang, Xiaodong","Wang, Qing","Dai, Fan",Symmetrical Dense-Shortcut Deep Fully Convolutional Networks for Semantic Segmentation of Very-High-Resolution Remote Sensing Images,,MAY 2018,122,"Semantic segmentation has emerged as a mainstream method in very-high-resolution remote sensing land-use/land-cover applications. In this paper, we first review the state-of-the-art semantic segmentation models in both computer vision and remote sensing fields. Subsequently, we introduce two semantic segmentation frameworks: SNFCN and SDFCN, both of which contain deep fully convolutional networks with shortcut blocks. We adopt an overlay strategy as the postprocessing method. Based on our frameworks, we conducted experiments on two online ISPRS datasets: Vaihingen and Potsdam. The results indicate that our frameworks achieve higher overall accuracy than the classic FCN-8s and Seg-Net models. In addition, our postprocessing method can increase the overall accuracy by about 1%-2% and help to eliminate ""salt and pepper"" phenomena and block effects.",Convolutional neural networks (CNN),deep learning (DL),fully convolutional networks (FCN),remote sensing,SDFCN,"Gong, Yuanfu","Zhu, Kun",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_418,"Zhang, Bin","Zhang, Yongjun","Li, Yansheng","Wan, Yi",Semi-supervised Deep Learning via Transformation Consistency Regularization for Remote Sensing Image Semantic Segmentation,,2023,13,"Deep convolutional neural networks have gotten a lot of press in the last several years, especially in domains like computer vision and remote sensing (RS). However, achieving superior performance with deep networks highly depends on a massive number of accurately labeled training samples. In real-world applications, gathering a large number of labeled samples is time consuming and labor intensive, especially for pixel-level data annotation. This dearth of labels in land-cover classification is especially pressing in the RS domain because high-precision high-quality labeled samples are extremely difficult to acquire, but unlabeled data are readily available. In this study, we offer a new semisupervised deep semantic labeling framework for the semantic segmentation of high-resolution RS images to take advantage of the limited amount of labeled examples and numerous unlabeled samples. Our model uses transformation consistency regularization to encourage consistent network predictions under different random transformations or perturbations. We try three different transforms to compute the consistency loss and analyze their performance. Then, we present a deep semisupervised semantic labeling technique by using a hybrid transformation consistency regularization. A weighted sum of losses, which contains a supervised term computed on labeled samples and an unsupervised regularization term computed on unlabeled data, may be used to update the network parameters in our technique. Our comprehensive experiments on two RS datasets confirmed that the suggested approach utilized latent information from unlabeled samples to obtain more precise predictions and outperformed existing semisupervised algorithms in terms of performance. Our experiments further demonstrated that our semisupervised semantic labeling strategy has the potential to partially tackle the problem of limited labeled samples for high-resolution RS image land-cover segmentation.",Consistency regularization,convolutional neural network (CNN),semantic segmentation,semisupervised learning (SSL),unlabeled data,"Guo, Haoyu","Zheng, Zhi","Yang, Kun",,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,remote sensing (RS) imagery,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_419,"Lu, Chen","Zhang, Xian","Du, Kaile","Xu, Han",CTCFNet: CNN-Transformer Complementary and Fusion Network for High-Resolution Remote Sensing Image Semantic Segmentation,,2024,0,"Semantic segmentation of high-resolution remote sensing images poses challenges such as scale variability, diverse objects, and obstruction by surface elements. These factors often lead existing methods to suffer from issues like missed and false detections, as well as coarse segmentation boundaries. To tackle these challenges, this article proposes a CNN-transformer complementary and fusion network, termed as CTCFNet. It aims to enhance segmentation accuracy and robustness by extracting and integrating the complementary global and local information from high-resolution remote sensing images. The CTCFNet operates through two primary stages: feature extraction and fusion. In the feature extraction stage, a feature extractor employs convolutional neural network (CNN) and pyramid vision transformer (PVT) blocks to extract both local and global features. A boundary loss is also proposed to improve the segmentation performance for object textures and boundaries. In the feature fusion stage, a feature aggregation module (FAM) is first designed to effectively fuse local and global features at the same scale, facilitating the feature extractor to obtain more comprehensive representations. On this basis, a bi-directional decoder (BiDecoder) reconstructs multiscale features through both top-down and bottom-up directions, resulting in more precise segmentation outputs. Experiments on several high-resolution remote sensing image datasets demonstrate that the proposed method outperforms the state-of-the-art methods in terms of segmentation accuracy and generalization. The code is available at https://github.com/ChenLu0000/CTCFNet.",Image segmentation,Feature extraction,Remote sensing,Transformers,Decoding,"Liu, Guangcan",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantic segmentation,Bidirectional control,Complementary information,convolutional neural network (CNN) transformer,feature fusion,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_420,"Yu, Lei","Jin, Qizhao","Wang, Wei",,Intra- and Inter-Image Causal Intervention for Robust Semantic Segmentation in Remote-Sensing Images,,2024,0,"Semantic segmentation in remote-sensing (RS) images is a critical component for advanced RS scene understanding. Traditional methods learn to segment using direct supervision of paired data, yet fall short by not fully addressing the noncausal biases behind RS images. These include intra-image biases, such as varying background and object appearance, and inter-image biases, such as inconsistent geographical environments and class distributions, which can mislead models to infer by relying on these biases. Given the high cost and effort needed for pixelwise annotation, the limited volume of RS images exacerbates this problem. In this letter, we propose a novel intra- and inter-image causal intervention (I-3-CI) learning paradigm, designed specifically to mitigate the influence of both biases on RS semantic segmentation. Specifically, to reduce the intra-image biases, our I-3-CI utilizes intra-image contrastive learning to promote closer alignment of features from the same category and distancing those of disparate categories in the same image. Consequently, the compact feature space is more robust to the varying appearance. As for eliminating the inter-image level biases, our I-3-CI further explores the inter-image contrastive learning of all pixels within the entire dataset. To overcome the difficulty of learning from all pixels of the dataset simultaneously, our I-3-CI introduces a set of proxy prototype features to keep track of the global centroids for the features of different categories. We substantiate the I-3-CI paradigm's efficacy through rigorous testing on the LoveDA, Vaihingen, and Potsdam datasets with an average improvement of 0.55% on the mIoU metric, proving the value of causal interventions in achieving robust and accurate semantic segmentation.",Causal intervention,contrastive learning,noncausal biases,remote-sensing (RS) image,semantic segmentation,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_421,"Jiang, Zhongze","Chen, Zhong","Ji, Kaixiang","Yang, Jian",Semantic segmentation network combined with edge detection for building extraction in remote sensing images,MIPPR 2019: PATTERN RECOGNITION AND COMPUTER VISION,2020,3,"Extracting buildings from remote sensing images is a significant task with many applications such as map drawing, city planning, population estimation, etc. However, traditional methods that rely on artificially designed features struggle to perform well due to the diverse appearance and complicated background. In this paper, we design an end-to-end convolutional neural network that combines semantic segmentation and edge detection for building extraction. In addition, we propose a residual unit combined with spatial pyramid pooling (SPP-RU) to yield representations of different size receptive fields by multi-branch network. We conduct experiments on WHU building dataset, and the experimental results demonstrate the effectiveness of our method in terms of quantitative and qualitative performance compared with state-of-the-art methods.",Building extraction,Edge detection,Semantic Segmentation,Convolutional neural network,Remote sensing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_422,"Zhang, Qi","Geng, Guohua","Zhou, Pengbo","Liu, Qinglin",Link Aggregation for Skip Connection-Mamba: Remote Sensing Image Segmentation Network Based on Link Aggregation Mamba,,OCT 2024,0,"The semantic segmentation of satellite and UAV remote sensing imagery is pivotal for address exploration, change detection, quantitative analysis and urban planning. Recent advancements have seen an influx of segmentation networks utilizing convolutional neural networks and transformers. However, the intricate geographical features and varied land cover boundary interferences in remote sensing imagery still challenge conventional segmentation networks' spatial representation and long-range dependency capabilities. This paper introduces a novel U-Net-like network for UAV image segmentation. We developed a link aggregation Mamba at the critical skip connection stage of UNetFormer. This approach maps and aggregates multi-scale features from different stages into a unified linear dimension through four Mamba branches containing state-space models (SSMs), ultimately decoupling and fusing these features to restore the contextual relationships in the mask. Moreover, the Mix-Mamba module is incorporated, leveraging a parallel self-attention mechanism with SSMs to merge the advantages of a global receptive field and reduce modeling complexity. This module facilitates nonlinear modeling across different channels and spaces through multipath activation, catering to international and local long-range dependencies. Evaluations on public remote sensing datasets like LovaDA, UAVid and Vaihingen underscore the state-of-the-art performance of our approach.",semantic segmentation,remote sensing,Mamba,state-space model,link aggregation,"Wang, Yong","Li, Kang",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_423,"Lin, Baihong","Zou, Zhengxia","Shi, Zhenwei",,RSBEV: Multiview Collaborative Segmentation of 3-D Remote Sensing Scenes With Bird's-Eye-View Representation,,2024,0,"Perception of 3-D remote sensing scenes plays a crucial role in accurately recognizing and locating ground objects, as it enables a deeper understanding of complex environments by capturing scene geometry, object relationships, and occlusion patterns. Inspired by the powerful multisensor fusion capabilities in autonomous driving, we explore a new task in this article: given a set of multiview images of a 3-D remote sensing scene, we aim to obtain bird's-eye-view (BEV) scene information under the common view area in the world coordinate system. In this work, we focus on the task of semantic segmentation to demonstrate the feasibility of our approach and introduce a BEV modeling technique tailored for remote sensing scenes, which facilitates the projection of 3-D scene details from multiple perspective views onto a BEV. We then utilize a dual-encoder structure based on the vision transformer (VIT) architecture to extract relevant spatial information using self-attention mechanisms. Within the decoder, we employ a feature pyramid network (FPN) to integrate BEV patch encoding with spatial feature residuals, enabling fine-grained segmentation results at the original input resolution. Furthermore, we curated the LEVIR-MDS multidrone segmentation dataset, comprising scenes from ten community-level areas across three continents, totaling 243k images and their corresponding annotated BEV semantic maps, amounting to approximately 500 GB. This dataset serves as a robust benchmark to assess the effectiveness and generalization capability of our proposed method. To our knowledge, this is the first semantic segmentation dataset designed specifically for collaborative multidrone applications. We further show that our method achieves a 12% improvement in mean IoU (mIoU), reaching 69.73%, compared to a pure convolutional network model.",Bird's-eye-view (BEV) representation,multiview collaborative segmentation,remote sensing,semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_424,"Zhou, Xuanyu","Zhou, Lifan","Gong, Shengrong","Zhong, Shan",Swin Transformer Embedding Dual-Stream for Semantic Segmentation of Remote Sensing Imagery,,2024,11,"The acquisition of global context and boundary information is crucial for the semantic segmentation of remote sensing (RS) images. In contrast to convolutional neural networks (CNNs), transformers exhibit superior performance in global modeling and shape feature encoding, which provides a novel avenue for obtaining global context and boundary information. However, current methods fail to effectively leverage these distinctive advantages of transformers. To address this issue, we propose a novel single encoder and dual decoders architecture called STDSNet, which embeds the Swin transformer into the dual-stream network for semantic segmentation of RS imagery. The proposed STDSNet employs the Swin transformer as the network backbone in the encoder to address the limitations of CNNs in global modeling and encoding shape features. The dual decoder comprises two parallel streams, namely the global stream (GS) and the shape stream (SS). The GS utilizes the global context fusion module (GCFM) to address the loss of global context during upsampling. It further integrates GCFMs with skip connections and a multiscale fusion strategy to mitigate large-scale regional object classification errors resulting from similar features or shadow occlusion in RS images. The SS introduces the gate convolution module (GCM) to filter out irrelevant features, allowing it to focus on processing boundary information, which improves the semantic segmentation performance of small targets and their boundaries in RS images. Extensive experiments demonstrate that STDSNet outperforms other state-of-the-art methods on the ISPRS Vaihingen and Potsdam benchmarks.",Transformers,Shape,Semantic segmentation,Feature extraction,Streams,"Yan, Wei","Huang, Yizhou",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Semantics,Decoding,Dual-stream,remote sensing (RS),semantic segmentation,Swin transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_425,"Wang, Hao","Guo, Mingning","Li, Shaoxian","Li, Haifeng",Global-Local Coupled Style Transfer for Semantic Segmentation of Bitemporal Remote Sensing Images,,2024,0,"Due to the different acquisition conditions, large variations in the feature distributions of two temporal domains generally exist, known as temporal domain shift. The temporal domain shift is primarily influenced by coupled dual-factor: global style variations (such as illumination and weather conditions) and local style variations (such as the inherent phenological properties of land cover classes). In this article, we first formulate the temporal domain shift problem as an issue of dual-factor coupled interference on feature distributions in remote sensing (RS) community. To address this issue, we propose a semantic-guided style transfer (SGST) framework seamlessly integrating global feature alignment with local feature semantic matching. We use an adaptive segmentation model to provide pseudosegmentation maps and feed them into the style transfer model as semantic guidance. Under semantic guidance, a semantic-constrained style normalization (SCSN) module is designed to achieve style transfer at both global and local levels. Furthermore, a dual learning approach is introduced to make the style transfer model and the adaptive segmentation model promote each other. As a result, the style transfer model generates high-quality style-transferred images, and the adaptive segmentation model progressively predicts more accurate pseudosegmentation maps. Extensive experiments demonstrate the superiority of our proposed framework over state-of-the-art methods in terms of both perceptual quality and quantitative performance.",Adaptation models,Semantics,Land surface,Semantic segmentation,Remote sensing,"Tao, Chao",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Predictive models,Visualization,Dual learning,global-local coupled style transfer,semantic guidance,temporal domain shift,,,,,,,,,,,,,,,,,,,,,,,,
Row_426,"Li, Yuanjun","Zhu, Zhiyu","Li, Yuanjiang","Zhang, Jinglin",CTMU-Net: An Improved U-Net for Semantic Segmentation of Remote-Sensing Images Based on the Combined Attention Mechanism,,2023,4,"With the development of remote-sensing technology, it is important to use semantic segmentation methods to obtain detailed information in remote-sensing images. However, the objects in the images reveal significant intraclass differences and slight interclass differences, thus affecting the acquisition of terrain information. To tackle the problems, this article proposes an improved U-Net for the semantic segmentation of remote-sensing images. First, the local importance-based pooling is introduced to alleviate the loss of feature details in the coding part. Second, a combined attention module with a double-branch structure is designed, which models the local relationship and the global relationship at the same time to obtain more typical features. Finally, in order to make full use of the feature information extracted from the coding part, the combined attention module and the channel attention module are added to different positions in U-Net. In order to validate the proposed method, we conduct experiments on the WHDLD dataset and compare the experimental results with other semantic segmentation methods. On the WHDLD dataset, the MPA, MIOU, and FWIOU of the proposed method reach 76$\%$, 64.11$\%$, and 75.64$\%$, respectively, revealing its priority. To demonstrate the generalization of the proposed method, generalization experiments are conducted via the LandCover.ai dataset and the Massachusetts-building dataset. The simulation results testify that the proposed method provides an excellent generalization ability.",Semantic segmentation,Remote sensing,Semantics,Feature extraction,Deep learning,"Li, Xi","Shang, Shuyao","Zhu, Dewen",,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Transformers,Electronic mail,Attention mechanism,deep learning,remote-sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_427,"Yuan, Genji","Li, Jianbo","Lv, Zhiqiang","Li, Yinong",DDCAttNet: Road Segmentation Network for Remote Sensing Images,,2021,1,"Semantic segmentation of remote sensing images based on deep convolutional neural networks has proven its effectiveness. However, due to the complexity of remote sensing images, deep convolutional neural networks have difficulties in segmenting objects with weak appearance coherences even though they can represent local features of object effectively. The road networks segmentation of remote sensing images faces two major problems: high inter-individual similarity and ubiquitous occlusion. In order to address these issues, this paper develops a novel method to extract roads from complex remote sensing images. We designed a Dual Dense Connected Attention network (DDCAttNet) that establishes long-range dependencies between road features. The architecture of the network is designed to incorporate both spatial attention and channel attention information into semantic segmentation for accurate road segmentation. Experimental results on the benchmark dataset demonstrate the superiority of our proposed approach both in quantitative and qualitative evaluation.",Remote sensing,Road segmentation,Attention mechanism,,,"Xu, Zhihao",,,,"WIRELESS ALGORITHMS, SYSTEMS, AND APPLICATIONS, WASA 2021, PT II",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_428,"Ma, Yuefeng","Wang, Yingli","Liu, Xingya","Wang, Haiying",SWINT-RESNet: An Improved Remote Sensing Image Segmentation Model Based on Transformer,,2024,2,"Deep neural networks have been widely used in remote sensing image segmentation. Nowadays, artificial intelligence methods are increasingly applied to remote sensing feature classification. Although convolutional neural networks (CNNs) are widely used for image segmentation tasks, their global feature extraction with increasing image samples is insufficient. Furthermore, transformer is now being focused on computer vision. However, although transformer can capture the global information of remote sensing images, it cannot adequately model the detailed information of image changes. To comprehensively compensate for the defects of CNNs and the transformer network in feature extraction, this study proposes a semantic segmentation network with multifeature fusion (SWINT-RESNet). This network combines the transformer-extracted global and local features and those of CNNs to improve the accuracy of remote sensing image segmentation. The experiments show that the segmentation performance of SWINT-RESNet is superior for both small and medium sample remote sensing image datasets.",Feature extraction,Transformers,Remote sensing,Accuracy,Semantics,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Convolutional neural networks,Semantic segmentation,Deep learning,remote sensing image segmentation,RESNet,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_429,"Liu, Yutong","Gao, Kun","Wang, Hong","Yang, Zhijia",A Transformer-based multi-modal fusion network for semantic segmentation of high-resolution remote sensing imagery,,SEP 2024,0,"Semantic segmentation of high-resolution multispectral remote sensing image has been intensely studied. However, the shadow occlusions, or the similar color and textures, between the categories influence the segmentation accuracy. Concomitantly, the size of targets in the remote sensing images is diverse and the network cannot balance their segmentation. This paper introduces a network, Transformer-based Multi-modal Fusion Network (TMFNet), which fuses the multi-modal features and incorporates height features from the digital surface model (DSM) to supplement the extra different features between each category. Particularly, we introduce two parallel encoders to extract the features from different modalities, a Multi-Modal fusion model based on the Transformer (MMformer) to complete the multi-modal fusion, and a Border Region Attention based multi-level Fusion Module (BRAFM) to integrate the cross-level features and enhance the small target segmentation by utilizing the details around the border. The experiment results on the ISPRS Vaihingen and Potsdam benchmark datasets indicate that the proposed TMFNet outperforms the SOTA methods on the segmentation performance.",High-resolution remote sensing,Semantic segmentation,Transformer,Multi-modal fusion,,"Wang, Pengyu","Ji, Shijing","Huang, Yanjun","Zhu, Zhenyu",INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,"Zhao, Xiaobin",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_430,"Zeng, Qiaolin","Zhou, Jingxiang","Tao, Jinhua","Chen, Liangfu",Multiscale Global Context Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,2024,5,"Semantic segmentation of high-resolution remote sensing images (HRSIs) is a challenging task because objects in HRSIs usually have great scale variance and appearance variance. Although deep convolutional neural networks (DCNNs) have been widely applied in the semantic segmentation of HRSIs, they have inherent limitations in capturing global context. Attention mechanisms and transformer can effectively model long-range dependencies, but they often result in high computational costs when being applied to process HRSIs. In this article, an encoder-decoder network (MSGCNet) is proposed to fully and efficiently model multiscale context and long-range dependencies of HRSIs. Specifically, the multiscale interaction (MSI) module employs an efficient cross-attention to facilitate interaction among multiscale features of the encoder, which bridges the semantic gap between high- and low-level features and introduces more scale information to the network. In order to efficiently model long-range dependencies in both spatial and channel dimensions, the transformer-based decoder block (TBDB) implements window-based efficient multihead self-attention (W-EMSA) and enables interactions cross windows. Furthermore, to further integrate the global context generated by TBDB, the scale-aware fusion (SAF) module is proposed to deeply supervise the decoder, which iteratively fuses hierarchical features through spatial attention. As demonstrated by both quantitative and qualitative experimental results on two publicly available datasets, the proposed MSGCNet exhibits superior performance compared to currently popular methods. The code will be available at http://github.com/JingxiangZhou/MSGCNet.",Fuses,Semantic segmentation,Computational modeling,Semantics,Transformers,"Niu, Xuerui","Zhang, Yumeng",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Decoding,Sensors,Attention mechanism,remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_431,"Wang, Xinyao","Wang, Haitao","Jing, Yuqian","Yang, Xianming",A Bio-Inspired Visual Perception Transformer for Cross-Domain Semantic Segmentation of High-Resolution Remote Sensing Images,,MAY 2024,1,"Pixel-level classification of very-high-resolution images is a crucial yet challenging task in remote sensing. While transformers have demonstrated effectiveness in capturing dependencies, their tendency to partition images into patches may restrict their applicability to highly detailed remote sensing images. To extract latent contextual semantic information from high-resolution remote sensing images, we proposed a gaze-saccade transformer (GSV-Trans) with visual perceptual attention. GSV-Trans incorporates a visual perceptual attention (VPA) mechanism that dynamically allocates computational resources based on the semantic complexity of the image. The VPA mechanism includes both gaze attention and eye movement attention, enabling the model to focus on the most critical parts of the image and acquire competitive semantic information. Additionally, to capture contextual semantic information across different levels in the image, we designed an inter-layer short-term visual memory module with bidirectional affinity propagation to guide attention allocation. Furthermore, we introduced a dual-branch pseudo-label module (DBPL) that imposes pixel-level and category-level semantic constraints on both gaze and saccade branches. DBPL encourages the model to extract domain-invariant features and align semantic information across different domains in the feature space. Extensive experiments on multiple pixel-level classification benchmarks confirm the effectiveness and superiority of our method over the state of the art.",transformer,semantic segmentation,pseudo-label,high-resolution remote-sensing images,,"Chu, Jianbo",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_432,"Lenczner, Gaston","Chan-Hon-Tong, Adrien","Le Saux, Bertrand","Luminari, Nicola",DIAL: Deep Interactive and Active Learning for Semantic Segmentation in Remote Sensing,,2022,17,"In this article, we propose to build up a collaboration between a deep neural network and a human in the loop to swiftly obtain accurate segmentation maps of remote sensing images. In a nutshell, the agent iteratively interacts with the network to correct its initially flawed predictions. Concretely, these interactions are annotations representing the semantic labels. Our methodological contribution is twofold. First, we propose two interactive learning schemes to integrate user inputs into deep neural networks. The first one concatenates the annotations with the other network's inputs. The second one uses the annotations as a sparse ground truth to retrain the network. Second, we propose an active learning (AL) strategy to guide the user toward the most relevant areas to annotate. To this purpose, we compare different state-of-the-art acquisition functions to evaluate the neural network uncertainty such as ConfidNet, entropy, or ODIN. Through experiments on three remote sensing datasets, we show the effectiveness of the proposed methods. Notably, we show that AL based on uncertainty estimation enables to quickly lead the user toward mistakes and that it is thus relevant to guide the user interventions. Code will be open-source and released in this repository.(1)",Neural networks,Annotations,Image segmentation,Uncertainty,Semantics,"Le Besnerais, Guy",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Remote sensing,Deep learning,Active learning (AL),deep learning,earth observation,interactive segmentation,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_433,"Wang, Zhaoxin","Zheng, Chengyu","Wang, Chenglong","Wang, Jingyu",Statistical texture involved multi-granularity attention network for remote sensing semantic segmentation,,MAR 2024,0,"In recent years,semantic segmentation technology plays an important role in land resource management tasks. However, many classic semantic segmentation models often fail to obtain satisfactory results for remote sensing images with large difference of object scale span. The statistical texture involved multi-granularity attention network has been proposed to improve this situation. Statistical texture involved multi-granularity attention network has an encoder-decoder structure which is similar to DeeplabV3+ 2018. On this basis,texture excitation module and multi-granularity attention module are proposed to improve the accuracy of semantic segmentation. Texture excitation module is designed to extract statistical texture feature that are hidden in the image so that they can be combined with spatial structure feature for better prediction. Specifically, this module calculates the pixel co-occurrence matrix in the horizontal, vertical, diagonal, and anti diagonal directions. Multi-granularity attention module provides a more comprehensive understanding of the image by simultaneously focusing on the image at coarse,medium and fine granularity. Specifically, this module designs coarse granularity, medium granularity, and fine granularity spatial attention mechanisms and coarse granularity, medium granularity, and fine granularity channel attention mechanisms based on the core idea of self attention mechanism. Statistical texture involved multi-granularity attention network is compared with several of the most advanced deep learning methods on the Vaihingen data set and Potsdam data set.Compared with the best performing method, our method has increased by 0.73%, 1.88% and 0.013 in pixel accuracy,mean intersection over union and F1 score respectively on Vaihingen data set; our method improves the pixel accuracy from 87.82% to 88.50%, the mean intersection over union from 74.53% to 75.82%, and the F1 score from 0.8488 to 0.8573 on Potsdam data set.",Statistical textures,Multi-granularity,Remote sensing images,Semantic segmentation,Attention mechanism,"Yu, Shusong","Nie, Jie",,,MULTIMEDIA TOOLS AND APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_434,"Wang, Guoying","Chen, Jiahao","Mo, Lufeng","Wu, Peng",Lightweight land cover classification via semantic segmentation of remote sensing imagery and analysis of influencing factors,,FEB 15 2024,2,"Land cover classification is of great value and can be widely used in many fields. Earlier land cover classification methods used traditional image segmentation techniques, which cannot fully and comprehensively extract the ground information in remote sensing images. Therefore, it is necessary to integrate the advanced techniques of deep learning into the study of semantic segmentation of remote sensing images. However, most of current high-resolution image segmentation networks have disadvantages such as large parameters and high network training cost. In view of the problems above, a lightweight land cover classification model via semantic segmentation, DeepGDLE, is proposed in this paper. The model DeepGDLE is designed on the basis of DeeplabV3+ network and utilizes the GhostNet network instead of the backbone feature extraction network in the encoder. Using Depthwise Separable Convolution (DSC) instead of dilation convolution. This reduces the number of parameters and increases the computational speed of the model. By optimizing the dilation rate of parallel convolution in the ASPP module, the ""grid effect"" is avoided. ECANet lightweight channel attention mechanism is added after the feature extraction module and the pyramid pooling module to focus on the important weights of the model. Finally, the loss function Focal Loss is utilized to solve the problem of category imbalance in the dataset. As a result, the model DeepGDLE effectively reduces the parameters of the network model and the network training cost. And extensive experiments compared with several existing semantic segmentation algorithms such as DeeplabV3+, UNet, SegNet, etc. show that DeepGDLE improves the quality and efficiency of image segmentation. Therefore, compared to other networks, the DeepGDLE network model can be more effectively applied to land cover classification. In addition, in order to investigate the effects of different factors on the semantic segmentation performance of remote sensing images and to verify the robustness of the DeepGDLE model, a new remote sensing image dataset, FRSID, is constructed in this paper. This dataset takes into account more influences than the public dataset. The experimental results show that on the WHDLD dataset, the experimental metrics mIoU, mPA, and mRecall of the proposed model, DeepGDLE, are 62.29%, 72.85%, and 72.46%, respectively. On the FRSID dataset, the metrics mIoU, mPA, and mRecall are 65.89%, 74.43%, and 74.08%, respectively. For the future scope of research in this field, it may focus on the fusion of multi-source remote sensing data and the intelligent interpretation of remote sensing images.",semantic Segmentation,remote sensing image,land cover classification,attention mechanism,classification of surface objects,"Yi, Xiaomei",,,,FRONTIERS IN ENVIRONMENTAL SCIENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_435,"He, You","Zhang, Hanchao","Ning, Xiaogang","Zhang, Ruiqian",Spatial-Temporal Semantic Perception Network for Remote Sensing Image Semantic Change Detection,,AUG 2023,11,"Semantic change detection (SCD) is a challenging task in remote sensing, which aims to locate and identify changes between the bi-temporal images, providing detailed ""from-to"" change information. This information is valuable for various remote sensing applications. Recent studies have shown that multi-task networks, with dual segmentation branches and single change branch, are effective in SCD tasks. However, these networks primarily focus on extracting contextual information and ignore spatial details, resulting in the missed or false detection of small targets and inaccurate boundaries. To address the limitations of the aforementioned methods, this paper proposed a spatial-temporal semantic perception network (STSP-Net) for SCD. It effectively utilizes spatial detail information through the detail-aware path (DAP) and generates spatial-temporal semantic-perception features through combining deep contextual features. Meanwhile, the network enhances the representation of semantic features in spatial and temporal dimensions by leveraging a spatial attention fusion module (SAFM) and a temporal refinement detection module (TRDM). This augmentation results in improved sensitivity to details and adaptive performance balancing between semantic segmentation (SS) and change detection (CD). In addition, by incorporating the invariant consistency loss function (ICLoss), the proposed method constrains the consistency of land cover (LC) categories in invariant regions, thereby improving the accuracy and robustness of SCD. The comparative experimental results on three SCD datasets demonstrate the superiority of the proposed method in SCD. It outperforms other methods in various evaluation metrics, achieving a significant improvement. The Sek improvements of 2.84%, 1.63%, and 0.78% have been observed, respectively.",semantic change detection,change detection,semantic segmentation,spatial detail,semantic perception,"Chang, Dong","Hao, Minghui",,,REMOTE SENSING,,spatial-temporal semantic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_436,"Xue, Guangkuo","Liu, Yikun","Huang, Yuwen","Li, Mingsong",A3Seg: An Annealing Augmented and Aligned Semantic Segmentation Network for High Spatial Resolution Remote Sensing Images,,2022,1,"Semantic segmentation of high spatial resolution (HSR) remote sensing images (RSIs) plays an important role in many applications. However, HSR RSIs have significantly larger spatial sizes than typical natural images, which results in fewer valuable samples when training models. In addition, fusing multiscale features is the key step in obtaining features with strong semantic and high spatial information. However, current feature fusion methods are too straightforward to address misalignment issues. To handle these two problems, we propose an annealing augmented and aligned segmentation network, named A(3)Seg. Specifically, we propose an annealing online hard example mining (AOHEM) strategy to automatically select more valuable samples during the training stage. Based on AOHEM, a contextual augmentation block is proposed to extract sufficient contextual information using three different attention mechanisms in consideration of three different feature properties. Finally, we propose a novel feature alignment block to fuse features at different levels by alignment with the guidance of a salient feature. Experimental results on three different HSR RSIs datasets demonstrate that the proposed method outperforms the state-of-the-art general semantic segmentation methods with a better tradeoff between accuracy and complexity.",Aligned feature fusion,remote sensing imagery,semantic segmentation,valuable sample mining,,"Yang, Gongping",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_437,"Wu, Honglin","Zhang, Min","Huang, Peng","Tang, Wenlong",CMLFormer: CNN and Multiscale Local-Context Transformer Network for Remote Sensing Images Semantic Segmentation,,2024,7,"The characteristics of remote sensing images, such as complex ground objects, rich feature details, large intraclass variance and small interclass variance, usually require deep learning semantic segmentation methods to have strong feature learning representation ability. Due to the limitation of convolutional operation, convolutional neural networks (CNNs) are good at capturing local details, but perform poorly at modeling long-range dependencies. Transformers rely on multihead self-attention mechanisms to extract global contextual information, but it usually leads to high complexity. Therefore, this article proposes CNN and multiscale local-context transformer network (CMLFormer), a novel encoder-decoder structured network for remote sensing image semantic segmentation. Specifically, for the features extracted by the lightweight ResNet18 encoder, we design a transformer decoder based on multiscale local-context transform block (MLTB) to enhance the ability of feature learning. By using a self-attention mechanism with nonoverlapping windows and with the help of multiscale horizontal and vertical interactive stripe convolution, MLTB is able to capture both local feature information and global feature information at different scales with low complexity. In addition, the feature enhanced module is introduced into the decoder to further facilitate the learning of global and local information. Experimental results show that our proposed CMLFormer exhibits excellent performance on the Vaihingen and Potsdam datasets.",Transformers,Semantic segmentation,Remote sensing,Feature extraction,Computational modeling,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Convolutional neural networks,Adaptation models,Convolutional neural network (CNN),multiscale,remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_438,"Geng, Jie","Song, Shuai","Jiang, Wen",,Dual-Path Feature Aware Network for Remote Sensing Image Semantic Segmentation,,MAY 2024,0,"Semantic segmentation is a significant task for remote sensing interpretation, which takes advantage of contextual semantic information to classify each pixel into a specific category. Most current methods apply convolutional neural networks (CNN) to learn feature representation from remote sensing images, which may ignore the global dependencies due to the limitation of convolutional kernels. Inspired by the global feature learning ability of Transformer, we propose a novel deep model called dual-path feature aware network (DPFANet), which combines the structure of CNN and Transformer for semantic segmentation of remote sensing images. DPFANet aims to learn effective modeling ability from local to global features of images. Simultaneously, an adaptive feature fusion network is developed to fuse features from dual-path networks. Moreover, an edge optimization block is applied to constrain the edge features, whose purpose is to obtain more representative features for segmentation. Experimental results on three public remote sensing datasets verify that our proposed network yields better segmentation performance compared to other related methods.",Semantics,Task analysis,feature fusion,transformer,remote sensing image,,,,,IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY,,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_439,Priyanka,"Sravya, N.","Lal, Shyam","Nalini, J.",DIResUNet: Architecture for multiclass semantic segmentation of high resolution remote sensing imagery data,,OCT 2022,21,"Scene understanding is an important task in information extraction from high-resolution aerial images, an operation which is often involved in remote sensing applications. Recently, semantic segmentation using deep learning has become an important method to achieve state-of-the-art performance in pixel-level classification of objects. This latter is still a challenging task due to large pixel variance within classes possibly coupled with small pixel variance between classes. This paper proposes an artificial-intelligence (AI)-based approach to this problem, by designing the DIResUNet deep learning model. The model is built by integrating the inception module, a modified residual block, and a dense global spatial pyramid pooling (DGSPP) module, in combination with the well-known U-Net scheme. The modified residual blocks and the inception module extract multi-level features, whereas DGSPP extracts contextual intelligence. In this way, both local and global information about the scene are extracted in parallel using dedicated processing structures, resulting in a more effective overall approach. The performance of the proposed DIResUNet model is evaluated on the Landcover and WHDLD high resolution remote sensing (HRRS) datasets. We compared DIResUNet performance with recent benchmark models such as U-Net, UNet++, Attention UNet, FPN, UNet+SPP, and DGRNet to prove the effectiveness of our proposed model. Results show that the proposed DIResUNet model outperforms benchmark models on two HRRS datasets.",Deep learning,Semantic segmentation,Spatial pyramid pooling,Remote sensing,Residual block and inception module,"Reddy, Chintala Sudhakar","Dell'Acqua, Fabio",,,APPLIED INTELLIGENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_440,"Zhang, Bin","Wan, Yi","Zhang, Yongjun","Li, Yansheng",JSH-Net: joint semantic segmentation and height estimation using deep convolutional networks from single high-resolution remote sensing imagery,,SEP 2 2022,7,"Semantic segmentation for high-resolution remote sensing imagery is a pivotal component of land use and land cover categorization, and height estimation is essential for rebuilding the 3D information of an image. Because of the higher intra-class variation and smaller inter-class dissimilarity, these two challenging tasks are generally treated separately. This paper proposes a fully convolutional network that can tackle these problems simultaneously by estimating the land-cover categories and height values of pixels from a single aerial image. To handle these tasks, we develop a multi-task learning architecture (JSH-Net) that employs a shared feature representation and exploits their potential consistency across tasks, resulting in robust features and better prediction accuracy. Specifically, we propose a novel skip connection module that aggregates the contexts from the encoder part to the decoder part, bridging the semantic gap between them. In addition, we propose a progressive refinement strategy to recover detailed information about the objects. Moreover, we also proposed a height estimation branch on the head of the model to utilize shared features. The experiments we conducted on ISPRS 2D Labelling dataset verified that our network provided precise results of semantic segmentation and height estimation from two output branches and outperformed other state-of-the-art approaches.",remote sensing,semantic segmentation,height estimation,deep learning,multi-task learning,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,convolutional neural network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_441,"Chong, Yanwen","Chen, Xiaoshu","Pan, Shaoming",,Context Union Edge Network for Semantic Segmentation of Small-Scale Objects in Very High Resolution Remote Sensing Images,,2022,17,"Semantic segmentation of small-scale objects in very high resolution (VHR) remote sensing images plays an important role in some special tasks, such as change detection and mapping of land cover. However, due to small size, small-scale objects are more likely to be completely obscured by shadows than large-scale objects, which make it difficult for the traditional convolutional neural network (CNN) to distinguish small-scale objects from shadows. Furthermore, even if small-scale objects are distinguished, their boundaries are still difficult to refine. To solve the above problems, a novel context union edge network (CEN) for small-scale objects semantic segmentation is proposed by comprehensively considering both the contextual and edge information. In CEN, a plug-and-play context-based feature enhancement module (CFEM) is designed to enhance the ability of CNNs to distinguish small-scale objects. Then, an information exchange mechanism (IEM) is proposed based on the dual-stream (semantic and edge stream) network to refine the boundaries of small-scale objects. Finally, some experiments based on the ISPRS Vaihingen data set are conducted in terms of both overall accuracy (OA) and F1-score. The proposed CEN achieves 89.9% of F1-score for small-scale objects (cars) and 90.9% of OA, harvesting new state-of-the-art results.",Semantics,Feature extraction,Image edge detection,Streaming media,Remote sensing,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Image segmentation,Information exchange,Context,edge guidance,information exchange mechanism (IEM),semantic segmentation,small-scale objects,,,,,,,,,,,,,,,,,,,,,,,
Row_442,"Shi, Shuting","Li, Baohong","Zhang, Laifu","Kuang, Kun",Causality-Guided Stepwise Intervention and Reweighting for Remote Sensing Image Semantic Segmentation,,2024,0,"Semantic segmentation is one of the most significant tasks in remote sensing (RS) image interpretation, which focuses on learning global and local information to infer the semantic label of each pixel. Previous studies devise encoder-decoder structured deep learning (DL) models to extract global and local features from RS images with the help of pretraining knowledge to predict semantic labels. However, due to the common heterogeneity between the data for pretraining and the data to be semantically segmented, these models fail to learn general features appropriate to RS datasets. In this article, we propose a novel formulation of the above problem from a causal perspective, where the learned features from pretrained models result from causality and spurious correlations, and only the former carries general information that remains invariant regardless of the exact task and dataset. Based on the above formulation, we propose stepwise intervention and reweighting (SIR). It can reduce the confounding bias introduced by the pretraining knowledge and improve the model's ability to learn general features, making semantic segmentation of RS images benefit more from pretraining. Besides, we conduct a detailed theoretical analysis of our methods and conduct extensive experiments on two widely used public RS datasets. Experimental results demonstrate that applying SIR to encoder-decoder semantic segmentation models achieves performance improvements, proving the effectiveness and application values of the proposed method.",Task analysis,Semantics,Transformers,Feature extraction,Unified modeling language,"Wu, Sensen","Feng, Tian","Yan, Yiming","Du, Zhenhong",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Data models,Causal inference,deep learning (DL),remote sensing (RS),semantic segmentation,transfer learning,transfer learning,,,,,,,,,,,,,,,,,,,,,,,
Row_443,"Zhang, Xiuwei","Yang, Yizhe","Ran, Lingyan","Chen, Liang",Remote Sensing Image Semantic Change Detection Boosted by Semi-Supervised Contrastive Learning of Semantic Segmentation,,2024,0,"Semantic change detection (SCD) is a challenging task in remote sensing image (RSI) interpretation, which adopts multitemporal images to detect, locate, and analyze pixel-level land-cover ""from-to"" changes. In SCD, the severe class imbalance problem and the occurrence of confusing categories are very typical, making it challenging to accurately distinguish the easily confused categories with limited semantic context information. However, previous works did not address these issues in depth. This article proposes a novel SCD method named semi-supervised contrastive learning (SSCLNet), in which a simple and effective SCD network is designed as a strong baseline, and a semi-supervised contrastive learning module of semantic segmentation (SS) is presented to enhance the distinguishability of categories. Our baseline extracts semantic context through high-resolution network (HRNet), gets change information simply through an absolute difference, and then directly performs SCD based on the fusion of semantic context and change information. To utilize the semantic context information of the unlabeled non-changed regions, we employ a self-training (ST) method for semi-supervised SS. To learn distinguishable feature representations for easily confused categories, we present contrastive learning with an adaptive sampling strategy for SS. It selects challenging negative samples for each category from the other categories that exhibit similar features or attributes. The sampling space includes both the labeled changed samples and the non-changed samples predicted by ST. The comprehensive experiments on the SECOND and the Landsat-SCD dataset demonstrate that the proposed SSCLNet achieves the state-of-the-art (SOTA) performance, with a significant improvement of 2.07% and 4.15% in the score value, respectively.",Semantics,Remote sensing,Semantic segmentation,Task analysis,Self-supervised learning,"Wang, Kangwei","Yu, Lei","Wang, Peng","Zhang, Yanning",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Earth,Training,Contrastive learning,self-training (ST),semantic change detection (SCD),,,,,,,,,,,,,,,,,,,,,,,,,
Row_444,"Song, Wanying","Nie, Fangxin","Wang, Chi","Jiang, Yinyin",Unsupervised Multi-Scale Hybrid Feature Extraction Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,OCT 2024,0,"Generating pixel-level annotations for semantic segmentation tasks of high-resolution remote sensing images is both time-consuming and labor-intensive, which has led to increased interest in unsupervised methods. Therefore, in this paper, we propose an unsupervised multi-scale hybrid feature extraction network based on the CNN-Transformer architecture, referred to as MSHFE-Net. The MSHFE-Net consists of three main modules: a Multi-Scale Pixel-Guided CNN Encoder, a Multi-Scale Aggregation Transformer Encoder, and a Parallel Attention Fusion Module. The Multi-Scale Pixel-Guided CNN Encoder is designed for multi-scale, fine-grained feature extraction in unsupervised tasks, efficiently recovering local spatial information in images. Meanwhile, the Multi-Scale Aggregation Transformer Encoder introduces a multi-scale aggregation module, which further enhances the unsupervised acquisition of multi-scale contextual information, obtaining global features with stronger feature representation. The Parallel Attention Fusion Module employs an attention mechanism to fuse global and local features in both channel and spatial dimensions in parallel, enriching the semantic relations extracted during unsupervised training and improving the performance of unsupervised semantic segmentation. K-means clustering is then performed on the fused features to achieve high-precision unsupervised semantic segmentation. Experiments with MSHFE-Net on the Potsdam and Vaihingen datasets demonstrate its effectiveness in significantly improving the accuracy of unsupervised semantic segmentation.",high-resolution remote sensing,unsupervised,semantic segmentation,global context information,fine-grained features,"Wu, Yan",,,,REMOTE SENSING,,feature fusion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_445,"Qi, Xiyu","Mao, Yongqiang","Zhang, Yidan","Deng, Yawen",PICS: Paradigms Integration and Contrastive Selection for Semisupervised Remote Sensing Images Semantic Segmentation,,2023,7,"Remote sensing images semantic segmentation is a fundamental yet challenging task, which has long relied heavily on sufficient pixelwise annotations. Semisupervised learning is proposed to address the problem of high dependence on labeled data by exploiting more learnable samples generated from the large amounts of accessible unlabeled data. However, affected by the complexity and diversity of remote sensing images, various misclassifications often occur and lead to errors accumulation during model training. Errors accumulation will destroy the consistency of model training and lead to degradation of final segmentation performance. In this article, in order to further alleviate the damage caused by the errors to the consistency of model training and improve final segmentation accuracy, we propose a novel semisupervised segmentation framework, paradigms integration and contrastive selection (PICS). First, multiple proven semisupervised paradigms are integrated to generate pseudolabeled samples with less noise. Second, a loss-based contrastive selection method is explored to distinguish generated samples that contain different degrees of inevitable misclassification, thereby further maintaining the approximation of the generated samples and the ground truth in the sample space. By generating and selecting high-quality pseudolabeled samples for selective self-training, we can better guarantee consistency during model training and obtain better segmentation results. Extensive experiments over the ISPRS Vaihingen, Potsdam, and the challenging iSAID benchmarks demonstrate that our method yields significant accuracy boosting on the segmentation results and achieves on-par performance with the state of the arts.",Remote sensing,Training,Semantic segmentation,Semisupervised learning,Data models,"Wei, Haoran","Wang, Lei",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Predictive models,Visualization,Paradigms integration,remote sensing,selective self-training,semantic segmentation,semisupervised learning (SSL),,,,,,,,,,,,,,,,,,,,,,,
Row_446,"Ma, Xiaowen","Che, Rui","Wang, Xinyu","Ma, Mengting",DOCNet: Dual-Domain Optimized Class-Aware Network for Remote Sensing Image Segmentation,,2024,4,"The spatial attention mechanism has been frequently employed for the semantic segmentation of remote sensing images, given its renowned capability to model long-range dependencies. As remote sensing images often exhibit intricate backgrounds, significant intraclass variability, and a foreground-background imbalance, spatial attention mechanism-based methods somehow tend to introduce an extensive amount of background context through intensive affinity operations, causing unsatisfactory segmentation outcomes. While several class-aware methods attempt to attenuate the interference of background context by generating class representations as representative features, they still encounter challenges related to independent correlation calculation and single-confidence scale class representations. We introduce a dual-domain optimized class-aware network designed to address these challenges. In the semantic domain, we use category confidence as a scaling criterion to derive class representations at multiple confidence scales, effectively modeling pixel-class relationships. In the spatial domain, we leverage pixel-class relationships and their consensus to enhance relevant correlations while suppressing erroneous ones. Experimental results on three datasets demonstrate that the proposed method surpasses previous state-of-the-art ones for remote sensing image segmentation. Code is available at https://github.com/xwmaxwma/rssegmentation.",Class-aware,pixel-class relation,remote sensing,semantic segmentation,,"Wu, Sensen","Feng, Tian","Zhang, Wei",,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_447,"Ni, Huan","Liu, Qingshan","Guan, Haiyan","Tang, Hong",Category-Level Assignment for Cross-Domain Semantic Segmentation in Remote Sensing Images,,2023,12,"Deep learning-based semantic segmentation has made great progress in understanding very-high-resolution (VHR) remote sensing images (RSIs). However, large-scale applications are still limited. The main reason is that diverse imaging modes and geographical differences make it difficult to transfer a model trained in the source domain to the target domain. To solve this problem, unsupervised domain adaptation (UDA) for VHR RSIs has received some attention, but the accuracy of cross-domain semantic segmentation still needs to be improved. Currently, one reasonable proposal for improving accuracy is to take a close look at the category-level information. In this article, we reveal an integer programming mechanism for modeling the category-level relationship between the source and target domains. The mechanism is based on the solution of the assignment problem, and thus, the proposed method is called category-level assignment for UDA (ClA-UDA). In ClA-UDA, a category-level assignment problem with additional constraints is defined for UDA tasks, and the solution is provided. Based on the solution, an assignment-based image-to-image transferring algorithm (AIT) is first proposed to transfer the source-domain images based on the style of the target-domain images. AIT minimizes a weighted discrepancy and provides an analytical solution for the transfer. Two assignment-based alignment losses are then introduced to align the source and target domains based on the category-level relationship in a concise way. To validate the performance of ClA-UDA, three VHR RSI datasets are employed, and six UDA tasks are designed. Extensive experiments are conducted, and the results demonstrate the superiority of ClA-UDA compared with the existing methods.",Training,Semantic segmentation,Remote sensing,Task analysis,Adaptation models,"Chanussot, Jocelyn",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantics,Adversarial machine learning,Assignment problem,category-level relationship,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,,,
Row_448,"Li, Zhenghong","Chen, Hao","Wu, Jiangjiang","Li, Jun",SegMind: Semisupervised Remote Sensing Image Semantic Segmentation With Masked Image Modeling and Contrastive Learning Method,,2023,6,"Remote sensing (RS) image semantic segmentation has attracted much attention due to its wide applications. However, deep learning-based RS image semantic segmentation methods usually require substantial manual pixelwise annotations, which are expensive and hard to obtain in practice. Although the existing semisupervised RS semantic segmentation methods effectively reduce dependence on labeled data, they generally focus on information consistency between labeled and unlabeled images, but ignore the potential context information between different areas of the RS image. In fact, the objects contained in an RS image usually have some long-range dependence between each other, since trees are usually on both sides of a road, and the middle of two rows of houses is commonly a road. Therefore, we believe that the potential dependencies between different areas of the RS image should be beneficial to reduce the label dependence of RS semantic segmentation. Based on this point, we propose a novel semisupervised RS image semantic segmentation network named SegMind, which is based on mean-teacher (MT) architecture and adopts masked image modeling (MIM) to enhance information interactions of different areas. Moreover, contrastive learning (CL) and entropy loss are introduced to SegMind framework to further improve the linear separability and prediction confidence of the proposed model. Experiments on three datasets have demonstrated the superiority of the proposed method over the state-of-the-art methods. The code is available at https://github.com/lzh-ggs-ddu/SegMind.",Contrastive learning (CL),masked image mod-eling (MIM),remote sensing (RS) image semantic segmentation,semisupervised learning,,"Jing, Ning",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_449,"da Silva, Caio C., V","Nogueira, Keiller","Oliveira, Hugo N.","dos Santos, Jefersson A.",TOWARDS OPEN-SET SEMANTIC SEGMENTATION OF AERIAL IMAGES,2020 IEEE LATIN AMERICAN GRSS & ISPRS REMOTE SENSING CONFERENCE (LAGIRS),2020,14,"Classical and more recently deep computer vision methods are optimized for visible spectrum images, commonly encoded in grayscale or ROB colorspaces acquired from smartphones or cameras. A more uncommon source of images exploited in the remote sensing field are satellite and aerial images. However the development of pattern recognition approaches for these data is relatively recent, mainly due to the limited availability of this type of images, as until recently they were used exclusively for military purposes. Access to aerial imagery, including spectral information, has been increasing mainly due to the low cost of drones, cheapening of imaging satellite launch costs, and novel public datasets. Usually remote sensing applications employ computer vision techniques strictly modeled for classification tasks in closed set scenarios. However, real-world tasks rarely fit into closed set contexts, frequently presenting previouSly unknown classes, characterizing them as open set scenarios. Focusing on this problem, this is the first paper to study and develop semantic segmentation techniques for open set scenarios applied to remote sensing images. The main contributions of this paper are: I) a discussion of related works in open set semantic segmentation, showing evidence that these techniques can be adapted for open set remote sensing tasks; 2) the development and evaluation of a novel approach for open set semantic segmentation. Our method yielded competitive results when compared to closed set methods for the same dataset.",Open Set,Deep Learning,Semantic Segmentation,Remote Sensing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_450,"Zheng, Wei","Feng, Jiangfan","Gu, Zhujun","Zeng, Maimai",A Stage-Adaptive Selective Network with Position Awareness for Semantic Segmentation of LULC Remote Sensing Images,,MAY 29 2023,3,"Deep learning has proven to be highly successful at semantic segmentation of remote sensing images (RSIs); however, it remains challenging due to the significant intraclass variation and interclass similarity, which limit the accuracy and continuity of feature recognition in land use and land cover (LULC) applications. Here, we develop a stage-adaptive selective network that can significantly improve the accuracy and continuity of multiscale ground objects. Our proposed framework can learn to implement multiscale details based on a specific attention method (SaSPE) and transformer that work collectively. In addition, we enhance the feature extraction capability of the backbone network at both local and global scales by improving the window attention mechanism of the Swin Transfer. We experimentally demonstrate the success of this framework through quantitative and qualitative results. This study demonstrates the strong potential of the prior knowledge of deep learning-based models for semantic segmentation of RSIs.",remote sensing,semantic segmentation,position awareness,attention network,land use and land cover (LULC),,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_451,"Xiao, Tao","Liu, Yikun","Huang, Yuwen","Li, Mingsong",Enhancing Multiscale Representations With Transformer for Remote Sensing Image Semantic Segmentation,,2023,49,"Semantic segmentation is an extremely challenging task in high-resolution remote sensing (HRRS) images as objects have complex spatial layouts and enormous variations in appearance. Convolutional neural networks (CNNs) have an excellent ability to extract local features and have been widely applied as a feature extractor for various vision tasks. However, due to the inherent inductive bias of convolution operation, CNNs inevitably have limitations in modeling long-range dependencies. Transformer can capture global representations well but unfortunately ignores the details of local features and has high computational and spatial complexity in processing high-resolution feature maps. In this article, we propose a novel hybrid architecture for HRRS image segmentation, termed Enhancing Multiscale Representations with Transformer (EMRT), to exploit the advantages of convolution operations and Transformer to enhance multiscale representation learning. We incorporate the deformable self-attention mechanism in the Transformer to automatically adjust the receptive field and design an encoder-decoder architecture accordingly to achieve efficient context modeling. Specifically, CNN is constructed to extract feature representations. In the encoder, local features and global representations at different resolutions are extracted by the CNN and Transformer, respectively, and fused in an interactive manner. Moreover, a separate spatial branch is designed to extract multiscale contextual information as queries, and global dependencies between features at different scales are efficiently established by the decoder. Extensive experiments on three public remote sensing datasets demonstrate the superiority of EMRT and indicate that the overall performance of our method outperforms state-of-the-art methods. Code is available at https://github.com/peach-xiao/EMRT.",Transformers,Feature extraction,Remote sensing,Task analysis,Convolutional neural networks,"Yang, Gongping",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantic segmentation,Semantics,Convolutional neural networks (CNNs),multiscale representation learning,remote sensing,semantic segmentation,Transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_452,"Li, Yansheng","Ouyang, Song","Zhang, Yongjun",,Combining deep learning and ontology reasoning for remote sensing image semantic segmentation,,MAY 11 2022,44,"Because of its wide potential applications, remote sensing (RS) image semantic segmentation has attracted increasing research interest in recent years. Until now, deep semantic segmentation network (DSSN) has achieved a certain degree of success on semantic segmentation of RS imagery and can obviously outperform the traditional methods based on hand-crafted features. As a classic data-driven technique, DSSN can be trained by an end-to-end mechanism and is competent for employing lowlevel and mid-level cues (i.e., the discriminative image structure) to understand RS images. However, its interpretability and reliability are poor due to the nature weakness of the data-driven deep learning methods. By contrast, human beings have an excellent inference capacity and can reliably interpret RS imagery with the basic RS domain knowledge. Ontological reasoning is an ideal way to imitate and employ the domain knowledge of human beings. However, it is still rarely explored and adopted in the RS domain. As a solution of the aforementioned critical limitation of DSSN, this study proposes a collaboratively boosting framework (CBF) to combine the data-driven deep learning module and knowledge-guided ontology reasoning module in an iterative manner. The deep learning module adopts the DSSN architecture and takes the integration of the original image and inferred channels as the input of the DSSN. In addition, the ontology reasoning module is composed of intra-and extra-taxonomy reasoning. More specifically, the intra-taxonomy reasoning directly corrects misclassifications of the deep learning module based on the domain knowledge, which is the key to improve the classification performance. The extra-taxonomy reasoning aims to generate the inferred channels beyond the current taxonomy to improve the discriminative performance of DSSN in the original RS image space. On the one hand, benefiting from the referred channels from the ontology reasoning module, the deep learning module using the integration of the original image and referred channels can achieve better classification performance than only using the original image. On the other hand, better classification results from the deep learning module further improve the performance of the ontology reasoning module. As a whole, the deep learning and ontology reasoning modules are mutually boosted in the iterations. Extensive experiments on two publicly open RS datasets such as UCM and ISPRS Potsdam show that our proposed CBF can outperform the competitive baselines with a large margin. (c) 2022 Elsevier B.V. All rights reserved.",Collaboratively boosting framework (CBF),Deep learning,Ontology reasoning,Deep semantic segmentation network (DSSN),Remote sensing (RS) imagery,,,,,KNOWLEDGE-BASED SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_453,"Wang, Xinhua","Yuan, Botao","Li, Zhuang","Wang, Heqi",A Fractal Curve-Inspired Framework for Enhanced Semantic Segmentation of Remote Sensing Images,,NOV 2024,1,"The classification and recognition of features play a vital role in production and daily life; however, the current semantic segmentation of remote sensing images is hampered by background interference and other factors, leading to issues such as fuzzy boundary segmentation. To address these challenges, we propose a novel module for encoding and reconstructing multi-dimensional feature layers. Our approach first utilizes a bilinear interpolation method to downsample the multi-dimensional feature layer in the coding stage of the U-shaped framework. Subsequently, we incorporate a fractal curve module into the encoder, which aggregates points on feature maps from different layers, effectively grouping points from diverse regions. Finally, we introduce an aggregation layer that combines the upsampling method from the UNet series, employing the multi-scale censoring of multi-dimensional feature map outputs from various layers to efficiently capture both spatial and feature information. The experimental results across diverse scenarios demonstrate that our model achieves excellent performance in aggregating point information from feature maps, significantly enhancing semantic segmentation tasks.",remote sensing images,bilinear interpolation,fractal curve,gather layers,encoder-decoder,,,,,SENSORS,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_454,"Zheng, Chen","Hu, Chen","Chen, Yuncheng","Li, Jingying",A Self-Learning-Update CNN Model for Semantic Segmentation of Remote Sensing Images,,2023,11,"Convolutional neural network (CNN) has been widely used in semantic segmentation for remote sensing images, and it has achieved great success. Due to the diversity of the spatial distribution of terrestrial objects in remote sensing images, it is difficult to effectively learn general geographical laws and apply them to a specific image. To introduce geographical knowledge into the CNN model more effectively, a self-learning-update CNN model (SLU-CNN) is proposed in this letter. It learns the representation of specific spatial dependence among different objects according to the CNN result, and then incorporates it with the CNN result to make semantic inference available. The proposed method mainly involves two modules. First, geographical objects generated from the CNN result are used as inference units. Second, the spatial dependence between inference units is learned to build a specific adaptive geographical relationship. And then, it is embedded as an adaptive penalty term into an object-based Markov random field model (OMRF) to achieve the collaboration between the CNN result and the semantic inference. Our method provides a general data-knowledge dual-driven framework for the deep neural network. Experiments of the Gaofen Image Dataset (GID) and Sentinel-2 datasets validate the effectiveness of the proposed method by comparing it with different state-of-the-art CNN methods.",Mathematical models,Convolutional neural networks,Semantics,Remote sensing,Semantic segmentation,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Knowledge engineering,Training,Convolutional neural network (CNN),geographical knowledge,Markov random field,semantic inference,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_455,"Cao, Yong","Shi, Yiwen","Liu, Yiwei","Huo, Chunlei",Dual Stream Fusion Network for Multi-spectral High Resolution Remote Sensing Image Segmentation,,2021,1,"Semantic segmentation is in-demand in High Resolution Remote Sensing (HRRS) image processing. Unlike natural images, HRRS images usually provide channels such as Near Infrared (NIR) in addition to RGB channels. However, in order to make use of the pre-trained model, the current semantic segmentation methods in remote sensing field usually only use the RGB channel and discard the information of other channels. In this paper, to make full use of the HRRS image information, a dual-stream fusion network is proposed to fuse the information of different channel combinations through a Feature Pyramid Network (FPN), then a Stage Pyramid Pooling (SPP) module is used to integrate the features of different scales and produce the final segmentation results. Experiments on the RSCUP competition dataset show that the proposed approach can effectively improve the segmentation performance.",Semantic segmentation,Remote sensing,Stream fusion,,,"Xiang, Shiming","Pan, Chunhong",,,"PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2021, PT II",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_456,"Bi, Hanbo","Feng, Yingchao","Mao, Yongqiang","Pei, Jianning",AgMTR: Agent Mining Transformer for Few-Shot Segmentation in Remote Sensing,,OCT 2024,0,"Few-shot Segmentation aims to segment the interested objects in the query image with just a handful of labeled samples (i.e., support images). Previous schemes would leverage the similarity between support-query pixel pairs to construct the pixel-level semantic correlation. However, in remote sensing scenarios with extreme intra-class variations and cluttered backgrounds, such pixel-level correlations may produce tremendous mismatches, resulting in semantic ambiguity between the query foreground (FG) and background (BG) pixels. To tackle this problem, we propose a novel Agent Mining Transformer, which adaptively mines a set of local-aware agents to construct agent-level semantic correlation. Compared with pixel-level semantics, the given agents are equipped with local-contextual information and possess a broader receptive field. At this point, different query pixels can selectively aggregate the fine-grained local semantics of different agents, thereby enhancing the semantic clarity between query FG and BG pixels. Concretely, the Agent Learning Encoder is first proposed to erect the optimal transport plan that arranges different agents to aggregate support semantics under different local regions. Then, for further optimizing the agents, the Agent Aggregation Decoder and the Semantic Alignment Decoder are constructed to break through the limited support set for mining valuable class-specific semantics from unlabeled data sources and the query image itself, respectively. Extensive experiments on the remote sensing benchmark iSAID indicate that the proposed method achieves state-of-the-art performance. Surprisingly, our method remains quite competitive when extended to more common natural scenarios, i.e., PASCAL-5i\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$5<^>i$$\end{document} and COCO-20i\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$20<^>{i}$$\end{document}.",Few-shot learning,Few-shot segmentation,Remote sensing,Semantic segmentation,,"Diao, Wenhui","Wang, Hongqi","Sun, Xian",,INTERNATIONAL JOURNAL OF COMPUTER VISION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_457,"Bokhovkin, Alexey","Burnaev, Evgeny",,,Boundary Loss for Remote Sensing Imagery Semantic Segmentation,,2019,84,"In response to the growing importance of geospatial data, its analysis including semantic segmentation becomes an increasingly popular task in computer vision today. Convolutional neural networks are powerful visual models that yield hierarchies of features and practitioners widely use them to process remote sensing data. When performing remote sensing image segmentation, multiple instances of one class with precisely defined boundaries are often the case, and it is crucial to extract those boundaries accurately. The accuracy of segments boundaries delineation influences the quality of the whole segmented areas explicitly. However, widely-used segmentation loss functions such as BCE, IoU loss or Dice loss do not penalize misalignment of boundaries sufficiently. In this paper, we propose a novel loss function, namely a differentiable surrogate of a metric accounting accuracy of boundary detection. We can use the loss function with any neural network for binary segmentation. We performed validation of our loss function with various modifications of UNet on a synthetic dataset, as well as using real-world data (ISPRS Potsdam, INRIA AIL). Trained with the proposed loss function, models outperform baseline methods in terms of IoU score.",Semantic segmentation,Deep learning,Aerial imagery,CNN,Loss function,,,,,"ADVANCES IN NEURAL NETWORKS - ISNN 2019, PT II",,Building detection,Computer vision,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_458,"Zhang, Zhaoyang","Ren, Zhen","Tao, Chao","Zhang, Yunsheng",GraSS: Contrastive Learning With Gradient-Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation,,2023,22,"Self-supervised contrastive learning (SSCL) has achieved significant milestones in remote sensing image (RSI) understanding. Its essence lies in designing an unsupervised instance discrimination pretext task to extract image features from a large number of unlabeled images that are beneficial for downstream tasks. However, existing instance discrimination-based SSCL suffers from two limitations when applied to the RSI semantic segmentation task: 1) positive sample confounding issue (SCI), SSCL treats different augmentations of the same RSI as positive samples, but the richness, complexity, and imbalance of RSI ground objects lead to the model actually pulling a variety of different ground objects closer while pulling positive samples closer, which confuse the feature of different ground objects, and 2) feature adaptation bias, SSCL treats RSI patches containing various ground objects as individual instances for discrimination and obtains instance-level features, which are not fully adapted to pixel-level or object-level semantic segmentation tasks. To address the above limitations, we consider constructing samples containing single ground objects to alleviate positive SCI and make the model obtain object-level features from the contrastive between single ground objects. Meanwhile, we observed that the discrimination information can be mapped to specific regions in RSI through the gradient of unsupervised contrastive loss, and these specific regions tend to contain single ground objects. Based on this, we propose contrastive learning with gradient-guided sampling strategy (GraSS) for RSI semantic segmentation. GraSS consists of two stages: 1) the instance discrimination warm-up stage to provide initial discrimination information to the contrastive loss gradients and 2) the gradient-guided sampling contrastive training stage to adaptively construct samples containing more singular ground objects using the discrimination information. Experimental results on three open datasets demonstrate that GraSS effectively enhances the performance of SSCL in high-resolution RSI semantic segmentation. Compared with eight baseline methods from six different types of SSCL, GraSS achieves an average improvement of 1.57% and a maximum improvement of 3.58% in terms of mean intersection over the union (mIoU). In addition, we discovered that the unsupervised contrastive loss gradients contain rich feature information, which inspires us to use gradient information more extensively during model training to attain additional model capacity. The source code is available at https://github.com/GeoX-Lab/GraSS.",Contrastive loss,gradient-guided,remote sensing image (RSI),self-supervised learning,semantic segmentation,"Peng, Chengli","Li, Haifeng",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_459,"Che, Zhihao","Shen, Li","Huo, Lianzhi","Hu, Changmiao",MAFF-HRNet: Multi-Attention Feature Fusion HRNet for Building Segmentation in Remote Sensing Images,,MAR 2023,13,"Built-up areas and buildings are two main targets in remote sensing research; consequently, automatic extraction of built-up areas and buildings has attracted extensive attention. This task is usually difficult because of boundary blur, object occlusion, and intra-class inconsistency. In this paper, we propose the multi-attention feature fusion HRNet, MAFF-HRNet, which can retain more detailed features to achieve accurate semantic segmentation. The design of a pyramidal feature attention (PFA) hierarchy enhances the multilevel semantic representation of the model. In addition, we develop a mixed convolutional attention (MCA) block, which increases the capture range of receptive fields and overcomes the problem of intra-class inconsistency. To alleviate interference due to occlusion, a multiscale attention feature aggregation (MAFA) block is also proposed to enhance the restoration of the final prediction map. Our approach was systematically tested on the WHU (Wuhan University) Building Dataset and the Massachusetts Buildings Dataset. Compared with other advanced semantic segmentation models, our model achieved the best IoU results of 91.69% and 68.32%, respectively. To further evaluate the application significance of the proposed model, we migrated a pretrained model based on the World-Cover Dataset training to the Gaofen 16 m dataset for testing. Quantitative and qualitative experiments show that our model can accurately segment buildings and built-up areas from remote sensing images.",remote sensing,building extraction,built-up extraction,semantic segmentation,,"Wang, Yanping","Lu, Yao","Bi, Fukun",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_460,"Luo, Wen","Deng, Fei","Jiang, Peifan","Dong, Xiujun",FSegNet: A Semantic Segmentation Network for High-Resolution Remote Sensing Images That Balances Efficiency and Performance,,2024,0,"In recent years, convolutional neural networks (CNNs) and vision transformers (ViTs) have become the mainstream segmentation methods for high-resolution remote sensing images (HRSIs). CNNs can quickly acquire the correlation between local neighboring pixels through convolutional operations, but it is difficult to establish global contextual relationships, resulting in limited segmentation accuracy. ViTs are able to establish reliable global semantic dependencies through the mechanism of self-attention, but the quadratic computational complexity of self-attention makes the ViTs present high accuracy but low efficiency. Therefore, in this letter, to balance the efficiency and accuracy of HRSIs segmentation, we combine the respective advantages of CNNs and ViTs to propose the FSegNet network. Specifically, we introduce FasterViT and utilize its efficient hierarchical attention (HAT) to mitigate the surge in self-attention computation due to the high resolution of HRSIs. On this basis, we construct a lightweight decoder based on intensive computation, which achieves fast generation of segmentation results by reshaping and mapping multilevel features. Experiments on the ISPRS Potsdam and Vaihingen datasets show that the proposed FSegNet best balances performance and efficiency. The code is available at https://github.com/Rowan-L/FSegNet.",Feature extraction,Image segmentation,Decoding,Semantics,Convolution,"Zhang, Gulan",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Spatial resolution,Remote sensing,FasterViT,high-resolution remote sensing images (HRSIs),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_461,"Li, Jiaju","Wang, Hefeng","Zhang, Anbing","Liu, Yuliang",Semantic Segmentation of Hyperspectral Remote Sensing Images Based on PSE-UNet Model,,DEC 2022,13,"With the development of deep learning, the use of convolutional neural networks (CNN) to improve the land cover classification accuracy of hyperspectral remote sensing images (HSRSI) has become a research hotspot. In HSRSI semantics segmentation, the traditional dataset partition method may cause information leakage, which poses challenges for a fair comparison between models. The performance of the model based on ""convolutional-pooling-fully connected"" structure is limited by small sample sizes and high dimensions of HSRSI. Moreover, most current studies did not involve how to choose the number of principal components with the application of the principal component analysis (PCA) to reduce dimensionality. To overcome the above challenges, firstly, the non-overlapping sliding window strategy combined with the judgment mechanism is introduced, used to split the hyperspectral dataset. Then, a PSE-UNet model for HSRSI semantic segmentation is designed by combining PCA, the attention mechanism, and UNet, and the factors affecting the performance of PSE-UNet are analyzed. Finally, the cumulative variance contribution rate (CVCR) is introduced as a dimensionality reduction metric of PCA to study the Hughes phenomenon. The experimental results with the Salinas dataset show that the PSE-UNet is superior to other semantic segmentation algorithms and the results can provide a reference for HSRSI semantic segmentation.",hyperspectral remote sensing images,dataset partition method,convolutional neural networks,PSE-UNet,Hughes phenomenon,,,,,SENSORS,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_462,"Su, Yanzhou","Wu, Yongjian","Wang, Min","Wang, Feng",SEMANTIC SEGMENTATION OF HIGH RESOLUTION REMOTE SENSING IMAGE BASED ON BATCH-ATTENTION MECHANISM,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),2019,15,"Deep convolution neural network has been widely used in recent works for semantic segmentation of High Resolution Remote Sensing(HRRS) images. Because of the limitation of GPU memory, HRRS images are usually split into several sub-images for training convolutional neural networks. For each sub-image, the segmentation model may not have enough information to predict the segmentation map very well. In order to alleviate this problem, we propose to apply a batch-attention module to capture the discriminative information from similar objects, which come from other sub-images in a mini-batch. We also utilize global attention upsample module as the decoder to provide global context and fuse high and low level information better. We evaluate our model on the Potsdam dataset and achieve 88.30% pixAcc and 73.78% mIoU.",HRRS images,Semantic Segmentation,batch-attention,Global Attention Upsample,,"Cheng, Jian",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_463,"Zhou, Yuan","Zhu, Jiahang","Huo, Leigang","Huo, Chunlei",MULTI-TASK LEARNING FOR SEMANTIC CHANGE DETECTION ON VHR REMOTE SENSING IMAGES,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,2,"Remote Sensing Images Change Detection (RSICD) aims to locate the changed regions between bitemporal very-high-resolution (VHR) sensing images. However, existing deep learning-based RSICD methods are from the requirements by practical application, mainly due to the low feature discrimination and limited accuracy. We propose a novel multi-task and multi-temporal encoder-decoder changed detection network (MMNet) for VHR images, which accomplished both semantic segmentation and change detection at the same time. The encoder extracts multi-level contextual information, which contains two semantic segmentation branches (SSB) and a change detection branch (CDB). In this way, change representation constrains semantic representation during training, which introduces a novel loss function to ensure the semantic consistency within the unchanged regions. Furthermore, to utilize multi-level feature representation for enhancing the separability of features, a multi-scale feature fusion module (MFFM) is presented.",Change detection,Multi-task learning,Semantic segmentation,Deep learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_464,"Li, Xin","Li, Tao","Chen, Ziqi","Zhang, Kaiwen",Attentively Learning Edge Distributions for Semantic Segmentation of Remote Sensing Imagery,,JAN 2022,18,"Semantic segmentation has been a fundamental task in interpreting remote sensing imagery (RSI) for various downstream applications. Due to the high intra-class variants and inter-class similarities, inflexibly transferring natural image-specific networks to RSI is inadvisable. To enhance the distinguishability of learnt representations, attention modules were developed and applied to RSI, resulting in satisfactory improvements. However, these designs capture contextual information by equally handling all the pixels regardless of whether they around edges. Therefore, blurry boundaries are generated, rising high uncertainties in classifying vast adjacent pixels. Hereby, we propose an edge distribution attention module (EDA) to highlight the edge distributions of leant feature maps in a self-attentive fashion. In this module, we first formulate and model column-wise and row-wise edge attention maps based on covariance matrix analysis. Furthermore, a hybrid attention module (HAM) that emphasizes the edge distributions and position-wise dependencies is devised combing with non-local block. Consequently, a conceptually end-to-end neural network, termed as EDENet, is proposed to integrate HAM hierarchically for the detailed strengthening of multi-level representations. EDENet implicitly learns representative and discriminative features, providing available and reasonable cues for dense prediction. The experimental results evaluated on ISPRS Vaihingen, Potsdam and DeepGlobe datasets show the efficacy and superiority to the state-of-the-art methods on overall accuracy (OA) and mean intersection over union (mIoU). In addition, the ablation study further validates the effects of EDA.",semantic segmentation,remote sensing imagery,covariance matrix analysis,edge distributions,end-to-end neural network,"Xia, Runliang",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_465,"Zheng, Jianhua","Fu, Yusha","Chen, Xiaohan","Zhao, Ruolin",EGCM-UNet: Edge Guided Hybrid CNN-Mamba UNet for farmland remote sensing image semantic segmentation,,DEC 31 2025,0,"Segmenting farmland images is challenging due to their high color similarity to the background and irregular shapes, resulting in over/undersegmentation. To tackle these challenges, we propose the Edge Guided Hybrid CNN-Mamba UNet (EGCM-UNet) and design the oriented residual convolutional edge branch (ORCEB) to mine prior edge information. Additionally, the model designs a MaUNet module, which introduces the Visual State Space (VSS) block fused with Mamba to manage long-distance dependencies of image features, and uses the Edge-Guided Semantic Aggregation Module (EGSAM) for precise segmentation by fusing edge features with the VSS block's output. Lastly, comparative experiments were conducted using selected baseline models on the AgriculturalField-Seg dataset. The results show that EGCM-UNet outperformed U-Net with a Mean Intersection over Union (mIoU) of 0.394 vs. 0.379 on the test set. This indicates the proposed model delivers good performance in the semantic segmentation task of farmland remote sensing images.",Remote sensing images of farmland,semantic segmentation,feature fusion,Mamba,,"Lu, Junde","Zhao, Huanghui","Chen, Qian",,GEOCARTO INTERNATIONAL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_466,"Zhao, Qi","Lyu, Shuchang","Zhao, Hongbo","Liu, Binghao",Self-training guided disentangled adaptation for cross-domain remote sensing image semantic segmentation,,MAR 2024,6,"Remote sensing (RS) image semantic segmentation using deep convolutional neural networks (DCNNs) has shown great success in various applications. However, the high dependence on annotated data makes challenging for DCNNs to adapt to different RS scenes. To address this challenge, we propose a cross domain RS image semantic segmentation task that considers ground sampling distance, remote sensing sensor variation, and different geographical landscapes as the main factors causing domain shifts between source and target images. To mitigate the negative impact of domain shift, we propose a self-training guided disentangled adaptation network (ST-DASegNet) that consists of source and target student backbones extract source-style and target-style features. To align cross-domain single-style features, we adopt feature-level adversarial learning. We also propose a domain disentangled module (DDM) to extract universal and distinct features from single-domain cross-style features. Finally, we fuse these features and generate predictions using source and target student decoders. Moreover, we employ an exponential moving average (EMA) based cross-domain separated self-training mechanism to ease the instability and disadvantageous effect during adversarial optimization. Our experiments on several prominent RS datasets (Potsdam, Vaihingen, and LoveDA) demonstrate that ST-DASegNet outperforms previous methods and achieves new state-of-theart results. Visualization and analysis also confirm the interpretability of ST-DASegNet. The code is publicly available at https://github.com/cv516Buaa/ST-DASegNet.",Remote sensing image semantic segmentation,Unsupervised domain adaptation,Self-training,Domain disentangling,Adversarial learning,"Chen, Lijiang","Cheng, Guangliang",,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_467,"Zheng, Chen","Wang, Leiguang",,,Semantic Segmentation of Remote Sensing Imagery Using Object-Based Markov Random Field Model With Regional Penalties,,MAY 2015,46,"This paper proposes a novel object-based Markov random field model (OMRF) for semantic segmentation of remote sensing images. First, the method employs the region size and edge information to build a weighted region adjacency graph (WRAG) for capturing the complicated interactions among objects. Thereafter, aimed at modeling object interactions in the OMRF, the size and edge information are further introduced into the Gibbs joint distribution of the random field as regional penalties. Finally, the semantic segmentation is achieved through a principled probabilistic inference of the OMRF with regional penalties. The proposed method is compared with other MRF-based methods and some state-of-the-art methods. Experiments are conducted on a series of synthetic and real-world images. Segmentation results demonstrate that our method provides better performance (an accuracy improvement about 3%). Moreover, we further discuss the application of the proposed method for classification.",Object-based Markov random field (OMRF),regional penalties,remote sensing images,semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_468,"Ning, Xiaogang","He, You","Zhang, Hanchao","Zhang, Ruiqian",Semantic Information Collaboration Network for Semantic Change Detection in Remote Sensing Images,,2024,1,"Semantic change detection (SCD) extends the traditional change detection (CD) task to simultaneously identify the change areas and their corresponding land cover categories in bi-temporal images. This ""from-to"" change information holds significant value in numerous practical applications and is increasingly garnering attention in the remote sensing domain. However, prevalent challenges such as loss of detail and class imbalance significantly hinder the efficacy of SCD applications. To address this challenge, we propose a novel semantic information collaboration network (SIC-Net). This network incorporates a detail capture path and a spatial-temporal semantic coordination module aiming to effectively execute SCD by fusing detailed information with contextual features and harnessing synergies between binary CD and semantic segmentation. Additionally, we introduce a pseudo-label growth algorithm to mitigate the substantial loss of sample category information. The experimental results on two widely used SCD datasets demonstrate that SIC-Net outperforms other methods across various evaluation metrics, achieving SeK of 23.96% and 61.29%, respectively. These findings not only validate the effectiveness of the SIC-Net but also provide new ideas and directions for future research in the field of remote sensing, particularly in SCD.",Semantics,Task analysis,Feature extraction,Remote sensing,Semantic segmentation,"Chang, Dong","Hao, Minghui",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Land surface,Accuracy,Change detection,pseudo-label growth algorithm (PLGA),semantic change detection (SCD),semantic coordination,semantic segmentation,spatial detail,,,,,,,,,,,,,,,,,,,,,,
Row_469,"Zhu, Xiaotong","Peng, Taile","Guo, Jia","Wang, Hao",CNN-transformer dual branch collaborative model for semantic segmentation of high-resolution remote sensing images,,NOV 2024,0,"High-resolution remote sensing images play an important role in geological surveys, disaster detection, and other fields. However, highly imbalanced ground target classes and easily confused small ground targets pose significant challenges to the semantic segmentation task. We propose IC-TransUNet, a new dual branch model based on an encoder-decoder structure that fully exploits the advantages of convolutional neural networks and transformers and considers both detailed information and semantic information capture. Specifically, a lightweight CSwin transformer and InceptionNeXt are used as the dual branch backbone of the model. To further improve the model performance, first, we designed the InceptionNeXt-CSwin Transformer Fusion Module (ICFM) and Edge Enhancement Module (EEM) to guide the dual branch backbone to obtain features. Second, a detachable Spatial-channel Attention Fusion Module (SCAFM) is designed to be flexibly inserted into multiple positions of the model. Finally, we designed a decoder with significant performance based on a global local transformer block, SCAFM, and a multilayer perceptron segmentation head. IC-TransUNet achieved highly competitive performance in experiments on the Vaihingen and Potsdam datasets from the International Society for Photogrammetry and Remote Sensing.",CNN,dual encoder,high-resolution remote sensing images,semantic segmentation transformer,transformer,"Cao, Taotao",,,,PHOTOGRAMMETRIC RECORD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_470,"Gao, Hao","Cao, Lin","Yu, Dingfeng","Xiong, Xuejun",Semantic Segmentation of Marine Remote Sensing Based on a Cross Direction Attention Mechanism,,2020,10,"With the development of remote sensing technology, the semantic segmentation and recognition of various things in the ocean have become more and more frequent. Due to the wide variety of marine things and the large differences in morphology, it has brought greater difficulties to the recognition of marine remote sensing images. In order to obtain better segmentation results of ocean remote sensing images, this paper proposes an cross attention mechanism(Horizontal and Vertical) of exponential operation combined with multi-scale convolution algorithm. Among them, the cross attention mechanism and expanded distribution weight coefficient mentioned in this paper are first proposed. First, Input the marine remote sensing image features into an cross attention mechanism algorithm of exponential operation to obtain feature weight coefficients and joint weight coefficients in multiple directions; Then, the features with weight coefficients are input into the multi-access convolutional layer and the multi-scale dilated convolutional layer respectively for deep feature mining; Then the above steps are repeated twice, and finally the semantic segmentation of marine remote sensing images is achieved by fusing multiple deep-level features afterwards. Experiments were conducted on three public marine remote sensing data sets, and the results proved the effectiveness of our proposed cross attention mechanism of extended operation algorithm. The F values of the MAMC model on Beach, Island and Sea ice data sets have reached 99.4%, 91.25%, 87.08% respectively. Compared with other models, the effect is significantly improved, and proved the powerful performance of the algorithm in the semantic segmentation of marine remote sensing images.",Remote sensing,Feature extraction,Convolution,Image segmentation,Semantics,"Cao, Maoyong",,,,IEEE ACCESS,,Data mining,Oceans,Cross direction attention mechanism,marine remote sensing,multi-access convolutional,deep learning,convolution and dilated convolution,,,,,,,,,,,,,,,,,,,,,,,
Row_471,"Lan, Yubin","Huang, Kanghua","Yang, Chang","Lei, Luocheng",Real-Time Identification of Rice Weeds by UAV Low-Altitude Remote Sensing Based on Improved Semantic Segmentation Model,,NOV 2021,29,"Real-time analysis of UAV low-altitude remote sensing images at airborne terminals facilitates the timely monitoring of weeds in the farmland. Aiming at the real-time identification of rice weeds by UAV low-altitude remote sensing, two improved identification models, MobileNetV2-UNet and FFB-BiSeNetV2, were proposed based on the semantic segmentation models U-Net and BiSeNetV2, respectively. The MobileNetV2-UNet model focuses on reducing the amount of calculation of the original model parameters, and the FFB-BiSeNetV2 model focuses on improving the segmentation accuracy of the original model. In this study, we first tested and compared the segmentation accuracy and operating efficiency of the models before and after the improvement on the computer platform, and then transplanted the improved models to the embedded hardware platform Jetson AGX Xavier, and used TensorRT to optimize the model structure to improve the inference speed. Finally, the real-time segmentation effect of the two improved models on rice weeds was further verified through the collected low-altitude remote sensing video data. The results show that on the computer platform, the MobileNetV2-UNet model reduced the amount of network parameters, model size, and floating point calculations by 89.12%, 86.16%, and 92.6%, and the inference speed also increased by 2.77 times, when compared with the U-Net model. The FFB-BiSeNetV2 model improved the segmentation accuracy compared with the BiSeNetV2 model and achieved the highest pixel accuracy and mean Intersection over Union ratio of 93.09% and 80.28%. On the embedded hardware platform, the optimized MobileNetV2-UNet model and FFB-BiSeNetV2 model inferred 45.05 FPS and 40.16 FPS for a single image under the weight accuracy of FP16, respectively, both meeting the performance requirements of real-time identification. The two methods proposed in this study realize the real-time identification of rice weeds under low-altitude remote sensing by UAV, which provide a reference for the subsequent integrated operation of plant protection drones in real-time rice weed identification and precision spraying.",low-altitude remote sensing,semantic segmentation model,real-time,rice weeds,target spraying,"Ye, Jiahang","Zhang, Jianling","Zeng, Wen","Zhang, Yali",REMOTE SENSING,"Deng, Jizhong",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_472,"Chen, Jie","Wang, Hao","Guo, Ya","Sun, Geng",Strengthen the Feature Distinguishability of Geo-Object Details in the Semantic Segmentation of High-Resolution Remote Sensing Images,,2021,11,"Semantic segmentation is one of the hot topics in the field of remote sensing image intelligent analysis. Deep convolutional neural network (DCNN) has become a mainstream technology in semantic segmentation due to its powerful semantic feature representation. The emergence of high-resolution remote sensing imagery has provided massive detail information, but difficulties and challenges remain in the ""feature representation of fine geo objects"" and ""feature distinction of easily confusing geo objects."" To this end, this article focuses on the distinguishing features of geo-object details and proposes a novel DCNN-based semantic segmentation. First, the cascaded relation attention module is adopted to determine the relationship among different channels or positions. Then, information connection and error correction are used to capture and fuse the features of geo-object details. The output feature representations are provided by the multiscale feature module. Besides, the proposed model uses the boundary affinity loss to gain accurate and clear geo-object boundary. The experimental results on the Potsdam and Vaihingen datasets demonstrate that the proposed model can achieve excellent segmentation performance on overall accuracy and mean intersection over union. Furthermore, the results of ablation and visualization analyses also verify the feasibility and effectiveness of the proposed method.",Feature extraction,Semantics,Image segmentation,Visualization,Remote sensing,"Zhang, Yi","Deng, Min",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Decoding,Data mining,Attention mechanism,geo-object details,high-resolution remote sensing imagery,multiscale feature representation,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_473,"Wang, Luhan","Xiao, Pengfeng","Zhang, Xueliang","Chen, Xinyang",A Fine-Grained Unsupervised Domain Adaptation Framework for Semantic Segmentation of Remote Sensing Images,,2023,10,"Unsupervised domain adaptation (UDA) aims at adapting a model from the source domain to the target domain by tackling the issue of domain shift. Cross-domain segmentation of remote sensing images (RSIs) remains a big challenge due to the unique properties of RSIs. On the one hand, the divergence of data distribution in different local regions leads to negative transfer by directly applying the global alignment method in RSIs. On the other hand, the underlying category-level structure in the target domain is often ignored, which confuses the decision of semantic boundaries on the dispersed category features caused by large intraclass variance and small interclass variance in RSIs. In this study, we propose a novel fine-grained adaptation framework combining two stages of global-local alignment and category-level alignment to solve the above-mentioned problems. In the first stage of global-local adaptation, an attention map is derived from an intermediate discriminator and focuses on hard-to-align regions to mitigate negative transfer due to global adversarial learning. In the second stage of category-level adaptation, the category feature compact module is utilized to address the issue of dispersed features in the target domain attained by the cross-domain network, which will facilitate the fine-grained alignment of categories. Experiments under various scenarios, including geographic location variation and spectral band composition variation, demonstrate that the local adaptation and category-level adaptation of RSIs are complementary in the cross-domain segmentation, and the integrated framework helps achieve outstanding performance for UDA semantic segmentation of RSIs.",Adversarial machine learning,Semantic segmentation,Semantics,Remote sensing,Adaptation models,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Training,Prototypes,Adversarial learning,category-level alignment,global-local alignment,remote sensing images (RSIs),semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,
Row_474,"Mo, Youda","Li, Huihui","Xiao, Xiangling","Zhao, Huimin",Swin-Conv-Dspp and Global Local Transformer for Remote Sensing Image Semantic Segmentation,,2023,6,"compared with the traditional method based on hand-crafted features, deep neural network has achieved a certain degree of success on remote sensing (RS) image semantic segmentation. However, there are still serious holes, rough edge segmentation, and false detection or even missed detection due to the light and its shadow in the segmentation. Aiming at the above problems, this article proposes a RS semantic segmentation model SCG-TransNet that is a hybrid model of Swin transformer and Deeplabv3+, which includes Swin-Conv-Dspp (SCD) and global local transformer block (GLTB). First, the SCD module which can efficiently extract feature information from objects at different scales is used to mitigate the hole phenomenon, reducing the loss of detailed information. Second, we construct a GLTB with spatial pyramid pooling shuffle module to extract critical detail information from the limited visible pixels of the occluded objects, which alleviates the problem of difficult object recognition due to occlusion effectively. Finally, the experimental results show that our SCG-TransNet achieves a mean intersection over union of 70.29% on the Vaihingen datasets, which is 3% higher than the baseline model. It also achieved good results on POSDAM datasets. These demonstrate the effectiveness, robustness, and superiority of our proposed method compared with existing state-of-the-art methods.",Global local transformer block (GLTB),remote sensing (RS) image,semantic segmentation,Swin transformer,Swin-Conv-Dspp (SCD),"Liu, Xiaoyong","Zhan, Jin",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_475,"Chen, Man","Xu, Kun","Chen, Enping","Zhang, Yao",Semantic Attention and Structured Model for Weakly Supervised Instance Segmentation in Optical and SAR Remote Sensing Imagery,,NOV 2023,1,"Instance segmentation in remote sensing (RS) imagery aims to predict the locations of instances and represent them with pixel-level masks. Thanks to the more accurate pixel-level information for each instance, instance segmentation has enormous potential applications in resource planning, urban surveillance, and military reconnaissance. However, current RS imagery instance segmentation methods mostly follow the fully supervised paradigm, relying on expensive pixel-level labels. Moreover, remote sensing imagery suffers from cluttered backgrounds and significant variations in target scales, making segmentation challenging. To accommodate these limitations, we propose a semantic attention enhancement and structured model-guided multi-scale weakly supervised instance segmentation network (SASM-Net). Building upon the modeling of spatial relationships for weakly supervised instance segmentation, we further design the multi-scale feature extraction module (MSFE module), semantic attention enhancement module (SAE module), and structured model guidance module (SMG module) for SASM-Net to enable a balance between label production costs and visual processing. The MSFE module adopts a hierarchical approach similar to the residual structure to establish equivalent feature scales and to adapt to the significant scale variations of instances in RS imagery. The SAE module is a dual-stream structure with semantic information prediction and attention enhancement streams. It can enhance the network's activation of instances in the images and reduce cluttered backgrounds' interference. The SMG module can assist the SAE module in the training process to construct supervision with edge information, which can implicitly lead the model to a representation with structured inductive bias, reducing the impact of the low sensitivity of the model to edge information caused by the lack of fine-grained pixel-level labeling. Experimental results indicate that the proposed SASM-Net is adaptable to optical and synthetic aperture radar (SAR) RS imagery instance segmentation tasks. It accurately predicts instance masks without relying on pixel-level labels, surpassing the segmentation accuracy of all weakly supervised methods. It also shows competitiveness when compared to hybrid and fully supervised paradigms. This research provides a low-cost, high-quality solution for the instance segmentation task in optical and SAR RS imagery.",weakly supervised instance segmentation,remote sensing imagery,semantic attention,structured model,multi-scale feature extraction,"Xie, Yifei","Hu, Yahao","Pan, Zhisong",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_476,"Zhou, Wujie","Fan, Xiaomin","Yan, Weiqing","Shan, Shengdao",Graph Attention Guidance Network With Knowledge Distillation for Semantic Segmentation of Remote Sensing Images,,2023,13,"Deep learning has become a popular method for studying the semantic segmentation of high-resolution remote sensing images (HRRSIs). Existing methods have adopted convolutional neural networks (CNNs) to achieve better segmentation accuracy of HRRSIs, and the success of these models often depends on the model complexity and parameter quantity. However, the deployment of these models on equipment with limited resources is a significant challenge. To solve this problem, a lightweight student network framework-a graph attention guidance network (GAGNet) with knowledge distillation (KD), called GAGNet-S*-is proposed in this study, which distills knowledge from pretrained large teacher network (GAGNet-T) and builds reliable weak labels to optimize untrained student network (GAGNet-S). Inspired by the graph convolution network, this study designs a graph convolution module called the attention-graph decoder (AGD), which combines attention mechanisms with graph convolution to optimize image features and improve segmentation accuracy in the semantic segmentation task of HRRSIs. In addition, a dense cross-decoder (DCD) was designed for multiscale dense fusion, which utilizes rich semantic information in the high-level features to guide and refine the low-level features from the bottom up. Extensive experiments showed that GAGNet-S* (GAGNet-S with KD) achieved excellent segmentation performance on two widely used datasets: Potsdam and Vaihingen. The code and models are available at https://github.com/F8AoMn/GAGNet-KD.",Index Terms-Dense cross-decoder (DCD),graph convolution,high-resolution remote sensing images (HRRSIs),knowledge distillation (KD),semantic segmentation,"Jiang, Qiuping","Hwang, Jenq-Neng",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_477,"Li, Zhiyang","Pan, Xuran","Xu, Kexing","Yang, Xinqi",Multi-modal remote sensing image segmentation based on attention-driven dual-branch encoding framework,,APR 1 2024,0,"The high resolution remote sensing images are characterized by rich surface details and diverse features, and the single-modality high-resolution images suffer from limited expressive ability in the earth object segmentation application scenarios. We propose a multi-modal remote sensing image segmentation method based on attention-driven dual-branch encoding framework. The method involves parallel encoding of multi-modal remote sensing data to thoroughly extract features from each modality. Furthermore, multistage multi-modal features are fused by attention-driven feature fusion modules to generate high-quality multi-modal feature representation. Extensive experiments are carried out on the International Society for Photogrammetry and Remote Sensing Vaihingen and Potsdam 2D semantic labeling datasets. The datasets include both RGB/IRRG images and digital surface model (DSM) images. Experimental results demonstrate that: (1) the elevation information of DSM images can bring obvious benefits to the earth objects with significant heights, and introducing DSM images properly can improve the segmentation accuracy compared to using only RGB/IRRG images; (2) the attention-driven feature fusion module outperforms traditional feature fusion methods in capturing cross-modal complementary features, leading to outstanding segmentation accuracy for each earth object.",multi-modal remote sensing images,semantic segmentation,convolutional neural networks,attention-driven feature fusion,,,,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_478,"Li, Zhen","Zhang, Zhenxin","Chen, Dong","Zhang, Liqiang",HCRB-MSAN: Horizontally Connected Residual Blocks-Based Multiscale Attention Network for Semantic Segmentation of Buildings in HSR Remote Sensing Images,,2022,11,"Accurate and efficient semantic segmentation of buildings in high spatial resolution (HSR) remote sensing images is the basis for applications such as fine urban management, high-precision mapping, land resource utilization investigation, and human settlement suitability evaluation. The current building extraction methods based on deep learning can obtain high-level abstract features of images. However, due to the limitation of convolution kernel size and the vanishing gradient, the extraction of some buildings is inaccurate, and some small-volume buildings are missing as the network deepens. In this regard, we design a horizontally connected residual blocks-based multiscale attention network to achieve high-quality extraction of buildings in HSR remote sensing image. In this network, we subdivide each residual block by channel grouping and feature horizontal connection to consider the difference and saliency of feature information between channels, and then combine the output features with multiscale attention module to consider the contextual semantic relationship of different regions and integrate multilevel local and global information of buildings. A stepwise up-sampling mechanism is designed in the decoding process to finally achieve precise semantic segmentation of buildings. We conduct experiments on two public datasets and compare the proposed method with state-of-the-art semantic segmentation methods. The experiments show that our method could achieve better building extraction results in HSR remote sensing image, which proves the effectiveness of our proposed method.",Feature extraction,Buildings,Remote sensing,Image segmentation,Semantics,"Zhu, Lin","Wang, Qiang","Chen, Siyun","Peng, Xueli",IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Data mining,Deep learning,Building semantic segmentation,deep learning,horizontally connected residual block,high spatial resolution (HSR) remote sensing image,multiscale attention,,,,,,,,,,,,,,,,,,,,,,,
Row_479,"Chen, Li","Dou, Xin","Peng, Jian","Li, Wenbo",EFCNet: Ensemble Full Convolutional Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,2022,15,"Convolutional neural networks (CNNs) have achieved remarkable results in semantic segmentation of high-resolution remote sensing images (HRRSIs). However, the scales and textures of HRRSIs are diverse, which makes it difficult for a fixed-layer CNN to obtain rich features. In this regard, we propose an end-to-end ensemble fully convolutional network (EFCNet), which mainly includes two modules: the adaptive fusion module (AFM) and the separable convolutional module (SCM). The AFM can fuse features of different scales based on ensemble learning, whereas the SCM can reduce the complexity of the model under multifeature fusion. In the experiment, we use UNet and PSPNet to verify the framework on the ISPRS Vaihingen and Potsdam datasets. The experimental results show that the EFCNet can effectively improve the final segmentation performance and reduce the complexity of the ensemble model.",Convolution,Feature extraction,Kernel,Training,Remote sensing,"Sun, Bingyu","Li, Haifeng",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Image segmentation,Spatial resolution,Convolutional neural network (CNN),ensemble learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_480,"Fang, Bo","Chen, Gang","Chen, Jifa","Ouyang, Guichong",CCT: Conditional Co-Training for Truly Unsupervised Remote Sensing Image Segmentation in Coastal Areas,,SEP 2021,6,"As the fastest growing trend in big data analysis, deep learning technology has proven to be both an unprecedented breakthrough and a powerful tool in many fields, particularly for image segmentation tasks. Nevertheless, most achievements depend on high-quality pre-labeled training samples, which are labor-intensive and time-consuming. Furthermore, different from conventional natural images, coastal remote sensing ones generally carry far more complicated and considerable land cover information, making it difficult to produce pre-labeled references for supervised image segmentation. In our research, motivated by this observation, we take an in-depth investigation on the utilization of neural networks for unsupervised learning and propose a novel method, namely conditional co-training (CCT), specifically for truly unsupervised remote sensing image segmentation in coastal areas. In our idea, a multi-model framework consisting of two parallel data streams, which are superpixel-based over-segmentation and pixel-level semantic segmentation, is proposed to simultaneously perform the pixel-level classification. The former processes the input image into multiple over-segments, providing self-constrained guidance for model training. Meanwhile, with this guidance, the latter continuously processes the input image into multi-channel response maps until the model converges. Incentivized by multiple conditional constraints, our framework learns to extract high-level semantic knowledge and produce full-resolution segmentation maps without pre-labeled ground truths. Compared to the black-box solutions in conventional supervised learning manners, this method is of stronger explainability and transparency for its specific architecture and mechanism. The experimental results on two representative real-world coastal remote sensing datasets of image segmentation and the comparison with other state-of-the-art truly unsupervised methods validate the plausible performance and excellent efficiency of our proposed CCT.",remote sensing,image segmentation,coastal areas,deep learning,co-training,"Kou, Rong","Wang, Lizhe",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_481,"Cui, Wei","Yao, Meng","Hao, Yuanjie","Wang, Ziwei",Knowledge and Geo-Object Based Graph Convolutional Network for Remote Sensing Semantic Segmentation,,JUN 2021,14,"Pixel-based semantic segmentation models fail to effectively express geographic objects and their topological relationships. Therefore, in semantic segmentation of remote sensing images, these models fail to avoid salt-and-pepper effects and cannot achieve high accuracy either. To solve these problems, object-based models such as graph neural networks (GNNs) are considered. However, traditional GNNs directly use similarity or spatial correlations between nodes to aggregate nodes' information, which rely too much on the contextual information of the sample. The contextual information of the sample is often distorted, which results in a reduction in the node classification accuracy. To solve this problem, a knowledge and geo-object-based graph convolutional network (KGGCN) is proposed. The KGGCN uses superpixel blocks as nodes of the graph network and combines prior knowledge with spatial correlations during information aggregation. By incorporating the prior knowledge obtained from all samples of the study area, the receptive field of the node is extended from its sample context to the study area. Thus, the distortion of the sample context is overcome effectively. Experiments demonstrate that our model is improved by 3.7% compared with the baseline model named Cluster GCN and 4.1% compared with U-Net.",remote sensing images,semantic segmentation,geo-object prior knowledge,graph neural network,,"He, Xin","Wu, Weijie","Li, Jie","Zhao, Huilin",SENSORS,"Xia, Cong",,,,,,,,,,"Wang, Jin",,,,,,,,,,,,,,,,,,,,
Row_482,"Ma, Xianping","Zhang, Xiaokang","Pun, Man-On",,A Crossmodal Multiscale Fusion Network for Semantic Segmentation of Remote Sensing Data,,2022,42,"Driven by the rapid development of Earth observation sensors, semantic segmentation using multimodal fusion of remote sensing data has drawn substantial research attention in recent years. However, existing multimodal fusion methods based on convolutional neural networks cannot capture long-range dependencies across multiscale feature maps of remote sensing data in different modalities. To circumvent this problem, this work proposes a crossmodal multiscale fusion network (CMFNet) by exploiting the transformer architecture. In contrast to the conventional early, late, or hybrid fusion networks, the proposed CMFNet fuses information of different modalities at multiple scales using the cross-attention mechanism. More specifically, the CMFNet utilizes a novel cross-modal attention architecture to fuse multiscale convolutional feature maps of optical remote sensing images and digital surface model data through a crossmodal multiscale transformer (CMTrans) and a multiscale context augmented transformer (MCATrans). The CMTrans can effectively model long-range dependencies across multiscale feature maps derived from multimodal data, while the MCATrans can learn discriminative integrated representations for semantic segmentation. Extensive experiments on two large-scale fine-resolution remote sensing datasets, namely ISPRS Vaihingen and Potsdam, confirm the excellent performance of the proposed CMFNet as compared to other multimodal fusion methods.",Transformers,Remote sensing,Semantics,Image segmentation,Feature extraction,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Fuses,Decoding,Combined squeeze-and-excitation (CSE),cross attention,crossmodal multiscale fusion,transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_483,"You, Chao","Jiao, Licheng","Liu, Xu","Li, Lingling",Boundary-Aware Multiscale Learning Perception for Remote Sensing Image Segmentation,,2023,3,"For remote sensing image segmentation, the boundaries of objects are difficult to distinguish, which is ignored by most methods. Therefore, it is challenging how to excavate and recover the boundaries of objects accurately. In this article, we propose a boundary-aware multiscale network (BMNet) to solve this problem. The key components of BMNet include the scale attention module (SAM) and boundary guidance module (BGM). Specifically, the SAM is proposed to guide the refinement of multiscale features in a context-aware way. It enhances the discriminability of multiscale features by establishing contextual dependencies, which enables the refinement of the prediction of objects. Then, the BGM is proposed to enable networks to distinguish the boundary of objects. It uses manifold information of features to generate BG maps and forces the network to focus more on the boundary of objects. The effectiveness of the proposed BMNet is demonstrated on two public remote sensing datasets: ISPRS 2-D semantic labeling Potsdam dataset and Vaihingen dataset, where BMNet achieves better segmentation than prevalent methods. Finally, the experimental results indicate that BMNet can produce sharper boundaries of objects to reconstruct more detailed segmentation results.",Boundary attention guidance,learning perception,multiscale,remote sensing,semantic segmentation,"Liu, Fang","Ma, Wenping","Yang, Shuyuan",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_484,"Gao, Yupeng","Luo, Xiaoling","Gao, Xiaojing","Yan, Weihong",Semantic segmentation of remote sensing images based on multiscale features and global information modeling,,SEP 1 2024,6,"The main difficulties in semantic segmentation of remote sensing images include the effect of shadows, the blurring of feature differences between categories, and loss of small-scale category features during processing. To deal with these challenges, we propose a semantic segmentation network for RSI based on multi -scale features and global information modeling. A skillfully designed two -branch fusion attention based on an adaptive fusion converter is added to the multilevel cascaded HRNet structure to better combine multiscale features and global modeling information. Prior to this, "" Coordinate attention "" was combined with "" Spatial attention "" designed in this paper to form a Feed -forward Attention Layer (FAL) to encode finer feature information cues. Meanwhile, a more refined multilayer decoder is designed to obtain better image category recovery. Various experiments were conducted on four different scenarios and types of datasets including WHDLD, Potsdam, Vaihingen, and LoveDa, and in terms of the significant evaluation metric MIOU, with dividing the training, validation, and test sets in the ratio of 6:2:2, the performance of our model on the above four datasets was obtained with 61.79%, 70.79%, 81.15%, and 52.55%.In addition, a comparison with other excellent works is made and the results show that our designed model has better performance.",Multiscale features,Global modeling information,Remote sensing image,Semantic segmentation,,"Pan, Xin","Fu, Xueliang",,,EXPERT SYSTEMS WITH APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_485,"Li, Yansheng","Chen, Wei","Huang, Xin","Gao, Zhi",MFVNet: a deep adaptive fusion network with multiple field-of-views for remote sensing image semantic segmentation,,APR 2023,44,"In recent years, the remote sensing image (RSI) semantic segmentation attracts increasing research interest due to its wide application. RSIs are difficult to be processed holistically on current GPU cards on account of their large field-of-views (FOVs). However, the prevailing practices such as downsampling and cropping will inevitably decrease the quality of semantic segmentation. To address this conflict, this paper proposes a new deep adaptive fusion network with multiple FOVs (MFVNet), which is specially designed for RSI semantic segmentation. Different from existing methods, MFVNet takes into consideration the differences among multiple FOVs. By pyramid sampling the RSI, we first obtain images on different scales with multiple FOVs. Images on the high scale with a large FOV can capture larger spatial contexts and complete object contours, while images on the low scale with a small FOV can keep the higher spatial resolution and more detailed information. Then scale-specific models are chosen to make the best predictions for all scales. Next, the output feature maps and score maps are aligned through the scale alignment module to overcome spatial misregistration among scales. Finally, the aligned score maps are fused with the help of adaptive weight maps generated by the adaptive fusion module, producing the fused prediction. The performance of MFVNet surpasses the previous state-of-the-art semantic segmentation models on three typical RSI datasets, demonstrating the effectiveness of the proposed MFVNet.",semantic segmentation,remote sensing image (RSI),field-of-view (FOV),adaptive fusion,convolutional neural network,"Li, Siwei","He, Tao","Zhang, Yongjun",,SCIENCE CHINA-INFORMATION SCIENCES,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_486,"Wang, Wei","Zhang, Yingfeng","Wang, Xin","Li, Ji",A Boundary Guided Cross Fusion Approach for Remote Sensing Image Segmentation,,2024,1,"Remote sensing images have a variety of application prospects because of their rich information. Due to recent advances in deep learning methods, solid improvements have been made in the semantic segmentation of high-resolution remote sensing images. However, achieving precise segmentation of small and crowded objects remains a challenge. To tackle this challenging task, a Boundary Guided Cross Fusion module (BGCFM) is proposed. The Bidirectional Boundary Gate module (BBGM) is designed to provide reliable boundary information for BGCFM. Based on these two models, a remote sensing images real-time semantic segmentation network, boundary guided cross fusion network (BGCFNet), is designed. The effectiveness of the boundary-guided fusion method and the performance of BGCFNet were verified on the GID-5 dataset without pretraining. The application of the boundary-guided fusion method on the SOTA dual-branch real-time semantic segmentation network improves segmentation accuracy. BGCFNet achieves the best performance with a Mean Intersection over Union (mIoU) of 88.82%. Its inference speed is about 1.5 times that of other networks in the experiment, achieving an excellent balance between accuracy and speed.",Boundary information,multiscale feature fusion method,real-time semantic segmentation,remote sensing image,small object,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_487,"Liu, Jingyi","Wu, Jiawei","Xie, Hongfei","Xiao, Dong",Semantic Segmentation of Urban Remote Sensing Images Based on Deep Learning,,SEP 2024,0,"In the realm of urban planning and environmental evaluation, the delineation and categorization of land types are pivotal. This study introduces a convolutional neural network-based image semantic segmentation approach to delineate parcel data in remote sensing imagery. The initial phase involved a comparative analysis of various CNN architectures. ResNet and VGG serve as the foundational networks for training, followed by a comparative assessment of the experimental outcomes. Subsequently, the VGG+U-Net model, which demonstrated superior efficacy, was chosen as the primary network. Enhancements to this model were made by integrating attention mechanisms. Specifically, three distinct attention mechanisms-spatial, SE, and channel-were incorporated into the VGG+U-Net framework, and various loss functions were evaluated and selected. The impact of these attention mechanisms, in conjunction with different loss functions, was scrutinized. This study proposes a novel network model, designated VGG+U-Net+Channel, that leverages the VGG architecture as the backbone network in conjunction with the U-Net structure and augments it with the channel attention mechanism to refine the model's performance. This refinement resulted in a 1.14% enhancement in the network's overall precision and marked improvements in MPA and MioU. A comparative analysis of the detection capabilities between the enhanced and original models was conducted, including a pixel count for each category to ascertain the extent of various semantic information. The experimental validation confirms the viability and efficacy of the proposed methodology.",deep learning,semantic segmentation,remote sensing image,convolutional neural network,attention mechanism,"Ran, Mengying",,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_488,"Li, Xiao","Lei, Lin","Kuang, Gangyao",,Multilevel Adaptive-Scale Context Aggregating Network for Semantic Segmentation in High-Resolution Remote Sensing Images,,2022,12,"High-resolution remote sensing ((HRS)-S-2) images contain complex land objects of difference sizes, and it is important for semantic segmentation of the (HRS)-S-2 images to extract multiscale information. In this letter, we introduce a novel multilevel adaptive-scale context aggregating network (MACANet) for semantic segmentation of the (HRS)-S-2 images, which mainly consists of two parts--adaptive-scale context extraction block (AS-CEB) and sequential aggregation block (SAB). In particular, the AS-CEB introduces an inflexible strategy to obtain the features with appropriate scale information based on different asymmetric convolutions and the gated mechanism. Meanwhile, the SAB progressively aggregates multilevel adaptive-scale features, which are used to relieve the semantic gap between different-level features and generate precise score maps. Experimental results on representative (HRS)-S-2 datasets show the advantages of our method. The code is available at https://github.com/RSIP-NUDT/MACANet.",Semantics,Image segmentation,Feature extraction,Data mining,Remote sensing,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Logic gates,Kernel,Fully convolutional network (FCN),S) images,multiscale information,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_489,"Lv, Liang","Zhang, Lefei",,,Advancing Data-Efficient Exploitation for Semi-Supervised Remote Sensing Images Semantic Segmentation,,2024,6,"To reduce the dependence of remote sensing (RS) image semantic segmentation models on extensive pixel-level annotated images, this article aims to address the issue of insufficient exploitation of RS images' potential within existing semi-supervised learning methods, introducing a novel semi-supervised RS image semantic segmentation method. Specifically, for unlabeled samples, the multiperturbation dynamic consistency (MDC) is proposed to align multiple predictions from diverse data augmentations; MDC leverages a dynamic decay threshold (DDT) instead of fixed thresholds to learn more reliable information, enriching the perturbation space and assisting the segmentation model in acquiring more discriminative feature representations. Furthermore, considering the rich contextual information in RS images, the class prototype memory (CPM) derived from labeled samples is maintained during the training stage, which is leveraged to guide the refinement of predictions from segmentation model at the inference stage. Extensive experiments are conducted on six RS image semantic segmentation datasets, including DFC22, iSAID, MER, MSL, GID-15, and Vaihingen. The experimental results demonstrate the superiority of the proposed method. The code is available at https://github.com/lvliang6879/MCSS.",Semantic segmentation,Training,Data augmentation,Predictive models,Deep learning,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Data models,Computational modeling,Class prototype memory (CPM),dynamic decay threshold (DDT),multiperturbation dynamic consistency (MDC),remote sensing (RS) images,semantic segmentation,semi-supervised learning,,,,,,,,,,,,,,,,,,,,,,
Row_490,"Wei, Guangyi","Xu, Jindong","Yan, Weiqing","Chong, Qianpeng",Dual-Domain Fusion Network Based on Wavelet Frequency Decomposition and Fuzzy Spatial Constraint for Remote Sensing Image Segmentation,,OCT 2024,0,"Semantic segmentation is crucial for a wide range of downstream applications in remote sensing, aiming to classify pixels in remote sensing images (RSIs) at the semantic level. The dramatic variations in grayscale and the stacking of categories within RSIs lead to unstable inter-class variance and exacerbate the uncertainty around category boundaries. However, existing methods typically emphasize spatial information while overlooking frequency insights, making it difficult to achieve desirable results. To address these challenges, we propose a novel dual-domain fusion network that integrates both spatial and frequency features. For grayscale variations, a multi-level wavelet frequency decomposition module (MWFD) is introduced to extract and integrate multi-level frequency features to enhance the distinctiveness between spatially similar categories. To mitigate the uncertainty of boundaries, a type-2 fuzzy spatial constraint module (T2FSC) is proposed to achieve flexible higher-order fuzzy modeling to adaptively constrain the boundary features in the spatial by constructing upper and lower membership functions. Furthermore, a dual-domain feature fusion (DFF) module bridges the semantic gap between the frequency and spatial features, effectively realizes semantic alignment and feature fusion between the dual domains, which further improves the accuracy of segmentation results. We conduct comprehensive experiments and extensive ablation studies on three well-known datasets: Vaihingen, Potsdam, and GID. In these three datasets, our method achieved 74.56%, 73.60%, and 81.01% mIoU, respectively. Quantitative and qualitative results demonstrate that the proposed method significantly outperforms state-of-the-art methods, achieving an excellent balance between segmentation accuracy and computational overhead.",remote sensing,semantic segmentation,wavelet transform,type-2 fuzzy,,"Xing, Haihua","Ni, Mengying",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_491,"Zhao, Weiheng","Cao, Jiannong","Dong, Xueyan",,U-shaped contourlet network for high-spatial-resolution remote sensing images segmentation,,JUL 1 2023,3,"Accurate semantic segmentation of images has long been a research priority in remote sensing. However, the presence of geometrically complex and spatially diverse objects increases the difficulty in simultaneously obtaining coherent and accurate labeling result. To solve this challenge, our study combined multiscale geometric feature extraction with convolutional neural network and proposed a new U-shaped contourlet network (USCNet) for segmentation from high-spatial-resolution remote sensing images (HSRRSIs). This network is designed to learn and characterize the geometric features present in HSRRSIs. The USCNet first transforms the original dataset into a pyramidal structure containing multiscale and multidirectional geometric information and then fuses the spatial and geometric features to extract high-level semantic information. This network has two advantages: (1) coarse-to-fine spatial features are learned efficiently using a hierarchical learning structure and (2) the multiscale learning scheme captures geometric information in different directions. The results of extensive experiments conducted on two remote sensing datasets (the International Society for Photogrammetry and Remote Sensing Vaihingen and Potsdam challenge datasets) show that the proposed approach outperforms several state-of-the-art semantic segmentation methods.",semantic segmentation,contourlet transform,convolutional neural network,multiscale,high-spatial-resolution remote sensing images,,,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_492,"Zhang, Kai","Han, Yu","Chen, Jian","Zhang, Zichao",Semantic Segmentation for Remote Sensing based on RGB Images and Lidar Data using Model-Agnostic Meta-Learning and Partical Swarm Optimization,IFAC PAPERSONLINE,2020,11,"Urban remote sensing has the problems that land cover categories are usually highly unbalanced and annotated samples are scarce, which brings great limitations to monitoring the change of urban coverage and periodically evaluating urban ecological conditions. Semantic segmentation is one of the main applications in urban remote sensing image analysis. Because the ground objects in remote sensing images have the characteristics of disordered distribution and irregular morphology. The classical semantic segmentation model based on deep learning U-Net cannot achieve high semantic segmentation accuracy for urban ground objects. This paper proposes to optimize the neural network structure and introduce lidar data to solve the above problems. In this paper, the Model-Agnostic Meta-Learning and fully convolutional neural networks are fused which be trained and tested by remote sensing images. It makes the training process into inner loop and outer loop. And Partical Swarm Optimization (PSO) is used to optimize the parameter updating process of neural network to further improve the test accuracy. This paper fuses Lidar data and remote sensing images, and comprehensively use the position and elevation information of Lidar data and the spectrum and texture information of remote sensing images to classify the ground features. The datasets used in this paper are RGB remote sensing images and Digital Elevation Model (DEM) images. The test accuracy of U-Net network optimized by MAML can be improved by 6%-7% under the same network parameters and training data sets. With the introduction of Lidar data, the accuracy of the test is increased by 3-5%. The experimental results show that the precision before and after PSO optimization is improved by 6%-9%, which verifies the idea in the paper. Copyright (C) 2020 The Authors.",Urban remote sensing,semantic segmentation,Model-Agnostic Meta-Learning (MAML),Partical Swarm Optimization (PSO),Lidar data,"Wang, Shubo",,,,,,RGB remote sensing images,Digital Elevation Model (DEM),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_493,"Weng, Mengqian","Hu, Zhibo","Xie, Xiaopeng","Li, Yunhong",Semantic Segmentation of Remote Sensing Images Based on Dual Attention and Multi-scale Feature Fusion,TWELFTH INTERNATIONAL CONFERENCE ON GRAPHICS AND IMAGE PROCESSING (ICGIP 2020),2021,0,"We propose a remote sensing image semantic segmentation model based on dual attention and multi-scale feature fusion to solve the problems of objects scale differences and missing small objects. This model uses ResNet50 in the coding part to extract features. First of all, the output features of each stage of ResNet50 are introduced into the pyramid pooling module, making full use of the multi-scale context information of the image to cope with the change of the object scales. Secondly, the dual attention is introduced in the final output features of ResNet50 to establish the semantic relationship between the spatial and channel dimensions, which enhances the ability of feature representation and improve the condition that small targets are difficult to segment. Finally, starting from the output features of the attention module, the features of all levels are gradually integrated to complete decoding to refine the target segmentation edge. The designed comparative experiments results show the effectiveness of the proposed method.",Deep learning,remote sensing image,semantic segmentation,pyramid pooling,dual attention,"Hu, Lei",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_494,"Wang, Wenna","Ran, Lingyan","Yin, Hanlin","Sun, Mingjun",Hierarchical Shared Architecture Search for Real-Time Semantic Segmentation of Remote Sensing Images,,2024,2,"Real-time semantic segmentation of remote-sensing images demands a trade-off between speed and accuracy, which makes it challenging. Apart from manually designed networks, researchers seek to adopt neural architecture search (NAS) to discover a real-time semantic segmentation model with optimal performance automatically. Most existing NAS methods stack up no more than two types of searched cells, omitting the characteristics of resolution variation. This article proposes the hierarchical shared architecture search (HAS) method to automatically build a real-time semantic segmentation model for remote sensing images. Our model contains a lightweight backbone and a multiscale feature fusion module. The lightweight backbone is carefully designed with low computational cost. The multiscale feature fusion module is searched using the NAS method, where only the blocks from the same layer share identical cells. Extensive experiments reveal that our searched real-time semantic segmentation model of remote sensing images achieves the state-of-the-art trade-off between accuracy and speed. Specifically, on the LoveDA, Potsdam, and Vaihingen datasets, the searched network achieves 54.5% mIoU, 87.8% mIoU, and 84.1% mIoU, respectively, with an inference speed of 132.7 FPS. Besides, our searched network achieves 72.6% mIoU at 164.0 FPS on the CityScapes dataset and 72.3% mIoU at 186.4 FPS on the CamVid dataset.",Feature aggregation module,hierarchical shared search strategy,neural architecture search (NAS),real-time semantic segmentation,,"Zhang, Xiuwei","Zhang, Yanning",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_495,"Yang, Yang","Zheng, Shunyi","Wang, Xiqi","Ao, Wei",AMMUNet: Multiscale Attention Map Merging for Remote Sensing Image Segmentation,,2025,0,"The advancement of deep learning has driven notable progress in remote sensing semantic segmentation. Multihead self-attention (MSA) mechanisms have been widely adopted in semantic segmentation tasks. Network architectures exemplified by Vision Transformers have implemented window-based operations in the spatial domain to reduce computational costs. However, this approach comes at the expense of a weakened capacity to capture long-range dependencies, potentially limiting their efficacy in remote sensing image processing. In this letter, we propose AMMUNet, a UNet-based framework that employs multiscale attention map (AM) merging, comprising two key innovations: the attention map merging mechanism (AMMM) module and the granular multihead self-attention (GMSA). AMMM effectively combines multiscale AMs into a unified representation using a fixed mask template, enabling the modeling of a global attention mechanism. By integrating precomputed AMs in preceding layers, AMMM reduces computational costs while preserving global correlations. The proposed GMSA efficiently acquires global information while substantially mitigating computational costs in contrast to the global MSA mechanism. This is accomplished through the strategic alignment of granularity and the reduction of relative position bias parameters, thereby optimizing computational efficiency. Experimental evaluations highlight the superior performance of our approach, achieving remarkable mean intersection over union (mIoU) scores of 75.48% on the challenging Vaihingen dataset and an exceptional 77.90% on the Potsdam dataset, demonstrating the superiority of our method in precise remote sensing semantic segmentation. Codes are available at https://github.com/interpretty/AMMUNet.",Remote sensing,Transformers,Merging,Decoding,Computational modeling,"Liu, Zhao",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Computational efficiency,Semantic segmentation,Feature extraction,Costs,Correlation,Attention map (AM) merging,global attention mechanism,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_496,"Zhou, Yongxiu","Wang, Honghui","Yao, Guangle","Liu, Mingzhe",A Novel Remote Sensing Landslide Semantic Segmentation Method: Using CycleGAN-Based Change Detection Algorithms,"ENGINEERING GEOLOGY FOR A HABITABLE EARTH, VOL 4, IAEG XIV CONGRESS 2023",2024,0,"The study of landslide segmentation using remote sensing images is now focused on change detection and deep learning semantic segmentation algorithm. Deep learning-based semantic segmentation algorithms often require a considerable amount of pixel-level labeled training data. In the study of landslide segmentation research, too high cost of labeling is a barrier to develop deep learning methods. Although the change detection method does not require training data, it is necessary to gather the pre-event and post-event remote sensing images. When change detection is applied to landslide segmentation, it is a big challenge to obtain pre-event remote sensing images. To address this issue, a cycleGAN-based change detection technique for remote sensing landslide semantic segmentation has been developed. First, we used landslide images and non-landslide images to train the cycleGAN model. Then, we applied the cycleGAN approach to produce a non-landslide image from the landslide remote sensing image. Finally, using change detection method, landslide regions are segregated. In addition, we evaluated the suggested technique on the Bijie remote sensing landslide dataset, and got 0.845 accuracy, 0.404 recall, and 0.184 mIoU, demonstrating the method's viability.",Weakly supervised learning,Landslide,Remote sensing,CycleGAN,,"Xu, Qiang",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_497,"Qi, Zipeng","Zou, Zhengxia","Chen, Hao","Shi, Zhenwei",Remote-Sensing Image Segmentation Based on Implicit 3-D Scene Representation,,2022,1,"Remote-sensing image segmentation, as a challenging but fundamental task, has drawn increasing attention in the remote-sensing field. Recent advances in deep learning have greatly boosted research on this task. However, the existing deep-learning-based segmentation methods heavily rely on a large amount of pixelwise labeled training data, and the labeling process is time-consuming and labor-intensive. In this letter, we focus on the scenario that leverages the 3-D structure of multiview images and a limited number of annotations to generate accurate novel view segmentation. Under this scenario, we propose a novel method for remote-sensing image segmentation based on implicit 3-D scene representation, which generates arbitrary-view segmentation output from limited segmentation annotations. The proposed method employs a two-stage training strategy. In the first stage, we optimize the implicit neural representations of a 3-D scene and encode their multiview images into a neural radiance field. In the second stage, we transform the scene color attribute into semantic labels and propose a ray-convolution network to aggregate local 3-D consistency cues across different locations. We also design a color-radiance network to help our method generalize to unseen views. Experiments on both synthetic and real-world data suggest that our method significantly outperforms deep convolutional neural networks (CNNs)-based methods and other view synthesis-based methods. We also show that the proposed method can be applied as a novel data augmentation approach that benefits CNN-based segmentation methods.",Image segmentation,implicit neural representations,neural radiance field,remote sensing,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_498,"Liang, Guozhen","Xie, Fengxi","Chien, Ying-Ren",,Class-Aware Self- and Cross-Attention Network for Few-Shot Semantic Segmentation of Remote Sensing Images,,SEP 2024,0,"Few-Shot Semantic Segmentation (FSS) has drawn massive attention recently due to its remarkable ability to segment novel-class objects given only a handful of support samples. However, current FSS methods mainly focus on natural images and pay little attention to more practical and challenging scenarios, e.g., remote sensing image segmentation. In the field of remote sensing image analysis, the characteristics of remote sensing images, like complex backgrounds and tiny foreground objects, make novel-class segmentation challenging. To cope with these obstacles, we propose a Class-Aware Self- and Cross-Attention Network (CSCANet) for FSS in remote sensing imagery, consisting of a lightweight self-attention module and a supervised prior-guided cross-attention module. Concretely, the self-attention module abstracts robust unseen-class information from support features, while the cross-attention module generates a superior quality query attention map for directing the network to focus on novel objects. Experiments demonstrate that our CSCANet achieves outstanding performance on the standard remote sensing FSS benchmark iSAID-5i, surpassing the existing state-of-the-art FSS models across all combinations of backbone networks and K-shot settings.",few-shot learning,few-shot semantic segmentation,remote sensing,class-aware self- and cross-attention,,,,,,MATHEMATICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_499,"Chantharaj, Sirinthra","Pornratthanapong, Kissada","Chitsinpchayakun, Pitchayut","Panboonyuen, Teerapong",Semantic Segmentation on Medium-Resolution Satellite Images using Deep Convolutional Networks with Remote Sensing Derived Indices,2018 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER SCIENCE AND SOFTWARE ENGINEERING (JCSSE),2018,6,"Semantic Segmentation is a fundamental task in computer vision and remote sensing imagery. Many applications, such as urban planning, change detection, and environmental monitoring, require the accurate segmentation; hence, most segmentation tasks are performed by humans. Currently, with the growth of Deep Convolutional Neural Network (DCNN), there are many works aiming to find the best network architecture fitting for this task. However, all of the studies are based on very-high resolution satellite images, and surprisingly; none of them are implemented on medium resolution satellite images. Moreover, no research has applied geoinformatics knowledge. Therefore, we purpose to compare the semantic segmentation models, which are FCN, SegNet, and GSN using medium resolution images from Landsat-8 satellite. In addition, we propose a modified SegNet model that can be used with remote sensing derived indices. The results show that the model that achieves the highest accuracy RGB bands of medium resolution aerial imagery is SegNet. The overall accuracy of the model increases when includes Near Infrared (NIR) and Short-Wave Infrared (SWIR) band. The results showed that our proposed method (our modified SegNet model, named RGB-IR-IDX-MSN method) outperforms all of the baselines in terms of mean F1 scores.",semantic segmentation,deep convolutional neural network,remote sensing,medium-resolution satellite image,landsat-8,"Vateekul, Peerapon","Lawavirojwong, Siam","Srestasathiern, Panu","Jitkajornwanich, Kulsawasd",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_500,"Ding, Lei","Tang, Hao","Bruzzone, Lorenzo",,LANet: Local Attention Embedding to Improve the Semantic Segmentation of Remote Sensing Images,,JAN 2021,255,"The trade-off between feature representation power and spatial localization accuracy is crucial for the dense classification/semantic segmentation of remote sensing images (RSIs). High-level features extracted from the late layers of a neural network are rich in semantic information, yet have blurred spatial details; low-level features extracted from the early layers of a network contain more pixel-level information but are isolated and noisy. It is therefore difficult to bridge the gap between high- and low-level features due to their difference in terms of physical information content and spatial distribution. In this article, we contribute to solve this problem by enhancing the feature representation in two ways. On the one hand, a patch attention module (PAM) is proposed to enhance the embedding of context information based on a patchwise calculation of local attention. On the other hand, an attention embedding module (AEM) is proposed to enrich the semantic information of low-level features by embedding local focus from high-level features. Both proposed modules are lightweight and can be applied to process the extracted features of convolutional neural networks (CNNs). Experiments show that, by integrating the proposed modules into a baseline fully convolutional network (FCN), the resulting local attention network (LANet) greatly improves the performance over the baseline and outperforms other attention-based methods on two RSI data sets.",Semantics,Image segmentation,Feature extraction,Decoding,Remote sensing,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Correlation,Convolutional neural networks,Convolutional neural network (CNN),deep learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_501,"Jiang, Xufeng","Zhou, Nan","Li, Xiang",,Few-Shot Segmentation of Remote Sensing Images Using Deep Metric Learning,,2022,20,"Current convolutional neural network (CNN)-based methods for remote sensing image segmentation require a large number of densely annotated images for model training and have limited generalization abilities for unseen object categories. In this letter, we propose a novel few-shot learning-based method for the semantic segmentation of remote sensing images. Our method can perform semantic labeling for unseen object categories with only a few annotated samples. More specifically, our model starts by using a deep CNN to extract high-level semantic features. The prototype representation of each class is then generated by using a masked average pooling on the feature embeddings of the support images with ground truth masks. Finally, our model performs semantic labeling over the query images by matching the feature embedding of each pixel to its nearest prototypes in the embedding space. Our model is optimized with a nonparametric metric learning-based loss function to maximize the intra-class similarity of learned prototypes while minimizing the inter-class similarity. Experiments on International Society for Photogrammetry and Remote Sensing (ISPRS) 2-D semantic labeling dataset demonstrate satisfying in-domain and cross-domain transferring abilities of our model.",Image segmentation,Semantics,Prototypes,Feature extraction,Training,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Remote sensing,Labeling,Few-shot learning,prototype representation,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_502,"Li, Xin","Xu, Feng","Gao, Hongmin","Liu, Fan",A Frequency Domain Feature-Guided Network for Semantic Segmentation of Remote Sensing Images,,2024,6,"Semantic segmentation of Remote Sensing Images (RSIs) entails assigning semantic labels to each pixel accurately. RSIs are rich in spatial and spectral data, revealing diverse material and object characteristics. Yet, current RSI-focused computer vision models struggle with significant intra-class variation and inter-class resemblance due to limited spectral data usage. We propose the Frequency Domain Feature-Guided Network (FFGNet) for RSI semantic segmentation, influenced by digital signal processing theories. FFGNet initially generates frequency domain features via patch partitioning and 2D discrete cosine transformation. Our Frequency Enhancement Attention module (FEA) then distinguishes and intensifies frequency components to retain detailed information. These enhanced features are integrated with the Spatial-Spectral Attention (SSA) for enriched spectral signals. In the inference phase, these features are upsampled and combined with decoded features, emphasizing spectral details. Additionally, our novel loss function combines frequency and cross-entropy losses. Experiments on LoveDA and ISPRS Potsdam datasets demonstrate FFGNet's effectiveness, surpassing other mainstream models. An ablation study further validates our dual-guidance design.",Frequency-domain analysis,Discrete cosine transforms,Spatial databases,Semantics,Remote sensing,"Lyu, Xin",,,,IEEE SIGNAL PROCESSING LETTERS,,Training,Logic gates,Semantic segmentation,remote sensing images,attention mechanism,spatial-spectral attention,,,,,,,,,,,,,,,,,,,,,,,,
Row_503,"Zhang, Lili","Lu, Wanxuan","Zhang, Jinming","Wang, Hongqi",A Semisupervised Convolution Neural Network for Partial Unlabeled Remote-Sensing Image Segmentation,,2022,7,"Semantic segmentation methods for remote-sensing images based on the deep learning framework have achieved significant performance improvements. However, most of the existing work is based on fully supervised methods, which rely on a large number of manually annotated pixel-level labels. However, for remote-sensing images, labeling the ground-truth takes time and effort. To solve the problem in existing methods of overly relying on manual labeling, in this study, we propose a semisupervised convolution neural network based on contrastive loss for partial unlabeled remote-sensing image segmentation. In the design of the contrastive loss function, to capture the semantic relationship of pixels and improve the separability between different categories, we propose pixel-level and region-level contrastive loss. The pixel-level contrastive loss is designed to learn the correlation between different images, while region-level contrastive loss is designed to improve the quality of generated pseudo-labels. In addition, we designed a propagated self-training method that further guarantees the quality of the pseudo-labels and improves the richness of the labeled data. Experiments on POTSDAM and Vaihingen datasets demonstrate that the proposed method achieves the highest Mean Intersection over Union (mIOU) and significantly outperforms previous methods.",Image segmentation,Remote sensing,Training,Semantics,Propagation losses,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Data models,Head,Deep learning,remote-sensing image,semantic segmentation,semisupervised,,,,,,,,,,,,,,,,,,,,,,,,
Row_504,"Wang, Wenxiu","Fu, Yutian","Dong, Feng","Li, Feng",Semantic segmentation of remote sensing ship image via a convolutional neural networks model,,MAY 2019,27,"Semantic segmentation of remote sensing ship targets is one of the most challenging works in image processing, especially for small and multi-scale ship target detection. To solve these problems, an efficient method based on convolutional neural networks (CNN) to detect ship targets is proposed. This method introduces the attention model to the network to enhance the characteristics of small targets and combines atrous convolution with traditional CNN to increase the receptive field. To preserve the information lost by pooling, the proposed method uses the passthrough layer method to retain more features and concatenate the high- and low-resolution features. To verify the effectiveness of the method proposed in this study, the performance was evaluated by using precision, recall, F1-Score, mean intersection-over-union (IoU), and pixel accuracy measurements. These performances are all higher than the traditional semantic segmentation network SegNet. Mean IoU increases to 0.783 and pixel accuracy increases to 0.935. This method can conclusively identify ship targets in remote sensing images and has a certain reference value for remote sensing target detection.",remote sensing,neural nets,feature extraction,image classification,object detection,,,,,IET IMAGE PROCESSING,,ships,image fusion,image segmentation,geophysical image processing,remote sensing ship image,convolutional neural networks model,remote sensing ship targets,image processing,multiscale ship target detection,,,,,,,,,,,,,attention model,combines atrous convolution,traditional CNN,passthrough layer method,high- resolution features,low-resolution features,remote sensing target detection,SegNet,traditional semantic segmentation network
Row_505,"Yuan, Min","Ren, Dingbang","Feng, Qisheng","Wang, Zhaobin",MCAFNet: A Multiscale Channel Attention Fusion Network for Semantic Segmentation of Remote Sensing Images,,JAN 2023,24,"Semantic segmentation for urban remote sensing images is one of the most-crucial tasks in the field of remote sensing. Remote sensing images contain rich information on ground objects, such as shape, location, and boundary and can be found in high-resolution remote sensing images. It is exceedingly challenging to identify remote sensing images because of the large intraclass variance and low interclass variance caused by these objects. In this article, we propose a multiscale hierarchical channel attention fusion network model based on a transformer and CNN, which we name the multiscale channel attention fusion network (MCAFNet). MCAFNet uses ResNet-50 and Vit-B/16 to learn the global-local context, and this strengthens the semantic feature representation. Specifically, a global-local transformer block (GLTB) is deployed in the encoder stage. This design handles image details at low resolution and extracts global image features better than previous methods. In the decoder module, a channel attention optimization module and a fusion module are added to better integrate high- and low-dimensional feature maps, which enhances the network's ability to obtain small-scale semantic information. The proposed method is conducted on the ISPRS Vaihingen and Potsdam datasets. Both quantitative and qualitative evaluations show the competitive performance of MCAFNet in comparison to the performance of the mainstream methods. In addition, we performed extensive ablation experiments on the Vaihingen dataset in order to test the effectiveness of multiple network components.",semantic segmentation,transformer,channel attention module,hybrid structure,,"Dong, Yongkang","Lu, Fuxiang","Wu, Xiaolin",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_506,"Yang, Jingyu","Zhao, Liang","Dang, Jianwu","Wang, Yangping",A Semantic Segmentation Method for High-resolution Remote Sensing Images Based on Encoder-Decoder,,2022,2,"Image segmentation is a key technology in remote sensing image interpretation, and it is widely used in many fields. Aiming at the common problems of low segmentation accuracy and blurred target boundary in the semantic segmentation of high-resolution remote sensing images, a semantic segmentation method of high-resolution remote sensing images based on encoder-decoder structure is proposed, in which an attention mechanism is introduced to highlight important features,and an optimized Pyramid pooling module is used to extract multi-scale features from different layers. Finally, a multi-level and multi-scale feature fusion strategy is adopted to achieve fine-grained segmentation of high-resolution remote sensing images. The method is also compared and tested on the ISPRS Vaihingen dataset to verify the effectiveness.",semantic segmentation,attention mechanism,multi-scale feature fusion,label smoothing,,"Yue, Biao","Gu, Zongliang",,,"COMPUTER VISION - ECCV 2018, PT VII",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_507,"Qi, Xingqun","Li, Kaiqi","Liu, Pengkun","Zhou, Xiaoguang",Deep Attention and Multi-Scale Networks for Accurate Remote Sensing Image Segmentation,,2020,36,"Remote sensing image segmentation is a challenging task in remote sensing image analysis. Remote sensing image segmentation has great significance in urban planning, crop planting, and other fields that need plentiful information about the land. Technically, this task suffers from the ultra-high resolution, large shooting angle, and feature complexity of the remote sensing images. To address these issues, we propose a deep learning-based network called ATD-LinkNet with several customized modules. Specifically, we propose a replaceable module named AT block using multi-scale convolution and attention mechanism as the building block in ATD-LinkNet. AT block fuses different scale features and effectively utilizes the abundant spatial and semantic information in remote sensing images. To refine the nonlinear boundaries of internal objects in remote sensing images, we adopt the dense upsampling convolution in the decoder part of ATD-LinkNet. Experimentally, we enforce sufficient comparative experiments on two public remote sensing datasets (Potsdam and DeepGlobe Road Extraction). The results show our ATD-LinkNet achieves better performance against most state-of-the-art networks. We obtain 89.0% for pixel-level accuracy in the Potsdam dataset and 62.68% for mean Intersection over Union in the DeepGlobe Road Extraction dataset.",Remote sensing,Image segmentation,Feature extraction,Semantics,Image resolution,"Sun, Muyi",,,,IEEE ACCESS,,Convolution,Roads,Remote sensing image,convolutional neural network,semantic segmentation,attention,multi-scale,dense upsampling convolution,,,,,,,,,,,,,,,,,,,,,,
Row_508,"Wang, Libo","Li, Rui","Zhang, Ce","Fang, Shenghui",UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery,,AUG 2022,402,"Semantic segmentation of remotely sensed urban scene images is required in a wide range of practical applications, such as land cover mapping, urban change detection, environmental protection, and economic assessment. Driven by rapid developments in deep learning technologies, the convolutional neural network (CNN) has dominated semantic segmentation for many years. CNN adopts hierarchical feature representation, demonstrating strong capabilities for information extraction. However, the local property of the convolution layer limits the network from capturing the global context. Recently, as a hot topic in the domain of computer vision, Transformer has demonstrated its great potential in global information modelling, boosting many vision-related tasks such as image classification, object detection, and particularly semantic segmentation. In this paper, we propose a Transformer-based decoder and construct an UNet-like Transformer (UNetFormer) for real-time urban scene segmentation. For efficient segmentation, the UNetFormer selects the lightweight ResNet18 as the encoder and develops an efficient global-local attention mechanism to model both global and local information in the decoder. Extensive experiments reveal that our method not only runs faster but also produces higher accuracy compared with state-of-the-art lightweight models. Specifically, the proposed UNetFormer achieved 67.8% and 52.4% mIoU on the UAVid and LoveDA datasets, respectively, while the inference speed can achieve up to 322.4 FPS with a 512 x 512 input on a single NVIDIA GTX 3090 GPU. In further exploration, the proposed Transformer-based decoder combined with a Swin Transformer encoder also achieves the state-of-the-art result (91.3% F1 and 84.1% mIoU) on the Vaihingen dataset. The source code will be freely available at https://github. com/WangLibo1995/GeoSeg.",Semantic Segmentation,Remote Sensing,Vision Transformer,Fully Transformer Network,Global-local Context,"Duan, Chenxi","Meng, Xiaoliang","Atkinson, Peter M.",,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,Urban Scene,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_509,"Adam, Jibril Muhammad","Liu, Weiquan","Yu, Zang","Afzal, Muhammad Kamran",Deep learning-based semantic segmentation of urban-scale 3D meshes in remote sensing: A survey,,JUL 2023,14,"Semantic segmentation in 3D meshes is the classification of its constituent element(s) into specific classes or categories. Using the powerful feature extraction abilities of deep neural networks (DNNs), significant results have been obtained in the semantic segmentation of various remotely sensed data formats. With the increased utilization of DNNs to segment remotely sensed data, there have been commensurate in-depth reviews and surveys summarizing the various learning-based techniques and methodologies that entail these methods. However, most of these surveys focused on methods that involve popular data formats like LiDAR point clouds, synthetic aperture radar (SAR) images, and hyperspectral images (HSI) while 3D meshes hardly received any attention. In this paper, to our best knowledge, we present the first comprehensive and contemporary survey of recent advances in utilizing deep learning techniques for the semantic segmentation of urban-scale 3D meshes. We first describe the different approaches employed by mesh-based learning methods to generalize and implement learning techniques on the mesh surface, and then describe how the element-wise classification tasks are achieved through these methods. We also provide an in-depth discussion and comparative analysis of the surveyed methods followed by a summary of the benchmark large-scale mesh datasets accompanied with the evaluation metrics for assessing the segmentation performance of the methods. Finally, we summarize some of the contemporary problems of the field and provide future research directions that may help researchers in the community.",Semantic segmentation,Deep learning,3D meshes,Urban-scale,Survey,"Bello, Saifullahi Aminu","Muhammad, Abdullahi Uwaisu","Wang, Cheng","Li, Jonathan",INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,Remote sensing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_510,"Hou, Yunlong","Zhu, Lei","Chen, Qin",,Remote Sensing Image Segmentation Based on U-shaped Network with Atrous Spatial Pyramid,PROCEEDINGS OF THE 39TH CHINESE CONTROL CONFERENCE,2020,0,"Semantic segmentation in remote sensing images has always been an important research direction of computer vision, and is widely used in land and resources related fields such as land mapping, hydrology and environmental testing, urban and rural construction. This paper focuses on remote sensing image segmentation problem, which is still challenging for current semantic segmentation networks. To address this problem, this paper takes the DeepLabv3+([1]) network as the baseline structure, and attaches a multi-level feature fusion module to well utilize the low-level information of the network. The proposed network performs better on multiple evaluations such as Precision, Recall, and Mean Absolute Error (MAE) compared to several state-of-the-art methods, and is able to extract details of the segmentation more precisely.",Semantic Segmentation,Remote Sensing Image,DeepLabv3+,FAM,MAE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_511,"Bello, Inuwa M.","Zhang, Ke","Wang, Jingyu","Li, Haoyu",Lightweight multiscale framework for segmentation of high-resolution remote sensing imagery,,AUG 5 2021,6,"Deep convolutional neural network semantic segmentation has played a significant role in remote sensing applications due to its capability of end-to-end training and automatic high-level features extraction. Highly accurate predictions have been recorded from different network architectures that are designed using the convolutional layers. However, the accuracy is mainly achieved at a very high cost of computation, which renders the network infeasible for real-time application on resource-constrained devices. Therefore, there is a need to establish a trade-off between accuracy and model efficiency. Our paper proposes a lightweight, highly accurate, memory-efficient segmentation network capable of deployment on resource-constrained devices. Our proposed network of 1.09 million trainable parameters attains an appreciable accuracy of 89.41% and 88.78% on the Vaihingen and Potsdam respective dataset of the ISPRS 2D Semantic Labeling Challenge. The result from the speed and the memory efficiency experiment shows that our proposed network is suitable for real-time remote sensing applications. (C) 2021 Society of Photo-Optical Instrumentation Engineers (SPIE)",multiscale multispectral image,neural networks,remote sensing,image segmentation,,,,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_512,"Jin, Huazhong","Bao, Zhixi","Chang, Xueli","Zhang, Tingtao",Semantic segmentation of remote sensing images based on dilated convolution and spatial-channel attention mechanism,,JAN 1 2023,3,"The rich context information and multiscale ground information in remote sensing images are crucial to improving the semantic segmentation accuracy. Therefore, we propose a remote sensing image semantic segmentation method that integrates multilevel spatial channel attention and multi-scale dilated convolution, effectively addressing the issue of poor segmentation performance of small target objects in remote sensing images. This method builds a multilevel characteristic fusion structure, combining deep-level semantic characteristics with the details of the shallow levels to generate multiscale feature diagrams. Then, we introduce the dilated convolution of the series combination in each layer of the atrous spatial pyramid pooling structure to reduce the loss of small target information. Finally, using convolutional conditional random field to describe the context information on the space and edges to improve the model's ability to extract details. We prove the effectiveness of the model on the three public datasets. From the quantitative point of view, we mainly evaluate the four indicators of the model's F1 score, overall accuracy (OA), Intersection over Union (IoU), and Mean Intersection over Union (MIoU). On GID dataset, F1 score, OA, and MIoU reach 87.27, 87.80, and 77.70, respectively, superior to most mainstream semantic segmentation networks.",semantic segmentation,dilated convolution,spatial-channel attention,convolutional conditional random field,,"Chen, Can",,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_513,"Zeng, Qiaolin","Zhou, Jingxiang","Niu, Xuerui",,Cross-Scale Feature Propagation Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,2023,8,"Over the past few years, various strategies have been proposed to improve the multiscale information capture capability of networks, such as encoder-decoder framework, convolution layers with different kernel sizes in parallel, and multiple branches framework. However, many methods only rely on one of the strategies, which limits their performance when processing remote sensing images (RSIs) with large-scale variance. To address this issue and enable the fast and effective extraction of multiscale semantic information, this manuscript introduces a novel cross-scale feature propagation network (CFPNet). Specifically, the multiscale convolution (MSC) module aims to capture fine-grained multiscale context with different receptive fields, and the attention upsample (AUS) module embeds the semantic information of high-level features into low-level features while maintaining spatial details. Besides, the feature semantic enhancement (FSE) module is proposed to aggregate the multilayer features of the decoder to enhance the final feature representation. The experimental results on the Beijing Land-Use (BLU) and GID datasets demonstrate the effectiveness and efficiency of our CFPNet.",Attention mechanism,deep learning,remote sensing images (RSIs),semantic segmentation,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_514,"Ma, Xianping","Zhang, Xiaokang","Ding, Xingchen","Pun, Man-On",Decomposition-Based Unsupervised Domain Adaptation for Remote Sensing Image Semantic Segmentation,,2024,1,"Unsupervised domain adaptation (UDA) techniques are vital for semantic segmentation in geosciences, effectively utilizing remote sensing imagery across diverse domains. However, most existing UDA methods, which focus on domain alignment at the high-level feature space, struggle to simultaneously retain local spatial details and global contextual semantics. To overcome these challenges, a novel decomposition scheme is proposed to guide domain-invariant representation learning. Specifically, multiscale high/low-frequency decomposition (HLFD) modules are proposed to decompose feature maps into high- and low-frequency components across different subspaces. This decomposition is integrated into a fully global-local generative adversarial network (GLGAN) that incorporates global-local transformer blocks (GLTBs) to enhance the alignment of decomposed features. By integrating the HLFD scheme and the GLGAN, a novel decomposition-based UDA framework called De-GLGAN is developed to improve the cross-domain transferability and generalization capability of semantic segmentation models. Extensive experiments on two UDA benchmarks, namely ISPRS Potsdam and Vaihingen, and LoveDA Rural and Urban, demonstrate the effectiveness and superiority of the proposed approach over existing state-of-the-art UDA methods. The source code for this work is accessible at https://github.com/sstary/SSRS.",Semantic segmentation,Remote sensing,Training,Semantics,Feature extraction,"Ma, Siwei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Context modeling,Transformers,Generators,Generative adversarial networks,Adaptation models,Global-local information,high/low-frequency decomposition (HLFD),remote sensing,semantic segmentation,,,,,,,,,,,,,unsupervised domain adaptation (UDA),,,,,,,,
Row_515,"Zou, Zhengxia","Shi, Tianyang","Li, Wenyuan","Zhang, Zhou",Do Game Data Generalize Well for Remote Sensing Image Segmentation?,,JAN 2020,15,"Despite the recent progress in deep learning and remote sensing image interpretation, the adaption of a deep learning model between different sources of remote sensing data still remains a challenge. This paper investigates an interesting question: do synthetic data generalize well for remote sensing image applications? To answer this question, we take the building segmentation as an example by training a deep learning model on the city map of a well-known video game ""Grand Theft Auto V"" and then adapting the model to real-world remote sensing images. We propose a generative adversarial training based segmentation framework to improve the adaptability of the segmentation model. Our model consists of a CycleGAN model and a ResNet based segmentation network, where the former one is a well-known image-to-image translation framework which learns a mapping of the image from the game domain to the remote sensing domain; and the latter one learns to predict pixel-wise building masks based on the transformed data. All models in our method can be trained in an end-to-end fashion. The segmentation model can be trained without using any additional ground truth reference of the real-world images. Experimental results on a public building segmentation dataset suggest the effectiveness of our adaptation method. Our method shows superiority over other state-of-the-art semantic segmentation methods, for example, Deeplab-v3 and UNet. Another advantage of our method is that by introducing semantic information to the image-to-image translation framework, the image style conversion can be further improved.",remote sensing,deep learning,video game,domain adaptation,building segmentation,"Shi, Zhenwei",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_516,"Ye, Ziran","Lin, Yue","Dong, Baiyu","Tan, Xiangfeng",An Object-Aware Network Embedding Deep Superpixel for Semantic Segmentation of Remote Sensing Images,,OCT 2024,0,"Semantic segmentation forms the foundation for understanding very high resolution (VHR) remote sensing images, with extensive demand and practical application value. The convolutional neural networks (CNNs), known for their prowess in hierarchical feature representation, have dominated the field of semantic image segmentation. Recently, hierarchical vision transformers such as Swin have also shown excellent performance for semantic segmentation tasks. However, the hierarchical structure enlarges the receptive field to accumulate features and inevitably leads to the blurring of object boundaries. We introduce a novel object-aware network, Embedding deep SuperPixel, for VHR image semantic segmentation called ESPNet, which integrates advanced ConvNeXt and the learnable superpixel algorithm. Specifically, the developed task-oriented superpixel generation module can refine the results of the semantic segmentation branch by preserving object boundaries. This study reveals the capability of utilizing deep convolutional neural networks to accomplish both superpixel generation and semantic segmentation of VHR images within an integrated end-to-end framework. The proposed method achieved mIoU scores of 84.32, 90.13, and 55.73 on the Vaihingen, Potsdam, and LoveDA datasets, respectively. These results indicate that our model surpasses the current advanced methods, thus demonstrating the effectiveness of the proposed scheme.",VHR image,convolutional neural network,deep superpixel,semantic segmentation,,"Dai, Mengdi","Kong, Dedong",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_517,"Wu, Xiaosuo","Wang, Liling","Wu, Chaoyang","Guo, Cunge",Semantic Segmentation of Remote Sensing Images Using Multiway Fusion Network,,FEB 2024,7,"To effectively solve the problems of intra-class dissimilarity and inter-class similarity, this study proposes a deep learning semantic segmentation model that fuses multiple path features. It utilizes Multipath Fusion Module (MFM) to extract input image features, and dynamically fuses the features extracted from each input path. In the fusion process, the segmentation model dynamically adjusts the fuse on ratio and feature threshold of each path according to the input image, enables highly accurate image segmentation. In the upsampling stage, a guided upsampling strategy helps to avoid edge classification errors due to bilinear interpolation. The proposed network was trained and tested on the Potsdam dataset with good results, with mean intersection over union (mIoU) of 83.38%, overall accuracy (OA) of 90.21% and an F1 score of 90.86%.",Semantic segmentation,remote sensing images,Multiway Fusion Network,guided upsampling,,"Yan, Haowen","Qiao, Ze",,,SIGNAL PROCESSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_518,"Wang, Xin","Zhang, Yu","Ca, Jingye","Qin, Qin",Semantic segmentation network for mangrove tree species based on UAV remote sensing images,,DEC 2 2024,0,"Mangroves are special vegetation that grows in the intertidal zone of the coast and has extremely high ecological and environmental value. Different mangrove species exhibit significant differences in ecological functions and environmental responses, so accurately identifying and distinguishing these species is crucial for ecological protection and monitoring. However, mangrove species recognition faces challenges, such as morphological similarity, environmental complexity, target size variability, and data scarcity. Traditional mangrove monitoring methods mainly rely on expensive and operationally complex multispectral or hyperspectral remote sensing sensors, which have high data processing and storage costs, hindering large-scale application and popularization. Although hyperspectral monitoring is still necessary in certain situations, the low identification accuracy in routine monitoring severely hinders ecological analysis. To address these issues, this paper proposes the UrmsNet segmentation network, aimed at improving identification accuracy in routine monitoring while reducing costs and complexity. It includes an improved lightweight convolution SCConv, an Adaptive Selective Attention Module (ASAM), and a Cross-Layer Feature Fusion Module (CLFFM). ASAM adaptively extracts and fuses features of different mangrove species, enhancing the network's ability to characterize mangrove species with similar morphology and in complex environments. CLFFM combines shallow details and deep semantic information to ensure accurate segmentation of mangrove boundaries and small targets.Additionally, this paper constructs a high-quality RGB image dataset for mangrove species segmentation to address the data scarcity problem. Compared to traditional methods, our approach is more precise and efficient. While maintaining relatively low parameters and computational complexity (FLOPs), it achieves excellent performance with mIoU and mPA metrics of 92.21% and 95.98%, respectively. This performance is comparable to the latest methods using multispectral or hyperspectral data but significantly reduces cost and complexity. By combining periodic hyperspectral monitoring with UrmsNet-supported routine monitoring, a more comprehensive and efficient mangrove ecological monitoring can be achieved.These research findings provide a new technical approach for large-scale, low-cost monitoring of important ecosystems such as mangroves, with significant theoretical and practical value. Furthermore, UrmsNet also demonstrates excellent performance on LoveDA, Potsdam, and Vaihingen datasets, showing potential for wider application.",Feature fusion,Mangrove species segmentation,Semantic segmentation,UAV remote sensing,,"Feng, Yi","Yan, Jingke",,,SCIENTIFIC REPORTS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_519,"Wu, Honglin","Zeng, Zhaobin","Huang, Peng","Yu, Xinyu",CCTNet: CNN and Cross-Shaped Transformer Hybrid Network for Remote Sensing Image Semantic Segmentation,,2024,0,"Deep learning methods have achieved great success in the field of remote sensing image segmentation in recent years, but building a lightweight segmentation model with comprehensive local and global feature extraction capabilities remains a challenging task. In this article, we propose a convolutional neural network (CNN) and cross-shaped transformer hybrid network (CCTNet) for semantic segmentation of high-resolution remote sensing images. This model follows an encoder-decoder structure. It employs ResNet18 as an encoder to extract hierarchical feature information, and constructs a transformer decoder based on efficient cross-shaped self-attention to fully model local and global feature information and achieve lightweighting of the network. Moreover, the transformer block introduces a mixed-scale convolutional feedforward network to further enhance multiscale information extraction. Furthermore, a simplified and efficient feature aggregation module is leveraged to gradually aggregate local and global information at different stages. Extensive comparison experiments on the ISPRS Vaihingen and Potsdam datasets reveal that our method obtains superior performance compared with state-of-the-art lightweight methods.",Transformers,Feature extraction,Semantic segmentation,Semantics,Remote sensing,"Zhang, Min",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Convolutional neural networks,Decoding,Computational efficiency,Data mining,Computer architecture,Convolutional neural network (CNN),cross-shaped transformer,global contextual information,remote sensing image,,,,,,,,,,,,,semantic segmentation,,,,,,,,
Row_520,"Li, Zhongyu","Wang, Huajun","Liu, Yang",,Semantic segmentation of remote sensing image based on bilateral branch network,,MAY 2024,2,"Due to the large intra-class differences between the same categories and the scale imbalance between different categories in the remote sensing image dataset, the semantic segmentation task presents the problem of small-scale object information loss, the imbalance between foreground and background, and simultaneously the background dominates, which seriously affects the performance of the network model. To solve the above problems, this paper proposes an efficient bilateral branch depth neural network model based on the U-Net depth neural network, named BBU-Net. Firstly, one branch of the network learns the distribution characteristics of the original data, and the other focuses on difficult samples. Then the two branches improve the representation and classification ability of the neural network by accumulating learning strategies. Finally, considering the geometric diversity of remote sensing images, this paper adopts test time augmentation and reflection padding strategies and proposes a balanced weighted loss function named CombineLoss to alleviate the imbalance in the training process. The depth neural network proposed in this paper was first tested on the Inria Aerial Image Labeling Dataset, and 87.53% of mean intersection over union and 97.4% of mean pixel accuracy were obtained, respectively. At the same time, to verify the model's complexity, the model proposed in this paper is compared with the neural network based on integrated learning. The comparison results show that the spatial complexity of the network proposed in this paper is much lower than the neural network obtained by integrated learning, and the parameters are also much smaller than the neural network based on integrated learning. Then use the satellite building dataset I in the WHU Building Dataset and mainstream semantic segmentation methods for multiple groups of comparative experiments. The experimental results show that the method proposed in this paper can effectively extract the semantic information of remote sensing images, significantly improve the imbalance of remote sensing image data, improve the performance of the network model, and achieve a good semantic segmentation effect, which fully proves the effectiveness of this method.",Feature extraction,Semantic segmentation,Deep neural network,Imbalance,Integrated learning,,,,,VISUAL COMPUTER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_521,"Liu, Mengjia","Liu, Peng","Zhao, Lingjun","Ma, Yan",Fast semantic segmentation for remote sensing images with an improved Short-Term Dense-Connection (STDC) network,,DEC 31 2024,0,"It is hard to accomplish fast semantic segmentation on large remote sensing images, since current neural networks with numerous parameters often rely on significant computational resources. Our team proposes an improved fast semantic segmentation model based on short-term dense-connection network (RepSTDC). We introduce a structure reparameterization and coordinate attention into STDC networks. By structure reparameterization, we transform the multi-branch structure into a comparable single-branch configuration during the inference process. By replacing the traditional channel attention with a coordinate attention mechanism, we enhance the attention mechanism with considering channel relationships and long-distance position information, and then it saves the memory usages. We conducted thorough experiments to assess the efficacy of network components of RepSTDC on the several benchmark datasets. Additionally, we compared our proposed approach with state-of-the-art methods. Our RepSTDC model can well balance the accuracy performances, computing speed, and memory usage in most cases. It achieves fast segmentation by significantly reducing parameters but without obviously compromising performances compared to other methods.",Convolutional neural network (CNN),structural reparameterization,remote sensing image,semantic segmentation,,"Chen, Lajiao","Xu, Mengzhen",,,INTERNATIONAL JOURNAL OF DIGITAL EARTH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_522,"Wang, Kai","He, Daojie","Sun, Qingqiang","Yi, Lizhi",A novel network for semantic segmentation of landslide areas in remote sensing images with multi-branch and multi-scale fusion,,JUN 2024,2,"Landslides pose significant risks as natural disasters, highlighting the importance of accurate mapping using remote sensing images for various practical applications. However, due to the challenges arising from incomplete and inaccurate boundary information of foreground landslide polygons, existing methods can only achieve suboptimal performance. To this premise, in this paper, we propose a segmentation network called GMNet that leverages global information extraction and multi -scale feature fusion to enhance the discrimination of landslides from other objects. Specifically, by employing a multi -branch mechanism, our method effectively captures global information, while an improved multi -scale feature fusion technique addresses the issue of varying scales in landslide polygons. Furthermore, semantic enhancement enhances the semantic information of low-level features, bridging the semantic gap and enhancing fusion efficacy. Experimental results demonstrate the effectiveness of our network in segmenting landslide areas accurately within the remote sensing image dataset. Especially, our F1_scores on three benchmarks outperform existing runner-ups by notable margins of 4.81%, 1.72%, and 1.16%, showcasing the value of our method in this domain.",Landslide,Transformer,Semantic segmentation,Remote sensing,Feature fusion,"Yuan, Xiaofeng","Wang, Yalin",,,APPLIED SOFT COMPUTING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_523,"Su, Yu-Chen","Liu, Tsung-Jung","Liuy, Kuan-Hsien",,Multi-scale Wavelet Frequency Channel Attention for Remote Sensing Image Segmentation,,2022,9,"Among recent developments in semantic segmentation, deep convolutional encoder-decoder has become the main-scheme model for remote sensing images. In this paper, we propose a architecture similar to U-Net for remote sensing image segmentation that uses wavelet frequency channel attention (WFCA) blocks as the attention mechanism to extract rich semantic features, which not only contain local information in spatial domain, but also consider frequency details in frequency domain. Then we fuse WFCA blocks with multi-scale skip connections to become multi-scale wavelet frequency channel attention (ms-WFCA) blocks for better utilizing features from different scales. Finally, the proposed method shows promising results on the Potsdam dataset.",Semantic segmentation,remote sensing,wavelet transformation,attention mechanism,multi-scale,,,,,DEEP LEARNING IN MEDICAL IMAGE ANALYSIS AND MULTIMODAL LEARNING FOR CLINICAL DECISION SUPPORT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_524,Ni Ruiwen,Mu Ye,Li Ji,Zhang Tong,Segmentation of Remote Sensing Images Based on U-Net Multi-Task Learning,,2022,2,"In order to accurately segment architectural features in highresolution remote sensing images, a semantic segmentation method based on U-net network multi-task learning is proposed. First, a boundary distance map was generated based on the remote sensing image of the ground truth map of the building. The remote sensing image and its truth map were used as the input in the U-net network, followed by the addition of the building ground prediction layer at the end of the U-net network. Based on the ResNet network, a multi-task network with the boundary distance prediction layer was built. Experiments involving the ISPRS aerial remote sensing image building and feature annotation data set show that compared with the full convolutional network combined with the multi-layer perceptron method, the intersection ratio of VGG16 network, VGG16 + boundary prediction, ResNet50 and the method in this paper were increased by 5.15%, 6.946%, 6.41% and 7.86%. The accuracy of the networks was increased to 94.71%, 95.39%, 95.30% and 96.10% respectively, which resulted in high-precision extraction of building features.",Multitasking learning,U-net,ResNet,remote sensing image,semantic segmentation,Luo Tianye,Feng Ruilong,Gong He,Hu Tianli,CMC-COMPUTERS MATERIALS & CONTINUA,Sun Yu,,,,,,,,,,Guo Ying,Li Shijun,"Tyasi, Thobela Louis",,,,,,,,,,,,,,,,,,
Row_525,"Li, Xin","Xu, Feng","Liu, Fan","Tong, Yao",Semantic Segmentation of Remote Sensing Images by Interactive Representation Refinement and Geometric Prior-Guided Inference,,2024,18,"High spatial resolution remote sensing images (HRRSIs) contain intricate details and varied spectral distributions, making their semantic segmentation a challenging task. To address this problem, it is crucial to adequately capture both local and global contexts to reduce semantic ambiguity. While self-attention modules in vision transformers capture long-range context, they tend to sacrifice local details. In this article, we propose a geometric prior-guided interactive network (GPINet), a hybrid network that refines features across encoder and decoder stages. First of all, a dual branch structure encoder with local-global interaction modules (LGIMs) is designed to fully exploit local and global contexts for feature refinement. Unlike commonly used skip connections or concatenations, the LGIMs bilaterally couple and exchange CNN features with transformer features by lossless transformation and elaborating cross-attention. Moreover, we introduce a geometric prior generation module (GPGM) that iteratively updates the randomly initialized geometric prior. Subsequently, the geometric priors are stored and used to guide feature recovery. Finally, a weighted summation is applied to the upsampled decoded features and geometric priors. By comprehensively capturing contexts and enabling lossless decoding and deterministic inference, GPINet allows the network to learn discriminative representations for accurately specifying pixel-level semantics. Experiments on three benchmark datasets demonstrate the superiority of the proposed GPINet over state-of-the-art methods. Furthermore, we validate the effectiveness of geometric priors and compare the model sizes.",Attention bias,contextual affinity,remote sensing images (RSIs),semantic segmentation,synergistic attention,"Lyu, Xin","Zhou, Jun",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_526,"Xu, Chunyan","Li, Chengzheng","Cui, Zhen","Zhang, Tong",Hierarchical Semantic Propagation for Object Detection in Remote Sensing Imagery,,JUN 2020,54,"Object detection in remote sensing imagery is a critical yet challenging task in the field of computer vision due to the bird & x2019;s-eye-view perspective. Although existing object detection approaches in remote sensing imagery have achieved great advances through the utilization of deep features or rotation proposals, but they give insufficient consideration to multilevel semantic information and its propagation for guiding the learning process. Accordingly, in this article, we propose a hierarchical semantic propagation (HSP) framework to boost object detection performance in remote sensing imagery, which is better able to propagate hierarchical semantic information among different components in a unified network. Given a remote sensing image as input, the HSP framework can detect instances of semantic objects belonging to certain categories in an end-to-end way. First, the multiscale representation is captured by a basic feature pyramid network, which can hierarchically combine spatial attention details and the global semantic structure in order to learn more discriminative visual features. Second, the soft-segmentation prediction is used as an auxiliary objective in the intermediate layer of our HSP; its output instance-aware semantic information can be propagated to suppress noisy background information and thereby guide the proposal generation in the region proposal network. By further propagating this hierarchical semantic information into the region of interest module, we can then predict the object category information and the corresponding horizontal and oriented bounding boxes. Comprehensive evaluations on three benchmark data sets demonstrate the superiority of our HSP to the existing state-of-the-art methods for object detection in remote sensing imagery.",Hierarchical semantic propagation (HSP),object detection,remote sensing imagery,,,"Yang, Jian",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_527,"Fan, Xiaomin","Zhou, Wujie","Qian, Xiaohong","Yan, Weiqing",Progressive Adjacent-Layer coordination symmetric cascade network for semantic segmentation of Multimodal remote sensing images,,MAR 15 2024,18,"Semantic segmentation of remote sensing images is a fundamental task in computer vision, with significant applications in forest and farmland cover surveys, geological disaster monitoring, and other related fields. The inclusion of digital surface models can enhance the segmentation performance compared to using unimodal imaging alone. However, most existing methods simply combine the features from both modalities without considering their differences and complementarity, leading to a loss of spatial details. In order to address this issue and improve segmentation accuracy, we propose a novel network called Progressive Adjacent-Layer Coordination Symmetric Cascade Network (PACSCNet). This network employs a two-stage fusion symmetric cascade encoder to leverage the similarities and differences between adjacent features for cross-layer fusion, thereby preserving spatial details. Additionally, our network includes a new dual-pyramid symmetric cascade decoder that extracts similarities in multimodal and cross-layer fusion features for merging. Furthermore, a pyramid residual integration module progressively integrates features at four scales to mitigate noise interference during large-scale fusion. Extensive experimental evaluations on the Vaihingen and Potsdam datasets demonstrate that PACSCNet achieves strong semantic segmentation performance, comparable to state-of-the-art approaches, in terms of accuracy and intersection-over-union. The source code and results of our proposed PACSCNet are publicly available at https://github.com/F8AoMn/PACSCNet.",Cross-layer fusion,Multimodal remote sensing image,Semantic segmentation,Symmetric cascade network,,,,,,EXPERT SYSTEMS WITH APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_528,"Zhang, Yuzhu","Gao, Di","Du, Yongxing","Li, Baoshan",Efficient multi-scale network for semantic segmentation of fine-resolution remotely sensed images,,SEP 1 2024,0,"Semantic segmentation of remote sensing urban scene images has diverse practical applications, including land cover mapping, urban change detection, environmental protection, and economic evaluation. However, classical semantic segmentation networks encounter challenges such as inadequate utilization of multi-scale semantic information and imprecise edge target segmentation in high-resolution remote sensing images. In response, this article introduces an efficient multi-scale network (EMNet) tailored for semantic segmentation of common features in remote sensing images. To address these challenges, EMNet integrates several key components. Firstly, the efficient atrous spatial pyramid pooling module is employed to enhance the relevance of multi-scale targets, facilitating improved extraction and processing of context information across different scales. Secondly, the efficient multi-scale attention mechanism and multi-scale jump connections are utilized to fuse semantic features from various levels, thereby achieving precise segmentation boundaries and accurate position information. Finally, an encoder-decoder structure is incorporated to refine the segmentation results. The effectiveness of the proposed network is validated through experiments conducted on the publicly available DroneDeploy image dataset and Potsdam dataset. Results indicate that EMNet achieves impressive performance metrics, with mean intersection over union (MIoU), mean precision (MPrecision), and mean recall (MRecall) reaching 75.99%, 86.76%, and 85.07%, respectively. Comparative analysis demonstrates that the network proposed in this article outperforms current mainstream semantic segmentation networks on both the DroneDeploy and Potsdam dataset.",remote sensing,deep learning,semantic segmentation,convolutional neural network,attention mechanism,"Qin, Ling",,,,MEASUREMENT SCIENCE AND TECHNOLOGY,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_529,"Cai, Jiajing","Shi, Jinmei","Leau, Yu-Beng","Meng, Shangyu",Res50-SimAM-ASPP-Unet: A Semantic Segmentation Model for High-Resolution Remote Sensing Images,,2024,0,"High-resolution remote sensing images contain intricate details and complex backgrounds, presenting challenges for traditional segmentation methods, which often struggle with accurate classification and contextual understanding. To address these issues, this study introduces the Res50-SimAM-ASPP-Unet model, a semantic segmentation approach for high-resolution remote sensing image processing tasks. The model integrates ResNet50 as the encoding layer of Unet for robust feature extraction, adds the SimAM attention mechanism to selectively enhance relevant details, and incorporates the ASPP module in the decoding layer to capture multi-scale contextual information. The methodology part analyzes the common ResNet model, the attention mechanism module, and the multi-scale feature extraction module, respectively, and then designs experiments to show the necessity and optimal position of adding Res50, SimAM, and ASPP. Comparative experiments on the LandCover.ai dataset demonstrate that the proposed model significantly outperforms common semantic segmentation networks, achieving a MIoU of 81.1%, MPA of 88.2%, Accuracy of 95.1%, Precision of 92.65%, and an F1 score of 90.45%. These results highlight the model's effectiveness in delivering high accuracy and adaptability across diverse remote sensing environments, establishing it as a valuable tool for applications requiring precise and scalable image segmentation.",Remote sensing,Feature extraction,Semantic segmentation,Accuracy,Interpolation,"Zheng, Xiuyan","Zhou, Jinghe",,,IEEE ACCESS,,Residual neural networks,Computer architecture,Computational modeling,Training,Image coding,Segmentation of high-resolution remote sensing images,multi-scale void space pyramid pool ASPP module,attention mechanism SimAM module,Res50-SimAM-ASPP-Unet,,,,,,,,,,,,,,,,,,,,,
Row_530,"Weng, Yijie","Li, Zongmei","Tang, Guofeng","Wang, Yang",OCNet-Based Water Body Extraction from Remote Sensing Images,,OCT 2023,2,"Water body extraction techniques from remotely sensed images are crucial in water resources distribution studies, climate change studies and other work. The traditional remote sensing water body extraction has the problems of low accuracy and being time-consuming and laborious, and the water body recognition technique based on deep learning is more efficient and accurate than the traditional threshold method; however, there is the problem that the basic model of semantic segmentation is not well-adapted to complex remote sensing images. Based on this, this study adopts an OCNet feature extraction network to modify the base model of semantic segmentation, and the resulting model achieves excellent performance on water body remote sensing images. Compared with the traditional water body extraction method and the base network, the OCNet modified model has obvious improvement, and is applicable to the extraction of water bodies in true-color remote sensing images such as high-score images and unmanned aerial vehicle remote sensing images. The results show that the model in this study can realize automatic and fast extraction of water bodies from remote sensing images, and the predicted water body image accuracy (ACC) can reach 85%. This study can realize fast and accurate extraction of water bodies, which is of great significance for water resources acquisition and flood disaster prediction.",water body extraction,remote sensing image,semantic segmentation,OCNet,deep learning,,,,,WATER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_531,Xu Zhaohong,Liu Yu,Wu Chen,Guan Shihao,Semantic Segmentation of Buildings in Remote Sensing Images Based on Dense Residual Learning and Channel Adaption,2019 4TH INTERNATIONAL CONFERENCE ON ELECTROMECHANICAL CONTROL TECHNOLOGY AND TRANSPORTATION (ICECTT 2019),2019,1,"Deep fully convolution neural network has opened a new field in semantic segmentation for remote sensing images. In this paper, an improved U-net model is proposed to extract buildings at the pixel level, so as to obtain its contour and size information. In this improved U-net model, the highly modular ResNeXt50 network is used as the encoder of U-net and a parallel dense residual module based on atrous convolution is proposed to extract multi-scale semantic information for segmentation. Moreover, the transposed convolution whose stride is 2 is used for upsampling to obtain the grayscale segmentation mask. This modified model adopts the sum of jaccard loss and binary cross entropy loss as the total loss function. Our experimental results have demonstrated that our improved U-net model has excellent performance on semantic information encoding and multi-scale feature extraction. Finally, the mean pixel precision (MPA), the mean intersection of union (MIoU), F-1 score and AUC of our improved model are 93.17%, 81.51%, 86.95% and 0.9928 respectively. The F-1 Score and AUC of our improved model are 11.3% and 11.4% higher than those of the standard U-net model respectively.",building extraction,remote sensing images,semantic segmentation,SE-Net,U-net,Zheng Ergong,Ma Yang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_532,"Shi, Jun","Jiang, Zhiguo","Feng, Hao","Ma, Yibing",SPARSE CODING-BASED TOPIC MODEL FOR REMOTE SENSING IMAGE SEGMENTATION,2013 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),2013,1,"Land cover segmentation can be viewed as topic assignment that the pixels are grouped into homogeneous regions according to different semantic topics in topic model. In this paper, we propose a novel topic model based on sparse coding for segmenting different kinds of land covers. Different from conventional topic models which generally assume each local feature descriptor is related to only one visual word of the codebook, our method utilizes sparse coding to characterize the potential correlation between the descriptor and multiple words. Therefore each descriptor can be represented by a small set of words. Furthermore, in this paper probabilistic Latent Semantic Analysis (pLSA) is applied to learn the latent relation among word, topic and document due to its simplicity and low computational cost. Experimental results on remote sensing image segmentation demonstrate the excellent superiority of our method over k-means clustering and conventional pLSA model.",remote sensing,sparse coding,pLSA,land cover segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_533,"Yang, Xuan","Li, Shanshan","Chen, Zhengchao","Chanussot, Jocelyn",An attention-fused network for semantic segmentation of very-high-resolution remote sensing imagery,,AUG 2021,107,"Semantic segmentation is an essential part of deep learning. In recent years, with the development of remote sensing big data, semantic segmentation has been increasingly used in remote sensing. Deep convolutional neural networks (DCNNs) face the challenge of feature fusion: very-high-resolution remote sensing image multisource data fusion can increase the network's learnable information, which is conducive to correctly classifying target objects by DCNNs; simultaneously, the fusion of high-level abstract features and low-level spatial features can improve the classification accuracy at the border between target objects. In this paper, we propose a multipath encoder structure to extract features of multipath inputs, a multipath attention-fused block module to fuse multipath features, and a refinement attention-fused block module to fuse high-level abstract features and low-level spatial features. Furthermore, we propose a novel convolutional neural network architecture, named attention-fused network (AFNet). Based on our AFNet, we achieve state-of-the-art performance with an overall accuracy of 91.7% and a mean F1 score of 90.96% on the ISPRS Vaihingen 2D dataset and an overall accuracy of 92.1% and a mean F1 score of 93.44% on the ISPRS Potsdam 2D dataset.",Semantic segmentation,Deep learning,Very-high-resolution imagery,Attention-fused network,ISPRS,"Jia, Xiuping","Zhang, Bing","Li, Baipeng","Chen, Pan",ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,Convolutional neural network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_534,"Zhang, Ming","Gu, Xin","Qi, Ji","Zhang, Zhenshi",CDEST: Class Distinguishability-Enhanced Self-Training Method for Adopting Pre-Trained Models to Downstream Remote Sensing Image Semantic Segmentation,,APR 2024,0,"The self-supervised learning (SSL) technique, driven by massive unlabeled data, is expected to be a promising solution for semantic segmentation of remote sensing images (RSIs) with limited labeled data, revolutionizing transfer learning. Traditional 'local-to-local' transfer from small, local datasets to another target dataset plays an ever-shrinking role due to RSIs' diverse distribution shifts. Instead, SSL promotes a 'global-to-local' transfer paradigm, in which generalized models pre-trained on arbitrarily large unlabeled datasets are fine-tuned to the target dataset to overcome data distribution shifts. However, the SSL pre-trained models may contain both useful and useless features for the downstream semantic segmentation task, due to the gap between the SSL tasks and the downstream task. To adapt such pre-trained models to semantic segmentation tasks, traditional supervised fine-tuning methods that use only a small number of labeled samples may drop out useful features due to overfitting. The main reason behind this is that supervised fine-tuning aims to map a few training samples from the high-dimensional, sparse image space to the low-dimensional, compact semantic space defined by the downstream labels, resulting in a degradation of the distinguishability. To address the above issues, we propose a class distinguishability-enhanced self-training (CDEST) method to support global-to-local transfer. First, the self-training module in CDEST introduces a semi-supervised learning mechanism to fully utilize the large amount of unlabeled data in the downstream task to increase the size and diversity of the training data, thus alleviating the problem of biased overfitting of the model. Second, the supervised and semi-supervised contrastive learning modules of CDEST can explicitly enhance the class distinguishability of features, helping to preserve the useful features learned from pre-training while adapting to downstream tasks. We evaluate the proposed CDEST method on four RSI semantic segmentation datasets, and our method achieves optimal experimental results on all four datasets compared to supervised fine-tuning as well as three semi-supervised fine-tuning methods.",semantic segmentation,remote sensing (RS),transfer learning,fine-tuning method,contrastive learning,"Yang, Hemeng","Xu, Jun","Peng, Chengli","Li, Haifeng",REMOTE SENSING,,self-training,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_535,"Hou, Jianlong","Guo, Zhi","Wu, Youming","Diao, Wenhui",BSNet: Dynamic Hybrid Gradient Convolution Based Boundary-Sensitive Network for Remote Sensing Image Segmentation,,2022,31,"Boundary information is essential for the semantic segmentation of remote sensing images. However, most existing methods were designed to establish strong contextual information while losing detailed information, making it challenging to extract and recover boundaries accurately. In this article, a boundary-sensitive network (BSNet) is proposed to address this problem via dynamic hybrid gradient convolution (DHGC) and coordinate sensitive attention (CSA). Specifically, in the feature extraction stage, we propose DHGC to replace vanilla convolution (VC), which adaptively aggregates one VC kernel and two gradient convolution kernels (GCKs) into a new operator to enhance boundary information extraction. The GCKs are proposed to explicitly encode boundary information, which is inspired by traditional Sobel operators. In the feature recovery stage, the CSA is introduced. This module is used to reconstruct the sharp and detailed segmentation results by adaptively modeling the boundary information and long-range dependencies in the low-level features as the assistance of high-level features. Note that DHGC and CSA are plug-and-play modules. We evaluate the proposed BSNet on three public datasets: the ISPRS 2-D semantic labeling Vaihingen, the Potsdam benchmark, and the iSAID dataset. The experimental results indicate that BSNet is a highly effective architecture that produces sharper predictions around object boundaries and significantly improves the segmentation accuracy. Our method demonstrates superior performance on the Vaihingen, the Potsdam benchmark, and the iSAID dataset in terms of the mean F-1, with improvements of 4.6%, 23%, and 2.4% over strong baselines, respectively. The code and models will be made publicly available.",Attention module,boundary information enhancement,remote sensing,semantic segmentation,,"Xu, Tao",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_536,"Yan, Yi","Zhang, Jing","Wu, Xinjia","Li, Jiafeng",When zero-padding position encoding encounters linear space reduction attention: an efficient semantic segmentation Transformer of remote sensing images,,JAN 17 2024,0,"Semantic segmentation of remote sensing images (RSIs) is of great significance for obtaining geospatial object information. Transformers win promising effect, whereas multi-head self-attention (MSA) is expensive. We propose an efficient semantic segmentation Transformer (ESST) of RSIs that combines zero-padding position encoding with linear space reduction attention (LSRA). First, to capture the coarse-to-fine features of RSI, a zero-padding position encoding is proposed by adding overlapping patch embedding (OPE) layers and convolution feed-forward networks (CFFN) to improve the local continuity of features. Then, we replace LSRA in the attention operation to extract multi-level features to reduce the computational cost of the encoder. Finally, we design a lightweight all multi-layer perceptron (all-MLP) head decoder to easily aggregate multi-level features to generate multi-scale features for semantic segmentation. Experimental results demonstrate that our method produces a trade-off in accuracy and speed for semantic segmentation of RSIs on the Potsdam and Vaihingen datasets, respectively.",Remote sensing images,semantic segmentation,Zero-padding position encoding,linear space reduction attention,All-MLP,"Zhuo, Li",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,Transformer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_537,"Lin, Dao-Yu","Wang, Yang","Xu, Guang-Luan","Fu, Kun",SYNTHESIZING REMOTE SENSING IMAGES BY CONDITIONAL ADVERSARIAL NETWORKS,2017 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),2017,7,"Automated annotation of urban areas from overhead imagery plays an essential role in many remote sensing applications. Generative Adversarial Nets (GANs) is one of the most effective ways to handle this problem. In this manuscript, two tricks were added in conditional GANs(cGANs) which learn the mapping from input image to output remote sensing image. All the experimental results demonstrated that cGANs was a reliable way to generate high-quality remote sensing images. What's more, when this method be applied to semantic segmentation and accurate classification was made by using ISPRS 2D semantic labelling challenge dataset.",Generative adversarial nets,synthesize remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_538,"Lang, Chunbo","Wang, Junyi","Cheng, Gong","Tu, Binfei",Progressive Parsing and Commonality Distillation for Few-Shot Remote Sensing Segmentation,,2023,41,"In recent years, few-shot segmentation (FSS) has received widespread attention from scholars by virtue of its superiority in low-data regimes. Most existing research focuses on natural image processing, and very few studies are dedicated to the practical but challenging topic of remote sensing image understanding. Related experimental results show that directly transferring the previously proposed framework to the current domain is prone to produce unsatisfactory results with incomplete objects and irrelevant distractors. Such phenomena can be attributed to the lack of modules specifically designed for the complex characteristics of remote sensing images, e.g., great intra-class diversity and low target-background contrast. In this article, we propose a conceptually simple and easy-to-implement framework to tackle the aforementioned problems. Specifically, our innovative design embodies two main aspects: 1) the support mask is progressively parsed into multiple valuable subregions that can be further exploited to compute local descriptors with segmentation cues about intractable parts; and 2) the base-class memories stored in the meta-training phase are replayed and leveraged for the distillation of novel-class prototypes, where the commonalities between classes are adequately explored, more in line with the concept of learning to learn. These two components, i.e., the progressive parsing module and commonality distillation module, contribute to each other and together constitute the proposed PCNet. We conduct extensive experiments on the standard benchmark to evaluate segmentation performance in few-shot settings. Quantitative and qualitative results illustrate that our PCNet distinctly outperforms previous FSS approaches and sets a new state-of-the-art.",Few-shot learning,few-shot segmentation (FSS),image processing,remote sensing,semantic segmentation,"Han, Junwei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_539,"Hosseinpour, H. R.","Samadzadegan, F.","Javan, F. Dadrass","Motayyeb, S.",IMPROVING SEMANTIC SEGMENTATION OF HIGH-RESOLUTION REMOTE SENSING IMAGES USING WASSERSTEIN GENERATIVE ADVERSARIAL NETWORK,"ISPRS GEOSPATIAL CONFERENCE 2022, JOINT 6TH SENSORS AND MODELS IN PHOTOGRAMMETRY AND REMOTE SENSING, SMPR/ 4TH GEOSPATIAL INFORMATION RESEARCH, GIRESEARCH CONFERENCES, VOL. 48-4",2023,0,"Semantic segmentation of remote sensing images with high spatial resolution has many applications in a wide range of problems in this field. In recent years, the use of advanced techniques based on fully convolutional neural networks have achieved high and impressive accuracies. However, the labels of different classes are estimated independently in this method. In general, the segmentation effect is too coarse to take the relationship between pixels into account. On the other hand, due to the use of convolution filters and limitations of calculations, the field of view information of these filters will be limited in deep layers. In this study, a method based on generative adversarial network (GAN) is proposed to strengthen spatial vicinity in the output segmentation map. The segmentation model receive assistance from the GAN model in the form of a higher order potential loss. Furthermore, for better stability and performance in model training the Wasserstein GAN is used for optimization of the model. We successfully show an increase in semantic segmentation accuracy using the challenging ISPRS Vaihingen benchmark dataset.",Semantic Segmentation,Deep Learning,Wasserstein GAN,Generative Adversarial Network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_540,"Kumar, Satyawant","Kumar, Abhishek","Lee, Dong-Gyu",,RSSGLT: Remote Sensing Image Segmentation Network Based on Global-Local Transformer,,2024,4,"Remotely captured images possess an immense scale and object appearance variability due to the complex scene. It becomes challenging to capture the underlying attributes in the global and local context for their segmentation. Existing networks struggle to capture the inherent features due to the cluttered background. To address these issues, we propose a remote sensing image segmentation network, RSSGLT, for semantic segmentation of remote sensing images. We capture the global and local features by leveraging the benefits of the transformer and convolution mechanisms. RSSGLT is an encoder-decoder design that uses multiscale features. We construct an attention map module (AMM) to generate channelwise attention scores for fusing these features. We construct a global-local transformer block (GLTB) in the decoder network to support learning robust representations during a decoding phase. Furthermore, we designed a feature refinement module (FRM) to refine the fused output of the shallow stage encoder feature and the deepest GLTB feature of the decoder. Experimental findings on the two public datasets show the effectiveness of the proposed RSSGLT.",Transformers,Decoding,Image segmentation,Remote sensing,Convolution,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Semantics,Context details,multiscale features,remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_541,"Ran, Lingyan","Wang, Lushuang","Zhuo, Tao","Xing, Yinghui",DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image Semantic Segmentation With Unsupervised Domain Adaptation,,2024,1,"The semantic segmentation of remote sensing (RS) images is a challenging and hot issue due to the large amount of unlabeled data and domain variation. Unsupervised domain adaptation (UDA) has proven to be advantageous in leveraging unlabeled information from the target domain. However, traditional approaches of independently fine-tuning UDA models in the source and target domains have a limited effect on the result. In this article, we propose a hybrid training strategy that boosts self-training methods with domain fusion images. First, we introduce a novel dual-domain image fusion (DDF) strategy to effectively utilize the original image, the style-transferred image, and the intermediate-domain information. Second, to further refine the precision of pseudolabels, we present a region-specific reweighting strategy that assigns different weights to pseudolabel regions based on their spatial context. Finally, we conduct a series of extensive benchmark experiments and ablation studies on the ISPRS Vaihingen and Potsdam datasets. These results show the efficiency of our approach and establish a practical basis for implementing semantic segmentation in remote sensors.",Training,Adaptation models,Remote sensing,Semantic segmentation,Task analysis,"Zhang, Yanning",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Accuracy,Semantics,Domain adaptation,feature fusion,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_542,"Li, Haifeng","Jing, Wenxuan","Wei, Guo","Wu, Kai",RiSSNet: Contrastive Learning Network with a Relaxed Identity Sampling Strategy for Remote Sensing Image Semantic Segmentation,,JUL 2023,1,"Contrastive learning techniques make it possible to pretrain a general model in a self-supervised paradigm using a large number of unlabeled remote sensing images. The core idea is to pull positive samples defined by data augmentation techniques closer together while pushing apart randomly sampled negative samples to serve as supervised learning signals. This strategy is based on the strict identity hypothesis, i.e., positive samples are strictly defined by each (anchor) sample's own augmentation transformation. However, this leads to the over-instancing of the features learned by the model and the loss of the ability to fully identify ground objects. Therefore, we proposed a relaxed identity hypothesis governing the feature distribution of different instances within the same class of features. The implementation of the relaxed identity hypothesis requires the sampling and discrimination of the relaxed identical samples. In this study, to realize the sampling of relaxed identical samples under the unsupervised learning paradigm, the remote sensing image was used to show that nearby objects often present a large correlation; neighborhood sampling was carried out around the anchor sample; and the similarity between the sampled samples and the anchor samples was defined as the semantic similarity. To achieve sample discrimination under the relaxed identity hypothesis, the feature loss was calculated and reordered for the samples in the relaxed identical sample queue and the anchor samples, and the feature loss between the anchor samples and the sample queue was defined as the feature similarity. Through the sampling and discrimination of the relaxed identical samples, the leap from instance-level features to class-level features was achieved to a certain extent while enhancing the network's invariant learning of features. We validated the effectiveness of the proposed method on three datasets, and our method achieved the best experimental results on all three datasets compared to six self-supervised methods.",semantic segmentation,remote sensing (RS),self-supervised learning,contrastive learning,,"Su, Mingming","Liu, Lu","Wu, Hao","Li, Penglong",REMOTE SENSING,"Qi, Ji",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_543,"Sui, Baikai","Cao, Yungang","Bai, Xueqin","Zhang, Shuang",BIBED-Seg: Block-in-Block Edge Detection Network for Guiding Semantic Segmentation Task of High-Resolution Remote Sensing Images,,2023,18,"Edge optimization of semantic segmentation results is a challenging issue in remote sensing image processing. This article proposes a semantic segmentation model guided by a block-in-block edge detection network named BIBED-Seg. This is a two-stage semantic segmentation model, where edges are extracted first and then segmented. We do two key works: The first work is edge detection, and we present BIBED, a block-in-block edge detection network, to extract the accurate boundary features. Here, the edge detection of multiscale feature fusion is first realized by creating the block-in-block residual network structure and devising the multilevel loss function. Second, we add the channel and spatial attention module into the residual structure to improve high-resolution remote sensing images' boundary positioning and detection accuracy by focusing on their channel and spatial dimensions. Finally, we evaluate our method on International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Vaihingen data sets and obtain ODS F-measure of 0.6671 and 0.7432, higher than other excellent edge detection methods. The second work is two-stage segmentation. First, the proposed BIBED is individually pretrained, and subsequently, the pretrained model is introduced into the entire segmentation network to extract boundary features. In the second segmentation stage, the edge detection network is used to constrain semantic segmentation results by loss cycles and feature bootstrapping. Our best model obtains the OA of 90.2%, 87.7%, and 81.5%, the IOU of 76.0%, 69.6%, and 61.3% on the ISPRS and WHDLD datasets, respectively.",Image edge detection,Feature extraction,Remote sensing,Semantic segmentation,Convolution,"Wu, Renzhe",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Sensors,Semantics,Channel attention mechanism,edge detection,high-resolution remote sensing,multiple-residual convolution blocks,semantic segmentation,spatial attention mechanism,,,,,,,,,,,,,,,,,,,,,,
Row_544,"Nogueira, Keiller","Dalla Mura, Mauro","Chanussot, Jocelyn","Schwartz, William Robson",Dynamic Multicontext Segmentation of Remote Sensing Images Based on Convolutional Networks,,OCT 2019,106,"Semantic segmentation requires methods capable of learning high-level features while dealing with large volume of data. Toward such goal, convolutional networks can learn specific and adaptable features based on the data. However, these networks are not capable of processing a whole remote sensing image, given its huge size. To overcome such limitation, the image is processed using fixed size patches. The definition of the input patch size is usually performed empirically (evaluating several sizes) or imposed (by network constraint). Both strategies suffer from drawbacks and could not lead to the best patch size. To alleviate this problem, several works exploited multicontext information by combining networks or layers. This process increases the number of parameters, resulting in a more difficult model to train. In this paper, we propose a novel technique to perform semantic segmentation of remote sensing images that exploits a multicontext paradigm without increasing the number of parameters while defining, in training time, the best patch size. The main idea is to train a dilated network with distinct patch sizes, allowing it to capture multicontext characteristics from heterogeneous contexts. While processing these varying patches, the network provides a score for each patch size, helping in the definition of the best size for the current scenario. A systematic evaluation of the proposed algorithm is conducted using four high-resolution remote sensing data sets with very distinct properties. Our results show that the proposed algorithm provides improvements in pixelwise classification accuracy when compared to the state-of-the-art methods.",Convolutional networks (ConvNets),deep learning,multicontext,multiscale,remote sensing,"dos Santos, Jefersson Alex",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_545,"Gao, Kuiliang","You, Xiong","Li, Ke","Chen, Lingyu",Attention Prompt-Driven Source-Free Adaptation for Remote Sensing Images Semantic Segmentation,,2024,1,"Recently, remote sensing images (RSIs) domain adaptation segmentation has been extensively studied. However, existing methods generally assume that source RSIs must be available, which is obviously an overly demanding condition and will increase unnecessary costs in practice. To this end, this letter takes the lead in exploring RSIs source-free adaptation segmentation, where only the offline model pretrained on the source domain and target RSIs are available. A novel method featuring prompt learning and vision foundation models is proposed, and the novelty design includes two aspects. First, to better adapt the general-purpose knowledge in the foundation model to different target RSIs, an attention-guided prompt tuning strategy is proposed, which can dynamically steer the knowledge at different layers and positions through prompts with different weights. Second, a feature alignment strategy with similarity distance is proposed for source-free domain adaptation by taking full advantage of the representation ability of the foundation model and the flexibility of prompt learning. Extensive experiments indicate that the performance of the proposed method is significantly superior to that of existing methods. Specifically, the mIoU of target RSIs has been improved by at least 3.14%similar to 4.18%.",Foundation models,prompt learning,remote sensing images (RSIs),semantic segmentation,source-free adaptation,"Lei, Juan","Zuo, Xibing",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_546,"Yao, Hongtai","Wang, Xianpei","Zhao, Le","Tian, Meng",Semantic Segmentation for Remote Sensing Images Using Pyramid Object-Based Markov Random Field With Dual-Track Information Transmission,,2022,2,"Semantic segmentation is one of the most important tasks in remote sensing image processing. According to task requirements, the semantic depth given to the same remote sensing image can be different, and many people have studied it through a pyramid or multilayer structure. The Markov random field (MRF) is widely used in single-layer modeling due to its outstanding spatial relationship capturing ability and feature description ability, but it is not sufficient enough to mine the interlayer information, and the way of information transmission between layers is relatively simple direct projection segmentation results. To solve this problem, new dual-track information transmission is proposed in this letter. The proposed method first constructs a triple-multi (multiresolution, multiregion adjacency graph (RAG), and multisemantic)-pyramid (TMP) structure with the original resolution image as the middle layer in the pyramid. Then, the MRF model is defined on each layer; its likelihood function and the prior function that are related to the adjacent layer are constructed. Finally, the dual-track information transmission circulation is carried out to traverse the entire pyramid structure starting from the original resolution layer. The proposed method is tested on different remote sensing images obtained by the SPOT5, Gaofen-2, and unmanned aerial vehicle (UAV) sensors. Experimental results show that the proposed method has better segmentation performance than other multilayer MRF methods.",Image segmentation,Semantics,Remote sensing,Markov processes,Information processing,"Gong, Li","Li, Bowen",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Image resolution,Task analysis,Information transmission,Markov random field (MRF),pyramid structure,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_547,"Zhao, Lei","Qiao, Peng","Dou, Yong",,Aircraft Segmentation Based On Deep Learning framework : from extreme points to remote sensing image segmentation,2019 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI 2019),2019,1,"Remote sensing image segmentation is a very important technology. Although the segmentation method based on convolutional neural networks (CNNs) has achieved promising results in natural image test set, e.g. VOC or COCO, they provide inferior performance when being transferred to remote sensing images. Due to the limits of labeled remote sensing images, fine-tuning pre-trained CNNs using remote sensing images do not benefit the image segmentation performance. Inspired by the recent works of interactive segmentation methods which exploit several extreme clicks that are fed into CNNs to improve the accuracy of the segmentation, we propose an effective method to improve the segmentation accuracy, which uses four extreme points (the top, bottom, left, and right) as the guide information. In terms of mIoU, our method achieves 84.4% on remote sensing image dataset, which outperforms the previous work by 23.1%. Compared with the previous interactive segmentation methods, the proposed method achieves superior performance. In addition, an improved method with an extra point is proposed based on the inaccurate part of results obtained by four extreme points. It is very feasible to be applied in an interactive segmentation toolbox.",Semantic segmentation,Interactive segmentation,Remote sensing images,Deep learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_548,"Ma, Xiaowen","Che, Rui","Hong, Tingfeng","Ma, Mengting",SACANet: scene-aware class attention network for semantic segmentation of remote sensing images,,2023,6,"Spatial attention mechanism has been widely used in semantic segmentation of remote sensing images given its capability to model long-range dependencies. Many methods adopting spatial attention mechanism aggregate contextual information using direct relationships between pixels within an image, while ignoring the scene awareness of pixels (i.e., being aware of the global context of the scene where the pixels are located and perceiving their relative positions). Given the observation that scene awareness benefits context modeling with spatial correlations of ground objects, we design a scene-aware attention module based on a refined spatial attention mechanism embedding scene awareness. Besides, we present a local-global class attention mechanism to address the problem that general attention mechanism introduces excessive background noises while hardly considering the large intra-class variance in remote sensing images. In this paper, we integrate both scene-aware and class attentions to propose a scene-aware class attention network (SACANet) for semantic segmentation of remote sensing images. Experimental results on three datasets show that SACANet outperforms other state-of-the-art methods and validate its effectiveness. Code is available at https://github.com/xwmaxwma/rssegmentation.",Semantic Segmentation,Scene Awareness,Class Attention,,,"Zhao, Ziyan","Feng, Tian","Zhang, Wei",,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_549,"Wang, Shanshan","Zuo, Zhiqi","Yan, Shuhao","Zeng, Weimin",A Novel Global-Local Feature Aggregation Framework for Semantic Segmentation of Large-Format High-Resolution Remote Sensing Images,,AUG 2024,0,"In high-resolution remote sensing images, there are areas with weak textures such as large building roofs, which occupy a large number of pixels in the image. These areas pose a challenge for traditional semantic segmentation networks to obtain ideal results. Common strategies like downsampling, patch cropping, and cascade models often sacrifice fine details or global context, resulting in limited accuracy. To address these issues, a novel semantic segmentation framework has been designed specifically for large-format high-resolution remote sensing images by aggregating global and local features in this paper. The framework consists of two branches: one branch deals with low-resolution downsampled images to capture global features, while the other branch focuses on cropped patches to extract high-resolution local details. Also, this paper introduces a feature aggregation module based on the Transformer structure, which effectively aggregates global and local information. Additionally, to save GPU memory usage, a novel three-step training method has been developed. Extensive experiments on two public datasets demonstrate the effectiveness of the proposed approach, with an IoU of 90.83% on the AIDS dataset and 90.30% on the WBDS dataset, surpassing state-of-the-art methods such as DANet, DeepLab v3+, U-Net, ViT, TransUNet, CMTFNet, and UANet.",high-resolution remote sensing images,optical large-format images,semantic segmentation,transformer,building extraction,"Pang, Shiyan",,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_550,"Zhang, Jinglin","Li, Yuxia","Tong, Zhonggui","He, Lei",GLCANet: Global-Local Context Aggregation Network for Cropland Segmentation from Multi-Source Remote Sensing Images,,DEC 2024,0,"Cropland is a fundamental basis for agricultural development and a prerequisite for ensuring food security. The segmentation and extraction of croplands using remote sensing images are important measures and prerequisites for detecting and protecting farmland. This study addresses the challenges of diverse image sources, multi-scale representations of cropland, and the confusion of features between croplands and other land types in large-area remote sensing image information extraction. To this end, a multi-source self-annotated dataset was developed using satellite images from GaoFen-2, GaoFen-7, and WorldView, which was integrated with public datasets GID and LoveDA to create the CRMS dataset. A novel semantic segmentation network, the Global-Local Context Aggregation Network (GLCANet), was proposed. This method integrates the Bilateral Feature Encoder (BFE) of CNNs and Transformers with a global-local information mining module (GLM) to enhance global context extraction and improve cropland separability. It also employs a multi-scale progressive upsampling structure (MPUS) to refine the accuracy of diverse arable land representations from multi-source imagery. To tackle the issue of inconsistent features within the cropland class, a loss function based on hard sample mining and multi-scale features was constructed. The experimental results demonstrate that GLCANet improves OA and mIoU by 3.2% and 2.6%, respectively, compared to the existing advanced networks on the CRMS dataset. Additionally, the proposed method also demonstrated high precision and practicality in segmenting large-area croplands in Chongzhou City, Sichuan Province, China.",semantic segmentation,remote sensing,multi-source image,cropland,deep learning,"Zhang, Mingheng","Niu, Zhenye","He, Haiping",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_551,Zhang Yinhui,Zhang Feng,He Zifen,Yang Xiaogang,Remote Sensing Image Segmentation Based on Attention Guidance and Multi-Feature Fusion,,DEC 2023,2,"Objective Remote sensing images have a large detection range, long dynamic monitoring time, and a large amount of carrying information, making the obtained ground feature information more comprehensive and rich. By extracting ground object targets from remote sensing images, more detailed and accurate ground object information in the imaging area can be obtained, providing data support for high-altitude reconnaissance, precision guidance, and terrain matching. However, with the rapid increase in data volume, the current low level of intelligent and automated target extraction methods is difficult to embrace the demand. Traditional image extraction techniques contain edge detection, threshold segmentation, and region segmentation. These methods have good segmentation performance for remote sensing targets with significant contour boundaries but lack the ability of adaptive adjustment while facing complex and ever-changing remote sensing targets. Convolutional neural networks have stronger representation ability, scalability, and robustness than traditional methods by providing multi-level semantic information in images. Due to the uneven distribution, blurred edges, and variable scales of ground objects in remote sensing images, convolutional neural networks are prone to losing edge information and multi-scale feature information during feature extraction. In addition, cloud cover of remote sensing targets in complex scenes exacerbates the loss of target edge and multi-scale information, making it more difficult for convolutional neural networks to accurately segment remote sensing ground objects. In order to solve the above problems, we propose a segmentation method that uses deep residual networks as the backbone and combines attention guidance and multi-feature fusion to enhance the network's ability to segment remote sensing image ground object edges and multi-scale objects.Methods We propose a remote sensing image semantic segmentation network called AMSNet, which combines attention guidance and multi-feature fusion. In the Encoder Section, D_ Resnet50 is applied as the backbone network to extract the main feature information from remote sensing images, which can enhance the acquisition of detailed information such as edge and small-scale targets in remote sensing images. The category guidance channel attention module is inserted into the backbone to enhance the network's segmentation ability for difficult-to-distinguish and irregularly shaped areas in remote sensing images. A feature reuse module is added to the backbone network to solve the loss of edge detail information and the disappearance of scattered small-scale targets during feature extraction. In the Decoder Section, the cross-regional feature fusion module is applied to fuse the multi-feature information, improving the acquisition of multi-scale target information. Multi-scale loss fusion module is also joined to further enhance the segmentation performance of the network for multi-scale targets.Results and Discussions From the analysis of experimental results on the remote sensing image dataset of the plateau region and the remote sensing image dataset of the plateau region under cloud interference, compared with other semantic segmentation networks, the proposed network has better segmentation performance (Table 6 and Table 7) regardless of cloud interference. In addition, the segmentation performance is less affected by cloud interference. Even under cloud interference, the segmentation accuracy of ground targets is only 1. 10 percentage points lower than that without cloud interference in mIoU, 0. 58 percentage points lower than that in mPa, and 0. 71 percentage points lower than that in mF1, which is lower than the influence of other semantic segmentation networks on segmentation effect under different cloud meteorological interference conditions. In addition, in order to verify the generalization performance of the AMSNet network segmentation effect, the International Society for Photogrammetry and Remote Sensing (ISPRS) dataset in the Vaihingen region of Germany is selected. In order to better fit the picture size, number of grouping convolutions of feature multiplexing modules in the AMSNet network is reduced to four groups. From the experimental results in Table 8, the network still performs better than other networks. This network is compared with PspNet and OCNet, with mIoU increased by 5. 09 percentage points and 5. 57 percentage points, Deeplabv3+ network with mIoU by 3. 47 percentage points, mPa by 3. 56 percentage points, and mF1 by 2. 78 percentage points. From the segmenting effect diagram of Fig. 8, this network has a lower error rate, fewer omission, and a more accurate segmenting boundary for building edges and small-scale cars than other networks.Conclusions We propose a network model based on encoding-decoding structure-AMSNet. In the encoding part, the D_Resnet50 network is applied as the backbone to extract the main feature information of remote sensing images. We also use a category-guided channel attention module to reduce the interference of channel noise on segmented objects and improve the segmentation effect of targets in difficult-to-distinguish areas. We embed a feature reuse module to compensate for the problem of target edge loss and small-scale target loss during the feature extraction process. In the decoding part, the cross-regional feature fusion module is designed to integrate multi-layer features and combine the multiscale loss fusion module to calculate the feature loss at different scales to improve the segmentation effect of the network on multi-scale targets. This network conducts experiments on the remote sensing image dataset of the plateau region, remote sensing image dataset of the plateau region under cloud interference, and a public dataset. Compared with semantic segmentation networks such as BiseNetv2, PspNet, and Deeplabv3+, the proposed network achieves better results in the evaluation indicators of mIoU, mPa, and mF1. The visualization results show that the proposed network can effectively segment the ground object targets and scattered multi-scale targets in the interlaced and hard-to-distinguish areas in the remote sensing images, and it has good segmentation performance and good robustness in cloud interference.",remote sensing image,semantic segmentation,attention mechanism,multi-scale feature,,Lu Ruitao,Chen Guangchen,,,ACTA OPTICA SINICA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_552,"Jin, Jidong","Lu, Wanxuan","Yu, Hongfeng","Rong, Xuee",Dynamic and Adaptive Self-Training for Semi-Supervised Remote Sensing Image Semantic Segmentation,,2024,0,"Remote sensing (RS) technology has made remarkable progress, providing a wealth of data for various applications, such as ecological conservation and urban planning. However, the meticulous annotation of this data is labor-intensive, leading to a shortage of labeled data, particularly in tasks like semantic segmentation. Semi-supervised methods, combining consistency regularization (CR) with self-training, offer a solution to efficiently utilize labeled and unlabeled data. However, these methods encounter challenges due to imbalanced data ratios. To tackle these challenges, we introduce a self-training approach named dynamic and adaptive self-training (DAST), which is combined with dynamic pseudo-label sampling (DPS), distribution matching (DM), and adaptive threshold updating (ATU). DPS is tailored to address the issue of class distribution imbalance by giving priority to classes with fewer samples. Meanwhile, DM and ATU aim to reduce distribution disparities by adjusting model predictions across augmented images within the framework of CR, ensuring they align with the actual data distribution. Experimental results on the Potsdam and iSAID datasets demonstrate that DAST effectively balances class distribution, aligns model predictions with data distribution, and stabilizes pseudo-labels, leading to state-of-the-art performance on both datasets. These findings highlight the potential of DAST in overcoming the challenges associated with significant disparities in labeled-to-unlabeled data ratios.",Remote sensing,Semantic segmentation,Transformers,Data models,Training,"Sun, Xian","Wu, Yirong",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantics,Predictive models,Consistency regularization (CR),remote sensing (RS) image,self-training,semantic segmentation,semisupervised learning (SSL),,,,,,,,,,,,,,,,,,,,,,,
Row_553,"Li, Weijia","Zhao, Wenqian","Yu, Jinhua","Zheng, Juepeng",Joint semantic-geometric learning for polygonal building segmentation from-resolution remote,,JUL 2023,18,"As a fundamental task for geographical information updating, 3D city modeling, and other critical applications, the automatic extraction of building footprints from high-resolution remote sensing images has been substan-tially explored and received increasing attention over recent years. Among different types of building extraction methods, the polygonal segmentation methods produce vector building polygons that are in a more realistic format compared with those obtained from pixel-wise semantic labeling and contour-based methods. However, existing polygonal building segmentation methods usually require a perfect segmentation map and a complex post-processing procedure to guarantee the polygonization quality, or produce inaccurate vertex prediction results that suffer from wrong vertex sequence, self-intersections, fixed vertex quantity, etc. In our previous work, we have proposed a method for polygonal building segmentation from remote sensing images that addresses the above limitations of existing methods. In this paper, we propose PolyCity, which further extends and improves our previous work in terms of the application scenario, methodology design, and experimental results. Our proposed PolyCity contains the following three components: (1) a pixel-wise multi-task network for learning the semantic and geometric information via three tasks, i.e., building segmentation, boundary prediction, and edge orientation prediction; (2) a simple but effective vertex selection module (VSM), which effectively bridges the gap between pixel-wise and graph-based models via transforming the segmentation map into valid polygon vertices; (3) a graph-based vertex refinement network (VRN) for automatically adjusting the coordinates of VSM-generated valid polygon vertices, producing the final building polygons with more precise vertices. Results on three large-scale building extraction datasets demonstrate that our proposed PolyCity generates vector building footprints with more accurate vertices, edges, shapes, etc., achieving significant vertex score improvements while maintaining high segmentation and boundary scores compared with the current state-of-the-art. The code of PolyCity will be released at https://github.com/liweijia/polycity.",Building extraction,Semantic segmentation,Graph neural networks,High-resolution remote sensing images,,"He, Conghui","Fu, Haohuan","Lin, Dahua",,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_554,"Fu, Xiaomeng","Qu, Huiming",,,Research on Semantic Segmentation of High-resolution Remote Sensing Image Based on Full Convolutional Neural Network,"2018 12TH INTERNATIONAL SYMPOSIUM ON ANTENNAS, PROPAGATION AND ELECTROMAGNETIC THEORY (ISAPE)",2018,9,"Remote sensing data is an important way to reflect the comprehensive information of surface. In this paper, based on the semantic segmentation of high-resolution remote sensing images, a segmentation method based on full convolutional neural network (FCN) is proposed. The method improves the traditional convolutional neural network (CNN) and replaces the final fully connected layer of the CNN network with a convolutional layer. And then optimize the convolution operation by using the matrix expansion technique. The experimental results show that the FCN network with sufficient training and fine-tuning can effectively perform automatic semantic segmentation of high-resolution remote sensing images. The correct segmentation accuracy is higher than 85%, which improves the efficiency of convolution operations.",semantic image segmentation,full convolutional neural network,deep learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_555,"Xin, Yi","Fan, Zide","Qi, Xiyu","Zhang, Yidan",Confidence-Weighted Dual-Teacher Networks With Biased Contrastive Learning for Semi-Supervised Semantic Segmentation in Remote Sensing Images,,2024,3,"Semantic segmentation of remote sensing images is vital in remote sensing technology. High-quality models for this task require a vast amount of images, and manual annotation is a process that is time-consuming and labor-intensive. Consequently, this has catalyzed the emergence of semi-supervised semantic segmentation methods. However, the complexity of foreground categories in remote sensing images poses a challenge to maintaining prediction consistency. Moreover, inherent characteristics such as intraclass variations and interclass similarities result in a certain degree of confusion among features of different classes in the feature space. This impacts the final classification results. To improve the model's consistency and optimize the classification of categories based on features, this article proposes a new semi-supervised semantic segmentation framework that combines consistency regularization and contrastive learning (CL). In terms of consistency regularization, the proposed method incorporates dual-teacher networks, introduces ClassMix for image augmentation, and uses confidence levels to integrate the predictions from these networks. By introducing perturbations at both the network and image levels, while simultaneously maintaining consistency, the predictive prowess and generalization ability of the model are enhanced. For CL, positive-unlabeled learning (PU-Learning) is used to improve the problem of mis-sampling when selecting features. At the same time, higher biased weights are allocated to more challenging negative samples, thereby elevating the complexity of feature learning and enhancing the discriminative capability of the final feature representation space. Our extensive experiments on the ISPRS Vaihingen dataset and the challenging iSAID dataset have served to underscore the superior performance of our approach.",Remote sensing,Self-supervised learning,Perturbation methods,Semantic segmentation,Training,"Li, Xinming",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Visualization,Sensors,Consistency regularization,contrastive learning (CL),remote sensing,semantic segmentation,semi-supervised learning,,,,,,,,,,,,,,,,,,,,,,,
Row_556,"Zhu, Jingru","Guo, Ya","Sun, Geng","Yang, Libo",Unsupervised Domain Adaptation Semantic Segmentation of High-Resolution Remote Sensing Imagery With Invariant Domain-Level Prototype Memory,,2023,31,"Semantic segmentation is a key technique involved in automatic interpretation of high-resolution remote sensing (HRS) imagery and has drawn much attention in the remote sensing community. Deep convolutional neural networks (DCNNs) have been successfully applied to the HRS imagery semantic segmentation task due to their hierarchical representation ability. However, the heavy dependence on a large number of training data with dense annotation and the sensitiveness to the variation of data distribution severely restrict the potential application of DCNNs for the semantic segmentation of HRS imagery. This study proposes a novel unsupervised domain adaptation semantic segmentation network (MemoryAdaptNet) for the semantic segmentation of HRS imagery. MemoryAdaptNet constructs an output space adversarial learning scheme to bridge the domain distribution discrepancy between the source domain and the target domain and to narrow the influence of domain shift. Specifically, we embed an invariant feature memory module to store invariant domain-level prototype information because the features obtained from adversarial learning only tend to represent the variant feature of current limited inputs. This module is integrated by a category attention-driven invariant domain-level memory aggregation module to current pseudo-invariant feature for further augmenting the representations. An entropy-based pseudo label filtering strategy is used to update the memory module with high-confident pseudo-invariant feature of current target images. Extensive experiments under three cross-domain tasks indicate that our proposed MemoryAdaptNet is remarkably superior to the state-of-the-art methods. Our code is available at https://github.com/RS-CSU/MemoryAdaptNet-master.",Semantic segmentation,Semantics,Remote sensing,Adversarial machine learning,Prototypes,"Deng, Min","Chen, Jie",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Memory modules,Adaptation models,Category attention,high-resolution remote sensing (HRS) imagery,invariant domain-level context,memory module,pseudo label filtering strategy,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,
Row_557,"Zhang, Mei","Liu, Lingling","Pei, Yongtao","Xie, Guojing",Semantic segmentation of multi-scale remote sensing images with contextual feature enhancement,,AUG 2024,0,"Remote sensing images exhibit complex characteristics such as irregular multi-scale feature shapes, significant scale variations, and imbalanced sizes between different categories. These characteristics lead to a decrease in the accuracy of semantic segmentation in remote sensing images. In view of this problem, this paper presents a context feature-enhanced multi-scale remote sensing image semantic segmentation method. It utilizes a context aggregation module for global context co-aggregation, obtaining feature representations at different levels through self-similarity calculation and convolution operations. The processed features are input into a feature enhancement module, introducing a channel gate mechanism to enhance the expressive power of feature maps. This mechanism enhances feature representations by leveraging channel correlations and weighted fusion operations. Additionally, pyramid pooling is employed to capture multi-scale information from the enhanced features, so as to improve the performance and accuracy of the semantic segmentation model. Experimental results on the Vaihingen and Potsdam datasets (which are indeed publicly released at the URL: https://www.isprs.org/education/benchmarks/UrbanSemLab/Default.aspx) demonstrate significant improvements in the performance and accuracy of the proposed method (whose algorithm source code is indeed publicly released in Sect. 3.4), compared to previous multi-scale remote sensing image semantic segmentation approaches, verifying its effectiveness.",Context aggregation,Self-similarity calculation,Feature enhancement,Channel gate mechanism,Multi-scale remote sensing image,"Wen, Jinghua",,,,VISUAL COMPUTER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_558,"Jiang, Liangcun","Li, Feng","Huang, Li","Peng, Feifei",TTNet: A Temporal-Transform Network for Semantic Change Detection Based on Bi-Temporal Remote Sensing Images,,SEP 2023,4,"Semantic change detection (SCD) holds a critical place in remote sensing image interpretation, as it aims to locate changing regions and identify their associated land cover classes. Presently, post-classification techniques stand as the predominant strategy for SCD due to their simplicity and efficacy. However, these methods often overlook the intricate relationships between alterations in land cover. In this paper, we argue that comprehending the interplay of changes within land cover maps holds the key to enhancing SCD's performance. With this insight, a Temporal-Transform Module (TTM) is designed to capture change relationships across temporal dimensions. TTM selectively aggregates features across all temporal images, enhancing the unique features of each temporal image at distinct pixels. Moreover, we build a Temporal-Transform Network (TTNet) for SCD, comprising two semantic segmentation branches and a binary change detection branch. TTM is embedded into the decoder of each semantic segmentation branch, thus enabling TTNet to obtain better land cover classification results. Experimental results on the SECOND dataset show that TTNet achieves enhanced performance when compared to other benchmark methods in the SCD task. In particular, TTNet elevates mIoU accuracy by a minimum of 1.5% in the SCD task and 3.1% in the semantic segmentation task.",semantic change detection,change relationship,siamese convolutional neural network,deep learning,,"Hu, Lei",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_559,"He, Qibin","Sun, Xian","Diao, Wenhui","Yan, Zhiyuan",Multimodal Remote Sensing Image Segmentation With Intuition-Inspired Hypergraph Modeling,,2023,34,"Multimodal remote sensing (RS) image segmentation aims to comprehensively utilize multiple RS modalities to assign pixel-level semantics to the studied scenes, which can provide a new perspective for global city understanding. Multimodal segmentation inevitably encounters the challenge of modeling intra-and inter-modal relationships, i.e., object diversity and modal gaps. However, the previous methods are usually designed for a single RS modality, limited by the noisy collection environment and poor discrimination information. Neuropsychology and neuroanatomy confirm that the human brain performs the guiding perception and integrative cognition of multimodal semantics through intuitive reasoning. Therefore, establishing a semantic understanding framework inspired by intuition to realize multimodal RS segmentation becomes the main motivation of this work. Drived by the superiority of hypergraphs in modeling high-order relationships, we propose an intuition-inspired hypergraph network ((IH)-H-2 N) for multimodal RS segmentation. Specifically, we present a hypergraph parser to imitate guiding perception to learn intra-modal object-wise relationships. It parses the input modality into irregular hyper graphs to mine semantic clues and generate robust mono modal representations. In addition, we also design a hypergraph matcher to dynamically update the hypergraph structure from the explicit correspondence of visual concepts, similar to integrative cognition, to improve cross-modal compatibility when fusing multimodal features. Extensive experiments on two multimodal RS datasets show that the proposed I2H N outperforms the stateof-the-art models, achieving F-1/mIoU accuracy 91.4%/82.9% on the ISPRS Vaihingen dataset, and 92.1%/84.2% on the MSAW dataset.",Cognition,Semantics,Image segmentation,Remote sensing,Optical interferometry,"Yao, Fanglong","Fu, Kun",,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,Vegetation,Optical sensors,Multimodal remote sensing,intuitive reasoning,hypergraph learning,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_560,"Wu, Shulei","Zhao, Yuchen","Wang, Yaoru","Chen, Jinbiao",Convolution Feature Inference-Based Semantic Understanding Method for Remote Sensing Images of Mangrove Forests,,FEB 2023,2,"The semantic segmentation and understanding of remote sensing images applying computer technology has become an important component of monitoring mangrove forests' ecological changes due to the rapid advancement of remote sensing technology. To improve the semantic segmentation capability of various surface features, this paper proposes a semantic understanding method for mangrove remote sensing images based on convolution feature inference. Firstly, the sample data is randomly selected, and next a model of convolution feature extraction is used to obtain the features of the selected sample data and build an initial feature set. Then, the convolution feature space and rule base are generated by establishing the three-dimensional color space distribution map for each class and domain similarity is introduced to construct the feature set and rules for reasoning. Next, a confidence reasoning method based on the convolution feature region growth, which introduces an improved similarity calculation, is put forward to obtain the first-time reasoning results. Finally, this approach adds a correction module, which removes the boundary information and reduces the noise from the results of the first-time reasoning as a new sample to correct the original feature set and rules, and uses the corrected feature set and rules for reasoning and understanding to obtain the final image segmentation results. It uses the corrected feature set and rules for reasoning and understanding to obtain the final image segmentation results. Experiments show that this algorithm has the benefits of a simple process, a short training time, and easy feature acquisition. The effect has been obviously improved compared to a single threshold segmentation method, K-Nearest Neighbor (KNN), Support Vector Machine (SVM), and other image segmentation methods.",convolution features,semantic understanding,remote sensing image segmentation,semantic inference,feature rule base,"Zang, Tao","Chen, Huandong",,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_561,"Du, Shouji","Du, Shihong","Liu, Bo","Zhang, Xiuyuan",Incorporating DeepLabv3+and object-based image analysis for semantic segmentation of very high resolution remote sensing images,,MAR 4 2021,79,"Semantic segmentation of remote sensing images is an important but unsolved problem in the remote sensing society. Advanced image semantic segmentation models, such as DeepLabv3+, have achieved astonishing performance for semantically labeling very high resolution (VHR) remote sensing images. However, it is difficult for these models to capture the precise outlines of ground objects and explore the context information that revealing relationships among image objects for optimizing segmentation results. Consequently, this study proposes a semantic segmentation method for VHR images by incorporating deep learning semantic segmentation model (DeepLabv3+) and object-based image analysis (OBIA), wherein DSM is employed to provide geometric information to enhance the interpretation of VHR images. The proposed method first obtains two initial probabilistic labeling predictions using a DeepLabv3+ network on spectral image and a random forest (RF) classifier on hand-crafted features, respectively. These two predictions are then integrated by Dempster-Shafer (D-S) evidence theory to be fed into an object-constrained higher-order conditional random field (CRF) framework to estimate the final semantic labeling results with the consideration of the spatial contextual information. The proposed method is applied to the ISPRS 2D semantic labeling benchmark, and competitive overall accuracies of 90.6% and 85.0% are achieved for Vaihingen and Potsdam datasets, respectively.",Semantic segmentation,DeepLabv3+,object-based image analysis,Dempster-Shafer evidence theory,conditional random field,,,,,INTERNATIONAL JOURNAL OF DIGITAL EARTH,,VHR images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_562,"Liu, Yansong","Piramanayagam, Sankaranarayanan","Monteiro, Sildomar T.","Saber, Eli",SEMANTIC SEGMENTATION OF REMOTE SENSING DATA USING GAUSSIAN PROCESSES AND HIGHER-ORDER CRFS,2017 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),2017,2,"Automatic recognition for complex scenes from aerial images and other sensor data (e.g. LiDAR) has become an active topic in the remote sensing community. In this paper, we proposed a novel framework that utilizes higher-order CRFs (HCRFs) to capture the spatial contextual information for the RGB aerial images along with their co-registered LiDAR data (DSMs). Our proposed HCRFs framework exploits the spatial contextual information in two levels. The first level encourages harmonic label co-existence within one segment, which can be generated by an unsupervised superpixel algorithm. The second level takes into account the local object co-occurrence among adjacent segments. We then show that how to apply the move making graph cuts algorithm to perform efficient inference for our proposed CRFs framework. Based on the experiments on a challenging data set, our proposed higher-order CRF framework generated state-of-the-art semantic segmentation results.",Semantic segmentation,higher-order CRFs,aerial images,graph cuts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_563,"Marsocci, Valerio","Scardapane, Simone",,,Continual Barlow Twins: Continual Self-Supervised Learning for Remote Sensing Semantic Segmentation,,2023,10,"In the field of earth observation (EO), continual learning (CL) algorithms have been proposed to deal with large datasets by decomposing them into several subsets and processing them incrementally. The majority of these algorithms assume that data are, first, coming from a single source, and second, fully labeled. Real-world EO datasets are instead characterized by a large heterogeneity (e.g., coming from aerial, satellite, or drone scenarios), and for the most part they are unlabeled, meaning they can be fully exploited only through the emerging self-supervised learning (SSL) paradigm. For these reasons, in this article, we present a new algorithm for merging SSL and CL for remote sensing applications that we call continual Barlow twins. It combines the advantages of one of the simplest self-supervision techniques, i.e., Barlow twins, with the elastic weight consolidation method to avoid catastrophic forgetting. In addition, we evaluate the proposed continual SSL approach on a highly heterogeneous EO dataset, showing the effectiveness of this strategy on a novel combination of three almost non-overlapping domains datasets (airborne Potsdam, satellite US3D, and drone unmanned aerial vehicle semantic segmentation dataset), on a crucial downstream task in EO, i.e., semantic segmentation. Encouraging results show the superiority of SSL in this setting, and the effectiveness of creating an incremental effective pretrained feature extractor, based on ResNet50, without the need of relying on the complete availability of all the data, with a valuable saving of time and resources.",Task analysis,Semantic segmentation,Satellites,Feature extraction,Remote sensing,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Electo-optic effects,Drones,Continual learning (CL),remote sensing,self-supervised learning (SSL),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_564,"Datla, Eshreddy","Vishnu, Chalavadi","Mohan, C. Krishna",,A Multimodal Semantic Segmentation for Airport Runway Delineation in Panchromatic Remote Sensing Images,,2022,3,"Monitoring airport runways in panchromatic remote sensing images is helpful for both civil and strategic communities in effective utilization of the large-area acquisitions. This paper proposes a novel multimodal semantic segmentation approach for effective delineation of the runways in panchromatic remote sensing images. The proposed approach aims to learn complementary information from two modalities, namely, panchromatic image and digital elevation model (DEM) to obtain discriminative features of the runway. The fusion of image features and the corresponding terrain information is performed by stacking the image and DEM by leveraging the merits of both Transformers and U-Net architecture. We perform the experiments on Cartosat-1 panchromatic satellite images with the corresponding Cartosat-1 DEM scenes. The experimental results demonstrate a significant contribution of terrain information to the segmentation process in achieving the contours of airport runways effectively.",Airport runway,remote sensing images,multimodal segmentation,digital elevation model,Cartosat-1,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_565,"Ding, Lei","Lin, Dong","Lin, Shaofu","Zhang, Jing",Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images,,2022,82,"Long-range contextual information is crucial for the semantic segmentation of high-resolution (HR) remote sensing images (RSIs). However, image cropping operations, commonly used for training neural networks, limit the perception of long-range contexts in large RSIs. To overcome this limitation, we propose a wide-context network (WiCoNet) for the semantic segmentation of HR RSIs. Apart from extracting local features with a conventional convolutional neural network (CNN), the WiCoNet has an extra context branch to aggregate information from a larger image area. Moreover, we introduce a context transformer to embed contextual information from the context branch and selectively project it onto the local features. The context transformer extends the vision transformer, an emerging kind of neural networks, to model the dual-branch semantic correlations. It overcomes the locality limitation of CNNs and enables the WiCoNet to see the bigger picture before segmenting the land-cover/land-use (LCLU) classes. Ablation studies and comparative experiments conducted on several benchmark datasets demonstrate the effectiveness of the proposed method. In addition, we present a new Beijing Land-Use (BLU) dataset. This is a large-scale HR satellite dataset with high-quality and fine-grained reference labels, which can facilitate future studies in this field.",Transformers,Semantics,Image segmentation,Feature extraction,Task analysis,"Cui, Xiaojie","Wang, Yuebin","Tang, Hao","Bruzzone, Lorenzo",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Convolutional neural networks,Context modeling,Convolutional neural network,remote sensing,semantic segmentation,vision transformer (ViT),,,,,,,,,,,,,,,,,,,,,,,,
Row_566,"Wang, Xiaofeng","Kang, Menglei","Chen, Yan","Jiang, Wenxiang",Adaptive Local Cross-Channel Vector Pooling Attention Module for Semantic Segmentation of Remote Sensing Imagery,,APR 2023,6,"Adding an attention module to the deep convolution semantic segmentation network has significantly enhanced the network performance. However, the existing channel attention module focusing on the channel dimension neglects the spatial relationship, causing location noise to transmit to the decoder. In addition, the spatial attention module exemplified by self-attention has a high training cost and challenges in execution efficiency, making it unsuitable to handle large-scale remote sensing data. We propose an efficient vector pooling attention (VPA) module for building the channel and spatial location relationship. The module can locate spatial information better by performing a unique vector average pooling in the vertical and horizontal dimensions of the feature maps. Furthermore, it can also learn the weights directly by using the adaptive local cross-channel interaction. Multiple weight learning ablation studies and comparison experiments with the classical attention modules were conducted by connecting the VPA module to a modified DeepLabV3 network using ResNet50 as the encoder. The results show that the mIoU of our network with the addition of an adaptive local cross-channel interaction VPA module increases by 3% compared to the standard network on the MO-CSSSD. The VPA-based semantic segmentation network can significantly improve precision efficiency compared with other conventional attention networks. Furthermore, the results on the WHU Building dataset present an improvement in IoU and F1-score by 1.69% and 0.97%, respectively. Our network raises the mIoU by 1.24% on the ISPRS Vaihingen dataset. The VPA module can also significantly improve the network's performance on small target segmentation.",adaptive local cross-channel interaction,vector average pooling,attention mechanism,remote sensing imagery,semantic segmentation,"Wang, Mengyuan","Weise, Thomas","Tan, Ming","Xu, Lixiang",REMOTE SENSING,"Li, Xinlu",deep learning,,,,,,,,,"Zou, Le","Zhang, Chen",,,,,,,,,,,,,,,,,,,
Row_567,"Lu, Hui","Liu, Qi","Liu, Xiaodong","Zhang, Yonghong",A Survey of Semantic Construction and Application of Satellite Remote Sensing Images and Data,,NOV-DEC 2021,25,"With the rapid development of satellite technology, remote sensing data has entered the era of big data, and the intelligent processing of remote sensing images has been paid more attention. Through the semantic research of remote sensing data, the processing ability of remote sensing data is greatly improved. This paper aims to introduce and analyze the research and application progress of remote sensing image satellite data processing from the perspective of semantics. Firstly, it introduces the characteristics and semantic knowledge of remote sensing big data. Secondly, the semantic concept, semantic construction, and application fields are introduced in detail. Then, for remote sensing big data, the technical progress in the study field of semantic construction is analyzed from four aspects-semantic description and understanding, semantic segmentation, semantic classification, and semantic search-focusing on deep learning technology. Finally, the problems and challenges in the four aspects are discussed in detail in order to find more directions to explore.",Automatic Analysis,Deep Learning,Remote Sensing,Satellite Remote Sensing,Semantic Construction,,,,,JOURNAL OF ORGANIZATIONAL AND END USER COMPUTING,,Semantic Knowledge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_568,"Zhu, Jingru","Guo, Ya","Sun, Geng","Hong, Liang",Causal Prototype-Inspired Contrast Adaptation for Unsupervised Domain Adaptive Semantic Segmentation of High-Resolution Remote Sensing Imagery,,2024,0,"Semantic segmentation of high-resolution remote sensing imagery (HRSI) suffers from the domain shift, resulting in poor performance of the model in another unseen domain. Unsupervised domain adaptive (UDA) semantic segmentation aims to adapt the semantic segmentation model trained on the labeled source domain to an unlabeled target domain. However, the existing UDA semantic segmentation models tend to align pixels or features based on statistical information related to labels in source and target domain data and make predictions accordingly, which leads to uncertainty and fragility of prediction results. In this article, we propose a causal prototype-inspired contrast adaptation (CPCA) method to explore the invariant causal mechanisms between different HRSIs domains and their semantic labels. It first disentangles causal features and bias features from the source and target domain images through a causal feature disentanglement (CFD) module. Then, a causal prototypical contrast (CPC) module is used to learn domain invariant causal features. To further de-correlate causal and bias features, a causal intervention (CI) module is introduced to intervene on the bias features to generate counterfactual unbiased samples. By forcing the causal features to meet the principles of separability, invariance, and intervention, CPCA can simulate the causal factors of source and target domains, and make decisions on the target domain based on the causal features, which can observe improved generalization ability. Extensive experiments under three cross-domain tasks indicate that CPCA is remarkably superior to the state-of-the-art methods.",Causal view,contrastive learning,counterfactuals,disentangled representation,high-resolution remote sensing imagery (HRSI),"Chen, Jie",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,semantic segmentation,unsupervised domain adaptation,Causal view,contrastive learning,counterfactuals,disentangled representation,high-resolution remote sensing imagery (HRSI),semantic segmentation,unsupervised domain adaptation,,,,,,,,,,,,,,,,,,,,,
Row_569,Sun Guowen,Luo Xiaobo,Zhang Kunqiang,,DeepLabV3_DHC: Semantic Segmentation of Urban Unmanned Aerial Vehicle Remote Sensing Image,,FEB 2024,0,"High-resolution unmanned aerial vehicle remote sensing images have extremely rich semantic and ground feature features, which are prone to problems such as incomplete target segmentation, missing edge information, and insufficient segmentation accuracy in semantic segmentation. To solve the above problems, based on DeepLabV3_plus model, an improved DeepLabV3_DHC is proposed. First of all, multiple backbone networks are used for down-sampling to collect low-level and high-level features of the image. Second, the atrous spatial pyramid pooling (ASPP) of the original model is replaced by a depthwise separable hybrid dilated convolution, and an adaptive coefficient is added to weaken the mesh effect. After that, the traditional sampling bilinear interpolation method is abandoned and replaced by the learnable dense upsampling convolution. Finally, cascade attention mechanism in low-level features. In this paper, a variety of backbone networks are selected for the experiment, and some images of Longchang City, Sichuan Province are selected for the dataset. The evaluation index uses the average intersection and combination ratio and the average pixel accuracy of the category as the reference basis. The experimental results show that the method in this paper not only has higher segmentation accuracy, but also reduces the amount of computation and parameters.",urban unmanned aerial vehicle remote sensing image,semantic segmentation,depthwise separable hybrid dilated convolution,dense upsampling convolution,attention mechanism,,,,,LASER & OPTOELECTRONICS PROGRESS,,grid effect,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_570,"Chen, Yuxia","Fang, Pengcheng","Zhong, Xiaoling","Yu, Jianhui",Hi-ResNet: Edge Detail Enhancement for High-Resolution Remote Sensing Segmentation,,2024,0,"High-resolution remote sensing (HRS) semantic segmentation extracts key objects from high-resolution coverage areas. However, objects of the same category within HRS images generally show significant differences in scale and shape across diverse geographical environments, making it difficult to fit the data distribution. In addition, a complex background environment causes similar appearances of objects of different categories, which precipitates a substantial number of objects into misclassification as background. These issues make existing learning algorithms suboptimal. In this work, we solve the abovementioned problems by proposing a high-resolution remote sensing network (Hi-ResNet) with efficient network structure designs, which consists of a funnel module, a multibranch module with stacks of information aggregation (IA) blocks, and a feature refinement module, sequentially, and class-agnostic edge-aware (CEA) loss. Specifically, we propose a funnel module to downsample, which reduces the computational cost and extracts high-resolution semantic information from the initial input image. Then, we downsample the processed feature images into multiresolution branches incrementally to capture image features at different scales. Furthermore, with the design of the window multihead self-attention, squeeze-and-excitation attention, and depthwise convolution, the light-efficient IA blocks are utilized to distinguish image features of the same class with variant scales and shapes. Finally, our feature refinement module integrates the CEA loss function, which disambiguates interclass objects with similar shapes and increases the data distribution distance for correct predictions. With effective pretraining strategies, we demonstrate the superiority of Hi-ResNet over the existing prevalent methods on three HRS segmentation benchmarks.",Feature extraction,Semantics,Convolution,Transformers,Shape,"Zhang, Xiaoming","Li, Tianrui",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Remote sensing,Task analysis,Attention,pretraining,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_571,"Wang, Zhaoqiu","Sun, Tao","Hu, Kun","Zhang, Yueting",A Deep Learning Semantic Segmentation Method for Landslide Scene Based on Transformer Architecture,,DEC 2022,9,"Semantic segmentation technology based on deep learning has developed rapidly. It is widely used in remote sensing image recognition, but is rarely used in natural disaster scenes, especially in landslide disasters. After a landslide disaster occurs, it is necessary to quickly carry out rescue and ecological restoration work, using satellite data or aerial photography data to quickly analyze the landslide area. However, the precise location and area estimation of the landslide area is still a difficult problem. Therefore, we propose a deep learning semantic segmentation method based on Encoder-Decoder architecture for landslide recognition, called the Separable Channel Attention Network (SCANet). The SCANet consists of a Poolformer encoder and a Separable Channel Attention Feature Pyramid Network (SCA-FPN) decoder. Firstly, the Poolformer can extract global semantic information at different levels with the help of transformer architecture, and it greatly reduces computational complexity of the network by using pooling operations instead of a self-attention mechanism. Secondly, the SCA-FPN we designed can fuse multi-scale semantic information and complete pixel-level prediction of remote sensing images. Without bells and whistles, our proposed SCANet outperformed the mainstream semantic segmentation networks with fewer model parameters on our self-built landslide dataset. The mIoU scores of SCANet are 1.95% higher than ResNet50-Unet, especially.",landslide,remote sensing images,semantic segmentation,Poolformer,separable channel attention,"Yu, Xiaqiong","Li, Ying",,,SUSTAINABILITY,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_572,"Li, Rui","Duan, Chenxi","Zheng, Shunyi","Zhang, Ce",MACU-Net for Semantic Segmentation of Fine-Resolution Remotely Sensed Images,,2022,71,"Semantic segmentation of remotely sensed images plays an important role in land resource management, yield estimation, and economic assessment. U-Net, a deep encoder-decoder architecture, has been used frequently for image segmentation with high accuracy. In this letter, we incorporate multiscale features generated by different layers of U-Net and design a multiscale skip connected and asymmetric-convolution-based U-Net (MACU-Net), for segmentation using fine-resolution remotely sensed images. Our design has the following advantages: (1) the multiscale skip connections combine and realign semantic features contained in both low-level and high-level feature maps; (2) the asymmetric convolution block strengthens the feature representation and feature extraction capability of a standard convolution layer. Experiments conducted on two remotely sensed data sets captured by different satellite sensors demonstrate that the proposed MACU-Net transcends the U-Net, U-Netpyramid pooling layers (PPL), U-Net 3+, among other benchmark approaches. Code is available at https://github.com/lironui/MACU-Net.",Convolution,Semantics,Remote sensing,Image segmentation,Kernel,"Atkinson, Peter M.",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Decoding,Asymmetric convolution block (ACB),fine-resolution remotely sensed images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_573,"Li, Xinghua","Xie, Linglin","Wang, Caifeng","Miao, Jianhao",Boundary-enhanced dual-stream network for semantic segmentation of high-resolution remote sensing images,,DEC 31 2024,16,"Deep convolutional neural networks (DCNNs) have been successfully used in semantic segmentation of high-resolution remote sensing images (HRSIs). However, this task still suffers from intra-class inconsistency and boundary blur due to high intra-class heterogeneity and inter-class homogeneity, considerable scale variance, and spatial information loss in conventional DCNN-based methods. Therefore, a novel boundary-enhanced dual-stream network (BEDSN) is proposed, in which an edge detection branch stream (EDBS) with a composite loss function is introduced to compensate for boundary loss in semantic segmentation branch stream (SSBS). EDBS and SSBS are integrated by highly coupled encoder and feature extractor. A lightweight multilevel information fusion module guided by channel attention mechanism is designed to reuse intermediate boundary information effectively. For aggregating multiscale contextual information, SSBS is enhanced by multiscale feature extraction module and hybrid atrous convolution module. Extensive experiments have been tested on ISPRS Vaihingen and Potsdam datasets. Results show that BEDSN can achieve significant improvements in intra-class consistency and boundary refinement. Compared with 11 state-of-the-art methods, BEDSN exhibits higher-level performance in both quantitative and visual assessments with low model complexity.",Semantic segmentation,boundary blur,intra-class inconsistency,HRSIs,CNN,"Shen, Huanfeng","Zhang, Liangpei",,,GISCIENCE & REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_574,"Hang, Renlong","Yang, Ping","Zhou, Feng","Liu, Qingshan",Multiscale Progressive Segmentation Network for High-Resolution Remote Sensing Imagery,,2022,58,"Semantic segmentation of high-resolution remote sensing imageries (HRSIs) is a critical task for a wide range of applications, such as precision agriculture and urban planning. Although convolutional neural networks (CNNs) have made great progress in accomplishing this task recently, there still exist some challenges to address, one of which is simultaneously segmenting objects with large-scale variations in a HRSI. Targeting at this challenge, previous CNNs often adopt multiple convolution kernels in one layer or skip-layer connections between different layers to extract multiscale representations. However, due to the limited learning capacity of each CNN, it tends to make tradeoffs in segmenting different-scale objects. This would lead to unsatisfactory segmentation results for some objects, especially the small or the large ones. In this article, we propose a multiscale progressive segmentation network to address this issue. Instead of forcing one network to deal with all scales of objects, our network attempts to cascade three subnetworks for gradually segmenting objects into small scale, large scale, and other scale. In order to make the subnetwork focus on the specific scale objects, a scale guidance module is designed. It takes advantage of segmentation results from the preceding subnetwork to guide the feature learning of the succeeding one. Additionally, to acquire the final segmentation results, we propose a position-sensitive module for adaptively combining the outputs of the three subnetworks. This module is capable of assigning combination weights of different subnetworks according to their importance. Experiments on two benchmark datasets named Vaihingen and Potsdam indicate that our proposed network can achieve considerable improvements in comparison with several state-of-the-art segmentation models.",Feature extraction,Image segmentation,Convolution,Kernel,Decoding,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Remote sensing,Convolutional neural networks,Convolutional neural networks,high-resolution remote sensing imagery,multiscale objects,position-sensitive modules,scale guidance modules,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,
Row_575,"Jia, Yuyu","Gao, Junyu","Huang, Wei","Yuan, Yuan",Holistic Mutual Representation Enhancement for Few-Shot Remote Sensing Segmentation,,2023,7,"Few-shot segmentation (FSS) endeavors to utilize a minimal amount of annotated samples (support) to guide the segmentation of unseen objects (query). Previous techniques primarily employ a support-to-query paradigm, neglecting to sufficiently leverage the mutual representation between query and support images, which leaves models suffering from intra-class variations and background interference in remote sensing images. This article proposes a holistic mutual representation enhancement (HMRE) method to bridge these gaps. First, a dual activation (DA) module is devised to establish information symmetry between the two branches and forms the foundation for mutual representation enhancement. Subsequently, the holistic mutual enhancement is jointly constructed by the global semantic (GS) and spatial dense (SD) mutual enhancement modules. In the prediction stage for segmentation, we integrate the enhanced mutual representation into the mutual-fusion decoder to activate the homologous object regions bidirectionally. To expedite the replication of investigation in this task, we further create a corresponding benchmark Flood-3i. The whole dataset is attainable at https://drive.google.com/drive/folders/ 1FMAKf2sszoFKjq0UrUmSLnJDbwQSpfxR. Extensive experiments on two benchmarks iSAID-5i and Flood-3i demonstrate the superiority of our proposed method, which also sets a new state-of-the-art.",Few-shot semantic segmentation,mutual representation enhancement,remote sensing images,,,"Wang, Qi",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_576,"Marsocci, Valerio","Scardapane, Simone","Komodakis, Nikos",,MARE: Self-Supervised Multi-Attention REsu-Net for Semantic Segmentation in Remote Sensing,,AUG 2021,16,"Scene understanding of satellite and aerial images is a pivotal task in various remote sensing (RS) practices, such as land cover and urban development monitoring. In recent years, neural networks have become a de-facto standard in many of these applications. However, semantic segmentation still remains a challenging task. With respect to other computer vision (CV) areas, in RS large labeled datasets are not very often available, due to their large cost and to the required manpower. On the other hand, self-supervised learning (SSL) is earning more and more interest in CV, reaching state-of-the-art in several tasks. In spite of this, most SSL models, pretrained on huge datasets like ImageNet, do not perform particularly well on RS data. For this reason, we propose a combination of a SSL algorithm (particularly, Online Bag of Words) and a semantic segmentation algorithm, shaped for aerial images (namely, Multistage Attention ResU-Net), to show new encouraging results (i.e., 81.76% mIoU with ResNet-18 backbone) on the ISPRS Vaihingen dataset.",semantic segmentation,self-supervised learning,linear attention,Vaihingen dataset,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_577,"Ulku, Irem",,,,ContexNestedU-Net: Efficient Context-Aware Semantic Segmentation Architecture for Precision Agriculture Applications Based on Multispectral Remote Sensing Imagery,,OCT 2024,0,"Precision agriculture relies on semantic segmentation models to optimize crop yield and minimize environmental impact. ContexNestedU-Net is proposed to improve the capture of contextual information for efficient utilization of multispectral remote sensing images in precision agriculture applications. For this purpose, it includes a novel redesign of the convolutional blocks in the Nested U-Net model. Through the application of depthwise separable convolution in the convolution blocks, the ContexNestedU-Net efficiently preserves unique spectral information. ASubsequently, dilated convolution is applied to capture rich contextual information. Three image sets are utilized in the experiments, one from the WorldView-3 satellite and the others from aerial vehicles. Extensive experiments demonstrate that the ContexNestedU-Net outperforms other U-Net-based models for various precision agriculture tasks. When using NDVI images, the proposed architecture improves the Jaccard index by 13% for tree objects, 0.9% for crop objects, and 4.5% for wheat yellow-rust objects compared to Nested U-Net. In addition, the ContexNestedU-Net model reduces the number of trainable parameters from 36.63 to 19 compared to Nested UNet, and the computational complexity (GLOPs) decreases from 849.3 to 302.4.",remote sensing,semantic segmentation,precision agriculture,,,,,,,TRAITEMENT DU SIGNAL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_578,"Wang, Hong","Chen, Xianzhong","Zhang, Tianxiang","Xu, Zhiyong",CCTNet: Coupled CNN and Transformer Network for Crop Segmentation of Remote Sensing Images,,MAY 2022,64,"Semantic segmentation by using remote sensing images is an efficient method for agricultural crop classification. Recent solutions in crop segmentation are mainly deep-learning-based methods, including two mainstream architectures: Convolutional Neural Networks (CNNs) and Transformer. However, these two architectures are not sufficiently good for the crop segmentation task due to the following three reasons. First, the ultra-high-resolution images need to be cut into small patches before processing, which leads to the incomplete structure of different categories' edges. Second, because of the deficiency of global information, categories inside the crop field may be wrongly classified. Third, to restore complete images, the patches need to be spliced together, causing the edge artifacts and small misclassified objects and holes. Therefore, we proposed a novel architecture named the Coupled CNN and Transformer Network (CCTNet), which combines the local details (e.g., edge and texture) by the CNN and global context by Transformer to cope with the aforementioned problems. In particular, two modules, namely the Light Adaptive Fusion Module (LAFM) and the Coupled Attention Fusion Module (CAFM), are also designed to efficiently fuse these advantages. Meanwhile, three effective methods named Overlapping Sliding Window (OSW), Testing Time Augmentation (TTA), and Post-Processing (PP) are proposed to remove small objects and holes embedded in the inference stage and restore complete images. The experimental results evaluated on the Barley Remote Sensing Dataset present that the CCTNet outperformed the single CNN or Transformer methods, achieving 72.97% mean Intersection over Union (mIoU) scores. As a consequence, it is believed that the proposed CCTNet can be a competitive method for crop segmentation by remote sensing images.",semantic segmentation,agricultural research,remote sensing,deep learning,CNN,"Li, Jiangyun",,,,REMOTE SENSING,,Transformer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_579,"Xi, Zhihao","He, Xiangyu","Meng, Yu","Yue, Anzhi",A Multilevel-Guided Curriculum Domain Adaptation Approach to Semantic Segmentation for High-Resolution Remote Sensing Images,,2023,9,"The semantic segmentation of high-resolution (HR) remote sensing images (RSIs) has been extensively researched in various applications. However, segmentation networks are prone to significant performance degradation on unlabeled data due to domain shift, such as data distribution shifts arising from distinct geographic locations. To address this issue, we propose a multilevel-guided curriculum domain adaptation (MuGCDA) approach for joint samplewise, categorywise, and pixelwise tasks, which facilitates the final fine-grained segmentation task by guiding the target domain to acquire samplewise and categorywise domain-robust properties. Concretely, at the sample level, we formulate a sample spatial relationship consistency guidance (SSCG) loss that guides the target domain to acquire similar sample spatial relationship properties to the source domain. At the category level, we propose a category layout structure consistency guidance (CLCG) module that guides the target domain to acquire consistent layout properties. At the pixel level, we design an adaptive hierarchical pseudolabel weight setting (AHPWS) method with a self-training (ST) paradigm to reduce the effect of label noise while improving the quality of the generated pseudolabels. Furthermore, to improve the stability of the training process, we use a momentum network (MN) as the teacher network to obtain the property knowledge and pseudolabels, and then guide the whole domain transfer process of the segmentation network, which acts as the student network. Extensive comparison and ablation experiments are conducted in several cross-space and cross-spectral scenes, and the results show that our method achieves significant performance improvements in cross-domain scenes for HR RSIs.",Domain adaptation (DA),high-resolution (HR) remote sensing,multilevel-guided curriculum learning,semantic segmentation,,"Chen, Jingbo","Deng, Yupeng","Chen, Jiansheng",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_580,"Liu, Tiancheng","Hu, Qingwu","Fan, Wenlei","Feng, Haixia",AMIANet: Asymmetric Multimodal Interactive Augmentation Network for Semantic Segmentation of Remote Sensing Imagery,,2024,0,"In recent years, the inherent 2-D characteristics of optical images have led to a plateau in semantic segmentation performance. The complementary nature of light detection and ranging (LiDAR) point clouds and camera images can effectively enhance semantic segmentation capabilities, and thus, research into multimodal joint semantic segmentation is garnering increasing attention. However, the domain gaps between different dimensions present challenges for the fusion of multimodal data. In this article, we introduce a novel asymmetric multimodal interaction augmented network (AMIANet), which directly processes heterogeneous data from images and point clouds. The treatment of the disparities in modal data ensures consistency in the features of both modes. Through the newly developed synergistic multimodal interaction module (SMI Module), AMIANet is capable of combining the complementary characteristics of cross-modal data. This is achieved by interactively fusing and extracting precise and rich structural information from point cloud features to enhance image characteristics. The experimental results on the N3C-California, WHU-RRDSD, and ISPRS Vaihingen datasets demonstrate that AMIANet surpasses benchmark methods and current state-of-the-art (SOTA) approaches. The code will be available at https://github.com/2012153946/AMIANet.",Point cloud compression,Feature extraction,Semantic segmentation,Semantics,Laser radar,"Zheng, Daoyuan",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Data mining,Deep learning,Light detection and ranging (LiDAR),multimodal fusion,remote sensing imagery,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_581,"Wang, Lunqian","Wang, Xinghua","Liu, Weilin","Ding, Hao",A unified architecture for super-resolution and segmentation of remote sensing images based on similarity feature fusion,,SEP 2024,0,"The resolution of the image has an important impact on the accuracy of segmentation. Integrating super- resolution (SR) techniques in the semantic segmentation of remote sensing images contributes to the improvement of precision and accuracy, especially when the images are blurred. In this paper, a novel and efficient SR semantic segmentation network (SRSEN) is designed by taking advantage of the similarity between SR and segmentation tasks in feature processing. SRSEN consists of the multi-scale feature encoder, the SR fusion decoder, and the multi-path feature refinement block, which adaptively establishes the feature associations between segmentation and SR tasks to improve the segmentation accuracy of blurred images. Experiments show that the proposed method achieves higher segmentation accuracy on fuzzy images compared to state-of-the-art models. Specifically, the mIoU of the proposed SRSEN is 3%-6% higher than other state-of-the-art models on low-resolution LoveDa, Vaihingen, and Potsdam datasets.",Deep learning,Semantic segmentation,Super-resolution,Remote sensing image,,"Xia, Bo","Zhang, Zekai","Zhang, Jinglin","Xu, Sen",DISPLAYS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_582,"Ismael, Sarmad F.","Aptoula, Erchan","Kayabol, Koray",,A JOINT SEMANTIC SEGMENTATION LOSS FUNCTION FOR IMBALANCED DATASETS,2022 IEEE MEDITERRANEAN AND MIDDLE-EAST GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (M2GARSS),2022,0,"Semantic segmentation is one of the most important applications in remote sensing image analysis. Since remote sensing datasets are often highly imbalanced in terms of class distribution, specialized loss functions such as focal loss are required. In this paper, a loss function that combines weighted focal loss with Jaccard loss has been developed. This loss function has been used to train U-Net and DeepLabV3+ semantic segmentation models on the recently introduced Landcover.ai dataset, which has a high level of class imbalance. It has been observed through our experiments that the combined loss function leads to a performance improvement.",Deep Learning,Weighted focal loss,Jaccard loss,Remote sensing imagery,Semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_583,"Pan, Xin","Zhao, Jian","Xu, Jun",,Conditional Generative Adversarial Network-Based Training Sample Set Improvement Model for the Semantic Segmentation of High-Resolution Remote Sensing Images,,SEP 2021,17,"To achieve high segmentation quality, deep semantic segmentation neural networks (DSSNNs) need to be trained on diverse direction, location, and neighboring category combinations for each pixel in the input-output image patches. Achieving this goal requires a large sample set. However, in many practical application scenarios, a very large training sample set is too expensive to achieve, or insufficient remote sensing image data are available. These limitations directly affect the quality of the results of DSSNNs. To address the above-mentioned problem, this article proposes a conditional generative adversarial network (CGAN)-based training sample set improvement model (CGAN-TSIM) for the semantic segmentation of high-resolution remote sensing images. In CGAN-TSIM, the generator model of the CGAN can generate a sample image when a ground-truth image is an input as a ""condition."" A condition generation mechanism is designed to create ground-truth images, and these ground-truth conditions are used to drive the CGAN to generate samples containing more diverse object combinations, directions, and locations. These generated images can be added to the original training sample set to improve their spatial information diversity. Rather than simply relying on passively finding samples that contain diverse spatial information, CGAN-TSIM extracts high-level spatial information from the original training images and actively generates new sample images. Experiments show that the samples generated by CGAN-TSIM can improve the quality of the sample set. Compared with other traditional methods, CGAN-TSIM enables better classification accuracy.",Remote sensing,Image segmentation,Training,Semantics,Generative adversarial networks,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Feature extraction,Generators,Condition generation,generative adversarial network (GAN),high-resolution remote sensing classification,sample generation,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_584,"Yang, Yang","Dong, Junwu","Wang, Yanhui","Yu, Bibo",DMAU-Net: An Attention-Based Multiscale Max-Pooling Dense Network for the Semantic Segmentation in VHR Remote-Sensing Images,,MAR 2023,4,"High-resolution remote-sensing images cover more feature information, including texture, structure, shape, and other geometric details, while the relationships among target features are more complex. These factors make it more complicated for classical convolutional neural networks to obtain ideal results when performing a feature classification on remote-sensing images. To address this issue, we proposed an attention-based multiscale max-pooling dense network (DMAU-Net), which is based on U-Net for ground object classification. The network is designed with an integrated max-pooling module that incorporates dense connections in the encoder part to enhance the quality of the feature map, and thus improve the feature-extraction capability of the network. Equally, in the decoding, we introduce the Efficient Channel Attention (ECA) module, which can strengthen the effective features and suppress the irrelevant information. To validate the ground object classification performance of the multi-pooling integration network proposed in this paper, we conducted experiments on the Vaihingen and Potsdam datasets provided by the International Society for Photogrammetry and Remote Sensing (ISPRS). We compared DMAU-Net with other mainstream semantic segmentation models. The experimental results show that the DMAU-Net proposed in this paper effectively improves the accuracy of the feature classification of high-resolution remote-sensing images. The feature boundaries obtained by DMAU-Net are clear and regionally complete, enhancing the ability to optimize the edges of features.",high-resolution remote-sensing images,ground object classification,dense connections,multiscale maximum pooling,semantic segmentation,"Yang, Zhigang",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_585,"Wang, Yu","Jing, Xin","Xu, Yang","Cui, Liangyi",Geometry-guided semantic segmentation for post-earthquake buildings using optical remote sensing images,,SEP 2023,11,"Deep-learning-based automatic recognition of post-earthquake damage for urban buildings is increasingly in demand for rapid and precise assessment of seismic hazards from optical remote sensing images. In this study, a novel loss function fusing geometric consistency constraint (GCC) with cross-entropy (CE) loss is designed for post-earthquake building segmentation with complex geometric features across multiple scales. Specifically, the GCC loss incorporates three critical components, namely, split line length, curvature, and area, and enables the exact extraction of the geometric constraints of boundary and region for damaged buildings. Through the optimization of multiple key coefficients of GCC loss, the proposed method achieves significant performance improvements in semantic segmentation, which is attributed to the enhanced ability to identify and capture the pixel relationship near the boundary. Merging GCC in the loss function enables faster and more accurate convergence of predicted values towards the ground truth during the training process, surpassing the performance of the CE loss alone. The results show that the combination of GCC and CE losses achieves the largest validation mIoU of 86.98% for damaged buildings segmentation, which facilitates post-earthquake assessment with high accuracy. Moreover, incorporating GCC leads to more precise and robust seismic damage segmentation by effectively improving edge closure, removing internal noise, and reducing false-positive and false-negative misrecognition. In addition, the GCC term further validates the effectiveness of improving segmentation tasks for other networks (e.g., DeepLabv3+). The GCC-derived method exhibits its desirable performance on segmentation accuracy, portability, and universality for building recognition with complex geometric features and post-earthquake scenes.",complex geometric features,dense post-earthquake buildings,geometric consistency loss,remote sensing images,semantic segmentation,"Zhang, Qiangqiang","Li, Hui",,,EARTHQUAKE ENGINEERING & STRUCTURAL DYNAMICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_586,"Zhao, Jiaqi","Zhou, Yong","Shi, Boyu","Yang, Jingsong",Multi-Stage Fusion and Multi-Source Attention Network for Multi-Modal Remote Sensing Image Segmentation,,NOV 2021,16,"With the rapid development of sensor technology, lots of remote sensing data have been collected. It effectively obtains good semantic segmentation performance by extracting feature maps based on multi-modal remote sensing images since extra modal data provides more information. How to make full use of multi-model remote sensing data for semantic segmentation is challenging. Toward this end, we propose a new network called Multi-Stage Fusion and Multi-Source Attention Network ((MS)(2)-Net) for multi-modal remote sensing data segmentation. The multi-stage fusion module fuses complementary information after calibrating the deviation information by filtering the noise from the multi-modal data. Besides, similar feature points are aggregated by the proposed multi-source attention for enhancing the discriminability of features with different modalities. The proposed model is evaluated on publicly available multi-modal remote sensing data sets, and results demonstrate the effectiveness of the proposed method.",Semantic segmentation,multi-modal remote sensing images,attention,feature fusion,,"Zhang, Di","Yao, Rui",,,ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_587,"Wang, Wei","Tang, Chen","Wang, Xin","Zheng, Bin",A ViT-Based Multiscale Feature Fusion Approach for Remote Sensing Image Segmentation,,2022,22,"Semantic segmentation plays an indispensable role in automatic analysis of remote sensing image data. However, the abundant semantic information and irregular shape patterns in remote sensing images are difficult to utilize, making it hard to segment remote sensing images only using convolution and single-scale feature maps. To achieve better segmentation performance, a multiscale feature pyramid decoder (MFPD) is proposed to fuse image features extracted by vision transformer (ViT). The decoder employs a novel 2-D-to-3-D transform method to obtain multiscale feature maps that contain rich context information and fuses the multiscale feature maps by channel concatenation. Furthermore, a dimension attention module (DAM) is designed to further aggregate the context information of the extracted remote sensing image features. This approach yields superior mean intersection over union (mIoU) on the Gaofen2-CZ dataset (60.42%) and GID-5 dataset (68.21%). Experimental results indicate that the comprehensive performance of our approach exceeds the compared segmentation methods based on convolutional neural network (CNN) and ViT.",Feature extraction,Image segmentation,Transformers,Decoding,Three-dimensional displays,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Dams,Transforms,Dimension attention module (DAM),remote sensing image,semantic segmentation,vision transformer (ViT),,,,,,,,,,,,,,,,,,,,,,,,
Row_588,"Cao, Kangjian","Wang, Sheng","Wei, Ziheng","Chen, Kexin",Unsupervised Domain Adaptation Semantic Segmentation of Remote Sensing Imagery with Scene Covariance Alignment,,DEC 2024,0,"Remote sensing imagery (RSI) segmentation plays a crucial role in environmental monitoring and geospatial analysis. However, in real-world practical applications, the domain shift problem between the source domain and target domain often leads to severe degradation of model performance. Most existing unsupervised domain adaptation methods focus on aligning global-local domain features or category features, neglecting the variations of ground object categories within local scenes. To capture these variations, we propose the scene covariance alignment (SCA) approach to guide the learning of scene-level features in the domain. Specifically, we propose a scene covariance alignment model to address the domain adaptation challenge in RSI segmentation. Unlike traditional global feature alignment methods, SCA incorporates a scene feature pooling (SFP) module and a covariance regularization (CR) mechanism to extract and align scene-level features effectively and focuses on aligning local regions with different scene characteristics between source and target domains. Experiments on both the LoveDA and Yanqing land cover datasets demonstrate that SCA exhibits excellent performance in cross-domain RSI segmentation tasks, particularly outperforming state-of-the-art baselines across various scenarios, including different noise levels, spatial resolutions, and environmental conditions.",unsupervised domain adaptation,remote sensing imagery,semantic segmentation,covariance alignment,,"Chang, Runlong","Xu, Fu",,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_589,"Li, Jiangyun","Cai, Yuanxiu","Li, Qing","Kou, Mingyin",A review of remote sensing image segmentation by deep learning methods,,DEC 31 2024,8,"Remote sensing (RS) images enable high-resolution information collection from complex ground objects and are increasingly utilized in the earth observation research. Recently, RS technologies are continuously enhanced by various characterized platforms and sensors. Simultaneously, artificial intelligence vision algorithms are also developing vigorously and playing a significant role in RS image analysis. In particular, aiming to divide images into different ground elements with specific semantic labels, RS image segmentation could realize the visual acquisition and interpretation. As one of the pioneering methods with the advantages of deep feature extraction ability, deep learning (DL) algorithms have been exploited and proved to be highly beneficial for precise segmentation in recent years. In this paper, a comprehensive review is performed on remote sensing survey systems and various kinds of specially designed deep learning architectures. Meanwhile, DL-based segmentation methods applied on four domains are also illustrated, including geography, precision agriculture, hydrology, and environmental protection issues. In the end, the existing challenges and promising research directions in RS image segmentation are discussed. It is envisioned that this review is able to provide a comprehensive and technical reference, deployment and successful exploitation of DL empowered RS image segmentation approaches.",Remote sensing,deep learning,image segmentation,,,"Zhang, Tianxiang",,,,INTERNATIONAL JOURNAL OF DIGITAL EARTH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_590,"Wang, Yang","Sun, Zhaochen","Zhao, Wei",,Encoder- and Decoder-Based Networks Using Multiscale Feature Fusion and Nonlocal Block for Remote Sensing Image Semantic Segmentation,,JUL 2021,11,"With the development of convolutional neural networks, the semantic segmentation of remote sensing images has been widely developed, but there are still some unsolved problems in this field due to the lack of multiscale information and the feature mismatch at the upsampling process. To solve these problems, we propose a network called multiscale feature fusion and alignment network (MFANet). MFANet is composed of an encoder and a decoder. The encoder contains a fully convolutional network, a multilevel feature fusion block (MLFFB), and a multiscale feature pyramid (MSFP). These subnetworks can obtain fine-grained feature maps that are full of multiscale and global features and improve segmentation results at multiple object scales. Moreover, MFANet uses a light convolution subnetwork, called decoder, to upsample the segmentation map stage by stage. Combining three scales of features, the decoder can promote the feature alignment at the upsampling stage. Along with the decoder, MFANet utilizes a multistage supervision loss to enhance the localization performance and boundary regression ability. Benefitting from the encoder and decoder structure and the innovative components inside encoder, MFANet is very powerful for the semantic segmentation of remote sensing images and can suit the complicated environment. We evaluate our MFANet on the Vaihingen and Potsdam data sets, and it outperforms the state-of-art methods both in the metric and visual effect.",Decoding,Feature extraction,Image segmentation,Convolution,Semantics,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Remote sensing,Interpolation,Channel attention block (CAB),decoder,multilevel feature fusion block (MLFFB),nonlocal block,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,
Row_591,"Wang, Yiqin",,,,Remote sensing image semantic segmentation network based on ENet,,SEP 2022,1,"The current image semantic segmentation methods cannot meet the requirements of high precision and high speed for remote sensing image analysis. The ENet network model builds a semantic segmentation network, which has the characteristics of few network parameters and fast operation speed. The attention mechanism module is integrated with the ENet network model, which can deeply mine image features in remote sensing datasets and ensure the accuracy of semantic segmentation. The author combines the ENet network with the attention mechanism to construct a new semantic segmentation network model. The model first constructed a remote sensing image semantic segmentation network model based on the ENet network, and simplified the model to further improve the speed of image segmentation and recognition. Then, the attention mechanism module is fused with the ENet network model, which can conduct deep and orderly mining of the image features of the remote sensing image data set. It can meet the accuracy requirements of remote sensing image semantic analysis. Simulations are performed based on three general datasets, and the experimental results show high accuracy and high speed.",,,,,,,,,,JOURNAL OF ENGINEERING-JOE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_592,"Yao, Hongtai","Wang, Xianpei","Zhao, Le","Tian, Meng",An Object-Based Markov Random Field with Partition-Global Alternately Updated for Semantic Segmentation of High Spatial Resolution Remote Sensing Image,,JAN 2022,3,"The Markov random field (MRF) method is widely used in remote sensing image semantic segmentation because of its excellent spatial (relationship description) ability. However, there are some targets that are relatively small and sparsely distributed in the entire image, which makes it easy to misclassify these pixels into different classes. To solve this problem, this paper proposes an object-based Markov random field method with partition-global alternately updated (OMRF-PGAU). First, four partition images are constructed based on the original image, they overlap with each other and can be reconstructed into the original image; the number of categories and region granularity for these partition images are set. Then, the MRF model is built on the partition images and the original image, their segmentations are alternately updated. The update path adopts a circular path, and the correlation assumption is adopted to establish the connection between the label fields of partition images and the original image. Finally, the relationship between each label field is constantly updated, and the final segmentation result is output after the segmentation has converged. Experiments on texture images and different remote sensing image datasets show that the proposed OMRF-PGAU algorithm has a better segmentation performance than other selected state-of-the-art MRF-based methods.",object-based Markov random field,high spatial resolution remote sensing image,semantic segmentation,correlation assumption,,"Jian, Zini","Gong, Li","Li, Bowen",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_593,"Wang, Shunli","Hu, Qingwu","Wang, Shaohua","Ai, Mingyao",Archaeological site segmentation of ancient city walls based on deep learning and LiDAR remote sensing,,MAR-APR 2024,7,"Ancient city walls, one of the most notable distinguishing features of Chinese ancient cities, are military defenses constructed of rammed earth. The ancient city walls have considerable study value because they served as the city's boundary and a symbol of power. However, as a result of natural erosion and human activities, many sites have been ruined. Existing optical remote sensing technologies, LiDAR point cloud processing algorithms, and deep learning methods are inadequate for the extraction and segmentation of ancient city wall sites. The novel semantic segmentation method for ancient city wall sites is described in this paper that extracts sites at the pixel level from LiDAR remote sensing data based on deep learning. To begin, the point cloud data collected by airborne laser scanning is processed into DEM data, and the distribution of ancient city walls in the study area is obtained through archaeological survey and expert interpretation. The dataset for deep learning semantic segmentation is then generated using image cropping and data augmentation techniques. Third, implement a U-Net semantic segmentation framework for microtopographic archaeological sites, and predict ancient city wall sites in the testing region after model training. Finally, the deep learning results are optimized using the connected component analysis method, and prediction mistakes such as holes and noise are removed. Taking Jinancheng, the capital city of the Chu kingdom, as an example, the proposed method process can identify and extract the ancient city wall sites at the pixel level, where the evaluation metrics reach 94.12% (Precision) and 81.38% (IoU). The experiment results are excellent due to improvement strategies in the dataset generation, model training, and post-processing steps. Thus, this study is significant for the current survey and protection of ancient city wall sites. The source code will be freely available at https://github.com/wshunli/Open- CHAI-CityWalls .(c) 2023 Consiglio Nazionale delle Ricerche (CNR). Published by Elsevier Masson SAS. All rights reserved.",Archaeological remote sensing,Archaeological site segmentation,Deep learning,LiDAR remote sensing,Semantic segmentation,"Zhao, Pengcheng",,,,JOURNAL OF CULTURAL HERITAGE,,Archaeological survey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_594,"Qi, Zipeng","Chen, Hao","Liu, Chenyang","Shi, Zhenwei",Implicit Ray Transformers for Multiview Remote Sensing Image Segmentation,,2023,5,"The mainstream convolutional neural network (CNN)-based remote sensing (RS) image semantic segmentation approaches typically rely on massively labeled training data. Such a paradigm struggles with the problem of RS multiview scene segmentation with limited labeled views due to the lack of consideration of 3-D information within the scene. In this article, we propose ""implicit ray transformer (IRT)"" based on implicit neural representation (INR) for RS scene semantic segmentation with sparse labels (5% of the images being labeled). We explore a new way of introducing the multiview 3-D structure priors to the task for accurate and view-consistent semantic segmentation. The proposed method includes a two-stage learning process. In the first stage, we optimize a neural field to encode the color and 3-D structure of the RS scene based on multiview images. In the second stage, we design a ray transformer to leverage the relations between the neural field 3-D features and 2-D texture features for learning better semantic representations. Different from previous methods that only consider 3-D priors or 2-D features, we incorporate additional 2-D texture information and 3-D priors by broadcasting CNN features to different point features along the sampled ray. To verify the effectiveness of the proposed method, we construct a challenging dataset containing six synthetic sub-datasets collected from the Carla platform and three real sub-datasets from Google Maps. Experiments show that the proposed method outperforms the CNN-based methods and the state-of-the-art INR-based segmentation methods in quantitative and qualitative metrics. The ablation study shows that under a limited number of fully annotated images, the combination of the 3-D structure priors and 2-D texture can significantly improve the performance and effectively complete missing semantic information in novel views. Experiments also demonstrate that the proposed method could yield geometry-consistent segmentation results against illumination changes and viewpoint changes. Our data and code will be public.",Three-dimensional displays,Feature extraction,Semantics,Task analysis,Remote sensing,"Zou, Zhengxia",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Transformers,Annotations,Implicit neural representation (INR),remote sensing (RS),semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_595,"Wang, Tao","Xu, Chao","Liu, Bin","Yang, Guang",MCAT-UNet: Convolutional and Cross-Shaped Window Attention Enhanced UNet for Efficient High-Resolution Remote Sensing Image Segmentation,,2024,3,"Semantic segmentation is a crucial step in the intelligent interpretation of high-resolution remote sensing images (HRSIs). Convolutional neural networks and transformers are widely used for semantic feature extraction in remote sensing images, but the former inevitably has limitations in modeling long-range spatial dependency information, while the latter lacks the ability to learn local semantic features. Existing remote sensing image segmentation methods are optimized and modified based on the backbone networks used in natural image processing. Despite achieving relatively good results, the complexity of their network structures leads to high computational costs and limited improvements in accuracy. These methods have limited boundary distinction for ground objects in complex environments, especially for small targets. In this article, we propose an efficient semantic segmentation architecture for HRSIs called MCAT-UNet, which utilizes multiscale convolutional attention (MSCA) and the cross-shaped window transformer (CSWT) to reconstruct UNet. The encoder stacks a sequence of MSCA to exploit the advantages of convolution attention to encode context information more effectively and enhance hierarchical multiscale representation learning. The proposed U-shaped decoder integrates three skip connections using the CSWT block to further capture long-range spatial dependency and gradually restore the size of the feature map. We benchmark MCAT-UNet on three common datasets, Potsdam, Vaihingen, and LoveDA. Comprehensive experiments and extensive ablation studies show that our proposed MCAT-UNet outperforms previous state-of-the-art methods with remarkable performance.",Transformers,Remote sensing,Feature extraction,Semantics,Task analysis,"Zhang, Erlei","Niu, Dangdang","Zhang, Hongming",,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Semantic segmentation,Computer vision,Convolutional attention,cross-shaped self-attention,remote sensing image,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_596,"Feng, Yongliang",,,,SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES USING DEEP LEARNING FROM THE PERSPECTIVE OF ECOLOGICAL ENVIRONMENT PROTECTION,,2022,0,"Traditional remote sensing image semantic segmentation has problems such as low segmentation accuracy and large model parameters and calculations. A semantic segmentation network of remote sensing images using deep learning from the perspective of ecological environment protection is proposed. Firstly, the ""Gaofen-5"" remote sensing image set is preprocessed by means of data cutting and image denoising to improve the data quality. Then, the local and global diffusion blocks (LGD) is introduced to improve the EfficientNet B0 network to construct an ENet-LGD network model to improve feature learning. Finally, the conditional random field model is improved based on superpixels. That is, each small block of the super pixel reflects the boundary of the feature, which further improves the segmentation effect. The experimental results based on the Pingshuo mining area in Shanxi Province show that the proposed network can accurately reflect the surface types and their boundaries. The average segmentation accuracy is 0.865, which is better than other comparison networks. It provides a certain theoretical basis for subsequent practical applications.",Semantic segmentation of remote sensing image,From the perspective of ecological environment protection,Global local interaction module,Efficientnet B0 network,Super pixel,,,,,FRESENIUS ENVIRONMENTAL BULLETIN,,Conditional random field model,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_597,"Ao, Wei","Zheng, Shunyi","Meng, Yan","Gao, Zhi",Few-Shot Aerial Image Semantic Segmentation Leveraging Pyramid Correlation Fusion,,2023,5,"Few-shot semantic segmentation (FSS) has gained significant attention due to its ability to segment novel objects using only a limited number of labeled samples, thereby addressing the problem of overfitting caused by a lack of training data. Although this technique is widely studied in the field of computer vision, there are few methods for remote-sensing images. Prevalent FSS methods can achieve remarkable results for natural images, but they are difficult to apply to remote-sensing image processing because existing methods rarely take into consideration the large-scale and resolution differences in remote-sensing images. Consequently, it is hard for them to obtain correct semantic guidance from a few annotated remote-sensing images. To tackle these problems, this article proposes the pyramid correlation fusion network (PCFNet) to promote the ability to mine helpful information by calculating multiscale pixel-wise semantic correspondence. Particularly, the dual-distance correlation (DDC) module is designed to simultaneously compute the cosine similarity and Euclidean distance between query features and support features, producing adequate guidance information to determine the category of each pixel. Moreover, to improve segmentation accuracy for small objects, the scale-aware cross-entropy loss (SACELoss) is introduced to dynamically assign loss weights according to the actual sizes of objects. This enables smaller objects to be assigned larger weight values and thus receive more attention during training. Comprehensive experiments on both the iSAID- 5(i) and DLRSD- 5(i) datasets demonstrate that our method outperforms state-of-the-art FSS methods. Our code is available at https://github.com/TinyAway/PCFNet.",Distance correlation,few-shot semantic segmentation (FSS),meta-learning,remote-sensing image processing,semantic correspondence,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_598,"Liu, Wenjie","Zhang, Yongjun","Fan, Haisheng","Zou, Yongjie",A New Multi-Channel Deep Convolutional Neural Network for Semantic Segmentation of Remote Sensing Image,,2020,9,"The semantic segmentation of remote sensing (RS) image is a hot research field. With the development of deep learning, the semantic segmentation based on a full convolution neural network greatly improves the segmentation accuracy. The amount of information on the RS image is very large, but the sample size is extremely uneven. Therefore, even the common network can segment RS images to a certain extent, but the segmentation accuracy can still be greatly improved. The common neural network deepens the network to improve the classification accuracy, but it has a lot of loss to the target spatial features and scale features, and the existing common feature fusion methods can only solve some problems. A segmentation network is built to solve the above problems very well. The network employs the InceptionV-4 network as the backbone and improves it. We modify the network structure and introduce the changed Atrous Spatial Pyramid Pooling module to extract the multi-scale features of the target from different training stages. Without losing the depth of the network, using Inception blocks to strengthen the width of the network can obtain more abstract features. At the same time, the backbone network is used for semantic fusion of the context, it can retain more spatial features, then an effective decoder network is designed. Finally, evaluate our model on the ISPRS 2D Semantic Labeling Contest Potsdam and Inria Aerial Image Labeling Dataset. The results show that the network has very superior performance, reaching 89.62% IOU score and 94.49% F1 score on the Potsdam dataset, and the IOU score on the Inria dataset has been greatly improved.",Image segmentation,Feature extraction,Convolution,Semantics,Machine learning,"Cui, Zhongwei",,,,IEEE ACCESS,,Neural networks,Image classification,Semantic segmentation,neural network,remote sensing,feature fusion,,,,,,,,,,,,,,,,,,,,,,,,
Row_599,"Tong, Zhonggui","Li, Yuxia","Zhang, Jinglin","He, Lei",MSFANet: Multiscale Fusion Attention Network for Road Segmentation of Multispectral Remote Sensing Data,,APR 2023,7,"With the development of deep learning and remote sensing technologies in recent years, many semantic segmentation methods based on convolutional neural networks (CNNs) have been applied to road extraction. However, previous deep learning-based road extraction methods primarily used RGB imagery as an input and did not take advantage of the spectral information contained in hyperspectral imagery. These methods can produce discontinuous outputs caused by objects with similar spectral signatures to roads. In addition, the images obtained from different Earth remote sensing sensors may have different spatial resolutions, enhancing the difficulty of the joint analysis. This work proposes the Multiscale Fusion Attention Network (MSFANet) to overcome these problems. Compared to traditional road extraction frameworks, the proposed MSFANet fuses information from different spectra at multiple scales. In MSFANet, multispectral remote sensing data is used as an additional input to the network, in addition to RGB remote sensing data, to obtain richer spectral information. The Cross-source Feature Fusion Module (CFFM) is used to calibrate and fuse spectral features at different scales, reducing the impact of noise and redundant features from different inputs. The Multiscale Semantic Aggregation Decoder (MSAD) fuses multiscale features and global context information from the upsampling process layer by layer, reducing information loss during the multiscale feature fusion. The proposed MSFANet network was applied to the SpaceNet dataset and self-annotated images from Chongzhou, a representative city in China. Our MSFANet performs better over the baseline HRNet by a large margin of +6.38 IoU and +5.11 F1-score on the SpaceNet dataset, +3.61 IoU and +2.32 F1-score on the self-annotated dataset (Chongzhou dataset). Moreover, the effectiveness of MSFANet was also proven by comparative experiments with other studies.",deep learning,semantic segmentation,attention mechanism,multispectral remote sensing data,,"Gong, Yushu",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_600,"Li, Peng","Zhang, Dezheng","Wulamu, Aziguli","Liu, Xin",Semantic Relation Model and Dataset for Remote Sensing Scene Understanding,,JUL 2021,8,"A deep understanding of our visual world is more than an isolated perception on a series of objects, and the relationships between them also contain rich semantic information. Especially for those satellite remote sensing images, the span is so large that the various objects are always of different sizes and complex spatial compositions. Therefore, the recognition of semantic relations is conducive to strengthen the understanding of remote sensing scenes. In this paper, we propose a novel multi-scale semantic fusion network (MSFN). In this framework, dilated convolution is introduced into a graph convolutional network (GCN) based on an attentional mechanism to fuse and refine multi-scale semantic context, which is crucial to strengthen the cognitive ability of our model Besides, based on the mapping between visual features and semantic embeddings, we design a sparse relationship extraction module to remove meaningless connections among entities and improve the efficiency of scene graph generation. Meanwhile, to further promote the research of scene understanding in remote sensing field, this paper also proposes a remote sensing scene graph dataset (RSSGD). We carry out extensive experiments and the results show that our model significantly outperforms previous methods on scene graph generation. In addition, RSSGD effectively bridges the huge semantic gap between low-level perception and high-level cognition of remote sensing images.",remote sensing scene understanding,semantic relation cognition,scene graph generation,multi-scale semantic fusion,attentional mechanism,"Chen, Peng",,,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,graph convolutional network,dilated convolution,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_601,"Zeng, Wankang","Cheng, Ming","Yuan, Zhimin","Dai, Wei",Domain adaptive remote sensing image semantic segmentation with prototype guidance,,MAY 1 2024,1,"Current unsupervised domain adaptation (UDA) techniques in semantic segmentation effectively decrease the domain discrepancy between the labeled source domain and unlabeled target domain, thereby enhancing the model's pixel -wise discriminative capability for target domain data. However, in remote sensing images (RSIs), our study uncovers that these approaches perform poorly in the presence of class distribution inconsistencies between the source and target domains. In this work, we propose a one -stage mean teacher framework with a novel auxiliary prototype classifier, named MTA, to address this issue. Specifically, the teacher model assigns pseudo labels at pixel level for target samples and captures knowledge from the student model via exponential moving average (EMA). With labeled source samples and target samples that have pseudo labels, the student model can alleviate the divergence between the source and target domains. In addition, the auxiliary prototype classifier (APC) reduces the performance degradation in the parametric softmax classifier of the student model caused by class distribution divergence. We also propose a prototype computation scheme to obtain each class prototype in the APC. Specifically, we build a memory bank for each class of the two domains to store feature embeddings dynamically. Then, we compute the class prototype by applying the clustering algorithm on memory banks corresponding to the class. Meanwhile, the APC reduces the intra-class domain discrepancy by optimizing the cross -entropy loss, which brings each class feature distribution of the two domains closer to the class prototype. The experimental results on RSIs UDA semantic segmentation tasks show the superiority of our approach over comparative methods.",Unsupervised domain adaptation,Semantic segmentation,Auxiliary prototype classifier,Mean teacher,,"Wu, Youming","Liu, Weiquan","Wang, Cheng",,NEUROCOMPUTING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_602,"Zhang, Chongyu",,,,Based on Multi-Feature Information Attention Fusion for Multi-Modal Remote Sensing Image Semantic Segmentation,2021 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (IEEE ICMA 2021),2021,4,"Semantic segmentation of remote sensing images are widely used in land census and agriculture. The scenes in remote sensing images are complex, easily affected by season, such as farmland. Besides, the size of the target in remote sensing image is different, the shape is irregular, and there is often the problem of missing detection, so the multi-source data information is directly fed into the neural network, resulting in fuzzy segmentation boundary, which is difficult to achieve fine segmentation. To solve this problem, we propose a Dual-way Feature attention Fusion Network (DFFNet), which consists of two branches, optical remote sensing image branch and elevation feature branch. The optical remote sensing image branch uses the spatial relationship module to learn and infer the global relationship between any two spatial positions or feature maps and then extracts the multi-level features of the image by capturing more context information and pyramid attention mechanism. The elevation feature branch strengthens the classification. Based on the boundary information, the remote sensing image segmentation is realized. Experiment results on ISPRS Vaihingen image dataset demonstrate the effectiveness of the proposed method.",Semantic segmentation,spatial attention,channel attention,feature fusion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_603,"Liu, Jiang","Cheng, Shuli","Du, Anyu",,ER-Swin: Feature Enhancement and Refinement Network Based on Swin Transformer for Semantic Segmentation of Remote Sensing Images,,2024,2,"As the field of remote sensing image processing continues to advance, semantic segmentation has become a focal point in this domain. The emergence of the swin transformer (SwinT) has greatly alleviated the computational complexities associated with transformers, leading to its widespread application in the field of semantic segmentation. However, most current network models lack a feature enhancement process internally, and the model's tail lacks refinement modules to prevent category misjudgments caused by feature redundancy. To address this issue, we propose ER-Swin to explore the potential of utilizing SwinT as the backbone network for semantic segmentation in remote sensing images. Addressing the need for feature enhancement in the backbone network, we propose interactive feature enhancement attention (IFEA), which leverages diagonal information interaction to augment features. Additionally, we design the semantic selective refinement module (SSRM) to refine the rich features at the tail end of the network, thereby enhancing segmentation outcomes. We evaluated our model on the Vaihingen, Potsdam, and LoveDA datasets and achieved accuracies of 84.89%, 87.20%, and 55.1%, respectively, on the mean intersection over union (mIoU) metric. Through comparative experiments, we demonstrate the superior segmentation performance of our model, affirming its competitiveness.",Feature enhancement,refinement,semantic segmentation,swin transformer (SwinT),,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_604,"Wang, Yupei","Zhang, Haoran","Hu, Yongkang","Hu, Xiaoxing",Geometric Boundary Guided Feature Fusion and Spatial-Semantic Context Aggregation for Semantic Segmentation of Remote Sensing Images,,2023,7,"Semantic segmentation of remote sensing images aims to achieve pixel-level semantic category assignment for input images. This task has achieved significant advances with the rapid development of deep neural network. Most current methods mainly focus on effectively fusing the low-level spatial details and high-level semantic cues. Other methods also propose to incorporate the boundary guidance to obtain boundary preserving segmentation. However, current methods treat the multi-level feature fusion and the boundary guidance as two separate tasks, resulting in sub-optimal solutions. Moreover, due to the large inter-class difference and small intra-class consistency within remote sensing images, current methods often fail to accurately aggregate the long-range contextual cues. These critical issues make current methods fail to achieve satisfactory segmentation predictions, which severely hinder downstream applications. To this end, we first propose a novel boundary guided multi-level feature fusion module to seamlessly incorporate the boundary guidance into the multi-level feature fusion operations. Meanwhile, in order to further enforce the boundary guidance effectively, we employ a geometric-similarity-based boundary loss function. In this way, under the explicit guidance of boundary constraint, the multi-level features are effectively combined. In addition, a channel-wise correlation guided spatial-semantic context aggregation module is presented to effectively aggregate the contextual cues. In this way, subtle but meaningful contextual cues about pixel-wise spatial context and channel-wise semantic correlation are effectively aggregated, leading to spatial-semantic context aggregation. Extensive qualitative and quantitative experimental results on ISPRS Vaihingen and GaoFen-2 datasets demonstrate the effectiveness of the proposed method.",Semantic segmentation,multi-level feature fusion,contextual cues,,,"Chen, Liang","Hu, Shanqing",,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_605,"Sun, Yu","Bi, Fukun","Gao, Yangte","Chen, Liang",A Multi-Attention UNet for Semantic Segmentation in Remote Sensing Images,,MAY 2022,34,"In recent years, with the development of deep learning, semantic segmentation for remote sensing images has gradually become a hot issue in computer vision. However, segmentation for multicategory targets is still a difficult problem. To address the issues regarding poor precision and multiple scales in different categories, we propose a UNet, based on multi-attention (MA-UNet). Specifically, we propose a residual encoder, based on a simple attention module, to improve the extraction capability of the backbone for fine-grained features. By using multi-head self-attention for the lowest level feature, the semantic representation of the given feature map is reconstructed, further implementing fine-grained segmentation for different categories of pixels. Then, to address the problem of multiple scales in different categories, we increase the number of down-sampling to subdivide the feature sizes of the target at different scales, and use channel attention and spatial attention in different feature fusion stages, to better fuse the feature information of the target at different scales. We conducted experiments on the WHDLD datasets and DLRSD datasets. The results show that, with multiple visual attention feature enhancements, our method achieves 63.94% mean intersection over union (IOU) on the WHDLD datasets; this result is 4.27% higher than that of UNet, and on the DLRSD datasets, the mean IOU of our methods improves UNet's 56.17% to 61.90%, while exceeding those of other advanced methods.",remote sensing,image segmentation,multi-head self-attention,channel attention,spatial attention,"Feng, Suting",,,,SYMMETRY-BASEL,,deep learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_606,"Zhao, Yanfeng","Yang, Zhenjian","Zhang, Yunjie","Chen, Yadong",BGFNet: boundary information-aided graph structure fusion network for semantic segmentation of remote sensing images,,JUN 2024,0,"Semantic segmentation of high-resolution remote sensing (RS) images faces challenges in multi-scale transformation. Although feature fusion is widely used in this task, the existing methods do not fully consider the spatial structure relationship between feature layers in the encoder stage. Firstly, this paper designs BGFNet network, combines the feature extraction in the encoder stage and the topological relationship modeling of graph structure, and proposes the graph structure-guided multi-scale feature fusion module to solve this problem. Secondly, in order to solve the problem of blurred object boundaries in RS image segmentation, we propose a multi-level deformable boundary guidance module, which emphasizes object boundaries by establishing long-range context. Finally, a shared enhanced attention module with shared parameters is proposed to enhance the characteristics of each class to improve the recognition ability of the model. The effectiveness of BGFNet is verified on Potsdam and Vaihingen public RS datasets, and its segmentation performance is obviously better than the existing mainstream methods. The source code will be freely available at https://github.com/zyf-cell/BGF_Net.",Graph structure,Boundary guidance,Attention mechanism,Remote sensing,Semantic segmentation,,,,,VISUAL COMPUTER,,Very high-resolution images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_607,"Xie, Hongbin","Pan, Yongzhuo","Luan, Jinhua","Yang, Xue",Open-pit Mining Area Segmentation of Remote Sensing Images Based on DUSegNet,,JUN 2021,27,"Remote sensing is an important technical means for monitoring and protecting mineral resources. However, because of the complex surface environment, very few good results have been achieved in the study of automatic open-pit mining area segmentation. Inspired by SegNet, UNet and D-LinkNet, this paper proposes a novel deep convolutional neural network for pixel-level semantic segmentation of optical remote sensing images termed DUSegNet. In this network, the pyramid model and upsampling method of pooling indices, similar to SegNet, are employed in the encoder-decoder architecture. In addition, the convolutional skip connection architecture, similar to UNet, is adopted to connect shallow features to the decoder. Additionally, the serial-parallel model, similar to D-LinkNet; the intensifier constructed by dilated convolution; and the classifier constructed by softmax layers are applied in the process of encoding and decoding. In the practical application stage, we present an effective open-pit mining area segmentation method for entire remote sensing images, which has great significance for practical work, such as environmental impact assessment procedures and mine management. In the experimental stage, we compared the open-pit mining area segmentation effects of SegNet, UNet, DecovNet, and DUSegNet on the same dataset manually collected from GF-2 remote sensing images and verified the advantages of DUSegNet using graphic results and optimal evaluation metrics, such as AP (0.94) and F-score (0.67).",Deep convolutional neural network,Semantic segmentation,Optical remote sensing image,Open-pit mining area,,"Xi, Yawen",,,,JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_608,"Su, Yanzhou","Cheng, Jian","Bai, Haiwei","Liu, Haijun",Semantic Segmentation of Very-High-Resolution Remote Sensing Images via Deep Multi-Feature Learning,,FEB 2022,22,"Currently, an increasing number of convolutional neural networks (CNNs) focus specifically on capturing contextual features (con. feat) to improve performance in semantic segmentation tasks. However, high-level con. feat are biased towards encoding features of large objects, disregard spatial details, and have a limited capacity to discriminate between easily confused classes (e.g., trees and grasses). As a result, we incorporate low-level features (low. feat) and class-specific discriminative features (dis. feat) to boost model performance further, with low. feat helping the model in recovering spatial information and dis. feat effectively reducing class confusion during segmentation. To this end, we propose a novel deep multi-feature learning framework for the semantic segmentation of VHR RSIs, dubbed MFNet. The proposed MFNet adopts a multi-feature learning mechanism to learn more complete features, including con. feat, low. feat, and dis. feat. More specifically, aside from a widely used context aggregation module for capturing con. feat, we additionally append two branches for learning low. feat and dis. feat. One focuses on learning low. feat at a shallow layer in the backbone network through local contrast processing, while the other groups con. feat and then optimizes each class individually to generate dis. feat with better inter-class discriminative capability. Extensive quantitative and qualitative evaluations demonstrate that the proposed MFNet outperforms most state-of-the-art models on the ISPRS Vaihingen and Potsdam datasets. In particular, thanks to the mechanism of multi-feature learning, our model achieves an overall accuracy score of 91.91% on the Potsdam test set with VGG16 as a backbone, performing favorably against advanced models with ResNet101.",very-high-resolution remote sensing images,semantic segmentation,multi-feature learning,,,"He, Changtao",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_609,"Xiong, Xuan","Wang, Xiaopeng","Zhang, Jiahua","Huang, Baoxiang",TCUNet: A Lightweight Dual-Branch Parallel Network for Sea-Land Segmentation in Remote Sensing Images,,SEP 2023,7,"Remote sensing techniques for shoreline extraction are crucial for monitoring changes in erosion rates, surface hydrology, and ecosystem structure. In recent years, Convolutional neural networks (CNNs) have developed as a cutting-edge deep learning technique that has been extensively used in shoreline extraction from remote sensing images, owing to their exceptional feature extraction capabilities. They are progressively replacing traditional methods in this field. However, most CNN models only focus on the features in local receptive fields, and overlook the consideration of global contextual information, which will hamper the model's ability to perform a precise segmentation of boundaries and small objects, consequently leading to unsatisfactory segmentation results. To solve this problem, we propose a parallel semantic segmentation network (TCU-Net) combining CNN and Transformer, to extract shorelines from multispectral remote sensing images, and improve the extraction accuracy. Firstly, TCU-Net imports the Pyramid Vision Transformer V2 (PVT V2) network and ResNet, which serve as backbones for the Transformer branch and CNN branch, respectively, forming a parallel dual-encoder structure for the extraction of both global and local features. Furthermore, a feature interaction module is designed to achieve information exchange, and complementary advantages of features, between the two branches. Secondly, for the decoder part, we propose a cross-scale multi-source feature fusion module to replace the original UNet decoder block, to aggregate multi-scale semantic features more effectively. In addition, a sea-land segmentation dataset covering the Yellow Sea region (GF Dataset) is constructed through the processing of three scenes from Gaofen-6 remote sensing images. We perform a comprehensive experiment with the GF dataset to compare the proposed method with mainstream semantic segmentation models, and the results demonstrate that TCU-Net outperforms the competing models in all three evaluation indices: the PA (pixel accuracy), F1-score, and MIoU (mean intersection over union), while requiring significantly fewer parameters and computational resources compared to other models. These results indicate that the TCU-Net model proposed in this article can extract the shoreline from remote sensing images more effectively, with a shorter time, and lower computational overhead.",double-branch,sea-land segmentation,GF-6,CNN,transformer,"Du, Runfeng",,,,REMOTE SENSING,,remote sensing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_610,"Hao, Ting","Bai, Shuya","Wu, Tianyu","Zhang, Libao",Weakly Supervised Semantic Segmentation of Remote Sensing Images Based on Progressive Mining and Saliency-Enhanced Self-Attention,,2024,0,"Given the high demands of effort in generating pixel-level annotations, weakly supervised semantic segmentation (WSSS) has become an important approach for remote sensing image (RSI) interpretation. However, current methods are mostly borrowed from natural scene studies, regardless of the significant variation in object sizes as well as the highly confusing intraclass heterogeneity and interclass homogeneity which are characteristic of RSIs. In this letter, we propose a WSSS method based on progressive mining and saliency-enhanced self-attention (PMSA), to efficiently segment RSIs with image-level labels. First, we exploit multiscale orientation patterns to sufficiently extract the rich texture in RSIs which can help to discern between the different classes, and combine this information with contrast and luminance features to generate fine saliency maps. Second, we design a progressive mining process to gradually discover both the large objects, representative of semantics, and the small objects, rich in patterns. Finally, we employ self-attention mechanism to capture global dependencies in RSIs for refining category areas. To inhibit the misspread of attention, we use saliency as a mask discerning between the background and the object classes. Experiments on different datasets demonstrate the competence of the proposed method, in terms of both metrical results and visual effects.",Remote sensing,saliency detection,self-attention,weakly supervised semantic segmentation (WSSS),,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_611,"Luo, Zhenglian","He, Lingmin",,,A two-stage domain adaptive remote sensing image semantic segmentation network combined with self-training,,2024,1,"Unsupervised domain adaptive (UDA) is a popular technology to solve the semantic segmentation of remote sensing images. UDA can alleviate the need for large-scale pixel-level labeling by pre-training on the labeled source domain and verifying on the target domain. However, domain offset exists between different data sets, which will greatly affect the final performance of the semantic segmentation model. Based on this, this paper proposes a segmented interdomain adaptive network framework of dual-path image transformation, which combines self-training with interdomain adaptive segmentation. Firstly, the image style conversion is used to reduce the domain offset of the image level, and the dual-path method is used to promote each other to improve the image conversion quality and the pseudo-label quality of the target domain. Secondly, the pseudo-tags of the first-stage training are used to prioritize, and the target domain is divided into easy and difficult to partition for selective retraining. By adversarial learning, the difficult-to-segment features are aligned with easily-segmented features, and the influence of domain migration is gradually reduced by adaptive repetition in the domain, thus improving the segmentation performance of the network.",Unsupervised domain adaptive,Semantic segmentation of remote sensing images,Dual path image conversion,self-training,Adversarial learning,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_612,"Yin, Jichong","Wu, Fang","Qi, Yuyang",,Vector Mapping Method for Buildings in Remote Sensing Images Based on Joint Semantic-Geometric Learning,,2023,3,"An important high-precision building vector mapping method automatically delineates building polygons from high-resolution remote sensing images. Deep learning methods have greatly improved the accuracy of automatic building segmentation in remote sensing images. However, building polygons in vector forms have a compact and regular structured expression effect, which corresponds more with the application requirements of cartography and geographic information systems (GIS). We propose a vector mapping method for buildings in remote sensing images with joint semantic-geometric learning to generate building polygon vectors in remote sensing images automatically. The method, aiming to provide cartography and GIS data sources, consists of three modules: multi-task segmentation, contour regularization, and polygon optimization. To reduce missing extractions and mis-extractions and obtain a complete building segmentation mask, the multitask segmentation module performs joint semantic-geometric learning on three related tasks: building instance detection, pixel-wise contour segmentation, and edge extraction. The regularization module normalizes the segmentation mask expression using geometric constraints and image information, whereas the polygon optimization module combines geometric constraints with deep learning methods to ensure vectorization quality. The experimental results show that the proposed method adapts well to building vector extraction tasks under different scenarios and can generate building vector polygons that match the ground truth labels. This method offers significant advantages in solving problems, such as building polygon irregularity and vertex offset.",Contour regularization,joint semantic-geometric learning,multitask segmentation,polygon optimization,remote sensing images,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_613,"Zhang, Zhaoyang","Wang, Xuying","Mei, Xiaoming","Tao, Chao",FALSE: False Negative Samples Aware Contrastive Learning for Semantic Segmentation of High-Resolution Remote Sensing Image,,2022,15,"Self-supervised contrastive learning (SSCL) is a potential learning paradigm for learning remote sensing image (RSI)-invariant features through the label-free method. The existing SSCL of RSI is built based on constructing positive and negative sample pairs. However, due to the richness of RSI ground objects and the complexity of the RSI contextual semantics, the same RSI patches have the coexistence and imbalance of positive and negative samples, which causes the SSCL pushing negative samples far away while pushing positive samples far away, and vice versa. We call this the sample confounding issue (SCI). To solve this problem, we propose a False negAtive sampLes aware contraStive lEarning model (FALSE) for the semantic segmentation of high-resolution RSIs. Since SSCL pretraining is unsupervised, the lack of definable criteria for false negative sample (FNS) leads to theoretical undecidability, and we designed two steps to implement the FNS approximation determination: coarse determination of FNS and precise calibration of FNS. We achieve coarse determination of FNS by the FNS self-determination (FNSD) strategy and achieve calibration of FNS by the FNS confidence calibration (FNCC) loss function. Experimental results on three RSI semantic segmentation datasets demonstrated that the FALSE effectively improves the accuracy of the downstream RSI semantic segmentation task compared with the current three models, which represent three different types of SSCL models. The mean intersection over union (mIoU) on the ISPRS Potsdam dataset is improved by 0.7% on average; on the CVPR DGLC dataset, it is improved by 12.28% on average; and on the Xiangtan dataset, this is improved by 1.17% on average. This indicates that the SSCL model has the ability to self-differentiate FNS and that the FALSE effectively mitigates the SCI in SSCL.",Calibration,Semantic segmentation,Benchmark testing,Feature extraction,Remote sensing,"Li, Haifeng",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Semantics,Complexity theory,False negative sample (FNS),remote sensing image (RSI),self-supervised contrastive learning (SSCL),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_614,"Zhu, Qiqi","Wan, Jiangqin","Zhong, Yanfei","Guan, Qingfeng",TOPIC MODEL FOR REMOTE SENSING DATA: A COMPREHENSIVE REVIEW,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,2,"From text analysis to image interpretation, the topic model (TM) always plays an important role. With its powerful semantic mining capabilities, it is able to capture the latent spectral and spatial information from remote sensing (RS) images. Recent years have witnessed widespread use of TM to solve the problems in RS image interpretation, i.e., semantic segmentation, target detection, and scene classification. However, there has not yet been a study expatiating and summarizing the current situation of RS applications with TM. This paper intends to systematically summarize the application of TM in RS images and to conduct several typical experiments for comparison. Specifically, the architecture of our work can be explained as follows: 1) the theory of TM; 2) the applications of RS based on TM; 3) experimental analysis of typical TM methods to provide reference for further understanding, and 4) summary and prospects for guiding further research into TM for RS data.",Topic model,remote sensing image,scene classification,semantic segmentation,,"Zhang, Liangpei","Li, Deren",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_615,"Liu, Xiaohui","Zhang, Lei","Wang, Rui","Li, Xiaoyu",Cascaded CNN and global-local attention transformer network-based semantic segmentation for high-resolution remote sensing image,,JUL 1 2024,0,"High-resolution remote sensing images (HRRSIs) contain rich local spatial information and long-distance location dependence, which play an important role in semantic segmentation tasks and have received more and more research attention. However, HRRSIs often exhibit large intraclass variance and small interclass variance due to the diversity and complexity of ground objects, thereby bringing great challenges to a semantic segmentation task. In most networks, there are numerous small-scale object omissions and large-scale object fragmentations in the segmentation results because of insufficient local feature extraction and low global information utilization. A network cascaded by convolution neural network and global-local attention transformer is proposed called CNN-transformer cascade network. First, convolution blocks and global-local attention transformer blocks are used to extract multiscale local features and long-range location information, respectively. Then a multilevel channel attention integration block is designed to fuse geometric features and semantic features of different depths and revise the channel weights through the channel attention module to resist the interference of redundant information. Finally, the smoothness of the segmentation is improved through the implementation of upsampling using a deconvolution operation. We compare our method with several state-of-the-art methods on the ISPRS Vaihingen and Potsdam datasets. Experimental results show that our method can improve the integrity and independence of multiscale objects segmentation results.",high-resolution remote sensing images,semantic segmentation,convolution neural network,transformer,global-local attention transformer block,"Xu, Jiyang","Lu, Xiaochen",,,JOURNAL OF APPLIED REMOTE SENSING,,multilevel channel attention integration block,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_616,"Xi, Mengfei","Li, Jie","He, Zhilin","Yu, Minmin",NRN-RSSEG: A Deep Neural Network Model for Combating Label Noise in Semantic Segmentation of Remote Sensing Images,,JAN 2023,3,"The performance of deep neural networks depends on the accuracy of labeled samples, as they usually contain label noise. This study examines the semantic segmentation of remote sensing images that include label noise and proposes an anti-label-noise network framework, termed Labeled Noise Robust Network in Remote Sensing Image Semantic Segmentation (NRN-RSSEG), to combat label noise. The algorithm combines three main components: network, attention mechanism, and a noise-robust loss function. Three different noise rates (containing both symmetric and asymmetric noise) were simulated to test the noise resistance of the network. Validation was performed in the Vaihingen region of the ISPRS Vaihingen 2D semantic labeling dataset, and the performance of the network was evaluated by comparing the NRN-RSSEG with the original U-Net model. The results show that NRN-RSSEG maintains a high accuracy on both clean and noisy datasets. Specifically, NRN-RSSEG outperforms UNET in terms of PA, MPA, Kappa, Mean_F1, and FWIoU in the presence of noisy datasets, and as the noise rate increases, each performance of UNET shows a decreasing trend while the performance of NRN-RSSEG decreases slowly and some performances show an increasing trend. At a noise rate of 0.5, the PA (-6.14%), MPA (-4.27%) Kappa (-8.55%), Mean_F1 (-5.11%), and FWIOU (-9.75%) of UNET degrade faster; while the PA (-2.51%), Kappa (-3.33%), and FWIoU of NRN-RSSEG (-3.26) degraded more slowly, MPA (+1.41) and Mean_F1 (+2.69%) showed an increasing trend. Furthermore, comparing the proposed model with the baseline method, the results demonstrate that the proposed NRN-RSSEG anti-noise framework can effectively help the current segmentation model to overcome the adverse effects of noisy label training.",remote sensing image segmentation,noisy labels,deep learning,noise-robust network,,"Qin, Fen",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_617,"Xu, LeiLei","Shi, ShanQiu","Liu, YuJun","Zhang, Hao",A large-scale remote sensing scene dataset construction for semantic segmentation,,OCT 2 2023,5,"As fuelled by the advancement of deep learning for computer vision tasks, its application in other fields has been boosted. This technology has been increasingly applied to the interpretation of remote sensing image, showing high potential economic and societal significance, such as automatically mapping land cover. However, the model requires a considerable number of samples for training, and it is now adversely affected by the lack of a large-scale dataset. Moreover, labelling samples is a time-consuming and laborious task, and a complete land classification system suitable for deep learning has not been established. This limitation hinders the development and application of deep learning. To meet the data needs of deep learning in the field of remote sensing, this study develops JSsampleP, a large-scale dataset for segmentation, generating 110,170 data samples that cover various categories of scenes within Jiangsu Province, China. The existing Geographical Condition Dataset (GCD) and Basic Surveying and Mapping Dataset (BSMD) in Jiangsu were fully utilised, significantly reducing the cost of labelling samples. Furthermore, the samples were subject to a rigorous cleaning process to ensure data quality. Finally, the accuracy of the dataset is verified using the U-Net model, and the future version will be optimised continuously.",Deep learning,CNN,sample,segmentation,remote sensing,"Wang, Dan","Zhang, Lu","Liang, Wan","Chen, Hao",INTERNATIONAL JOURNAL OF IMAGE AND DATA FUSION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_618,"Wang, Ming","She, Anqi","Chang, Hao","Cheng, Feifei",A deep inverse convolutional neural network-based semantic classification method for land cover remote sensing images,,MAR 27 2024,5,"The imbalance of land cover categories is a common problem. Some categories appear less frequently in the image, while others may occupy the vast majority of the proportion. This imbalance can lead the classifier to tend to predict categories with higher frequency of occurrence, while the recognition effect on minority categories is poor. In view of the difficulty of land cover remote sensing image multi-target semantic classification, a semantic classification method of land cover remote sensing image based on depth deconvolution neural network is proposed. In this method, the land cover remote sensing image semantic segmentation algorithm based on depth deconvolution neural network is used to segment the land cover remote sensing image with multi-target semantic segmentation; Four semantic features of color, texture, shape and size in land cover remote sensing image are extracted by using the semantic feature extraction method of remote sensing image based on improved sequential clustering algorithm; The classification and recognition method of remote sensing image semantic features based on random forest algorithm is adopted to classify and identify four semantic feature types of land cover remote sensing image, and realize the semantic classification of land cover remote sensing image. The experimental results show that after this method classifies the multi-target semantic types of land cover remote sensing images, the average values of Dice similarity coefficient and Hausdorff distance are 0.9877 and 0.9911 respectively, which can accurately classify the multi-target semantic types of land cover remote sensing images.",Deep inverse convolutional neural network,Land cover,Remote sensing images,Semantic classification,Semantic segmentation,"Yang, Heming",,,,SCIENTIFIC REPORTS,,Feature extraction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_619,"Zeng, Fuping","Yang, Bin","Zhao, Mengci","Xing, Ying",MASANet: Multi-Angle Self-Attention Network for Semantic Segmentation of Remote Sensing Images,,OCT 2022,5,"As an important research direction in the field of pattern recognition, semantic segmentation has become an important method for remote sensing image information extraction. However, due to the loss of global context information, the effect of semantic segmentation is still incomplete or misclassified. In this paper, we propose a multi-angle self-attention network (MASANet) to solve this problem. Specifically, we design a multi-angle self-attention module to enhance global context information, which uses three angles to enhance features and takes the obtained three features as the inputs of self-attention to further extract the global dependencies of features. In addition, atrous spatial pyramid pooling (ASPP) and global average pooling (GAP) further improve the overall performance. Finally, we concatenate the feature maps of different scales obtained in the feature extraction stage with the corresponding feature maps output by ASPP to further extract multi-scale features. The experimental results show that MASANet achieves good segmentation performance on high-resolution remote sensing images. In addition, the comparative experimental results show that MASANet is superior to some state-of-the-art models in terms of some widely used evaluation criteria.",global context information,MASANet,multi -scale features,semantic segmentation,,"Ma, Yiran",,,,TEHNICKI VJESNIK-TECHNICAL GAZETTE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_620,"Blaga, Bianca-Cerasela-Zelia","Nedevschi, Sergiu",,,Semantic Segmentation of Remote Sensing Images With Transformer-Based U-Net and Guided Focal-Axial Attention,,2024,0,"In the field of remote sensing, semantic segmentation of unmanned aerial vehicle (UAV) imagery is crucial for tasks such as land resource management, urban planning, precision agriculture, and economic assessment. Traditional methods use convolutional neural networks (CNNs) for hierarchical feature extraction but are limited by their local receptive fields, restricting comprehensive contextual understanding. To overcome these limitations, we propose a combination of transformer and attention mechanisms to improve object classification, leveraging their superior information modeling capabilities to enhance scene understanding. In this article, we present Swin-based focal axial attention network (SwinFAN), a U-Net framework featuring a Swin transformer as encoder, equipped with a novel decoder that introduces two new components for enhanced semantic segmentation of urban remote sensing images. The first proposed component is a guided focal-axial (GFA) attention module that combines local and global contextual information, enhancing the model's ability to discern intricate details and complex structures. The second component is an innovative attention-based feature refinement head (AFRH) designed to improve the precision and clarity of segmentation outputs through self-attention and convolutional techniques. Comprehensive experiments demonstrate that the accuracy of our proposed architecture significantly outperforms state-of-the-art models. More specifically, our method achieves mean intersection over union (mIoU) improvements of 1.9% on UAVid, 3.6% on Potsdam, 1.9% on Vaihingen, and 0.8% on LoveDA.",Transformers,Semantic segmentation,Remote sensing,Decoding,Computer architecture,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Computational modeling,Head,Context modeling,Accuracy,Feature extraction,Deep learning,global-local attention,remote sensing,semantic segmentation,,,,,,,,,,,,,transformer network,,,,,,,,
Row_621,"Chen, Fenglei","Liu, Haijun","Zeng, Zhihong","Zhou, Xichuan",BES-Net: Boundary Enhancing Semantic Context Network for High-Resolution Image Semantic Segmentation,,APR 2022,23,"This paper focuses on the high-resolution (HR) remote sensing images semantic segmentation task, whose goal is to predict semantic labels in a pixel-wise manner. Due to the rich complexity and heterogeneity of information in HR remote sensing images, the ability to extract spatial details (boundary information) and semantic context information dominates the performance in segmentation. In this paper, based on the frequently used fully convolutional network framework, we propose a boundary enhancing semantic context network (BES-Net) to explicitly use the boundary to enhance semantic context extraction. BES-Net mainly consists of three modules: (1) a boundary extraction module for extracting the semantic boundary information, (2) a multi-scale semantic context fusion module for fusing semantic features containing objects with multiple scales, and (3) a boundary enhancing semantic context module for explicitly enhancing the fused semantic features with the extracted boundary information to improve the intra-class semantic consistency, especially in those pixels containing boundaries. Extensive experimental evaluations and comprehensive ablation studies on the ISPRS Vaihingen and Potsdam datasets demonstrate the effectiveness of BES-Net, yielding an overall improvement of 1.28/2.36/0.72 percent in mF1/mIoU/OA over FCN_8s when the BE and MSF modules are combined by the BES module. In particular, our BES-Net achieves a state-of-the-art performance of 91.4% OA on the ISPRS Vaihingen dataset and 92.9%/91.5% mF1/OA on the ISPRS Potsdam dataset.",remote sensing images,semantic segmentation,boundary enhancing semantic context,fully convolutional network,,"Tan, Xiaoheng",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_622,"Cui, Jian","Liu, Jiahang","Ni, Yue","Wang, Jinjin",FDGSNet: A Multimodal Gated Segmentation Network for Remote Sensing Image Based on Frequency Decomposition,,2024,0,"Multiple modal data fusion can provide valuable and diverse information for remote sensing image segmentation. However, the existing fusion methods often lead to feature loss during the fusion of various modal data, and the complementarity among multimodal features is insufficient. To address these problems, we propose a multimodal gated segmentation network for remote sensing images based on the frequency decomposition. Complementary information from multimodal features is extracted by establishing a long-distance correlation between the low-frequency components of different modal data. In addition, high-frequency detailed features of different modal data are preserved by residual connection. The adaptive gated fusion method is then used to control the information flow between the complementary information and each modality feature map, enabling adaptive fusion between multimodal features. These operations can effectively improve the adaptability of the proposed method in various scenarios and data changes. Extensive experiments demonstrate that the proposed method has good effectiveness, robustness, and generalization and achieved state-of-the-art performance in several remote sensing image semantic segmentation tasks.",Feature extraction,Remote sensing,Semantic segmentation,Semantics,Data mining,"Li, Manchun",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Accuracy,Transformers,Logic gates,Vegetation mapping,Sensors,Frequency-domain decomposition,multimodal,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_623,"Li, Bingnan","Gao, Jiuchong","Chen, Shuiping","Lim, Samsung",POI Detection of High-Rise Buildings Using Remote Sensing Images: A Semantic Segmentation Method Based on Multitask Attention Res-U-Net,,2022,14,"A point-of-interest (POI) represents a specific point location that may be useful or interesting for people, and therefore, each and every building footprint in a topographic map can be recognized as a POI. Automatic extraction of building footprints using remote sensing images has become a challenging and important research topic, which is in demand for urban planning and development. Extensive studies have explored a variety of semantic segmentation methods using deep learning algorithms to achieve better performance in building footprint extraction; however, the existing algorithms were shown to have some limitations, which lead to poor segmentation results. Building roofs were recognized as building footprints in the previous studies. This is prone to error especially for high-rise buildings due to different sensor view angles. In this article, we propose a multitask Res-U-Net model with an attention mechanism for the extraction of the building roofs and the whole building shapes from remote sensing images and then use an offset vector method to detect the footprints of the high-rise buildings based on the boundaries of the corresponding building roofs and shapes. We also apply the online food delivery (OFD) data to parse the POI name of every building footprint. Several strategies are also developed in combination with the proposed model, including data augmentation and postprocessing. We conduct numerical experiments using real data of remote sensing images and OFD historical order data. Results demonstrate that our proposed model achieves a total F1-score of 77.05% and an intersection over union (IoU) of 63.55% in terms of the building roof segmentation, and an overall F1-score of 79.02% and an IoU of 66.05% for the whole building shape segmentation, which both achieve the best performance among all baseline models.",Buildings,Remote sensing,Shape,Image segmentation,Logic gates,"Jiang, Hai",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Global Positioning System,Multitasking,Attention mechanism,building footprint extraction,online food delivery (OFD),remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_624,"Lin, Yuting","Suzuki, Kumiko","Sogo, Shinichiro",,Practical Techniques for Vision-Language Segmentation Model in Remote Sensing,"MID-TERM SYMPOSIUM THE ROLE OF PHOTOGRAMMETRY FOR A SUSTAINABLE WORLD, VOL. 48-2",2024,0,"Traditional semantic segmentation models often struggle with poor generalizability in zero-shot scenarios such as recognizing attributes unseen in the training labels. On the other hands, language-vision models (VLMs) have shown promise in improving performance on zero-shot tasks by leveraging semantic information from textual inputs and fusing this information with visual features. However, existing VLM-based methods do not perform as effectively on remote sensing data due to the lack of such data in their training datasets. In this paper, we introduce a two-stage fine-tuning approach for a VLM-based segmentation model using a large remote sensing image-caption dataset, which we created using an existing image-caption model. Additionally, we propose a modified decoder and a visual prompt technique using a saliency map to enhance segmentation results. Through these methods, we achieve superior segmentation performance on remote sensing data, demonstrating the effectiveness of our approach.",Segmentation of Remote Sensing Data,Vision-Language Model,Fine-tuning,Visual Prompting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_625,"Li, Rui","Wang, Libo","Zhang, Ce","Duan, Chenxi",A2-FPN for semantic segmentation of fine-resolution remotely sensed images,,FEB 1 2022,74,"The thriving development of earth observation technology makes more and more high-resolution remote-sensing images easy to obtain. However, caused by fine-resolution, the huge spatial and spectral complexity leads to the automation of semantic segmentation becoming a challenging task. Addressing such an issue represents an exciting research field, which paves the way for scene-level landscape pattern analysis and decision-making. To tackle this problem, we propose an approach for automatic land segmentation based on the Feature Pyramid Network (FPN). As a classic architecture, FPN can build a feature pyramid with high-level semantics throughout. However, intrinsic defects in feature extraction and fusion hinder FPN from further aggregating more discriminative features. Hence, we propose an Attention Aggregation Module (AAM) to enhance multiscale feature learning through attention-guided feature aggregation. Based on FPN and AAM, a novel framework named Attention Aggregation Feature Pyramid Network (A(2)-FPN) is developed for semantic segmentation of fine-resolution remotely sensed images. Extensive experiments conducted on four datasets demonstrate the effectiveness of our A(2)-FPN in segmentation accuracy. Code is available at .",semantic segmentation,deep learning,attention mechanism,,,"Zheng, Shunyi",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_626,"Qu, Tingting","Xu, Jindong","Chong, Qianpeng","Liu, Zhaowei",Fuzzy neighbourhood neural network for high-resolution remote sensing image segmentation,,DEC 31 2023,9,"Remote sensing image segmentation plays an important role in many industrial-grade image processing applications. However, the problem of uncertainty caused by intraclass heterogeneity and interclass blurring is prevalent in high-resolution remote sensing images. Moreover, the complexity of information in high-resolution remote sensing images leads to a large amount of background information around objects. To solve this problem, a new fuzzy convolutional neural network is proposed in this paper. This network resolves the ambiguity and uncertainty of feature information by introducing a fuzzy neighbourhood module in the deep learning network structure. In addition, it adds a multi-attention gating module to highlight small object features and separate them from the complex background information to achieve fine segmentation of high-resolution remote sensing images. Experimental results on three different segmentation datasets suggest that the proposed method has higher segmentation accuracy and better performance than other deep learning networks, especially for complicated shadow information.",Fuzzy neighbourhood,high-resolution remote sensing image,image segmentation,multi-attention gating,,"Yan, Weiqing","Wang, Xuan","Song, Yongchao","Ni, Mengying",EUROPEAN JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_627,"Long, Jiang","Li, Mengmeng","Wang, Xiaoqin","Stein, Alfred",Semantic change detection using a hierarchical semantic graph interaction network from high-resolution remote sensing images,,MAY 2024,10,"Current semantic change detection (SCD) methods face challenges in modeling temporal correlations (TCs) between bitemporal semantic features and difference features. These methods lead to inaccurate detection results, particularly for complex SCD scenarios. This paper presents a hierarchical semantic graph interaction network (HGINet) for SCD from high-resolution remote sensing images. This multitask neural network combines semantic segmentation and change detection tasks. For semantic segmentation, we construct a multilevel perceptual aggregation network with a pyramidal architecture. It extracts semantic features that discriminate between different categories at multiple levels. We model the correlations between bitemporal semantic features using a TC module that enhances the identification of unchanged areas. For change detection, we design a semantic difference interaction module based on a graph convolutional network. It measures the interactions among bitemporal semantic features, their corresponding difference features, and the combination of both. Extensive experiments on four datasets, namely SECOND, HRSCD, Fuzhou, and Xiamen, show that HGINet performs better in identifying changed areas and categories across various scenarios and regions than nine existing methods. Compared with the existing methods applied on the four datasets, it achieves the highest F1scd values of 59.48%, 64.12%, 64.45%, and 84.93%, and SeK values of 19.34%, 14.55%, 18.28%, and 51.12%, respectively. Moreover, HGINet mitigates the influence of fake changes caused by seasonal effects, producing results with well-delineated boundaries and shapes. Furthermore, HGINet trained on the Fuzhou dataset is successfully transferred to the Xiamen dataset, demonstrating its effectiveness and robustness in identifying changed areas and categories from high-resolution remote sensing images. The code of our paper is accessible at https://github.com/long123524/HGINet-torch.",Semantic change detection,Hierarchical semantic graph interaction network,High-resolution remote sensing images,Temporal correlations,Semantic difference interaction,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_628,"Song, Zhenbo","Zhang, Zhenyuan","Fang, Feiyi","Fan, Zhaoxin",Deep semantic-aware remote sensing image deblurring,,OCT 2023,7,"This paper addresses the problem of blind deblurring of single remote sensing (RS) images with deep neural networks. Most existing deep learning-based methods are migrated from natural image deblurring models, disregarding the domain gap to remote sensing images. Besides, the image deblurring problem is typically considered as an independent low-level image pre-processing, taking no account of downstream tasks, such as classification and segmentation. In this paper, we first present a novel decoder with a par-allel fusion stream for fusing multi-scale RS features and expanding the receptive field. Then, to generate high-quality sharp RS images, we propose to calculate the perceptual loss on an RS-pre-trained network instead of computing on VGG19 pre-trained on natural images i.e. ImageNet. For bridging the RS im-age deblurring results to the downstream recognition tasks, we further propose a semantic loss, which is calculated on the last-layer feature map of an RS segmentation network. With extensive experiments con-ducted on public RS image datasets, we demonstrate that the proposed method improves results for RS image deblurring and achieves competitive performance both qualitatively and quantitatively. Moreover, downstream recognition experiments validate the superior quality of the recovered images over existing methods.& COPY; 2023 Elsevier B.V. All rights reserved.",Remote sensing image deblurring,Deep learning,Semantic supervision,,,"Lu, Jianfeng",,,,SIGNAL PROCESSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_629,"Wang, Bing","Wang, Zhirui","Sun, Xian","Wang, Hongqi",DMML-Net: Deep Metametric Learning for Few-Shot Geographic Object Segmentation in Remote Sensing Imagery,,2022,25,"Geographic object segmentation is a fundamental yet challenging problem for remote sensing image interpretation. The prevalent paradigm to solve this problem is to train deep neural networks on massive labeled samples. Although remarkable achievements have been attained, these methods suffer from the severe dependence on the large-scale dataset and require a long training process with high computation burden. To address these issues, a deep metametric learning framework, named DMML-Net, consisting of the metametric learner and the base-metric learner, is proposed for few-shot geographic object segmentation. First, DMML-Net formulates the segmentation as the metric-based pixel classification and develops a deep feature pyramid comparison network as the architecture of the metric learner for multiscale metric learning. Benefiting from this design, the segmentation can be efficiently solved, as well as being robust to deal with the scale variations of geographic objects. Second, an affinity-based fusion mechanism is introduced to adaptively reweight and fuse the semantic information across samples, effectively calibrating the deviation of prototypes induced by the intraclass variations. Third, considering the impact of the large interclass distribution divergences, DMML-Net presents a metametric training paradigm to provide the metric model with flexible scalability for fast adaptation to novel tasks. After metatraining, DMML-Net can be applied for the few-shot segmentation tasks of novel geographic objects with only a few gradient steps on the small training set. Experimental results on two benchmark remote sensing datasets demonstrate the validity and the superiority of our method in low-shot conditions where there are only one to ten labeled samples.",Task analysis,Measurement,Remote sensing,Training,Semantics,"Fu, Kun",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Object segmentation,Image segmentation,Few-shot learning,few-shot segmentation,metalearning,metric learning,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_630,"Deng, Guohui","Wu, Zhaocong","Wang, Chengjun","Xu, Miaozhong",CCANet: Class-Constraint Coarse-to-Fine Attentional Deep Network for Subdecimeter Aerial Image Semantic Segmentation,,2022,46,"Semantic segmentation is important for the understanding of subdecimeter aerial images. In recent years, deep convolutional neural networks (DCNNs) have been used widely for semantic segmentation in the field of remote sensing. However, because of the highly complex subdecimeter resolution of aerial images, inseparability often occurs among some geographic entities of interest in the spectral domain. In addition, the semantic segmentation methods based on DCNNs mostly obtain context information using extra information within the added receptive field. However, the context information obtained this way is not explicit. We propose a novel class-constraint coarse-to-fine attentional (CCA) deep network, which enables the formation of class information constraints to obtain explicit long-range context information. Further, the performance of subdecimeter aerial image semantic segmentation can be improved, particularly for fine-structured geographic entities. Based on coarse-to-fine technology, we obtained a coarse segmentation result and constructed an image class feature library. We propose the use of the attention mechanism to obtain strong class-constrained features. Consequently, pixels of different geographic entities can adaptively match the corresponding categories in the class feature library. Additionally, we employed a novel loss function, CCA-loss to realize end-to-end training. The experimental results obtained using two popular open benchmarks, International Society for Photogrammetry and Remote Sensing (ISPRS) 2-D semantic labeling Vaihingen data set and Institute of Electrical and Electronics Engineers (IEEE) Geoscience and Remote Sensing Society (GRSS) Data Fusion Contest Zeebrugge data set, validated the effectiveness and superiority of our proposed model. The proposed method achieved state-of-the-art performance on the IEEE GRSS Data Fusion Contest Zeebrugge data set.",Image segmentation,Semantics,Remote sensing,Libraries,Feature extraction,"Zhong, Yanfei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Automobiles,Training,Aerial imagery,context information,deep convolutional neural network (DCNN),remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_631,"Yu, Dawen","Ji, Shunping",,,Long-Range Correlation Supervision for Land-Cover Classification From Remote Sensing Images,,2023,0,"Long-range dependency modeling has been widely considered in modern deep learning-based semantic segmentation methods, especially those designed for large-size remote sensing images, to compensate the intrinsic locality of standard convolutions. However, in previous studies, the long-range dependency, modeled with an attention mechanism or transformer model, has been based on unsupervised learning, instead of explicit supervision from the objective ground truth (GT). In this article, we propose a novel supervised long-range correlation method for land-cover classification, called the supervised long-range correlation network (SLCNet), which is shown to be superior to the currently used unsupervised strategies. In SLCNet, pixels sharing the same category are considered highly correlated and those having different categories are less relevant, which can be easily supervised by the category consistency information available in the GT semantic segmentation map. Under such supervision, the recalibrated features are more consistent for pixels of the same category and more discriminative for pixels of other categories, regardless of their proximity. To complement the detailed information lacking in the global long-range correlation, we introduce an auxiliary adaptive receptive field feature extraction (ARFE) module, parallel to the long-range correlation module in the encoder, to capture finely detailed feature representations for multisize objects in multiscale remote sensing images. In addition, we apply multiscale side-output supervision and a hybrid loss function as local and global constraints to further boost the segmentation accuracy. Experiments were conducted on three public remote sensing datasets (the ISPRS Vaihingen dataset, the ISPRS Potsdam dataset, and the DeepGlobe dataset). Compared with the advanced segmentation methods from the computer vision, medicine, and remote sensing communities, the proposed SLCNet method achieved state-of-the-art performance on all the datasets. The code will be made available at gpcv.whu.edu.cn/data.",Correlation,Transformers,Remote sensing,Feature extraction,Semantics,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantic segmentation,Computational modeling,Convolutional neural network (CNN),land-cover classification,long-range correlation supervision,remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_632,"Yang, Yunsong","Yuan, Genji","Li, Jinjiang",,Correlated Mapping Attention Cooperative Network for Urban Remote Sensing Image Segmentation,,2024,0,"In current remote sensing segmentation tasks, the difficulty of segmenting spectrally similar objects is a significant issue. Solving this problem is crucial for improving segmentation accuracy. Traditional image-domain segmentation methods rely on color and texture features, but spectrally similar objects have negligible color differences, leading to suboptimal segmentation results. To address this, we propose a network framework called Correlated Mapping Attention Cooperative Network (CMACNet) by extending the problem from the image domain to the feature domain. Image-domain methods depend on color and texture features, whereas feature-domain methods process higher-level abstract features, avoiding issues caused by color similarity. Specifically, CMACNet first employs an autoencoder structure. The autoencoder compresses the input data and attempts to reconstruct the original data, ensuring that the latent space representations capture essential and representative features of the input data, thereby extracting highly generalized and versatile features. Next, we introduce the correlated mapping attention mechanism, which adaptively adjusts the attention to different features based on their correlations, effectively addressing the challenge of segmenting spectrally similar objects. Furthermore, to efficiently establish global relationships among features, we design a cross global interaction layer for global feature remapping. Comprehensive experiments on the Vaihingen and Potsdam datasets demonstrate that CMACNet outperforms existing state-of-the-art methods, achieving mean intersection over union scores of 84.77% and 87.69%, respectively.",Attention mechanism,global modeling,remote sensing (RS),semantic segmentation,transformer,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Attention mechanism,global modeling,remote sensing (RS),semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,,,
Row_633,"Ajibola, Segun","Cabral, Pedro",,,A Systematic Literature Review and Bibliometric Analysis of Semantic Segmentation Models in Land Cover Mapping,,JUN 2024,2,"Recent advancements in deep learning have spurred the development of numerous novel semantic segmentation models for land cover mapping, showcasing exceptional performance in delineating precise boundaries and producing highly accurate land cover maps. However, to date, no systematic literature review has comprehensively examined semantic segmentation models in the context of land cover mapping. This paper addresses this gap by synthesizing recent advancements in semantic segmentation models for land cover mapping from 2017 to 2023, drawing insights on trends, data sources, model structures, and performance metrics based on a review of 106 articles. Our analysis identifies top journals in the field, including MDPI Remote Sensing, IEEE Journal of Selected Topics in Earth Science, and IEEE Transactions on Geoscience and Remote Sensing, IEEE Geoscience and Remote Sensing Letters, and ISPRS Journal Of Photogrammetry And Remote Sensing. We find that research predominantly focuses on land cover, urban areas, precision agriculture, environment, coastal areas, and forests. Geographically, 35.29% of the study areas are located in China, followed by the USA (11.76%), France (5.88%), Spain (4%), and others. Sentinel-2, Sentinel-1, and Landsat satellites emerge as the most used data sources. Benchmark datasets such as ISPRS Vaihingen and Potsdam, LandCover.ai, DeepGlobe, and GID datasets are frequently employed. Model architectures predominantly utilize encoder-decoder and hybrid convolutional neural network-based structures because of their impressive performances, with limited adoption of transformer-based architectures due to its computational complexity issue and slow convergence speed. Lastly, this paper highlights existing key research gaps in the field to guide future research directions.",remote sensing,semantic segmentation,land cover mapping,deep learning,land cover classification,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_634,"Pastorino, Martina","Moser, Gabriele","Serpico, Sebastiano B.","Zerubia, Josiane",Semantic Segmentation of Remote-Sensing Images Through Fully Convolutional Neural Networks and Hierarchical Probabilistic Graphical Models,,2022,20,"Deep learning (DL) is currently the dominant approach to image classification and segmentation, but the performances of DL methods are remarkably influenced by the quantity and quality of the ground truth (GT) used for training. In this article, a DL method is presented to deal with the semantic segmentation of very-high-resolution (VHR) remote-sensing data in the case of scarce GT. The main idea is to combine a specific type of deep convolutional neural networks (CNNs), namely fully convolutional networks (FCNs), with probabilistic graphical models (PGMs). Our method takes advantage of the intrinsic multiscale behavior of FCNs to deal with multiscale data representations and to connect them to a hierarchical Markov model (e.g., making use of a quadtree). As a consequence, the spatial information present in the data is better exploited, allowing a reduced sensitivity to GT incompleteness to be obtained. The marginal posterior mode (MPM) criterion is used for inference in the proposed framework. To assess the capabilities of the proposed method, the experimental validation is conducted with the ISPRS 2D Semantic Labeling Challenge datasets on the cities of Vaihingen and Potsdam, with some modifications to simulate the spatially sparse GTs that are common in real remote-sensing applications. The results are quite significant, as the proposed approach exhibits a higher producer accuracy than the standard FCNs considered and especially mitigates the impact of scarce GTs on minority classes and small spatial details.",Semantics,Markov processes,Spatial resolution,Remote sensing,Image segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Deep learning,Convolutional neural network (CNN),fully convolutional network (FCN),hierarchical Markov model,multiscale analysis,probabilistic graphical model (PGM),remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_635,"Zhang, Bo","Chen, Tao","Wang, Bin",,Curriculum-Style Local-to-Global Adaptation for Cross-Domain Remote Sensing Image Segmentation,,2022,36,"Although domain adaptation has been extensively studied in natural image-based segmentation tasks, the research on cross-domain segmentation for very-high-resolution (VHR) remote sensing images (RSIs) still remains underexplored. The VHR RSI-based cross-domain segmentation mainly faces two critical challenges: 1) large area land covers with many diverse object categories bring severe local patch-level data distribution deviations, thus yielding different adaptation difficulties for different local patches and 2) different VHR sensor types or dynamically changing modes cause the VHR images to go through intensive data distribution differences even for the same geographical location, resulting in different global feature-level domain gaps. To address these challenges, we propose a curriculum-style local-to-global cross-domain adaptation framework for the segmentation of VHR RSIs. The proposed curriculum-style adaptation performs the adaptation process in an easy-to-hard way according to the adaptation difficulties that can be obtained using an entropy-based score for each patch of the target domain and, thus, well aligns the local patches in a domain image. The proposed local-to-global adaptation performs the feature alignment process from the locally semantic to globally structural feature discrepancies and consists of a semantic-level domain classifier and an entropy-level domain classifier that can reduce the above cross-domain feature discrepancies. Extensive experiments have been conducted in various cross-domain scenarios, including geographic location variations and imaging mode variations, and the experimental results demonstrate that the proposed method can significantly boost the domain adaptability of segmentation networks for VHR RSIs.",Image segmentation,Semantics,Imaging,Adaptation models,Task analysis,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Feature extraction,Remote sensing,Curriculum-style cross-domain adaptation (CCDA),local-to-global cross-domain adaptation,remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_636,"Xu, Yufen","Zhou, Shangbo","Huang, Yuhui",,Transformer-Based Model with Dynamic Attention Pyramid Head for Semantic Segmentation of VHR Remote Sensing Imagery,,NOV 2022,4,"Convolutional neural networks have long dominated semantic segmentation of very-high-resolution (VHR) remote sensing (RS) images. However, restricted by the fixed receptive field of convolution operation, convolution-based models cannot directly obtain contextual information. Meanwhile, Swin Transformer possesses great potential in modeling long-range dependencies. Nevertheless, Swin Transformer breaks images into patches that are single-dimension sequences without considering the position loss problem inside patches. Therefore, Inspired by Swin Transformer and Unet, we propose SUD-Net (Swin transformer-based Unet-like with Dynamic attention pyramid head Network), a new U-shaped architecture composed of Swin Transformer blocks and convolution layers simultaneously through a dual encoder and an upsampling decoder with a Dynamic Attention Pyramid Head (DAPH) attached to the backbone. First, we propose a dual encoder structure combining Swin Transformer blocks and reslayers in reverse order to complement global semantics with detailed representations. Second, aiming at the spatial loss problem inside each patch, we design a Multi-Path Fusion Model (MPFM) with specially devised Patch Attention (PA) to encode position information of patches and adaptively fuse features of different scales through attention mechanisms. Third, a Dynamic Attention Pyramid Head is constructed with deformable convolution to dynamically aggregate effective and important semantic information. SUD-Net achieves exceptional results on ISPRS Potsdam and Vaihingen datasets with 92.51%mF1, 86.4%mIoU, 92.98%OA, 89.49%mF1, 81.26%mIoU, and 90.95%OA, respectively.",swin transformer,remote sensing,semantic segmentation,dynamic attention pyramid head,,,,,,ENTROPY,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_637,Cai Mi,Cui Yaqi,Lv Yafei,Zhang Jing,A New Network Structure for Semantic Segmentation of Ship Targets in Remote Sensing,2019 22ND INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2019),2019,0,"Accurate detection of ship targets is a research hotspot in computer vision. Most of the researches have achieved instance-level detection in the way of bounding box. But we intend to achieve more accurate detection of ship targets in pixel-level through semantic segmentation. However, there are still two main challenges: the first one is the difficulty to segment small targets caused by the difference among ship targets' scales, and the other one is the lack of localization information caused by insufficient recovery ability of the decoder part. In this paper, we propose an effective solution. First, a multi-scale pooling fusion module is proposed to fuse multi-scale feature maps and acquire more multi-scale context information, then we improve the capability of precise decoding by taking the place of convolution operation with deconvolution in the decoder part to gather more localization information. At last, we integrate above two schemes into an encoder-decoder symmetry training network with less training parameters and less training time. Furthermore, we construct a dataset for ship semantic segmentation called HRSC2016-SS by labeling HRSC2016 dataset to evaluate our solution. Experiments show that comparing with the existing methods, our proposed solution has a better performance.",HRSC2016-SS,multi-scale,deconvolution,remote sensing,semantic segmentation,Xiong Wei,Pei Jiazheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_638,"Ahlswede, Steve","Madam, Nimisha Thekke","Schulz, Christian","Kleinschmit, Birgit",WEAKLY SUPERVISED SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES FOR TREE SPECIES CLASSIFICATION BASED ON EXPLANATION METHODS,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,1,"The collection of a high number of pixel-based labeled training samples for tree species identification is time consuming and costly in operational forestry applications. To address this problem, in this paper we investigate the effectiveness of explanation methods for deep neural networks in performing weakly supervised semantic segmentation using only image-level labels. Specifically, we consider four methods: i) class activation maps (CAM); ii) gradient-based CAM; iii) pixel correlation module; and iv) self-enhancing maps (SEM). We compare these methods with each other using both quantitative and qualitative measures of their segmentation accuracy, as well as their computational requirements. Experimental results obtained on an aerial image archive show that: i) considered explanation techniques are highly relevant for the identification of tree species with weak supervision; and ii) the SEM outperforms the other considered methods. The code for this paper is publicly available at https://git.tu-berlin.de/rsim/rs_wsss.",Tree species mapping,weakly supervised learning,semantic segmentation,explanation methods,remote sensing,"Demir, Begum",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_639,"Jia, Peiyan","Chen, Chen","Zhang, Delong","Sang, Yulong",Semantic segmentation of deep learning remote sensing images based on band combination principle: Application in urban planning and land use,,MAR 1 2024,7,"This study investigates the relevance of semantic segmentation of remote sensing images in urban planning and land use. We introduce a novel deep learning model that leverages the principle of band combination in remote sensing images to enhance the efficiency and accuracy of semantic segmentation. Our research focuses not only on advancing the segmentation capabilities of remote sensing images but also on applying this technology in urban planning and land use to foster sustainable development in smart cities. By integrating the band combination principle into the convolution operation, our approach improves feature extraction, thereby enhancing the quality of semantic segmentation in remote sensing images. This method outperforms traditional remote sensing image analysis techniques by combining automatic feature learning and the generalization capabilities of deep learning, thereby improving the segmentation model's performance. A unique aspect of this study is the direct application of remote sensing image segmentation in urban planning and land use. Our model accurately identifies various land uses such as residential, commercial, and industrial areas, and tracks land-use change trends, aiding urban planners in future development planning. Compared to conventional methods, our model significantly reduces training time and increases computational efficiency under identical training conditions. Experimental comparisons and analyses reveal that, within the same training duration, our model's accuracy surpasses that of similar models by 10%-15%. On the ISPRS dataset, our model achieved a segmentation accuracy of 82.43% for building surfaces, and 76.54% for trees. In scenarios with relatively uniform reflective surfaces, our model outperforms similar models by approximately 10%.",Semantic segmentation,Band combination,CNN,,,"Zhang, Lei",,,,COMPUTER COMMUNICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_640,"Zhang, Chenchen","Wang, Rongfang","Chen, Jia-Wei","Li, Weibin",A MULTI-BRANCH U-NET FOR WATER AREA SEGMENTATION WITH MULTI-MODALITY REMOTE SENSING IMAGES,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"Water area segmentation in remote sensing images is of great importance for flood monitoring. Convolutional neural networks have been successfully applied to various computer vision tasks. Among them, a U-shaped CNN known as U-Net achieves state-of-the-art performance on various types of image segmentation, including remote sensing images. However, there are still some difficulties in the water area segmentation of remote sensing images, such as complex backgrounds, cloud shading, and rough edges. In this work, we propose a multi-branch fusion U-Net (MFU-Net) method for water area segmentation with multi-modality remote sensing images. The experimental results showed that our MFU-Net can effectively and efficiently segment water area from Sentinel-1 and Sentinel-2 images, which F1, IoU and PA on the Sen1Floods11 dataset are 91.462%, 84.598% and 98.123%, respectively.",water area semantic segmentation,multi-modality fusion,multi-branch encoder,channel attention,remote sensing,"Huo, Chunlei","Niu, Yi",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_641,"Cai, Yuanzhi","Fan, Lei","Fang, Yuan",,SBSS: Stacking-Based Semantic Segmentation Framework for Very High-Resolution Remote Sensing Image,,2023,12,"Semantic segmentation of very high-resolution (VHR) remote sensing images is a fundamental task for many applications. However, large variations in the scales of objects in those VHR images pose a challenge for performing accurate semantic segmentation. Existing semantic segmentation networks are able to analyze an input image at up to four resizing scales, but this may be insufficient given the diversity of object scales. Therefore, multiscale (MS) test-time data augmentation is often used in practice to obtain more accurate segmentation results, which makes equal use of the segmentation results obtained at the different resizing scales. However, it was found in this study that different classes of objects had their preferred resizing scale for more accurate semantic segmentation. Based on this behavior, a stacking-based semantic segmentation (SBSS) framework is proposed to improve the segmentation results by learning this behavior, which contains a learnable error correction module (ECM) for segmentation result fusion and an error correction scheme (ECS) for computational complexity control. Two ECS, i.e., ECS-MS and ECS-single-scale (SS), are proposed and investigated in this study. The floating-point operations (Flops) required for ECS-MS and ECS-SS are similar to the commonly used MS test and the SS test, respectively. Extensive experiments on four datasets (i.e., Cityscapes, UAVid, LoveDA, and Potsdam) show that SBSS is an effective and flexible framework. It achieved higher accuracy than MS when using ECS-MS, and similar accuracy as SS with a quarter of the memory footprint when using ECS-SS.",Error correction,Feature extraction,Semantic segmentation,Spatial resolution,Decoding,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Bagging,Task analysis,Convolutional neural network,deep learning,ensemble learning,semantic segmentation,stacking,,,,,,,,,,,,,,,,,,,,,,,
Row_642,"Cheng, Dongcai","Meng, Gaofeng","Xiang, Shiming","Pan, Chunhong",FusionNet: Edge Aware Deep Convolutional Networks for Semantic Segmentation of Remote Sensing Harbor Images,,DEC 2017,96,"Sea-land segmentation and ship detection are two prevalent research domains for optical remote sensing harbor images and can find many applications in harbor supervision and management. As the spatial resolution of imaging technology improves, traditional methods struggle to perform well due to the complicated appearance and background distributions. In this paper, we unify the above two tasks into a single framework and apply the deep convolutional neural networks to predict pixelwise label for an input. Specifically, an edge aware convolutional network is proposed to parse a remote sensing harbor image into three typical objects, e. g., sea, land, and ship. Two innovations are made on top of the deep structure. First, we design a multitask model by simultaneously training the segmentation and edge detection networks. Hierarchical semantic features fromthe segmentation network are extracted to learn the edge network. Second, the outputs of edge pipeline are further employed to refine entire model by adding an edge aware regularization, which helps our method to yield very desirable results that are spatially consistent and well boundary located. It also benefits the segmentation of docked ships that are quite challenging for many previous methods. Experimental results on two datasets collected fromGoogleEarth have demonstrated the effectiveness of our approach both in quantitative and qualitative performance compared with state-of-the-art methods.",Edge aware regularization,harbor images,multitask learning,semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_643,"Yang, Zhujun","Yan, Zhiyuan","Diao, Wenhui","Ma, Yihang",Active Bidirectional Self-Training Network for Cross-Domain Segmentation in Remote-Sensing Images,,JUL 2024,1,"Semantic segmentation with cross-domain adaptation in remote-sensing images (RSIs) is crucial and mitigates the expense of manually labeling target data. However, the performance of existing unsupervised domain adaptation (UDA) methods is still significantly impacted by domain bias, leading to a considerable gap compared to supervised trained models. To address this, our work focuses on semi-supervised domain adaptation, selecting a small subset of target annotations through active learning (AL) that maximize information to improve domain adaptation. Overall, we propose a novel active bidirectional self-training network (ABSNet) for cross-domain semantic segmentation in RSIs. ABSNet consists of two sub-stages: a multi-prototype active region selection (MARS) stage and a source-weighted class-balanced self-training (SCBS) stage. The MARS approach captures the diversity in labeled source data by introducing multi-prototype density estimation based on Gaussian mixture models. We then measure inter-domain similarity to select complementary and representative target samples. Through fine-tuning with the selected active samples, we propose an enhanced self-training strategy SCBS, designed for weighted training on source data, aiming to avoid the negative effects of interfering samples. We conduct extensive experiments on the LoveDA and ISPRS datasets to validate the superiority of our method over existing state-of-the-art domain-adaptive semantic segmentation methods.",semantic segmentation,domain adaptation,active learning,self-training network,remote-sensing images,"Li, Xinming","Sun, Xian",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_644,Jian Yongsheng,Zhu Daming,Fu Zhitao,Wen Shiya,Remote Sensing Image Segmentation Network Based on Multi-Level Feature Refinement and Fusion,,FEB 2023,1,"To accurately segment ground objects from a high-resolution remote sensing image, we propose a remote sensing image segmentation network based on multi-level feature optimization fusion that focuses on the fusion of feature maps at different levels in the feature extraction skeleton network, performs reasonable and effective extractions, and analyzes output feature map information by fusing different types of information in the network feature map. Simultaneously, layer-by-layer multi-scale coding and decoding modules are used to refine the shallow feature map that merges with the high-level feature map, and the different types of information are optimized to the high-level feature map. The hollow convolution pyramid is then used to extract the information of different receptive fields on the high-level feature map, and the output feature map of semantic segmentation is optimized. When conducting experiments on the ISPRS Vaihingen dataset, the overall accuracy of the proposed network reaches 90.34%, which effectively improves the accuracy of remote sensing image target detection when compared with the classical semantic segmentation network. Moreover, to prove the generalization of the proposed algorithm, a generalization experiment on the ISPRS Potsdam dataset is conducted; the overall accuracy of this algorithm reaches 91.47%, proving its effectiveness.",remote sensing,semantic segmentation,multi-scale encoding and decoding,feature fusion,,,,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_645,"Geiss, Christian","Zhu, Yue","Qiu, Chunping","Mou, Lichao",Deep Relearning in the Geospatial Domain for Semantic Remote Sensing Image Segmentation,,2022,8,"We present a classification postprocessing (CPP) technique based on fully convolutional neural networks (CNNs) for semantic remote sensing image segmentation. Conventional CPP techniques aim to enhance the classification accuracy by imposing smoothness priors in the image domain. Contrary to that, here, a relearning strategy is proposed where the initial classification outcome of a CNN model is provided to a subsequent CNN model via an extended input space to guide the learning of discriminative feature representations in an end-to-end fashion. This deep relearning CNN (DRCNN) explicitly accounts for the geospatial domain by taking the spatial alignment of preliminary class labels into account. Hereby, we evaluate to learn the DRCNN in a cumulative and noncumulative way, i.e., extending the input space based on all previous or solely preceding model outputs, respectively, during an iterative procedure. Besides, the DRCNN can also be conveniently coupled with alternative CPP techniques such as object-based voting (OBV). The experimental results obtained from two test sites of WorldView-II imagery underline the beneficial performance properties of the DRCNN models. They can increase the accuracies of the initial CNN models on average from 72.64x0025; to 76.01x0025; and from 92.43x0025; to 94.52x0025; in terms of statistic. An additional increase of 1.65 and 2.84 percentage points can be achieved when combining the DRCNN models with an OBV strategy. From an epistemological point of view, our results underline that CNNs can benefit from the consideration of preliminary model outcomes and that conventional CPP techniques can profit from an upstream relearning strategy.",Image segmentation,Remote sensing,Training,Geospatial analysis,Computational modeling,"Zhu, Xiao Xiang","Taubenboeck, Hannes",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Training data,Partitioning algorithms,Classification postprocessing (CPP),convolutional neural networks (CNNs),deep learning,relearning,,,,,,,,,,,,,,,,,,,,,,,,
Row_646,"Wang, Ende","Jiang, Yanmei","Li, Yong","Yang, Jingchao",MFCSNet: Multi-Scale Deep Features Fusion and Cost-Sensitive Loss Function Based Segmentation Network for Remote Sensing Images,,OCT 2019,12,"Semantic segmentation of remote sensing images is an important technique for spatial analysis and geocomputation. It has important applications in the fields of military reconnaissance, urban planning, resource utilization and environmental monitoring. In order to accurately perform semantic segmentation of remote sensing images, we proposed a novel multi-scale deep features fusion and cost-sensitive loss function based segmentation network, named MFCSNet. To acquire the information of different levels in remote sensing images, we design a multi-scale feature encoding and decoding structure, which can fuse the low-level and high-level semantic information. Then a max-pooling indices up-sampling structure is designed to improve the recognition rate of the object edge and location information in the remote sensing image. In addition, the cost-sensitive loss function is designed to improve the classification accuracy of objects with fewer samples. The penalty coefficient of misclassification is designed to improve the robustness of the network model, and the batch normalization layer is also added to make the network converge faster. The experimental results show that the classification performance of MFCSNet outperforms U-Net and SegNet in classification accuracy, object details and prediction consistency.",Semantic segmentation,remote sensing images,feature fusion,cost-sensitive,,"Ren, Mengcheng","Zhang, Qingchun",,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_647,"Huang, Siyuan","Dong, Kaihui","Chen, Haobing","Yao, Wei",Semantic Segmentation of Ultra-high-resolution Remote Sensing Images Based on Global-local Branch Asynchronous Feature Interaction Structure,"39TH YOUTH ACADEMIC ANNUAL CONFERENCE OF CHINESE ASSOCIATION OF AUTOMATION, YAC 2024",2024,0,"In recent years, semantic segmentation of ultra-high-resolution remote sensing images (UHRRSI) has made certain progress. UHRRSI are characterized by large spatial resolution. Using the entire UHRRSI for model training will make the computational cost almost unaffordable. Common processing methods are to downsample or crop UHRRSI. However, both methods have their own shortcomings. The downsampling operation may destroy the details of the image, and the cropping operation may damage important contextual information. Therefore, this paper proposes a structure based on global-local branch asynchronous feature interaction. The global branch is used to process the downsampled images, while the local branch focuses on processing the cropped images. During the decoding stage, the decoding features of the two branches are asynchronously interacted. In comparative experiments conducted using the Potsdam and Vaihingen datasets, our model exhibits excellent results in semantic segmentation performance.",convolutional neural network,Ultra-high resolution remote sensing images (UHRRSI),semantic segmentation,Asynchronous feature interaction,,"Li, Bo","Cheng, Li",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_648,"Cao, Yong","Huo, Chunlei","Xiang, Shiming","Pan, Chunhong",GFFNet: Global Feature Fusion Network for Semantic Segmentation of Large-Scale Remote Sensing Images,,2024,2,"Semantic segmentation plays a pivotal role in interpreting high-resolution remote sensing images (RSIs), where contextual information is essential for achieving accurate segmentation. Despite the common practice of partitioning large RSIs into smaller patches for deep model input, existing methods often rely on adaptations from natural image semantic segmentation techniques, limiting their contextual scope to individual images. To address this limitation and harness a broader range of contextual information from original large-scale RSIs, this study introduces a global feature fusion network (GFFNet). GFFNet employs a novel approach by incorporating a group transformer structure alternated with group convolution, forming a lightweight global context learning branch. This design facilitates the extraction of global contextual features from the large-scale RSIs. In addition, we propose a cross feature fusion module that seamlessly integrates local features obtained from the convolutional network with the global contextual features. GFFNet serves as a versatile plugin for existing RSI semantic segmentation models, particularly beneficial when the target dataset involves cropping. This integration enhances the model's performance, especially in terms of segmenting large-scale objects. Experimental results on the ISPRS and GID-15 datasets validate the effectiveness of GFFNet in improving segmentation capabilities for large-scale objects in RSIs.",Cross feature fusion (CFF),global context learning,group transformer,semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_649,"Mao, Yong-Qiang","Jiang, Zhizhuo","Liu, Yu","Zhang, Yiming",Body Joint Boundary Prototype Match for Few-Shot Remote Sensing Semantic Segmentation,,2024,0,"Deep networks require a large number of samples for optimization, so few-shot segmentation in remote sensing scenes is still an open problem. However, this challenge is exacerbated by the feature blurring and aliasing of bodies (low frequency) and boundaries (high frequency). The existing methods usually only focus on the body part of the class, that is, the low-frequency part, and ignore the critical role of boundary information, that is, high-frequency details, on feature representation. In this letter, we propose a novel body joint boundary prototype match (B2PM) approach that aims to enable prior learning of low- and high-frequency information by explicitly modeling the body and boundary features of objects. First, body-aware prototype learning (BodyPL) realizes the adaptive modeling of the body part of the object through a precise farthest point sampling (FPS) initialization algorithm and an adaptive part shift (APS) strategy, which alleviates the feature ambiguity of the body. Second, boundary-aware prototype learning (BoundPL) explicitly models boundary prototypes by building a patch division and assignment strategy to alleviate feature aliasing at boundaries. Finally, prototype match performs prior knowledge aggregation by computing the affinity between query features and support prototypes. Extensive experiments on commonly used benchmarks (iSAID and PASCAL VOC) demonstrate that B2PM improves the state of the art by significant margins.",Prototypes,Feature extraction,Vectors,Adaptation models,Computational modeling,"Li, Yaowen","Yan, Chenggang","Zheng, Bolun",,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Remote sensing,Decoding,Few-shot learning,few-shot segmentation,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_650,"Lopez, Josue","Torres, Deni","Santos, Stewart","Atzberger, Clement",Spectral Imagery Tensor Decomposition for Semantic Segmentation of Remote Sensing Data through Fully Convolutional Networks,,FEB 2020,9,"This work aims at addressing two issues simultaneously: data compression at input space and semantic segmentation. Semantic segmentation of remotely sensed multi- or hyperspectral images through deep learning (DL) artificial neural networks (ANN) delivers as output the corresponding matrix of pixels classified elementwise, achieving competitive performance metrics. With technological progress, current remote sensing (RS) sensors have more spectral bands and higher spatial resolution than before, which means a greater number of pixels in the same area. Nevertheless, the more spectral bands and the greater number of pixels, the higher the computational complexity and the longer the processing times. Therefore, without dimensionality reduction, the classification task is challenging, particularly if large areas have to be processed. To solve this problem, our approach maps an RS-image or third-order tensor into a core tensor, representative of our input image, with the same spatial domain but with a lower number of new tensor bands using a Tucker decomposition (TKD). Then, a new input space with reduced dimensionality is built. To find the core tensor, the higher-order orthogonal iteration (HOOI) algorithm is used. A fully convolutional network (FCN) is employed afterwards to classify at the pixel domain, each core tensor. The whole framework, called here HOOI-FCN, achieves high performance metrics competitive with some RS-multispectral images (MSI) semantic segmentation state-of-the-art methods, while significantly reducing computational complexity, and thereby, processing time. We used a Sentinel-2 image data set from Central Europe as a case study, for which our framework outperformed other methods (included the FCN itself) with average pixel accuracy (PA) of 90% (computational time similar to 90s) and nine spectral bands, achieving a higher average PA of 91.97% (computational time similar to 36.5s), and average PA of 91.56% (computational time similar to 9.5s) for seven and five new tensor bands, respectively.",fully convolutional network,semantic segmentation,spectral image,tensor decomposition,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_651,"Wei, Kan","Dai, Jinkun","Hong, Danfeng","Ye, Yuanxin",MGFNet: An MLP-dominated gated fusion network for semantic segmentation of high-resolution multi-modal remote sensing images,,DEC 2024,0,"The heterogeneity and complexity of multimodal data in high-resolution remote sensing images significantly challenges existing cross-modal networks infusing the complementary information of high-resolution optical and synthetic aperture radar (SAR) images for precise semantic segmentation. To address this issue, this paper proposes a multi-layer perceptron (MLP) dominated gate fusion network (MGFNet). MGFNet consists of three modules: a multi-path feature extraction network, an MLP-gate fusion (MGF) module, and a decoder. Initially, MGFNet independently extracts features from high-resolution optical and SAR images while preserving spatial information. Then, the well-designed MGF module combines the multi-modal features through channel attention and gated fusion stages, utilizing MLP as a gate to exploit complementary information and filter redundant data. Additionally, we introduce a novel high-resolution multimodal remote sensing dataset, YESegOPT-SAR, with a spatial resolution of 0.5 m. To evaluate MGFNet, we compare it with several state-of-the-art (SOTA) models using YESeg-OPT-SAR and Pohang datasets, both of which are high-resolution multi-modal datasets. The experimental results demonstrate that MGFNet achieves higher evaluation metrics compared to other models, indicating its effectiveness in multi-modal feature fusion for segmentation. The source code and data are available at https://github.com/yeyuanxin110/YESeg-OPT-SAR.",Deep learning,Multi-modal data fusion,High-resolution remote sensing images,Semantic segmentation,,,,,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_652,"Zhu, Xiaotong","Peng, Taile","Hu, Xiaobin","Guo, Jia",SN-Unetformer: a dual encoder hybrid architecture for complex targets in high-resolution remote sensing images,,APR 1 2023,2,"Accurately identifying the semantic information of complex objects is a challenging problem in semantic segmentation of remote sensing images. We propose a bi-encoder network for semantic segmentation of complex targets, called the SN-Unetformer. It combines ConvNeXt and Swin Transformer into a bi-encoder and constructs a feature fusion module (FFM) to fully integrate the semantic information of the bi-encoder by exploiting channel dependence. Moreover, an efficient attention mechanism has been introduced to model the global-local relationship. To the best of our knowledge, our proposed network is innovative, as it is the first method to combine two popular networks, ConvNeXt and the Swin Transformer, into a dual encoder. Our SN-Unetformer has been tested on large-scale Vaihingen and Potsdam datasets, as well as the LoveDA dataset, with significant challenges. Compared to current advanced methods for semantic segmentation for remote sensing images, our accuracy is significantly better. In particular, our method achieves 84.3% of mean intersection over union on the Vaihingen dataset, which is the best result currently available for this dataset. (c) 2023 Society of PhotoOptical Instrumentation Engineers (SPIE) [DOI: 10.1117/1.JRS.17.026512]",semantic segmentation,remote sensing,Swin Transformer,ConvNeXt,dual encoder,"Cao, Taotao","Wang, Hao",,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_653,"Shi, Lukui","Wang, Ziyuan","Pan, Bin","Shi, Zhenwei",An End-to-End Network for Remote Sensing Imagery Semantic Segmentation via Joint Pixel- and Representation-Level Domain Adaptation,,NOV 2021,28,"It requires pixel-by-pixel annotations to obtain sufficient training data in supervised remote sensing image segmentation, which is a quite time-consuming process. In recent years, a series of domain-adaptation methods was developed for image semantic segmentation. In general, these methods are trained on the source domain and then validated on the target domain to avoid labeling new data repeatedly. However, most domain-adaptation algorithms only tried to align the source domain and the target domain in the pixel level or the representation level, while ignored their cooperation. In this letter, we propose an unsupervised domain-adaptation method by Joint Pixel and Representation level Network (JPRNet) alignment. The major novelty of the JPRNet is that it achieves joint domain adaptation in an end-to-end manner, so as to avoid the multisource problem in the remote sensing images. JPRNet is composed of two branches, each of which is a generative-adversarial network (GAN). In one branch, pixel-level domain adaptation is implemented by the style transfer with the Cycle GAN, which could transfer the source domain to a target domain. In the other branch, the representation-level domain adaptation is realized by adversarial learning between the transferred source-domain images and the target-domain images. The experimental results on the public data sets have indicated the effectiveness of the JPRNet.",Gallium nitride,Image segmentation,Semantics,Remote sensing,Feature extraction,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Adaptation models,Training,Domain adaptation,generative-adversarial network (GAN),remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_654,"Hao, Rong-Rong","Sun, Hong-Mei","Wang, Rui-Xuan","Pan, Ang",A novel semantic feature enhancement network for extracting lake water from remote sensing images,,SEP 2024,0,"The automatic lake water extraction method based on semantic segmentation is a research hotspot in the field of remote sensing image processing. In remote sensing images, the presence of complex noise information at the lake boundary hinders the normal expression of boundary information, which leads to methods cannot extract a coherent lake boundary. Moreover, partial small-scale lakes' texture features are weak and easily masked by the background information. To address the above issues, an end-to-end semantic segmentation network is designed. The network uses a symmetric encoder-decoder architecture to extract lake water in remote sensing images. On the one hand, a directional noise reduction filtering algorithm is proposed to reduce the impact of noise information on the network segmentation process. The algorithm utilizes a preset directional guide map to guide the nonlinear propagation of boundary noise and suppress low-contrast halo artifacts in the image, thereby better preserving the boundary sharpness of the lake. On the other hand, for the problem of missing small-scale lakes, an attention gate compression module is embedded in the skip connection. This module can adaptively integrate the correlation features between different ground objects, and selectively assign more attention to small-scale lakes, thereby improving the network's ability to recognize such lakes. In the experimental results, our method can produce more accurate lake water extraction results than the current mainstream methods. Besides it has an excellent performance in accurately identifying lake boundaries and small-scale lakes.",Semantic segmentation,Remote sensing images,Transformer,Directional noise reduction filtering,,"Jia, Rui-Sheng",,,,INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_655,"Cui, Mengtian","Li, Kai","Li, Yulan","Kamuhanda, Dany",Semi-Supervised Semantic Segmentation of Remote Sensing Images Based on Dual Cross-Entropy Consistency,,APR 2023,3,"Semantic segmentation is a growing topic in high-resolution remote sensing image processing. The information in remote sensing images is complex, and the effectiveness of most remote sensing image semantic segmentation methods depends on the number of labels; however, labeling images requires significant time and labor costs. To solve these problems, we propose a semi-supervised semantic segmentation method based on dual cross-entropy consistency and a teacher-student structure. First, we add a channel attention mechanism to the encoding network of the teacher model to reduce the predictive entropy of the pseudo label. Secondly, the two student networks share a common coding network to ensure consistent input information entropy, and a sharpening function is used to reduce the information entropy of unsupervised predictions for both student networks. Finally, we complete the alternate training of the models via two entropy-consistent tasks: (1) semi-supervising student prediction results via pseudo-labels generated from the teacher model, (2) cross-supervision between student models. Experimental results on publicly available datasets indicate that the suggested model can fully understand the hidden information in unlabeled images and reduce the information entropy in prediction, as well as reduce the number of required labeled images with guaranteed accuracy. This allows the new method to outperform the related semi-supervised semantic segmentation algorithm at half the proportion of labeled images.",cross-entropy consistency,information entropy,semi-supervised,channel attention mechanism,remote sensing image,"Tessone, Claudio J.",,,,ENTROPY,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_656,"Yi, Lina","Zhang, Guifeng","Wu, Zhaocong",,A Scale-Synthesis Method for High Spatial Resolution Remote Sensing Image Segmentation,,OCT 2012,60,"Multiscale segmentation is always needed to extract semantic meaningful objects for object-based remote sensing image analysis. Choosing the appropriate segmentation scales for distinct ground objects and intelligently combining them together are two crucial issues to get the appropriate segmentation result for target applications. With respect to these two issues, this paper proposes a simple scale-synthesis method which is highly flexible to be adjusted to meet the segmentation requirements of varying image-analysis tasks. The main idea of this method is to first divide the whole image area into multiple regions; each region consisted of ground objects that have similar optimal segmentation scale. Then, synthesize the suboptimal segmentations of each region to get the final segmentation result. The result is the combination of suboptimal scales of objects and is therefore more coherent to ground objects. To validate this method, the land-cover-category map is used to guide the scale synthesis of multiscale image segmentations for the Quickbird-image land-use classification. First, the image is coarsely divided into multiple regions; each region belongs to a certain land-cover category. Then, multiscale-segmentation results are generated by the Mumford-Shah function based region-merging method. For each land-cover category, the optimal segmentation scale is selected by the supervised segmentation-accuracy-assessment method. Finally, the optimal scales of segmentation results are synthesized under the guide of land-cover category. It is proved that the proposed scale-synthesis method can generate a more accurate segmentation result that benefits the latter classification. The land-use-classification accuracy reaches to 77.8%.",Image segmentation,multiscale,object-oriented classification,remote sensing,scale synthesis,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_657,"Li, Yuxuan","Li, Xiang","Dai, Yimain","Hou, Qibin",LSKNet: A Foundation Lightweight Backbone for Remote Sensing,,OCT 2024,1,"Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection, semantic segmentation and change detection, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightweight LSKNet backbone network sets new state-of-the-art scores on standard remote sensing classification, object detection, semantic segmentation and change detection benchmarks. Our comprehensive analysis further validated the significance of the identified priors and the effectiveness of LSKNet. The code is available at https://github.com/zcablii/LSKNet.",Remote sensing,CNN backbone,Large kernel,Attention,Object detection,"Liu, Li","Liu, Yongxiang","Cheng, Ming-Ming","Yang, Jian",INTERNATIONAL JOURNAL OF COMPUTER VISION,,Semantic segmentation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_658,"Chen, Guangsheng","Li, Chao","Wei, Wei","Jing, Weipeng",Fully Convolutional Neural Network with Augmented Atrous Spatial Pyramid Pool and Fully Connected Fusion Path for High Resolution Remote Sensing Image Segmentation,,MAY 1 2019,61,"Recent developments in Convolutional Neural Networks (CNNs) have allowed for the achievement of solid advances in semantic segmentation of high-resolution remote sensing (HRRS) images. Nevertheless, the problems of poor classification of small objects and unclear boundaries caused by the characteristics of the HRRS image data have not been fully considered by previous works. To tackle these challenging problems, we propose an improved semantic segmentation neural network, which adopts dilated convolution, a fully connected (FC) fusion path and pre-trained encoder for the semantic segmentation task of HRRS imagery. The network is built with the computationally-efficient DeepLabv3 architecture, with added Augmented Atrous Spatial Pyramid Pool and FC Fusion Path layers. Dilated convolution enlarges the receptive field of feature points without decreasing the feature map resolution. The improved neural network architecture enhances HRRS image segmentation, reaching the classification accuracy of 91%, and the precision of recognition of small objects is improved. The applicability of the improved model to the remote sensing image segmentation task is verified.",semantic segmentation,remote sensing,dilated convolution,fully convolutional neural network,deep learning,"Wozniak, Marcin","Blazauskas, Tomas","Damasevicius, Robertas",,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_659,"Hu, Xiaoxing","Wang, Yupei","Chen, Liang",,Domain Adaptive Semantic Segmentation of Remote Sensing Images via Self-Training-Based Dual-Level Data Augmentation,,2024,0,"Semantic segmentation models experience a significant performance degradation due to domain shifts between the source and target domains. This issue is particularly prevalent in remote sensing imagery, where a semantic segmentation model trained on images from one satellite is tested on images from another. Previous research has often overlooked the role of data augmentation in enhancing a model's adaptability to target domains. In contrast, this article proposes a novel self-training framework that incorporates data augmentation at both the input and feature levels, yielding excellent results. Specifically, we introduce a regularized online self-training framework that effectively addresses the challenges of overconfidence and class imbalance inherent in self-training. Based on this framework, we implement two robust data augmentation strategies at the input and feature levels to facilitate the learning of cross-domain invariant knowledge. At the input level, we employ a large-scale domain mixing strategy, termed multidomain mixing, to enhance the model's generalization capability. At the feature level, we introduce masked feature augmentation, a masking-based perturbation technique applied to the semantic features of the target domain. This approach enhances the consistency of teacher-student network predictions in the target domain feature space, thereby improving the robustness of the model's recognition of target domain features. The integration of the proposed self-training framework with dual-level data augmentation culminates in our innovative self-training-based dual-level data augmentation (STDA) method. Extensive experimental results on the ISPRS semantic segmentation benchmark demonstrate that STDA outperforms existing state-of-the-art methods, showcasing its effectiveness.",Adaptation models,Remote sensing,Semantic segmentation,Data models,Data augmentation,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Training,Semantics,Target recognition,Perturbation methods,Earth,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,,
Row_660,"Kang, Yuhan","Wu, Jie","Liu, Qiang","Yue, Jun",Trans-Diff: Heterogeneous Domain Adaptation for Remote Sensing Segmentation With Transfer Diffusion,,2024,0,"Domain adaptation has been demonstrated to be an important technique to reduce the expensive annotation costs for remote sensing segmentation. However, for remote sensing images (RSIs) acquired from different imaging modalities with significant differences, a model trained on one modality can hardly be utilized for images of other modalities. This leads to a greater challenge in domain adaptation, called heterogeneous domain adaptation (HDA). To address this issue, we propose a novel method called transfer diffusion (Trans-Diff), which is the first work to explore the diffusion model for HDA remote sensing segmentation. The proposed Trans-Diff constructs cross-domain unified prompts for the diffusion model. This approach enables the generation of images from different modalities with specific semantics, leading to efficient HDA segmentation. Specifically, we first propose an interrelated semantic modeling method to establish semantic interrelation between heterogeneous RSIs and annotations in a high-dimensional feature space and extract the unified features as the cross-domain prompts. Then, we construct a semantic guidance diffusion model to further improve the semantic guidance of images generated with the cross-domain prompts, which effectively facilitates the semantic transfer of RSIs from source modality to target modality. In addition, we design an adaptive sampling strategy to dynamically regulate the generated images' stylistic consistency and semantic consistency. This can effectively reduce the cross-domain discrepancies between different modalities of RSIs, ultimately significantly improving the HDA remote sensing segmentation performance. Experimental results demonstrate the superior performance of Trans-Diff over advanced methods on several heterogeneous RSI datasets.",Cross-domain prompt,diffusion model,hetero- geneous domain adaptation,hetero- geneous domain adaptation,remote sensing segmentation,"Fang, Leyuan",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,remote sensing segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_661,"Zhong, Letian","Lin, Yong","Sul, Yian","Fang, Xianbao",Improved U-Net Network Segmentation Method for Remote Sensing Image,"2022 IEEE 6TH ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC)",2022,1,"Semantic segmentation and extraction based on remote sensing images has important theory and significance. Deep learning has become one of the mainstream methods to extract information from remote sensing images. In this paper, based on the improvement of U-Net network structure, we combine ASPP and skip connection. Improve the residual module to improve the information extraction method. The main improvements of this paper are:0 Based on the U-Net network structure, we use the multi-scale feature detection capabilities of Pyramid to introduce. The ASPP module and the residual structure are improved, paying more attention to semantic and detail informatization, overcoming the limitations of U-Net in small target detection;20 We have improved the U-Net network, using skip connections to get more layers of information. Experiments show that the model proposed in this paper has significantly higher MPA and MIOU than the U-Net model on both the VOC dataset and the Vaihingen dataset. It means that ARU-Net can extract information better.",semantic segmentation,U-Net,Pyramid,ASPP,Residual block,,,,,,,Remote sensing image,Skip connection,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_662,"Lei, Jingxiong","Liu, Xuzhi","Yang, Haolang","Zeng, Zeyu",Dual Hybrid Attention Mechanism-Based U-Net for Building Segmentation in Remote Sensing Images,,FEB 2024,2,"High-resolution remote sensing images (HRRSI) have important theoretical and practical value in urban planning. However, current segmentation methods often struggle with issues like blurred edges and loss of detailed information due to the intricate backgrounds and rich semantics in high-resolution remote sensing images. To tackle these challenges, this paper proposes an end-to-end attention-based Convolutional Neural Network (CNN) called Double Hybrid Attention U-Net (DHAU-Net). We designed a new Double Hybrid Attention structure consisting of dual-parallel hybrid attention modules to replace the skip connections in U-Net, which can eliminate redundant information interference and enhances the collection and utilization of important shallow features. Comprehensive experiments on the Massachusetts remote sensing building dataset and the Inria aerial image labeling dataset demonstrate that our proposed method achieves effective pixel-level building segmentation in urban remote sensing images by eliminating redundant information interference and making full use of shallow features, and improves the segmentation performance without significant time costs (approximately 15%). The evaluation metrics reveal significant results, with an accuracy rate of 0.9808, precision reaching 0.9300, an F1 score of 0.9112, a mean intersection over union (mIoU) of 0.9088, and a recall rate of 0.8932.",deep learning,remote sensing image,attention mechanism,U-net,semantic segmentation,"Feng, Jun",,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_663,"Zhou, Wujie","Jin, Jianhui","Lei, Jingsheng","Yu, Lu",CIMFNet: Cross-Layer Interaction and Multiscale Fusion Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,JUN 2022,50,"Semantic segmentation of remote sensing images has received increasing attention in recent years; however, using a single imaging modality limits the segmentation performance. Thus, digital surface models have been integrated into semantic segmentation to improve performance. Nevertheless, existing methods based on neural networks simply combine data from the two modalities, mostly neglecting the similarities and differences between multimodal features. Consequently, the complementarity between multimodal features cannot be exploited, and excess noise is introduced during feature processing. To solve these problems, we propose a multimodal fusion module to explore the similarities and differences between features from the two information modalities for adequate fusion. In addition, although downsampling operations such as pooling and striding can improve the feature representativeness, they discard spatial details and often lead to segmentation errors. Thus, we introduce hierarchical feature interactions to mitigate the adverse effects of downsampling and introduce a two-way interactive pyramid pooling module to extract multiscale context features for guiding feature fusion. Extensive experiments performed on two benchmark datasets show that the proposed network integrating our novel modules substantially outperforms state-of-the-art semantic segmentation methods. The code and results can be found at https://github.com/NIT-JJH/CIMFNet.",Semantics,Image segmentation,Feature extraction,Logic gates,Fuses,,,,,IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING,,Data mining,Convolution,Cross-layer interaction,cross-modal fusion,high-resolution remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_664,"Wei, Youhua","Liu, Xuzhi","Lei, Jingxiong","Yue, Ruihan",Multiscale feature U-Net for remote sensing image segmentation,,JAN 1 2022,6,"The segmentation and extraction of buildings in high-resolution remote sensing images has good application prospects in military, civil, and other fields. With a depth encoder-decoder structure, U-Net is a frequently used model for high-precision image segmentation. However, the design of U-Net makes it hard to retain the detailed information of edges when processing the building segmentation. Specifically, the low-level features extracted from the shallow layer and the abstract features extracted from the deep layer cannot be completely merged, resulting in inaccurate segmentation. In response to this problem, we design a new multiscale feature extraction module that extracts target information through three convolution kernels of different scales. Taking U-Net as the baseline, by replacing skip connections with this module, we propose a multiscale feature extraction U-Net. This method can perform secondary feature extraction on the shallow feature information in the skip connection, refine the detailed information, and narrow the semantic gap between the low-level features and high-level features. It can not only improve the ability of the network to extract multiscale feature information, from a larger range to more layers to extract the edge detail information of the building in the remote sensing image, but also increase the number of skip connections to reduce network overfitting. Experimental results on Massachusetts remote sensing data and Massachusetts building data show that the method proposed offers significant improvement in terms of precision and accuracy compared with the methods full convolutional network, U-Net, SegNet, and high-resolution network, with an F1 score of 88.73%, mean IoU of 91.15%, precision of 89.74%, accuracy of 97.36%, and recall of 87.74%. (C) 2022 Society of Photo-Optical Instrumentation Engineers (SPIE)",remote sensing image,deep learning,multiscale information,building segmentation,U-Net,"Feng, Jun",,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_665,"Xu, Lewei","Hu, Zhuhua","Zhang, Chong","Wu, Wei",Remote Sensing Image Segmentation of Mariculture Cage Using Ensemble Learning Strategy,,AUG 2022,4,"Featured Application By introducing the method of deep learning, the precise segmentation of the aquaculture cages in a specific aquaculture sea area can be achieved, so as to realize the efficient statistics of the cage culture density and reduce the cost of manual statistics. In harbour areas, the irrational layout and high density of mariculture cages can lead to a dramatic deterioration of the culture's ecology. Therefore, it is important to analyze and regulate the distribution of cages using intelligent analysis based on deep learning. We propose a remote sensing image segmentation method based on the Swin Transformer and ensemble learning strategy. Firstly, we collect multiple remote sensing images of cages and annotate them, while using data expansion techniques to construct a remote sensing image dataset of mariculture cages. Secondly, the Swin Transformer is used as the backbone network to extract the remote sensing image features of cages. A strategy of alternating the local attention module and the global attention module is used for model training, which has the benefit of reducing the attention computation while exchanging global information. Then, the ensemble learning strategy is used to improve the accuracy of remote sensing cage segmentation. We carry out quantitative and qualitative analyses of remote sensing image segmentation of cages at the ports of Li'an, Xincun and Potou in Hainan Province, China. The results show that our proposed segmentation scheme has significant performance improvement compared to other models. In particular, the mIoU reaches 82.34% and pixel accuracy reaches 99.71%.",aquaculture,remote sensing image,semantic segmentation,smart agriculture,deep learning,,,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_666,"Dong, Sijun","Chen, Zhengchao",,,Block Multi-Dimensional Attention for Road Segmentation in Remote Sensing Imagery,,2022,12,"High-resolution remote sensing image (RSI) segmentation is a relatively mature application in various deep learning projects. In this study, aiming at slender objects in road RSIs, BMDANet combines cross-layer information exchange and block multi-dimensional attention (BMDA) module and optimizes road feature extraction by using multi-dimensional information to construct a global attention module. The experimental results based on the Ottawa road dataset show that our algorithm improved the recognition results of the road in RSI, and excelled the existing RSI road segmentation algorithm and reached the state-of-the-art. In addition, based on comparative experiments, the addition of the BMDA module to different algorithms can effectively improve the accuracy of the algorithm. It has proven the effectiveness and embedding of our BMDA module in RSI road segmentation algorithms.",Roads,Feature extraction,Image segmentation,Semantics,Remote sensing,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Data mining,Convolution,Block multi-dimensional attention (BMDA),cross-layer information exchange,remote sensing image (RSI),road semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_667,Yuan Wei,Xu Wenbo,Zhou Tian,,TopPixelLoss: a loss function for semantic segmentation of remote sensing images with class imbalance,,DEC 25 2021,0,"Aiming at the problem that the segmentation effect of small target in remote sensing image is not ideal, a loss function named TopPixelLoss was proposed. Firstly, the cross entropy of each pixel was calculated, and then the cross entropy of all pixels was sorted from large to small. After that, a K value was determined. According to the threshold K, the pixels with the largest cross entropy of the top K were selected. Finally, the cross entropy of the K pixels was averaged as the final loss value. Experiments using PSPNet network with cross entropy, FocalLoss and TopPixelLoss were carried out respectively through Vaihingen data set of ISPRS. The results show that, for different K values, the mean intersection over union (MIoU) F1-score and accuracy (ACC) arc all higher than FocalLoss, and that the effect is the best when K is 50 000 (MIoU, F1-score and ACC arc improved by 3.0%, 5.0% and 0.1% respectively compared with FocalLoss). The proposed TopPixelLoss function is a very effective loss function for imbalanced class segmentation.",remote sensing image,semantic segmentation,deep learning,class imbalance,small target segmentation,,,,,CHINESE SPACE SCIENCE AND TECHNOLOGY,,unbalanced sample,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_668,"Zuo, Renxiang","Zhang, Guangyun","Zhang, Rongting","Jia, Xiuping",A Deformable Attention Network for High-Resolution Remote Sensing Images Semantic Segmentation,,2022,25,"Deformable convolutional networks (DCNs) can mitigate the inherent limited geometric transformation. We reformulate the spatialwise attention mechanism using DCNs in this article for semantic segmentation of high-resolution remote sensing (HRRS) images. It combines the sparse spatial sampling strategy and the long-range relationship modeling capability, namely, deformable attention module (DAM). Such locality awareness, more adaptable to HRRS image structures, can capture each pixel's neighboring structural information. A reasonable multiscale deformable attention net (MDANet) is designed for the HRRS image semantic segmentation with a slightly increased computational cost based on the proposed DAM. Specifically, standard convolutional layers in the raw ResNet50 are equipped with a DAM to control sampling over a broader range of feature levels and aggregate multiscale context information. The experimental results evaluated on Vaihingen and DeepGlobe Land Cover Classification datasets show that the performance accuracy of MDANet is improved by 7.77% and 8.45% compared with the backbone network (ResNet50) in terms of Miou evaluation, respectively. Furthermore, a DAM can perform better than a global spatial attention mechanism with less computation on the 3 x 64 x 64 feature map. In addition, the added ablation studies demonstrate the effectiveness and efficiency of the DAM and multiscale strategy, respectively. Moreover, the sensitivity of critical hyperparameters is analyzed.",Deformable convolutional networks (DCNs),high-resolution remote sensing (HRRS) images,multiscale,spatialwise attention mechanism,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_669,"Chong, Qianpeng","Ni, Mengying","Huang, Jianjun","Wei, Guangyi",Rethinking high-resolution remote sensing image segmentation not limited to technology: a review of segmentation methods and outlook on technical interpretability,,JUN 2 2024,0,"The intelligent segmentation of high-resolution remote sensing (HRS) image, also called as dense prediction task for HRS image, has been and will continue to be important research in the remote sensing community. In recent years, the growing wave of artificial intelligence (AI) technology has introduced innovative paradigms to this domain, yielding outstanding results and overcoming many challenges with conventional segmentation techniques. This paper provides a comprehensive review of these intelligent segmentation methodologies, including traditional pattern recognition, convolution neural network (CNN)-based, and Transformer-based techniques. However, the explosive but incomplete development of intelligent segmentation techniques also poses more challenges for earth observation experts, the most of which is the technical interpretability. Consequently, we consider these segmentation techniques in the aspect of explainable artificial intelligence (XAI). Data-centric XAI thinks the practical applications of the segmentation model while model-centric XAI will facilitate the understanding of decision-making processes and the adjustment of structural features. Moreover, this review identifies novel research questions and provides constructive insights and recommendations to HRS image segmentation tasks, which may shed new light on the intelligent segmentation methods within the remote sensing image understanding community.",AI,earth observation,remote sensing,segmentation,,"Li, Ziyi","Xu, Jindong",,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_670,"Iizuka, Reo","Xia, Junshi","Yokoya, Naoto",,Frequency-Based Optimal Style Mix for Domain Generalization in Semantic Segmentation of Remote Sensing Images,,2024,2,"Supervised learning methods assume that training and test data are sampled from the same distribution. However, this assumption is not always satisfied in practical situations of land cover semantic segmentation when models trained in a particular source domain are applied to other regions. This is because domain shifts caused by variations in location, time, and sensor alter the distribution of images in the target domain from that of the source domain, resulting in significant degradation of model performance. To mitigate this limitation, domain generalization (DG) has gained attention as a way of generalizing from source domain features to unseen target domains. One approach is style randomization (SR), which enables models to learn domain-invariant features through randomizing styles of images in the source domain. Despite its potential, existing methods face several challenges, such as inflexible frequency decomposition, high computational and data preparation demands, slow speed of randomization, and lack of consistency in learning. To address these limitations, we propose a frequency-based optimal style mix (FOSMix), which consists of three components: 1) full mix (FM) enhances the data space by maximally mixing the style of reference images into the source domain; 2) optimal mix (OM) keeps the essential frequencies for segmentation and randomizes others to promote generalization; and 3) regularization of consistency ensures that the model can stably learn different images with the same semantics. Extensive experiments that require the model's generalization ability, with domain shift caused by variations in regions and resolutions, demonstrate that the proposed method achieves superior segmentation in remote sensing. The source code is available at https://github.com/Reo-I/FOSMix.",Frequency-domain analysis,Data models,Training,Semantic segmentation,Task analysis,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Remote sensing,Predictive models,Domain generalization (DG),semantic segmentation,style randomization (SR),,,,,,,,,,,,,,,,,,,,,,,,,
Row_671,"Duan, Sining","Zhao, Jingyi","Huang, Xinyi","Zhao, Shuhe",Semantic Segmentation of Remote Sensing Data Based on Channel Attention and Feature Information Entropy,,FEB 2024,2,"The common channel attention mechanism maps feature statistics to feature weights. However, the effectiveness of this mechanism may not be assured in remotely sensing images due to statistical differences across multiple bands. This paper proposes a novel channel attention mechanism based on feature information called the feature information entropy attention mechanism (FEM). The FEM constructs a relationship between features based on feature information entropy and then maps this relationship to their importance. The Vaihingen dataset and OpenEarthMap dataset are selected for experiments. The proposed method was compared with the squeeze-and-excitation mechanism (SEM), the convolutional block attention mechanism (CBAM), and the frequency channel attention mechanism (FCA). Compared with these three channel attention mechanisms, the mIoU of the FEM in the Vaihingen dataset is improved by 0.90%, 1.10%, and 0.40%, and in the OpenEarthMap dataset, it is improved by 2.30%, 2.20%, and 2.10%, respectively. The proposed channel attention mechanism in this paper shows better performance in remote sensing land use classification.",channel attention mechanism,land use classification,semantic segmentation,,,,,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_672,"Cao, Yiwen","Jiang, Nanfeng","Wang, Da-Han","Wu, Yun",UAM-Net: An Attention-Based Multi-level Feature Fusion UNet for Remote Sensing Image Segmentation,,2024,1,"Semantic segmentation of Remote Sensing Images (RSIs) is an essential application for precision agriculture, environmental protection, and economic assessment. While UNet-based networks have made significant progress, they still face challenges in capturing long-range dependencies and preserving fine-grained details. To address these limitations and improve segmentation accuracy, we propose an effective method, namely UAM-Net (UNet with Attention-based Multi-level feature fusion), to enhance global contextual understanding and maintain fine-grained information. To be specific, UAM-Net incorporates three key modules. Firstly, the Global Context Guidance Module (GCGM) integrates semantic information from the Pyramid Pooling Module (PPM) into each decoder stage. Secondly, the Triple Attention Module (TAM) effectively addresses feature discrepancies between the encoder and decoder. Finally, the computation-effective Linear Attention Module (LAM) seamlessly fuses coarse-level feature maps with multiple decoder stages. With the corporations of these modules, UAM-Net significantly outperforms the most state-of-the-art methods on two popular benchmarks.",Semantic segmentation,U-shape architecture,Attention mechanism,Feature fusion,Remote sensing images,"Zhu, Shunzhi",,,,"PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2023, PT IV",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_673,"Chen, Yantong","Li, Yuyang","Wang, Junsheng","Chen, Weinan",Remote Sensing Image Ship Detection under Complex Sea Conditions Based on Deep Semantic Segmentation,,FEB 2020,17,"Under complex sea conditions, ship detection from remote sensing images is easily affected by sea clutter, thin clouds, and islands, resulting in unreliable detection results. In this paper, an end-to-end convolution neural network method is introduced that combines a deep convolution neural network with a fully connected conditional random field. Based on the Resnet architecture, the remote sensing image is roughly segmented using a deep convolution neural network as the input. Using the Gaussian pairwise potential method and mean field approximation theorem, a conditional random field is established as the output of the recurrent neural network, thus achieving end-to-end connection. We compared the proposed method with other state-of-the-art methods on the dataset established by Google Earth and NWPU-RESISC45. Experiments show that the target detection accuracy of the proposed method and the ability of capturing fine details of images are improved. The mean intersection over union is 83.2% compared with other models, which indicates obvious advantages. The proposed method is fast enough to meet the needs for ship detection in remote sensing images.",remote sensing image,semantic segmentation,convolution neural network,atrous convolution,fully connected conditional random field,"Zhang, Xianzhong",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_674,"Liu, Yuheng","Zhang, Yifan","Wang, Ye","Mei, Shaohui",BiTSRS: A Bi-Decoder Transformer Segmentor for High-Spatial-Resolution Remote Sensing Images,,FEB 2023,5,"Semantic segmentation of high-spatial-resolution (HSR) remote sensing (RS) images has been extensively studied, and most of the existing methods are based on convolutional neural network (CNN) models. However, the CNN is regarded to have less power in global representation modeling. In the past few years, methods using transformer have attracted increasing attention and generate improved results in semantic segmentation of natural images, owing to their powerful ability in global information acquisition. Nevertheless, these transformer-based methods exhibit limited performance in semantic segmentation of RS images, probably because of the lack of comprehensive understanding in the feature decoding process. In this paper, a novel transformer-based model named the bi-decoder transformer segmentor for remote sensing (BiTSRS) is proposed, aiming at alleviating the problem of flexible feature decoding, through a bi-decoder design for semantic segmentation of RS images. In the proposed BiTSRS, the Swin transformer is adopted as encoder to take both global and local representations into consideration, and a unique design module (ITM) is designed to deal with the limitation of input size for Swin transformer. Furthermore, BiTSRS adopts a bi-decoder structure consisting of a Dilated-Uper decoder and a fully deformable convolutional network (FDCN) module embedded with focal loss, with which it is capable of decoding a wide range of features and local detail deformations. Both ablation experiments and comparison experiments were conducted on three representative RS images datasets. The ablation analysis demonstrates the contributions of specifically designed modules in the proposed BiTSRS to performance improvement. The comparison experimental results illustrate that the proposed BiTSRS clearly outperforms some state-of-the-art semantic segmentation methods.",remote sensing,semantic segmentation,global-local modeling,bi-decoder transformer,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_675,"Zhou, Wujie","Yang, Penghan","Qiu, Weiwei","Qiang, Fangfang",STONet-S*: A Knowledge-Distilled Approach for Semantic Segmentation in Remote Sensing Images,,2024,0,"Semantic segmentation of remote sensing images is a critical research domain. The integration of cross-modal features enhances stability in intricate environments. Despite the impressive performance of existing methods, their complexity and parameter demands remain significant. Our proposed STONet-S $<^>{\ast }$ , a stepped transmission optimization network (STONet) with knowledge distillation (KD), extracts insights from a pretrained extensive teacher network and transfers them to an untrained compact student network. Initially, a group enhancement and interaction unit (GEIU) correct for background noise influence and seamlessly integrates cross-modal features. Additionally, we introduce a stepped transmission decoder (STD) comprising a stepped capture module (SCM) and a self-reverse revision module (SRRM) to capture multiscale information from the ground up. Furthermore, leveraging the frequency domain, we employ frequency-awareness KD using a discrete cosine transform (DCT) and octave convolution to separate high and low-frequency maps, which are subsequently transferred to the student network. Last, detail-delivery and stepped-response KD (SRKD) mechanisms enhance the learning capacity of the student network. Through extensive experimentation on two datasets, STONet-S $<^>{\ast }$ demonstrates superior segmentation accuracy by achieving remarkable results with only 7.19 M parameters. The corresponding code repository can be accessed at: https://github.com/MAXHAN22/STONet.",Feature extraction,Semantics,Frequency-domain analysis,Convolution,Decoding,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Data mining,Semantic segmentation,Frequency-awareness knowledge distillation (FAKD),group enhancement and interaction unit (GEIU),remote sensing images (RSIs),semantic segmentation,stepped transmission decoder (STD),,,,,,,,,,,,,,,,,,,,,,,
Row_676,"Zhang, Boning","Zhang, Xiaokang","Pun, Man-On","Liu, Ming",PROTOTYPE-BASED CLUSTERED FEDERATED LEARNING FOR SEMANTIC SEGMENTATION OF AERIAL IMAGES,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,2,"Despite its impressive performance on semantic segmentation of remote sensing imagery, deep learning requires a large amount of labeled data for model training, which is both laborious and time-consuming for an individual institution. To cope with this obstacle, federated Learning (FL) has been proposed to enable multiple institutions to train a global model collaboratively without violating privacy rules. However, the performance of FL is poor in the presence of heterogeneous training data, i.e. the data is not independently and identically distributed (non-i.i.d) among participating clients, especially for remote sensing images with high spatial and spectral heterogeneity. In this paper, we propose an FL algorithm combined with prototype-based hierarchical clustering (FedPHC). Instead of updating a single global model to capture the shared knowledge of all clients, we utilize a mixture of multiple global models to handle the heterogeneity between various clients using hierarchical clustering (HC) based on the prototypical representations of clients' datasets. As a result, FedPHC can reduce the domain discrepancy within each group and obtain more representative models for heterogeneous datasets. Extensive experiments on the Inria Aerial Image Dataset confirm the effectiveness of FedPHC.",Semantic segmentation,federated learning,remote sensing images,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_677,"Ma, Siteng","Hou, Biao","Guo, Xianpeng","Wu, Zitong",Unsupervised Prototype-Wise Contrastive Learning for Domain Adaptive Semantic Segmentation in Remote Sensing Image,,2023,2,"Labeling data in the field of remote sensing is time-consuming and labor-intensive, making domain adaptation between different domains an urgently needed solution. To address the domain gap between diverse datasets in the remote-sensing domain, numerous methods tailored for domain adaptation in high-resolution remote-sensing imagery (RSI) have emerged. Some of the existing methods focus on reducing the domain gap at either the feature level or the pixel level, often overlooking their underlying connection. To tackle this issue, we introduce a prototype-wise contrastive feature alignment (PCFA) paradigm aimed at bridging the representations between the feature and pixel levels. By dynamically updating, we acquire prototype information encompassed by different mini-batches and employ an optimal transport mechanism to reasonably apply the prototype feature distribution in guiding the learning of target domain features. We conduct extensive domain adaptation semantic segmentation (DASS) experiments on the ISPRS Vaihingen and Potsdam datasets, achieving an improvement of about 4%-5% in mean intersection over union (mIoU) compared to previous methods using the DeepLabV2 framework.",Contrastive feature alignment,domain adap-tation semantic segmentation,domain gap,feature distribution,optimal transport mechanism,"Li, Zhihao","Wu, Hang","Jiao, Licheng",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,prototype,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_678,"Sun, Yihao","Wang, Mingrui","Huang, Xiaoyi","Xin, Chengshu",Fast Semantic Segmentation of Ultra-High-Resolution Remote Sensing Images via Score Map and Fast Transformer-Based Fusion,,SEP 2024,0,"For ultra-high-resolution (UHR) image semantic segmentation, striking a balance between computational efficiency and storage space is a crucial research direction. This paper proposes a Feature Fusion Network (EFFNet) to improve UHR image semantic segmentation performance. EFFNet designs a score map that can be embedded into the network for training purposes, enabling the selection of the most valuable features to reduce storage consumption, accelerate speed, and enhance accuracy. In the fusion stage, we improve upon previous redundant multiple feature fusion methods by utilizing a transformer structure for one-time fusion. Additionally, our combination of the transformer structure and multibranch structure allows it to be employed for feature fusion, significantly improving accuracy while ensuring calculations remain within an acceptable range. We evaluated EFFNet on the ISPRS two-dimensional semantic labeling Vaihingen and Potsdam datasets, demonstrating that its architecture offers an exceptionally effective solution with outstanding semantic segmentation precision and optimized inference speed. EFFNet substantially enhances critical performance metrics such as Intersection over Union (IoU), overall accuracy, and F1-score, highlighting its superiority as an architectural innovation in ultra-high-resolution remote sensing image semantic segmentation.",semantic segmentation,ultra-high-resolution,multiscale feature fusion,attention mechanism,,"Sun, Yinan",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_679,"Gao, Tianyi","Gao, Zhi","Ji, Hong","Ao, Wei",Query Adaptive Transformer and Multiprototype Rectification for Few-Shot Remote Sensing Image Segmentation,,2024,0,"Deep learning has emerged as a powerful tool for semantic segmentation tasks. However, in some data-deficient and resource-limited scenarios, networks are constrained to learn novel concepts. To tackle this problem, few-shot segmentation (FSS) has been proposed by the machine learning community, aiming at segmenting novel objects by leveraging a handful of annotated samples. However, most advanced FSS algorithms suffer from severe performance degradation when directly applied to remote sensing image (RSI) domains. Challenges arise due to the unique characteristics of RSIs. To address the large intraclass variation brought by various imaging conditions and large-scale variations, a query adaptive transformer (QAT) is proposed. QAT incorporates query priors into the feature extraction process, adapting RSI features to novel objects from query RSIs. It is noteworthy that query priors are extracted via a query prior generation (QPG) module devoid of any query label information. To alleviate interference brought by complex object distribution, a multiprototype strategy is adopted instead of representing objects with a single prototype ambiguously. Moreover, we rectify the prototypes using a prior injection module (PIM), thereby fully leveraging the advantages offered by query priors. The superiority of our method is validated through comprehensive experiments on the public iSAID-5(i) dataset and comparisons with state-of-the-art methods. Finally, we propose a novel cross-domain setting to investigate the potential and generalizability of few-shot RSI segmentation for several Earth observation applications.",Prototypes,Transformers,Feature extraction,Remote sensing,Earth,"Song, Weiwei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Semantic segmentation,Measurement,Graphical models,Geospatial analysis,Cross-domain,data scarcity,few-shot segmentation (FSS),remote sensing image (RSI),,,,,,,,,,,,,Transformer,,,,,,,,
Row_680,"Chen, Jifa","Chen, Gang","Zhang, Li","Huang, Min",Category-sensitive semi-supervised semantic segmentation framework for land-use/land-cover mapping with optical remote sensing images,,NOV 2024,0,"High-quality land-use/land-cover mapping with optical remote sensing images yet presents significant work. Even though fully convolutional semantic segmentation models have recently contributed to popular solutions, the lack of annotation data may lead to severe degradations in their inference performance. Besides, the category confusion in high-resolution representations will further exacerbate the adverse effects. In this paper, we propose a category-sensitive semi-supervised semantic segmentation framework to address these weaknesses by employing massive unlabeled data. With the perturbations from adopted hybrid data augmentation structures, we first focus on the output space and execute regularization constraints to learn category-specific discriminative features. It is formulated with a consistency self-training procedure where a dynamic class-balanced threshold selection scheme is proposed to provide high-confident pseudo supervisions for each category. In addition, we introduce pixel-wise contrastive learning on the common embedding space from both labeled and unlabeled data domains to further facilitate the semantic dependencies among category features, in which the reliable labels are leveraged as guidance for pixel sample selection. We verify the proposed framework on two benchmark land-use/ land-cover datasets, and the experimental results demonstrate its competitive performance to other state-of-theart semi-supervised methods.",Land-use/land-cover mapping,Semantic segmentation,Semi-supervised learning,Optical remote sensing images,,"Luo, Jin","Ding, Mingjun","Ge, Yong",,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_681,"Liu, Zeping","Tang, Hong",,,Learning Sparse Geometric Features for Building Segmentation from Low-Resolution Remote-Sensing Images,,APR 2023,3,"High-resolution remote-sensing imagery has proven useful for building extraction. Unfortunately, due to the high acquisition costs and infrequent availability of high-resolution imagery, low-resolution images are more practical for large-scale mapping or change tracking of buildings. However, extracting buildings from low-resolution images is a challenging task. Compared with high-resolution images, low-resolution images pose two critical challenges in terms of building segmentation: the effects of fuzzy boundary details on buildings and the lack of local textures. In this study, we propose a sparse geometric feature attention network (SGFANet) based on multi-level feature fusion to address the aforementioned issues. From the perspective of the fuzzy effect, SGFANet enhances the representative boundary features by calculating the point-wise affinity of the selected feature points in a top-down manner. From the perspective of lacking local textures, we convert the top-down propagation from local to non-local by introducing the grounding transformer harvesting the global attention of the input image. SGFANet outperforms competing baselines on remote-sensing images collected worldwide and multiple sensors at 4 and 10 m resolution, thereby, improving the IoU by at least 0.66%. Notably, our method is robust and generalizable, which makes it useful for extending the accessibility and scalability of building dynamic tracking across developing areas (e.g., the Xiong'an New Area in China) by using low-resolution images.",artificial intelligence,deep learning,remote sensing,semantic segmentation,building extraction,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_682,"Nanni, Loris","Brahnam, Sheryl","Loreggia, Andrea",,An Enhanced Loss Function for Semantic Road Segmentation in Remote Sensing Images,,2024,0,"The analysis of road continuity in satellite images is a complex challenge. This is due to the difficulty in identifying the directional vector of road sections, especially when the satellite view of roads is obstructed by trees or other structures. Today, most research focuses on optimizing the deep learning network topology, however, the accuracy of segmentation is affected by the loss function used in training; currently, little research has been published on ad-hoc loss functions for road segmentation. To solve this problem, we proposed loss functions based on topological pixel analysis, in which more weight is given to problematic pixels representing non-real road breaks. We report the results of different tests, obtaining state-of-the-art performance among convolution neural network-based approaches. For instance, on the Massachusetts Roads dataset, our method achieved a Dice score of 75.34% and an IoU of 60.44%, compared to the best baseline scores of 74.64% and 59.51% achieved by GapLoss. Similarly, on the DeepGlobe Roads dataset, our method obtained a Dice score of 79.78% and an IoU of 66.36%, outperforming the best baseline scores of 78.62% and 64.47% by GapLoss. Both the code and information for replicating our experiments are available at https://github.com/LorisNanni/An-Enhanced-Loss-Function-for-Semantic-Road-Segmentation-in-Remote-Sensing-Images, so as to enable future reliable comparisons.",Roads,Image segmentation,Remote sensing,Task analysis,Convolution,,,,,IEEE ACCESS,,Feature extraction,Deep learning,Convolutional neural networks,road segmentation,optimization,ensemble,,,,,,,,,,,,,,,,,,,,,,,,
Row_683,"Zhang, Chao","Weng, Liguo","Ding, Li","Xia, Min",CRSNet: Cloud and Cloud Shadow Refinement Segmentation Networks for Remote Sensing Imagery,,MAR 2023,26,"Cloud detection is a critical task in remote sensing image tasks. Due to the influence of ground objects and other noises, the traditional detection methods are prone to miss or false detection and rough edge segmentation in the detection process. To avoid the defects of traditional methods, Cloud and Cloud Shadow Refinement Segmentation Networks are proposed in this paper. The network can correctly and efficiently detect smaller clouds and obtain finer edges. The model takes ResNet-18 as the backbone to extract features at different levels, and the Multi-scale Global Attention Module is used to strengthen the channel and spatial information to improve the accuracy of detection. The Strip Pyramid Channel Attention Module is used to learn spatial information at multiple scales to detect small clouds better. Finally, the high-dimensional feature and low-dimensional feature are fused by the Hierarchical Feature Aggregation Module, and the final segmentation effect is obtained by up-sampling layer by layer. The proposed model attains excellent results compared to methods with classic or special cloud segmentation tasks on Cloud and Cloud Shadow Dataset and the public dataset CSWV.",semantic segmentation,deep learning,remote sensing imagery,attention,,"Lin, Haifeng",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_684,"He, Shuke","Jin, Chen","Shu, Lisheng","He, Xuzhi",A new framework for improving semantic segmentation in aerial imagery,,MAR 19 2024,1,"High spatial resolution (HSR) remote sensing imagery presents a rich tapestry of foreground-background intricacies, rendering semantic segmentation in aerial contexts a formidable and vital undertaking. At its core, this challenge revolves around two pivotal questions: 1) Mitigating Background Interference and Enhancing Foreground Clarity. 2) Accurate Segmentation in Dense Small Object Cluster. Conventional semantic segmentation methods primarily cater to the segmentation of large-scale objects in natural scenes, yet they often falter when confronted with aerial imagery's characteristic traits such as vast background areas, diminutive foreground objects, and densely clustered targets. In response, we propose a novel semantic segmentation framework tailored to overcome these obstacles. To address the first challenge, we leverage PointFlow modules in tandem with the Foreground-Scene (F-S) module. PointFlow modules act as a barrier against extraneous background information, while the F-S module fosters a symbiotic relationship between the scene and foreground, enhancing clarity. For the second challenge, we adopt a dual-branch structure termed disentangled learning, comprising Foreground Precedence Estimation and Small Object Edge Alignment (SOEA). Our foreground saliency guided loss optimally directs the training process by prioritizing foreground examples and challenging background instances. Extensive experimentation on the iSAID and Vaihingen datasets validates the efficacy of our approach. Not only does our method surpass prevailing generic semantic segmentation techniques, but it also outperforms state-of-the-art remote sensing segmentation methods.",deep learning,aerial imagery,remote sensing segmentation,foreground saliency enhancement,small objects semantic segmentation,"Wang, Mingyi","Liu, Gang",,,FRONTIERS IN REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_685,"Yang, Yunsong","Li, Jinjiang","Chen, Zheng","Ren, Lu",GVANet: A Grouped Multiview Aggregation Network for Remote Sensing Image Segmentation,,2024,0,"In remote sensing image segmentation tasks, various challenges arise, including difficulties in recognizing objects due to differences in perspective, difficulty in distinguishing objects with similar colors, and challenges in segmentation caused by occlusions. To address these issues, we propose a method called the grouped multiview aggregation network (GVANet), which leverages multiview information for image analysis. This approach enables global multiview expansion and fine-grained cross-layer information interaction within the network. Within this network framework, to better utilize a wider range of multiview information to tackle challenges in remote sensing segmentation, we introduce the multiview feature aggregation block for extracting multiview information. Furthermore, to overcome the limitations of same-level shortcuts when dealing with multiview problems, we propose the channel group fusion block for cross-layer feature information interaction through a grouped fusion approach. Finally, to enhance the utilization of global features during the feature reconstruction phase, we introduce the aggregation-inhibition-activation block for feature selection and focus, which captures the key features for segmentation. Comprehensive experimental results on the Vaihingen and Potsdam datasets demonstrate that GVANet outperforms current state-of-the-art methods, achieving mIoU scores of 84.5% and 87.6%, respectively.",Remote sensing,Feature extraction,Accuracy,Semantic segmentation,Semantics,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Convolutional neural networks,Buildings,Attention mechanism,multiscale fusion,remote sensing,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_686,"Li, Xiaoxiang","Huang, Liang","Sun, Yu","Wu, Chunyan",Semantic segmentation of buildings in remote sensing images based on dual-path network with rich-scale features,,SEP 1 2022,2,"To solve the problems of low utilization of spatial features and incomplete contour segmentation in building semantic segmentation of remote sensing images, a building semantic segmentation method in remote sensing images based on dual-path with rich-scale features is proposed. In the shallow spatial path of proposed method, Res2Net module and inception module are used to extract shallow rich scale features to avoid improper use of shallow features affecting segmentation results. In the deep semantic path, ResNet50 combined with hybrid dilated convolution is used as the backbone network, and the obtained high-level semantic features are pooled by a spatial pyramid to capture the deeper multi-scale features. Finally, a new feature fusion module is designed to assign weights to feature maps of different levels extracted from two paths. Experimental results on WHU and Massachusetts building datasets show that the proposed method has higher building extraction accuracy and better generalization ability compared with other semantic segmentation networks. (c) 2022 SPIE and IS&T",semantic segmentation of buildings,rich scale,dilated convolution,pyramid pooling,feature fusion,"Li, Wenguo","Ji, Xinran",,,JOURNAL OF ELECTRONIC IMAGING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_687,"Gao, Kuiliang","Yu, Anzhu","You, Xiong","Guo, Wenyue",Integrating Multiple Sources Knowledge for Class Asymmetry Domain Adaptation Segmentation of Remote Sensing Images,,2024,5,"In the existing unsupervised domain adaptation (UDA) methods for remote sensing images (RSIs) semantic segmentation, class symmetry is a widely followed ideal assumption, where the source and target RSIs have exactly the same class space. In practice, however, it is often very difficult to find a source RSI with exactly the same classes as the target RSI. More commonly, there are multiple source RSIs available. And there is always an intersection or inclusion relationship between the class spaces of each source-target pair, which can be referred to as class asymmetry. Nevertheless, the class asymmetry domain adaptation segmentation of RSIs with multiple sources has not yet been explored. To this end, a novel class asymmetry RSIs domain adaptation method is proposed for the first time in this article, which consists of four key components. First, a multibranch segmentation network is built to learn an expert for each source RSI. Second, a novel collaborative learning method with the cross-domain mixing strategy is proposed, to supplement the class information for each source while achieving the domain adaptation of each source-target pair. Third, a pseudolabel generation strategy is proposed to effectively combine the strengths of different experts, which can be flexibly applied to two cases where the source class union is equal to or includes the target class set. Fourth, a multiview-enhanced knowledge integration module is developed for high-level knowledge routing and transfer from multiple domains to target predictions. The experimental results of six different class settings on airborne and spaceborne RSIs show that the proposed method can effectively perform the multisource domain adaptation in the case of class asymmetry, and the obtained segmentation performance of target RSIs is significantly better than the existing relevant methods.",Class asymmetry,multiple sources,remote sensing images (RSIs),semantic segmentation,unsupervised domain adaptation (UDA),"Li, Ke","Huang, Ningbo",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_688,"Cheng, Xu","Liu, Lihua","Song, Chen",,A Cyclic Information-Interaction Model for Remote Sensing Image Segmentation,,OCT 2021,3,"Object detection and segmentation have recently shown encouraging results toward image analysis and interpretation due to their promising applications in remote sensing image fusion field. Although numerous methods have been proposed, implementing effective and efficient object detection is still very challenging for now, especially for the limitation of single modal data. The use of a single modal data is not always enough to reach proper spectral and spatial resolutions. The rapid expansion in the number and the availability of multi-source data causes new challenges for their effective and efficient processing. In this paper, we propose an effective feature information-interaction visual attention model for multimodal data segmentation and enhancement, which utilizes channel information to weight self-attentive feature maps of different sources, completing extraction, fusion, and enhancement of global semantic features with local contextual information of the object. Additionally, we further propose an adaptively cyclic feature information-interaction model, which adopts branch prediction to decide the number of visual perceptions, accomplishing adaptive fusion of global semantic features and local fine-grained information. Numerous experiments on several benchmarks show that the proposed approach can achieve significant improvements over baseline model.",deep learning,image segmentation,transfer learning,remote sensing image,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_689,"Zhang, Yijie","Cheng, Jian","Su, Yanzhou","Deng, Changjian",Global Adaptive Second-Order Transformer for Remote Sensing Image Semantic Segmentation,,2024,0,"In the domain of remote sensing (RS) image analysis, capturing global context is the key for precise semantic segmentation. Current vision transformer (ViT) advance this field by addressing convolutional neural network's (CNN) local receptive field limitations. However, ViT predominantly rely on the first-order information in image to establish global relationships, often overlooking the potential of second-order information, which is crucial for enhancing the discrimination of ground objects that exhibit high similarity and constant changes. To address this issue, we propose a global adaptive second-order transformer network (GASOT-Net). Specifically, the proposed global adaptive second-order transformer (GASOT) enhances the existing ViT structure by mining second-order information and adaptively fusing it with the first-order information during the process of establishing global dependency relationships. This approach enables the extraction of more discriminative features, thereby enriching the representation of global features. In addition, the local feature aggregation module (LFAM) is proposed to effectively aggregate features from different stages of CNN as input to the GASOT blocks. Moreover, to refine boundaries of complex ground objects, the global feature enhancement module (GFEM) is used in the decoder stage. In particular, GFEM includes two sub modules-feature shift module (FSM) and hierarchical feature fusion module (HFFM). FSM is used to enhance the local feature representation at first, and then, HFFM hierarchically aggregates local and global features from different stages. We conduct extensive experiments on four benchmark RS datasets, and the results show that our GASOT-Net outperforms other state-of-the-art methods. The code will be available at: https://github.com/j136812832/GASOT-Net.",Feature enhancement,global feature,second-order transformer,semantic segmentation,Feature enhancement,"Xia, Ziying","Tashi, Nyima",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,global feature,second-order transformer,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_690,"Jin, Yuanhang","Liu, Xiaosheng","Huang, Xiaobin",,EMR-HRNet: A Multi-Scale Feature Fusion Network for Landslide Segmentation from Remote Sensing Images,,JUN 2024,1,"Landslides constitute a significant hazard to human life, safety and natural resources. Traditional landslide investigation methods demand considerable human effort and expertise. To address this issue, this study introduces an innovative landslide segmentation framework, EMR-HRNet, aimed at enhancing accuracy. Initially, a novel data augmentation technique, CenterRep, is proposed, not only augmenting the training dataset but also enabling the model to more effectively capture the intricate features of landslides. Furthermore, this paper integrates a RefConv and Multi-Dconv Head Transposed Attention (RMA) feature pyramid structure into the HRNet model, augmenting the model's capacity for semantic recognition and expression at various levels. Last, the incorporation of the Dilated Efficient Multi-Scale Attention (DEMA) block substantially widens the model's receptive field, bolstering its capability to discern local features. Rigorous evaluations on the Bijie dataset and the Sichuan and surrounding area dataset demonstrate that EMR-HRNet outperforms other advanced semantic segmentation models, achieving mIoU scores of 81.70% and 71.68%, respectively. Additionally, ablation studies conducted across the comprehensive dataset further corroborate the enhancements' efficacy. The results indicate that EMR-HRNet excels in processing satellite and UAV remote sensing imagery, showcasing its significant potential in multi-source optical remote sensing for landslide segmentation.",remote sensing,landslide segmentation,HRNet,attention mechanism,,,,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_691,"Sheng, Jiajia","Sun, Youqiang","Huang, He","Xu, Wenyu",HBRNet: Boundary Enhancement Segmentation Network for Cropland Extraction in High-Resolution Remote Sensing Images,,AUG 2022,7,"Cropland extraction has great significance in crop area statistics, intelligent farm machinery operations, agricultural yield estimates, and so on. Semantic segmentation is widely applied to remote sensing image cropland extraction. Traditional semantic segmentation methods using convolutional networks result in a lack of contextual and boundary information when extracting large areas of cropland. In this paper, we propose a boundary enhancement segmentation network for cropland extraction in high-resolution remote sensing images (HBRNet). HBRNet uses Swin Transformer with the pyramidal hierarchy as the backbone to enhance the boundary details while obtaining context. We separate the boundary features and body features from the low-level features, and then perform a boundary detail enhancement module (BDE) on the high-level features. Endeavoring to fuse the boundary features and body features, the module for interaction between boundary information and body information (IBBM) is proposed. We select remote sensing images containing large-scale cropland in Yizheng City, Jiangsu Province as the Agricultural dataset for cropland extraction. Our algorithm is applied to the Agriculture dataset to extract cropland with mIoU of 79.61%, OA of 89.4%, and IoU of 84.59% for cropland. In addition, we conduct experiments on the DeepGlobe, which focuses on the rural areas and has a diversity of cropland cover types. The experimental results indicate that HBRNet improves the segmentation performance of the cropland.",high-resolution remote sensing images,semantic segmentation,transformer,boundary refinement,cropland extraction,"Pei, Haotian","Zhang, Wei","Wu, Xiaowei",,AGRICULTURE-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_692,"Hong, Eungi","Koo, Jamyoung","Pyo, Seongmin","Choi, Haechul",Tackling Dual Gaps in Remote Sensing Segmentation: Task-Oriented Super-Resolution for Domain Adaptation,,2024,0,"Semantic segmentation of remote sensing images plays a crucial role in various applications, such as land cover mapping and urban planning. However, the performance of semantic segmentation models often degrades when applied to images from different domains or with varying spatial resolutions. In this paper, we propose a novel task-oriented super-resolution method for domain adaptation in remote sensing semantic segmentation. Our approach aims to adapt a segmentation model trained on high-resolution images from a source domain to perform accurately on low-resolution images from a target domain. We introduce a super-resolution network that learns to enhance the spatial resolution of the target domain images while simultaneously optimizing the segmentation performance of a pre-trained and fixed segmentation model. The super-resolution network is trained using a combination of losses, including a segmentation loss, a perceptual loss, and a contrastive loss, which together ensure that the adapted images are both visually similar to the source domain images and semantically consistent with the ground-truth segmentation masks. We evaluate our method on two challenging remote sensing datasets, ISPRS Potsdam and Vaihingen, and demonstrate significant improvements in segmentation accuracy compared to state-of-the-art domain adaptation techniques. Our approach achieves mean Intersection over Union (mIoU) scores of 0.523 and 0.567 on the Potsdam and Vaihingen datasets, respectively. The proposed task-oriented super-resolution method offers a promising solution for adapting semantic segmentation models to new domains and resolutions in remote sensing applications.",Adaptation models,Superresolution,Remote sensing,Feature extraction,Predictive models,"Kim, Eunkyung","Jang, Haneol",,,IEEE ACCESS,,Semantic segmentation,Spatial resolution,Computer architecture,Training,Semantics,Domain adaptation,remote sensing imagery,semantic segmentation,task-oriented super-resolution,,,,,,,,,,,,,,,,,,,,,
Row_693,"Qiu, Kevin","Bulatov, Dimitri","Budde, Lina E.","Kullmann, Timo",INFLUENCE OF OUT-OF-DISTRIBUTION EXAMPLES ON THE QUALITY OF SEMANTIC SEGMENTATION IN REMOTE SENSING,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"Semantic segmentation for land cover maps follows the closed world assumption, where each pixel must be classified into a set of predefined classes. In order to fulfill this assumption, an additional class is usually introduced to describe all areas not covered by the main classes, called ""clutter"" or ""other"". Consequently, this class is extremely heterogeneous, and the classification is usually subpar. Using a common approach for uncertainty assessment of land cover classification, we analyze the influence of the clutter class being present or absent during training on the semantic segmentation. We assess the model uncertainties of two different deep learning models, U-Net and DeepLab V3+, and different training configurations by using a Monte-Carlo dropout based uncertainty metric. The corresponding uncertainty maps and histograms show a correlation between clutter class and the uncertainty metric.",Semantic segmentation,Monte-Carlo dropout,out-of-distribution,quality assessment,uncertainty,"Iwaszczuk, Dorota",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_694,"Qiao, Wenfan","Shen, Li","Wang, Jicheng","Yang, Xiaotian",A Weakly Supervised Semantic Segmentation Approach for Damaged Building Extraction From Postearthquake High-Resolution Remote-Sensing Images,,2023,20,"Quick and accurate building damage assessment following a disaster is critical to making a preliminary estimate of losses. Remote-sensing image analysis based on convolutional neural networks (CNNs) and their relatives has shown a growing potential in this task, but faces the challenge of collecting dense pixel-level annotations. In this letter, we propose a novel weakly supervised semantic segmentation (WSSS) method based on image-level labels for pixel-wise damaged building extraction from postearthquake high-resolution remote-sensing (HRRS) images. The proposed method aims to improve the quality of the class activation map (CAM) to boost model performance. To be specific, a multiscale dependence (MSD) module and a spatial correlation refinement (SCR) module are designed by considering the special characteristics of the damaged building and are integrated into an encoder-decoder network. The former is used for complete and dense localization of damaged buildings in CAM, and the latter contributes to noise suppression. Extensive experimental evaluations over three datasets are conducted to confirm the effectiveness of the proposed approach. Both generated CAMs and extracted damaged building results of our methods are better than that of current state-of-the-art methods.",Class activation map (CAM),convolutional neural network (CNN),damaged building extraction,high-resolution remote-sensing (HRRS) image,weakly supervised semantic segmentation (WSSS),"Li, Zhilin",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_695,"Li, Ruoyang","Xiong, Shuping","Che, Yinchao","Shi, Lei",Research on Efficient Feature Generation and Spatial Aggregation for Remote Sensing Semantic Segmentation,,APR 2024,0,"Semantic segmentation algorithms leveraging deep convolutional neural networks often encounter challenges due to their extensive parameters, high computational complexity, and slow execution. To address these issues, we introduce a semantic segmentation network model emphasizing the rapid generation of redundant features and multi-level spatial aggregation. This model applies cost-efficient linear transformations instead of standard convolution operations during feature map generation, effectively managing memory usage and reducing computational complexity. To enhance the feature maps' representation ability post-linear transformation, a specifically designed dual-attention mechanism is implemented, enhancing the model's capacity for semantic understanding of both local and global image information. Moreover, the model integrates sparse self-attention with multi-scale contextual strategies, effectively combining features across different scales and spatial extents. This approach optimizes computational efficiency and retains crucial information, enabling precise and quick image segmentation. To assess the model's segmentation performance, we conducted experiments in Changge City, Henan Province, using datasets such as LoveDA, PASCAL VOC, LandCoverNet, and DroneDeploy. These experiments demonstrated the model's outstanding performance on public remote sensing datasets, significantly reducing the parameter count and computational complexity while maintaining high accuracy in segmentation tasks. This advancement offers substantial technical benefits for applications in agriculture and forestry, including land cover classification and crop health monitoring, thereby underscoring the model's potential to support these critical sectors effectively.",semantic segmentation,lightweight architecture,attention mechanism,linear transformation,neighborhood feature optimization,"Ma, Xinming","Xi, Lei",,,ALGORITHMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_696,"Boulila, Wadii","Ghandorh, Hamza","Masood, Sharjeel","Alzahem, Ayyub",A transformer-based approach empowered by a self-attention technique for semantic segmentation in remote sensing,,APR 30 2024,2,"Semantic segmentation of Remote Sensing (RS) images involves the classification of each pixel in a satellite image into distinct and non-overlapping regions or segments. This task is crucial in various domains, including land cover classification, autonomous driving, and scene understanding. While deep learning has shown promising results, there is limited research that specifically addresses the challenge of processing fine details in RS images while also considering the high computational demands. To tackle this issue, we propose a novel approach that combines convolutional and transformer architectures. Our design incorporates convolutional layers with a low receptive field to generate fine-grained feature maps for small objects in very high-resolution images. On the other hand, transformer blocks are utilized to capture contextual information from the input. By leveraging convolution and self-attention in this manner, we reduce the need for extensive downsampling and enable the network to work with full-resolution features, which is particularly beneficial for handling small objects. Additionally, our approach eliminates the requirement for vast datasets, which is often necessary for purely transformer-based networks. In our experimental results, we demonstrate the effectiveness of our method in generating local and contextual features using convolutional and transformer layers, respectively. Our approach achieves a mean dice score of 80.41%, outperforming other well-known techniques such as UNet, Fully-Connected Network (FCN), Pyramid Scene Parsing Network (PSP Net), and the recent Convolutional vision Transformer (CvT) model, which achieved mean dice scores of 78.57%, 74.57%, 73.45%, and 62.97% respectively, under the same training conditions and using the same training dataset.",Semantic segmentation,Self-attention,Vision transformer,Satellite images,Remote sensing,"Koubaa, Anis","Ahmed, Fawad","Khan, Zahid","Ahmad, Jawad",HELIYON,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_697,"Li, Xin","Xu, Feng","Liu, Fan","Xia, Runliang",Hybridizing Euclidean and Hyperbolic Similarities for Attentively Refining Representations in Semantic Segmentation of Remote Sensing Images,,2022,12,"Attention mechanisms (AMs) have revolutionized the semantic segmentation network in interpreting remote sensing images (RSIs) due to their amazing ability in establishing contextual dependencies. Nevertheless, due to the complex scenes and diverse objects in RSIs, a variety of details and correlations are not available in Euclidean space. Therefore, a similarity-hybrid attention module (SHAM) is devised to attentively learn the hyperbolic and Euclidean attention maps between any two positions, followed by a weighted elementwise summation. The hybrid attention maps posses latent geometric properties of both Euclidean and hyperboloid. Taking commonly used fully convolutional network (FCN) as baseline, hybrid attention-enhanced neural network (HAENet) that embeds SHAM is presented. Experiments on International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and DeepGlobe benchmarks reveal its superiority to comparative methods. In addition, the ablation study validates the effectiveness of SHAM compared with other attention modules.",Attention mechanism (AM),hyperbolic geometry,semantic segmentation,similarity-hybrid attention,,"Tong, Yao","Li, Linyang","Xu, Zhennan","Lyu, Xin",IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_698,"Wu, Junfeng","Tang, Zhenjie","Xu, Congan","Liu, Enhai",Super-resolution domain adaptation networks for semantic segmentation via pixel and output level aligning,,AUG 25 2022,4,"Recently, unsupervised domain adaptation (UDA) has attracted increasing attention to address the domain shift problem in the semantic segmentation task. Although previous UDA methods have achieved promising performance, they still suffer from the distribution gaps between source and target domains, especially the resolution discrepancy in the remote sensing images. To address this problem, this study designs a novel end-to-end semantic segmentation network, namely, Super-Resolution Domain Adaptation Network (SRDA-Net). SRDA-Net can simultaneously achieve the super-resolution task and the domain adaptation task, thus satisfying the requirement of semantic segmentation for remote sensing images, which usually involve various resolution images. The proposed SRDA-Net includes three parts: a super-resolution and segmentation (SRS) model, which focuses on recovering high-resolution image and predicting segmentation map, a pixel-level domain classifier (PDC) for determining which domain the pixel belongs to, and an output-space domain classifier (ODC) for distinguishing which domain the pixel contribution is from. By jointly optimizing SRS with two classifiers, the proposed method can not only eliminate the resolution difference between source and target domains but also improve the performance of the semantic segmentation task. Experimental results on two remote sensing datasets with different resolutions demonstrate that SRDA-Net performs favorably against some state-of-the-art methods in terms of accuracy and visual quality. Code and models are available at .",remote sensing,semantic segmentation,domain adaptation,super resolution,deep learning,"Gao, Long","Yan, Wenjun",,,FRONTIERS IN EARTH SCIENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_699,"Yang, Yunsong","Yuan, Genji","Li, Jinjiang",,SFFNet: A Wavelet-Based Spatial and Frequency Domain Fusion Network for Remote Sensing Segmentation,,2024,0,"To fully utilize spatial information for segmentation and address the challenge of handling areas with significant grayscale variations in remote sensing segmentation, we propose the spatial and frequency domain fusion network (SFFNet) framework. This framework employs a two-stage network design: the first stage extracts features using spatial methods to obtain features with sufficient spatial details and semantic information; the second stage maps these features in both spatial and frequency domains. In the frequency domain mapping, we introduce the wavelet transform feature decomposer (WTFD) structure, which decomposes features into low-frequency and high-frequency components using the Haar wavelet transform and integrates them with spatial features. To bridge the semantic gap between frequency and spatial features, facilitating significant feature selection to promote the combination of features from different representation domains, we design the multiscale dual-representation alignment filter (MDAF). This structure utilizes multiscale convolutions and dual-cross attentions. Comprehensive experimental results demonstrate that, compared to existing methods, SFFNet achieves superior performance in terms of mean intersection over union (mIoU), reaching 84.80% and 87.73%, respectively. The code is located at https://github.com/yysdck/SFFNet.",Image segmentation,Remote sensing,Frequency-domain analysis,Wavelet transforms,Feature extraction,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantics,Wavelet domain,Attention mechanism,frequency domain features,global modeling,remote sensing,semantic segmentation,wavelet transform,,,,,,,,,,,,,,,,,,,,,,
Row_700,"Liu, Hao","Sun, Bin","Gao, Zhihai","Chen, Zhulin",High resolution remote sensing recognition of elm sparse forest via deep-learning-based semantic segmentation,,SEP 2024,1,"Elm (Ulmus pumila L.) sparse forest plays an vital role in maintaining local ecological stability and security in the Otingdag Sandy Land area. Prior studies on elm canopy extraction have predominantly relied on manual parameter configuration, resulting in unsatisfactory levels of generalization. To meet the needs of high-precision and rapid recognition of elm sparse forests in large areas, this study proposed a recognition method for elm sparse forest that orients to high spatial resolution remote sensing imageries, using deep-learning-based semantic segmentation techniques. It can automatically learn features that are conducive to segmenting the canopy of elm trees, and retains good generalization ability on the Gaofen-2 imageries obtained in different regions. First, we constructed a dataset specialized for elm canopy semantic segmentation task, and annotated over 130,000 elm canopies based on Gaofen-2 imageries. In addition, we trained 7 deep-learning semantic segmentation model candidates. Among them, MANet showed the best performance, with its F1-score reaching 81.44%. Lastly, we applied edge detection to the elm canopy coverage area, and automatically extract the elm canopy. The proposed method can provide technical support for the investigation and monitoring of elm sparse forests, while facilitates local desertification prevention efforts in the entire Otingdag Sandy Region.",Otingdag Sandy Land,Elm Sparse Forest,Gaofen-2,Semantic Segmentation,Deep Learning,"Zhu, Zhongzheng",,,,ECOLOGICAL INDICATORS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_701,"Yu, Zhimin","Wan, Fang","Lei, Guangbo","Xiong, Ying",RSLC-Deeplab: A Ground Object Classification Method for High-Resolution Remote Sensing Images,,SEP 2023,2,"With the continuous advancement of remote sensing technology, the semantic segmentation of different ground objects in remote sensing images has become an active research topic. For complex and diverse remote sensing imagery, deep learning methods have the ability to automatically discern features from image data and capture intricate spatial dependencies, thus outperforming traditional image segmentation methods. To address the problems of low segmentation accuracy in remote sensing image semantic segmentation, this paper proposes a new remote sensing image semantic segmentation network, RSLC-Deeplab, based on DeeplabV3+. Firstly, ResNet-50 is used as the backbone feature extraction network, which can extract deep semantic information more effectively and improve the segmentation accuracy. Secondly, the coordinate attention (CA) mechanism is introduced into the model to improve the feature representation generated by the network by embedding position information into the channel attention mechanism, effectively capturing the relationship between position information and channels. Finally, a multi-level feature fusion (MFF) module based on asymmetric convolution is proposed, which captures and refines low-level spatial features using asymmetric convolution and then fuses them with high-level abstract features to mitigate the influence of background noise and restore the lost detailed information in deep features. The experimental results on the WHDLD dataset show that the mean intersection over union (mIoU) of RSLC-Deeplab reached 72.63%, the pixel accuracy (PA) reached 83.49%, and the mean pixel accuracy (mPA) reached 83.72%. Compared to the original DeeplabV3+, the proposed method achieved a 4.13% improvement in mIoU and outperformed the PSP-NET, U-NET, MACU-NET, and DeeplabV3+ networks.",high-resolution remote sensing images,semantic segmentation,feature fusion,attention mechanism,,"Xu, Li","Ye, Zhiwei","Liu, Wei","Zhou, Wen",ELECTRONICS,"Xu, Chengzhi",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_702,"Ren, Yan","Long, Jie","Gao, Xiaowen","Zhang, Ming",TPL-DA: A Novel Threshold-Free Pseudolabel Learning Framework for Domain Adaptive Semantic Segmentation of High-Resolution Remote Sensing Images,,2025,0,"Semantic segmentation techniques for remote sensing scene understanding have significantly advanced, enhancing the refined Earth observation. However, most methods highly depend on extensive annotated data, leading to performance deterioration in complex high-resolution remote sensing cross-domain scenes, where variations in image conditions and environments are prevalent. Domain adaptive semantic segmentation (DASS) has been proposed to mitigate the reliance on dense and costly annotations, typically using stagewise training. This article addresses three key challenges in existing DASS methods: 1) insufficient warmup training, limiting potential performance gains; 2) rigid pseudolabel threshold settings in self-training (ST) result in performance bottlenecks; 3) entropy-based prediction bias alone fails to effectively identify high-confidence noise early in ST. To address these issues, we propose a novel threshold-free pseudolabel learning framework, TPL-DA. During the warmup stage, we introduce a multiview bidirectional consistency learning mechanism within a teacher-student architecture. This mechanism employs a bias-free data augmentation strategy, fostering consistent bidirectional predictions in teacher-student networks, thereby enhancing domain generalization and feature robustness. Our multiscale context-enhanced prediction module further amplifies this. In the ST stage, we propose a dynamic threshold-free pseudolabel learning strategy that utilizes well-aligned feature prototypes in the feature space to guide pseudolabel generation in the probability space, eliminating the threshold constraints. In addition, we model uncertainty using relative entropy and incorporate it into the optimization objective to manage high-confidence noise. Extensive experiments on the LoveDA, Potsdam, and Vaihingen datasets demonstrate that TPL-DA consistently outperforms existing methods and popular benchmarks, significantly enhancing DASS performance across diverse cross-domain scenes.",Domain adaptive semantic segmentation (DASS),high-resolution remote sensing (HRRS),self-training (ST),threshold,uncertainty estimation,"Liu, Guoqing","Su, Nan",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Domain adaptive semantic segmentation (DASS),high-resolution remote sensing (HRRS),self-training (ST),threshold,uncertainty estimation,,,,,,,,,,,,,,,,,,,,,,,,,
Row_703,"Chen, Dong","Wang, Yuebin","Zhang, Liqiang",,DA&MTSS: An End-to-End Remote Sensing Image Domain Adaptive Semantic Segmentation Framework Combining Data Augmentation and Mobile Threshold Self-Supervision,,2024,0,"The application of deep learning-based semantic segmentation in remote sensing (RS) images has achieved considerable success. However, many supervised methods still heavily rely on a large amount of labeled data, requiring time-consuming and labor-intensive manual annotations. Besides, networks trained on labeled source domain data often perform poorly in inference tasks with target domain data due to the domain shift phenomenon. To address these challenges, we construct a novel end-to-end unsupervised domain adaptation (UDA) framework, named data augmentation and mobile threshold self-supervision (DA&MTSS), which integrates data augmentation with self-supervision. Specifically, we analyze the common factors that cause domain shifts in RS images and adopt different data augmentation techniques to attenuate the domain shift and enhance the generalization ability and robustness of the network in cross-domain inference. In the self-supervision phase, we design a new sample-based mobile threshold method to dynamically control the thresholds of both dominant and long-tail classes during training and generate stable pseudo-labels. Therefore, our method eliminates the need for additional training or expert knowledge and achieves the co-evolution of network parameters and pseudo-label quality in the training process. The results of comprehensive experiments on five tasks across the ISPRS Vaihingen, Potsdam, and LoveDA datasets demonstrate that this method consistently achieves higher mIoU scores, showcasing the performance advantage of DA&MTSS in UDA for the semantic segmentation of RS image.",Training,Task analysis,Semantic segmentation,Semantics,Data augmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Spatial resolution,Sensors,reliable pseudo-labels,remote sensing (RS) image,self-supervision,self-training,unsupervised domain adaptation (UDA) semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_704,"Zhong, Hai-Feng","Sun, Hong-Mei","Han, Dong-Nuo","Li, Zeng-Hu",Lake water body extraction of optical remote sensing images based on semantic segmentation,,DEC 2022,18,"Automatically extract lake water bodies of optical remote sensing images is a very challenging task, because there are many small lakes in such images, these small lakes have the characteristics of weak target information and are easily interfered by noise information. Regarding above problems, this paper proposes an automatic extraction method of lake water based on semantic segmentation. Firstly, a multi-scale information enhancement network is designed based on the encoder-decoder structure, and the deep dilation residual structure is used in the encoder module of the network to improve the network's ability to mine the deep feature information and the context information of the lake water bodies. Secondly, the two-way channel attention mechanism is introduced into the network, which can reduce the interference of noise information on the lake boundaries and improve the accuracy of the network to the lake boundaries segmentation. Finally, the up-sampling convolution operation is used in the decoder module of the network to reduce the information loss during the up-sampling process. In this paper, the performance of the designed network is tested by using remote sensing images of lakes of different map scales and various evaluation indexes. The experimental results show that the designed network has better segmentation accuracy than other semantic segmentation networks.",Lake water body extraction,Optical remote sensing images,Weak target information,Deep dilation residual structure,,"Jia, Rui-Sheng",,,,APPLIED INTELLIGENCE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_705,"Liang, Zhengyin","Wang, Xili",,,Semantic Segmentation of Multispectral Remote Sensing Images Based on Band-Location Adaptive Selection,,JUL 2023,1,"Multispectral remote sensing images (MSIs) provide a substantial amount of ground object information spread over various spectral bands of the image. The quantity of information contained in different bands or different spatial locations within the same band varies significantly. How to capture useful information from MSIs is a challenging task in semantic segmentation of remote sensing images. An end -to -end semantic segmentation network (BLASeNet) based on band -location adaptive selection is proposed here. The proposed network adopts an encoder -decoder structure. In the coding phase, a band -location adaptive selection mechanism is proposed to adaptively learn the weights of different bands and different spatial locations within the same band, enhancing the effective features expression. The spectral -spatial features of 3D residual block -coded images are further proposed to make use of the band correlation of MSIs. During the decoding phase, an adaptive feature fusion module is proposed to adaptively adjust the fusion ratio of low-level detail features and high-level semantic features via network learning, as well as investigate the impact of three fusion strategies, namely, addition (BLASeNet-A), element multiplication (BLASeNet-M), and concatenation (BLASeNet-C), on the model's performance gain. Furthermore, channel attention is extended to 3D data, and the fused feature map is recalibrated on the channel dimension to produce a more accurate multi -level interactive feature map. The effectiveness of BLASeNet has been demonstrated by experimental results on ISPRS Potsdam, Qinghai and Tibet Plateau datasets.",image processing,semantic segmentation,3D convolution,band-location adaptive selection mechanism,attention mechanism,,,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_706,"Lyu, Xinran","Zhang, Libao",,,PROGRESSIVE REFINEMENT LEARNING BASED ON FEATURE INTERACTIVE FUSION FOR SEMANTIC SEGMENTATION OF REMOTE SENSING LIMITED DATASET,"2023 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",2023,0,"Due to the labor cost and the accuracy of manual identification, it is very difficult to make a strong label dataset of remote sensing images with a large amount of data. Therefore, the limited remote sensing dataset has become a research hotspot in recent years. However, due to insufficient precision and the lack of label accuracy, these methods often have insufficient expression ability. In this paper, we proposed a semantic segmentation method for remote sensing images by progressive refinement learning. Firstly, we construct multiple classification networks to vote for label noise cleaning, and select a network to retrain. Then, the method based on hierarchical feature learning is used to realize the pixel-level pseudo label calculation. Secondly, we proposed to construct feature interactive fusion module in the multi-level codec to achieve image group semantic segmentation. Comprehensive evaluations and the comparison with 7 methods validate the superiority of the proposed model.",Remote sensing,limited dataset,progressive refinement learning,feature interactive fusion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_707,"Zhang, Chunsen","Ge, Yingwei",,,Semantic Segmentation of Buildings in High Resolution Remote Sensing Images Using Conditional Random Fields,MIPPR 2019: PATTERN RECOGNITION AND COMPUTER VISION,2020,0,"This paper proposes a method of building a semantic segmentation method for high-resolution remote sensing images of conditional random fields. Through a large number of actual data operations comparison, U-Net semantic segmentation model is selected as the improved basic model in many deep convolutional neural network models. In order to improve the singularity of the upsampling operation, the U-Net semantic segmentation model is improved as follows: First, the model's crop-copy connection structure is changed to the pyramid pooling layer, and then the multi-scale representation feature image is used, and the multi-scale is used. The resampling of the feature image and the fine bilinear interpolation yield the maximum response at different scales. The improved U-Net model extracts more complete image features. The rough segmentation results are used as the initial input values of the fully connected conditional random fields (CRFs). The global pixel potential energy is inferred through the fully connected graph, and the feature images are refined. Target matching. Finally, the image features are input to the sigmoid classifier for analysis. The results show that the CRF-SUNet model with introduced conditional random field has high segmentation precision, and the boundary of the segmented building is clear, smooth and complete.",Deep learning,Semantic segmentation,Convolutional neural network,Conditional random field,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_708,"Gao, Kuiliang","Yu, Anzhu","You, Xiong","Qiu, Chunping",Prototype and Context-Enhanced Learning for Unsupervised Domain Adaptation Semantic Segmentation of Remote Sensing Images,,2023,12,"In unsupervised domain adaptation (UDA) of remote sensing images (RSIs), the huge interdomain discrepancies and intradomain variances lead to complicated class-level relations. Specifically, the instances of the same class differ greatly, while instances of different classes are similar, whether across different RSIs domains or within the same RSIs domain. However, existing methods cannot fully consider these problems, limiting the performance of UDA semantic segmentation of RSIs. To this end, this article proposes a novel cross-domain multiprototypes learning method, the core idea of which is to abstract the cross-domain and intradomain class-level relations into multiple prototypes. Specifically, the multiple prototypes belonging to different classes can detailedly describe complex interclass relations, and the multiple prototypes within the same class can better model rich intraclass relations. Furthermore, the source and target samples are jointly used for prototypes calculation, to fully fuse the feature information of different RSIs. In a nutshell, utilizing the samples from different RSIs domains to learn multiple prototypes for each class can achieve better domain alignment at the class level. In addition, considering that RSIs simultaneously contain large targets with wide coverage and important small targets, two masked consistency learning strategies are designed to better explore the contextual structure of target RSIs and improve the quality of pseudo labels for prototype updating. The global consistency strategy can strengthen the utilization of global context relations, while the local consistency strategy can further improve the learning of local context details. Therefore, the proposed method is actually a prototype and context-enhanced learning method for UDA semantic segmentation of RSIs. Extensive experiments demonstrate that the proposed method can achieve better performance than existing state-of-the-art UDA methods.",Prototypes,Semantic segmentation,Semantics,Adaptation models,Training,"Liu, Bing",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Task analysis,Supervised learning,Context enhancement,cross-domain multiprototypes,remote sensing images (RSIs),semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,,
Row_709,"Zhang, Qing","Pan, Shulin","Min, Fan","Wu, Yinghe",Noisy Supervised Deep Learning for Remote Sensing Image Segmentation Using Electronic Maps,,2023,5,"Deep learning has made substantial progress in remote sensing image segmentation tasks. It usually requires a large number of high-quality annotation maps (i.e., clean labels), which are labor-intensive. In this letter, we propose to use abundant electronic maps (i.e., noisy labels) to supplement a small number of clean labels to solve the problem of massive label production. In addition, we propose a multistage noise supervised framework to prevent noise from deteriorating the performance of deep model. Noise-supervised deep learning framework (NSDI) consists of clean training, weight initialization, and hybrid training stages. In the clean training stage, we train a segmentation model using a small amount of clean labels and remote sensing images to compute the confusion probability matrix. In the weight initialization stage, we use the confusion probability matrix and the prediction probability to calculate the label reliability of the electronic map. In the hybrid training stage, an adaptive weighted loss function based on cross-entropy is used to dynamically update the label reliability. Then we train the model further using electronic maps with the support of the adaptive weighted loss function and label reliability. Experiments were undertaken on 2656 images of 512 x 512 pixels. Ablation studies show that NSDI improves the model robustness and the segmentation quality.",Electronic map,remote sensing image,SegFormer,semantic segmentation,weakly supervision,"Zhang, Zilin","Luo, Haoran",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_710,"Chen, Hongyu","Zhang, Hongyan","Yang, Guangyi","Li, Shengyang",A Mutual Information Domain Adaptation Network for Remotely Sensed Semantic Segmentation,,2022,10,"Although deep learning has made semantic segmentation of very-high-resolution (VHR) remote sensing (RS) images practical and efficient, its large-scale application is still limited. Given the diversity of imaging sensors, acquisition conditions, and regional styles, a deep learning network well-trained on one source domain dataset often suffers from drastic performance drops when applied to other target domain datasets. Thus, we propose a novel end-to-end mutual information domain adaptation network (MIDANet) that can shift between semantic segmentation domains by integrating multitask learning in the convolutional neural networks within an entropy adversarial learning (EAL) framework. Through the joint learning of semantic segmentation and elevation estimation, the features extracted by MIDANet can concentrate more on the elevation clues while dropping the domain-variant information (i.e., texture, spectral information). First, one encoder is applied to excavate general semantic features. Two decoders that share the same architecture are used to perform pixel-level classification and digital surface model (DSM) regression. Second, feature interaction modules (FIMs) and a mutual information attention unit (MIAU) are designed to mine the latent relationships between the two tasks and enhance their feature representations. Finally, a final MIDANet is obtained for semantic segmentation that does not require any semantic segmentation labels in the target domain after the adversarial learning of the classification entropy at the output level. Extensive comparative experiments and ablation studies were conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Vaihingen test datasets. The results show that MIDANet outperforms other state-of-the-art domain adaptation (DA) methods in both evaluation metrics and visual assessment.",Adversarial learning,domain adaptation (DA),multitask learning,semantic segmentation,,"Zhang, Liangpei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_711,Lu Junyan,Jia Hongguang,Gao Fang,Li Wentao,Reconstruction of Digital Surface Model of Single-view Remote Sensing Image by Semantic Segmentation Network,,APR 2021,2,"A novel method for Digital Surface Model (DSM) reconstruction of single-view remote sensing image is proposed which only relies on light detection and ranging data. Based on deep learning technology, a semantic segmentation network with an encode-decode structure is designed. The network uses Multi-scale Residual Fusion Encode and Decode (MRFED) blocks to extract semantic information from the input image, and then predicts the height value pixel by pixel, as well as adopts a strategy of skip connections with feature maps to preserves the detailed features and structural information of the input image. The model is trained and tested on a public dataset of remote sensing images containing DSM data. Experiments show that, the Mean Absolute Error ( MAE) between DSM reconstruction results and true values is 2.1e-02, the Root Mean Square Error (RMSE) is 3.8e-02, and the Structural SIMilarity (SSIM) is 92.89%, which are all better than the classic deep learning semantic segmentation networks. Experiments confirm that the method can effectively reconstruct the DSM of single-view remote sensing images with high accuracy, as well as the structure of feature distribution.",Semantic segmentation network,Encode,decode,Multi-scale residual fusion,Skip connections,Lu Qing,,,,JOURNAL OF ELECTRONICS & INFORMATION TECHNOLOGY,,Digital Surface Model (DSM),,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_712,"Zhu, Xinyu","Zhang, Zhihua","He, Yi","Wang, Wei",LandslideNet: A landslide semantic segmentation network based on single-temporal optical remote sensing images,,NOV 15 2024,2,"Swiftly and accurately acquiring the spatial distribution, location, and magnitude of landslides while documenting them in a landslide cataloging database can furnish crucial information for precise disaster mitigation measures and secondary hazard prevention. The extraction of landslides using existing semantic segmentation algorithms may give rise to issues such as false detection and missed detection due to the diverse shape and texture features of landslides in remote sensing images, the abundance of spectral features, and the complexity of the environment. In this article, we proposed LandslideNet, a novel model specifically designed for accurate segmentation of landslides in single-temporal high spatial resolution optical remote sensing images. By constructing a landslide image dataset and employing the LandslideNet model, we successfully identify and segment landslides with high precision. Quantitative experimental results demonstrate that our LandslideNet achieves superior performance compared to widely used semantic segmentation models including U-Net, PSPNet, Deeplabv3+, HRNetv2, Segformer and GELAN-c with F1-score, mIoU, FWIoU, mPA and OA reaching 72.53 %, 78.41 %, 99.86 %, 83.33 % and 99.93 % respectively. Moreover, our model exhibits lower complexity while demonstrating improved capability in detecting landslides with complex shapes and different sizes. (c) 2024 COSPAR. Published by Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",Remote sensing,Landslide extraction,Deep learning,YOLOv8,Google earth,"Yang, Shuwen","Hou, Yuhao",,,ADVANCES IN SPACE RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_713,"Li, Jiaojiao","Liu, Yuzhe","Liu, Jiachao","Song, Rui",Feature Guide Network With Context Aggregation Pyramid for Remote Sensing Image Segmentation,,2022,6,"In recent years, the deep learning method based on fully convolution networks has proven to be an effective method for the semantic segmentation of remote sensing images (RSIs). However, the rich information and complex content of RSIs make networks training for segmentation more challenging. Specifically, the observing distance between the space-borne cameras and the ground objects is extraordinarily far, resulting in that some smaller objects only occupy a few pixels in the image. However, due to the rapid degeneration of tiny objects during the training process, most algorithms cannot properly handle these common small objects in RSIs with satisfactory results. In this article, we propose a novel feature guide network with a context aggregation pyramid (CAP) for RSIs segmentation to conquer these issues. An innovative edge-guide feature transform module is designed to take advantage of the edge and body information of objects to strengthen edge contours and the internal consistency in homogeneous regions, which can explicitly enhance the representation of tiny objects and relieve the degradation of small objects. Furthermore, we design a CAP pooling strategy to adaptively capture optimal feature characterization that can assemble multiscale features according to the significance of different contexts. Extensive experiments on three large-scale remote sensing datasets demonstrate that our method not only can outperform the state-of-the-art methods for objects of different scales but can also achieve robust segmentation results, especially for tiny objects.",Context aggregation pyramid (CAP),deep learning,edge guide,remote sensing images (RSIs),semantic segmentation,"Liu, Wei","Han, Kailiang","Du, Qian",,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_714,"Chong, Qianpeng","Ni, Mengying","Huang, Jianjun","Wei, Guangyi",Let the loss impartial: a hierarchical unbiased loss for small object segmentation in high-resolution remote sensing images,,DEC 31 2023,0,"The progress in optical remote sensing technology presents both a possibility and challenge for small object segmentation task. However, the gap between human vision cognition and machine behavior still poses an inherent constrains to the interpretation of small but key objects in large-scale remote sensing scenes. This paper summarizes this gap as a bias of the machine against small object segmentation task, called scale-induced bias. The scale-induced bias causes the degradation in the performance of conventional remote sensing image segmentation methods. Therefore, this paper applies a straightforward but innovative insight to mitigate the scale-induced bias. Specifically, we propose a universal impartial loss, which leverages the hierarchical approach to alleviate two sub-problems separately. The pixel-level statistical methodology is applied to remove the bias between the background and small objects, and an emendation vector is introduced to alleviate the bias between small object categories. Extensive experiments explicitly manifest that our method is fully compatible with the existing segmentation structures, armed with the hierarchical unbiased loss, these structures will achieve satisfactory improvement. The proposed method is validated on two benchmark remote sensing image datasets, where it achieved a competitive performance and could narrow the gap between the human vision cognition and machine behavior.",Hierarchical solution,remote sensing,small object,segmentation,,"Li, Ziyi","Xu, Jindong",,,EUROPEAN JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_715,"Xue, Gunagkuo","Liu, Yikun","Huang, Yuwen","Li, Mingsong",AANet: an attention-based alignment semantic segmentation network for high spatial resolution remote sensing images,,JUL 3 2022,1,"In this paper, we present an efficient network to tackle three critical problems in high spatial resolution (HSR) remote sensing image segmentation: ( i ) feature misalignment, ( i i ) insufficient contextual information extraction and ( i i i ) various class imbalance issues. In detail, we propose a novel Feature Alignment Block (FAB) to suppress misalignment issues with the guide of an anchor map. Further, to extract sufficient information, we design a Contextual Augmentation Block (CAB) to augment features of different semantic levels. Finally, we present an Annealing Online Hard Example Mining (AOHEM) strategy to handle the various class imbalance issues with a view to dynamically adjust the focus of the network. We apply the above proposed designs to FPN to form our Attention-based Alignment Network (AANet). Experimental results demonstrate that the proposed method achieves promising results on the challenging iSAID and Vaihingen datasets with a better trade-off between accuracy and complexity.",Semantic segmentation,attention,feature alignment,hard example mining,,"Yang, Gongping",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_716,"Qiu, Luyi","Yu, Dayu","Zhang, Chenxiao","Zhang, Xiaofeng",A Local-Global Framework for Semantic Segmentation of Multisource Remote Sensing Images,,JAN 2023,5,"Recently, deep learning has been widely used in the segmentation tasks of remote sensing images. However, the existing deep learning method most focus on local contextual information and has limited field of perception, which makes it difficult to capture the long-range contextual feature of objects at large scales form very-high-resolution (VHR) images. In this paper, we present a novel Local-global Framework consisting of the dual-source fusion network and local-global transformer modules, which efficiently utilize features extracted from multiple sources and fully capture features of local and global regions. The dual-source fusion network is an encoder designed to extract features from multiple sources such as spectra, synthetic aperture radar, and elevations, which selective fuse features from multiple sources and reduce the interference of redundant features. The local-global transformer module is proposed to capture fine-grained local features and coarse-grained global features, which enables the framework to focus on recognizing multiple-scale objects from the local and global regions. Moreover, we propose a pixelwise contrastive loss, which could encourage that the prediction is pulled closer to the ground truth. The Local-global Framework achieves state-of-the-art performance with 90.45% mean f1 score on the ISPRS Vaihingen dataset and 93.20% mean f1 score on the ISPRS Potsdam dataset.",semantic segmentation,deep learning,multisource image,global-local feature fusion,contrastive learning,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_717,"Shi, Hao","Fan, Jiahe","Wang, Yupei","Chen, Liang",Dual Attention Feature Fusion and Adaptive Context for Accurate Segmentation of Very High-Resolution Remote Sensing Images,,SEP 2021,11,"Land cover classification of high-resolution remote sensing images aims to obtain pixel-level land cover understanding, which is often modeled as semantic segmentation of remote sensing images. In recent years, convolutional network (CNN)-based land cover classification methods have achieved great advancement. However, previous methods fail to generate fine segmentation results, especially for the object boundary pixels. In order to obtain boundary-preserving predictions, we first propose to incorporate spatially adapting contextual cues. In this way, objects with similar appearance can be effectively distinguished with the extracted global contextual cues, which are very helpful to identify pixels near object boundaries. On this basis, low-level spatial details and high-level semantic cues are effectively fused with the help of our proposed dual attention mechanism. Concretely, when fusing multi-level features, we utilize the dual attention feature fusion module based on both spatial and channel attention mechanisms to relieve the influence of the large gap, and further improve the segmentation accuracy of pixels near object boundaries. Extensive experiments were carried out on the ISPRS 2D Semantic Labeling Vaihingen data and GaoFen-2 data to demonstrate the effectiveness of our proposed method. Our method achieves better performance compared with other state-of-the-art methods.",deep learning,land cover classification,semantic segmentation,,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_718,"Du, Bowen","Zhao, Zirong","Hu, Xiao","Wu, Guanghui",Landslide susceptibility prediction based on image semantic segmentation,,OCT 2021,46,"The visual characteristics of landslide susceptibility have not yet been fully explored. Professional or trained technicians have to take much time and effort to interpret remote sensing images and locate landslides accordingly. Although conventional machine learning methods based on hand-crafted features for landslide susceptibility prediction (LSP) have acquired remarkable performance, they have certain requirements for prior knowledge. Aiming to learn complex and inherent visual patterns of landslides through minimal manual intervention and achieve fine-grained prediction, in this paper, we define LSP as a semantic segmentation problem on optical remote sensing images. Six widely used semantic segmentation models including Fully Convolutional Network, U-Net, Pyramid Scene Parsing Network, Global Convolutional Network (GCN), DeepLab v3 and DeepLab v3+ are introduced and evaluated for LSP. As the lack of landslide datasets, an open labeled landslide dataset of remote sensing imagery is created for research. The results show that GCN and DeepLab v3 are more applicable for this problem scenario, and the best Mean Intersection-over-Union and Pixel Accuracy of models are 54.2% and 74.0% respectively, which could be further improved by more targeted network architectures. In conclusion, semantic segmentation methods are demonstrated to be effctive for predicting new potential landslides based on remote sensing images.",Landslide susceptibility prediction,Deep learning,Computer vision,Remote sensing,Semantic segmentation,"Han, Liangzhe","Sun, Leilei","Gao, Qiang",,COMPUTERS & GEOSCIENCES,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_719,"Bai, Lubin","Du, Shihong","Zhang, Xiuyuan","Wang, Haoyu",Domain Adaptation for Remote Sensing Image Semantic Segmentation: An Integrated Approach of Contrastive Learning and Adversarial Learning,,2022,30,"Although semantic segmentation models based on deep neural networks (DNNs) have achieved excellent results, generalizing well from one remote sensing dataset (source domain) to another dataset with different acquisition conditions (target domain) remains a major challenge. Many domain adaptation (DA) approaches have been proposed to address this problem. DA aims to help DNNs learn a generalizable representation space in which source and target domains have similar feature distributions, but most of the existing DA approaches have difficulty in aligning the high-dimensional image representations of two domains directly. In this study, we proposed a model integrating contrastive learning and adversarial learning in a unified framework for aligning two domains in both representation space and spatial layout. Specifically, the model consists of a semantic segmentation network for feature extraction and two branches for DA. The first branch is used for adaptation in representation space directly by a proposed pixelwise contrastive loss, while the second branch is used for adaptation in predicted results to help two domains have similar spatial layouts through a novel but simple entropy-based similarity discriminator. Additionally, a training strategy called category similarity matching sampling was proposed to provide source and target image pairs with similar category composition for each training iteration, which can help the two branches work better. Extensive experiments indicated that the two branches can benefit each other to gain a superior performance and DA pretraining by our methods can achieve impressive results with only a small number of target labeled samples.",Image segmentation,Semantics,Training,Feature extraction,Task analysis,"Liu, Bo","Ouyang, Song",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Adversarial machine learning,Adaptation models,Adversarial learning,contrastive learning,domain adaptation (DA),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_720,"Ma, Zhanming","Xia, Min","Weng, Liguo","Lin, Haifeng",Local Feature Search Network for Building and Water Segmentation of Remote Sensing Image,,FEB 2023,28,"Extracting buildings and water bodies from high-resolution remote sensing images is of great significance for urban development planning. However, when studying buildings and water bodies through high-resolution remote sensing images, water bodies are very easy to be confused with the spectra of dark objects such as building shadows, asphalt roads and dense vegetation. The existing semantic segmentation methods do not pay enough attention to the local feature information between horizontal direction and position, which leads to the problem of misjudgment of buildings and loss of local information of water area. In order to improve this problem, this paper proposes a local feature search network (DFSNet) application in remote sensing image building and water segmentation. By paying more attention to the local feature information between horizontal direction and position, we can reduce the problems of misjudgment of buildings and loss of local information of water bodies. The discarding attention module (DAM) introduced in this paper reads sensitive information through direction and location, and proposes the slice pooling module (SPM) to obtain a large receptive field in the pixel by pixel prediction task through parallel pooling operation, so as to reduce the misjudgment of large areas of buildings and the edge blurring in the process of water body segmentation. The fusion attention up sampling module (FAUM) guides the backbone network to obtain local information between horizontal directions and positions in spatial dimensions, provide better pixel level attention for high-level feature maps, and obtain more detailed segmentation output. The experimental results of our method on building and water data sets show that compared with the existing classical semantic segmentation model, the proposed method achieves 2.89% improvement on the indicator MIoU, and the final MIoU reaches 83.73%.",semantic segmentation,building and water segmentation,local feature search,horizontal direction,high-resolution remote sensing image,,,,,SUSTAINABILITY,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_721,"Zheng, Xiaoxiong","Chen, Tao",,,High spatial resolution remote sensing image segmentation based on the multiclassification model and the binary classification model,,FEB 2023,29,"Semantic segmentation technology is an important step in the interpretation of remote sensing images. High spatial resolution remote sensing images have clear features. Traditional image segmentation methods cannot fully represent the information in high spatial resolution images and tend to yield unsatisfactory segmentation accuracy. With the rapid development of deep learning, many researchers have tried to use deep learning algorithms for remote sensing image segmentation. This paper uses U-Net for multiclassification and binary classification of Gaofen-2 high spatial resolution remote sensing image data. Six types of features, which were build-up, farmland, water, meadow, forest and others, were labeled in the image. A ""neighborhood voting"" method was used to determine the category of uncertain pixels based on spatial heterogeneity and homogeneity. Through U-Net neural network multiclassification, the overall accuracy of the training data is 93.83%; the overall accuracy of the test data is 82.27%; and the test accuracy of the binary classification algorithm is 79.75%. The results show that the two models yield high accuracy and credibility in remote sensing image segmentation.",High spatial resolution remote sensing image,Semantic segmentation,U-Net,Neighborhood voting,,,,,,NEURAL COMPUTING & APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_722,"Zhang, Panli","Zhang, Sheng","Wang, Jiquan","Sun, Xiaobo",Identifying rice lodging based on semantic segmentation architecture optimization with UAV remote sensing imaging,,DEC 2024,0,"Lodging, a prevalent issue during rice growth, detrimentally impacts both yield and quality. It also complicates the harvesting process, reducing the efficiency of mechanized collection. Existing monitoring methods, predominantly based on manual observation and satellite remote sensing, fall short in addressing the requirements of contemporary, efficient, and real-time agriculture. This research integrates image analysis techniques with advanced optimization algorithms to develop a semantic segmentation model specifically designed for detecting rice lodging in remote sensing images. The model, named MI-UConvNeXt, employs a ConvNeXt-based feature extraction network utilizing UNet architecture (UConvNeXt) and incorporates an improved multi-objective salp swarm algorithm with Latin hypercube sampling and an elite opposition-based learning strategy (ISSA-LE) to dynamically adjusting the number of UConvNeXt channels. MI-UConvNeXt achieves a balance between accuracy and complexity. Compared to seven other semantic segmentation models from the literature, MI-UConvNeXt exhibits enhanced performance, with a Pixel Accuracy (PA) of 95.59%, mean Pixel Accuracy (mPA) of 95.62%, and mean Intersection over Union ( mIoU ) of 91.91% on the validation set. This demonstrates the model's superior accuracy, lower computational resource demands, and enhanced efficiency. By integrating deep learning with intelligent optimization algorithms, this study offers a novel and effective approach for monitoring crop lodging in agricultural production, providing robust technical support for the accurate extraction of crop phenotypic information.",Rice lodging,Salp swarm algorithm,Convolutional neural network,Semantic segmentation,,,,,,COMPUTERS AND ELECTRONICS IN AGRICULTURE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_723,"Rao, Zhibo","He, Mingyi","Zhu, Zhidong","Dai, Yuchao",Bidirectional Guided Attention Network for 3-D Semantic Detection of Remote Sensing Images,,JUL 2021,31,"Semantic segmentation and disparity estimation are in the research frontier of the computer vision and remote sensing (RS) fields. However, existing methods mostly deal with these two problems separately or use a combination of multiple models to solve these two tasks. Due to a lack of sufficient information sharing and fusion, they still have difficulties in coping with seasonal appearance differences in 3-D RS problems. In this article, we propose a novel multitask learning architecture that considers the bottomx2013;up and upx2013;bottom visual attention mechanism for 3-D semantic detection, named bidirectional guided attention network (BGA-Net). BGA-Net consists of five modules: unified backbone module (UBM), bidirectional guided attention module (BGAM), semantic segmentation module (SSM), feature matching module (FMM), and bidirectional fusion module (BFM). First, in UBM, we use a shared backbone to extract unified features and share them with three branches/modules (BGAM, SSM, and FMM). Then, SSM and FMM branches are applied to estimate segmentation and disparity maps, whereas the third branch/module (BGAM) shares the global features to guide the task-specific learning via attention mechanism. Finally, we fuse the results of the two tasks by BFM to improve the final performance. Extensive experiments demonstrate that: 1) our BGA-Net can handle the two tasks simultaneously and can be trained in an end-to-end way; 2) these modules fully take advantage of the two tasksx2019; information to share features and enhance the scene understanding ability, effectively against seasons change of RS images; and 3) BGA-Net has notable superiority and greater flexibility and also sets a new state of the art on the urban semantic 3-D (US3D) benchmark. Moreover, BGA-Net also provides insights into the intelligent interpretation of RS data images.",Task analysis,Semantics,Image segmentation,Feature extraction,Visualization,"He, Renjie",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Remote sensing,Computer architecture,Bidirectional guided aggregation,remote sensing image,semantic segmentation,stereo matching,visual attention mechanism,,,,,,,,,,,,,,,,,,,,,,,
Row_724,"Sun, Jingxi","Li, Weihong","Zhang, Yan","Gong, Weiguo",Building segmentation of remote sensing images using deep neural networks and domain transform CRF,IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXV,2019,2,"Automatic building segmentation from remote sensing images is critical in the remote sensing image semantic segmentation. The success of deep neural networks has led to advances in using fully convolutional neural networks (FCN) to extract buildings from the high-resolution image. However, the downsampling processing inevitably leads to loss of details of the segmentation results. To solve this problem, some methods try to refine the results of FCN by using probability graph models such as fully connected CRF (Conditional Random Fields). Nevertheless, many fully connected CRF based methods are too time-consuming and not suitable for building segmentation tasks in some situations. In this paper, we propose a novel time- efficient end-to-end CRF model with the domain transform algorithm called DT-CRF. In the proposed model, in order to accelerate the message passing in the mean-field approximate inference algorithm, we take the edge maps as the joint image for DT-CRF and use the domain transformation algorithm to calculate the pair-wise potential instead of the Gaussian kernel function. Meanwhile, we design a multi-task network which can generate masks and edges simultaneously, and the network can make the DT-CRF to easily optimize the segmentation results using model information. The evaluation of remote sensing image datasets verifies the time and space efficiency of the proposed DTCRF and demonstrates a distinct improvement.",Remote Sensing Image,Convolutional Neural Networks,Building Segmentation,Conditional Random Field,Domain Transform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_725,"Liu, Ming","Ren, Dong","Sun, Hang","Yang, Simon X.",Orchard Areas Segmentation in Remote Sensing Images via Class Feature Aggregate Discriminator,,2022,0,"Accurate evaluation of orchard areas from remote sensing images is of great importance in economic and ecological aspects. In practice, the differences in distributions between remote sensing images and the lack of data labels make the semantic segmentation model impossible to use in new data. Unsupervised domain adaptation (UDA) methods can improve the performance of the model in the target domain by aligning the source domain and the target domain. However, due to the class mismatch problem and the interference of high-dimensional feature complexity, most UDA methods cannot achieve satisfactory results in orchard areas segmentation task. To address these issues, we propose an UDA model for orchard areas segmentation by developing a class feature aggregate discriminator (CFUDA). The class feature aggregate discriminator is designed to distinguish intradomain classes and align interdomain classes, and class feature aggregate can represent class information of different domains, which helps the model to avoid the interference of complex information. In addition, adversarial loss reweighting is introduced to the novel model, which makes the segmentation model pay more attention to the orchard areas. To verify the effectiveness of the proposed method, we conducted extensive experiments in three different remote sensing images around Yichang City. Compared to the baseline model, the proposed approach improves intersection over union (IoU) by 27.68%, and we achieve high gains of 6.07% in IoU over other UDA methods. The larger gain indicates that our proposed method has great potential in cross-domain orchard areas segmentation.",Aggregates,Task analysis,Remote sensing,Adaptation models,Image resolution,"Shao, Pan",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Training,Semantics,Generative adversarial networks,remote sensing,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,,,
Row_726,"Song, Ahram","Kim, Yongil",,,Semantic Segmentation of Remote-Sensing Imagery Using Heterogeneous Big Data: International Society for Photogrammetry and Remote Sensing Potsdam and Cityscape Datasets,,OCT 2020,18,"Although semantic segmentation of remote-sensing (RS) images using deep-learning networks has demonstrated its effectiveness recently, compared with natural-image datasets, obtaining RS images under the same conditions to construct data labels is difficult. Indeed, small datasets limit the effective learning of deep-learning networks. To address this problem, we propose a combined U-net model that is trained using a combined weighted loss function and can handle heterogeneous datasets. The network consists of encoder and decoder blocks. The convolutional layers that form the encoder blocks are shared with the heterogeneous datasets, and the decoder blocks are assigned separate training weights. Herein, the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Cityscape datasets are used as the RS and natural-image datasets, respectively. When the layers are shared, only visible bands of the ISPRS Potsdam data are used. Experimental results show that when same-sized heterogeneous datasets are used, the semantic segmentation accuracy of the Potsdam data obtained using our proposed method is lower than that obtained using only the Potsdam data (four bands) with other methods, such as SegNet, DeepLab-V3+, and the simplified version of U-net. However, the segmentation accuracy of the Potsdam images is improved when the larger Cityscape dataset is used. The combined U-net model can effectively train heterogeneous datasets and overcome the insufficient training data problem in the context of RS-image datasets. Furthermore, it is expected that the proposed method can not only be applied to segmentation tasks of aerial images but also to tasks with various purposes of using big heterogeneous datasets.",semantic segmentation,deep learning,big dataset,ISPRS Potsdam dataset,Cityscape dataset,,,,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_727,Zhao Tianyu,"Xu, Jindong",,,Hyperspectral Remote Sensing Image Segmentation Based on the Fuzzy Deep Convolutional Neural Network,"2020 13TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2020)",2020,3,"The ""synonyms spectrum"" and ""foreign body with the spectrum"" of remote sensing images have caused the traditional segmentation methods to be greatly limited. Existing segmentation methods represented by deep convolution neural network have made breakthrough progress. However, traditional deep learning is a completely deterministic model, which can not describe the data uncertainty well. To solve this problem, a new fuzzy deep neural network is proposed in this paper, called RSFCNN (Remote Sensing image segmentation with Fuzzy Convolutional Neural Network). The network integrates fuzzy unit and traditional convolution unit. Convolution unit is used to extract discriminant features with different proportions, thus providing comprehensive information for pixel-level remote sensing image segmentation. Fuzzy logic unit is used to deal with various uncertainties and provide more reliable segmentation results. In this paper, end-to-end training scheme is used to learn the parameters of fuzzy and convolution units. Experiments were carried out on the data set of ISPRS Vaihingen. According to the experimental results, the proposed method has higher segmentation accuracy and better performance than other algorithms.",remote sensing image segmentation,semantic segmentation,fuzzy neural network,convolutional neural network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_728,"Xiang, Jianjian","Liu, Jia","Chen, Du","Xiong, Qi",CTFuseNet: A Multi-Scale CNN-Transformer Feature Fused Network for Crop Type Segmentation on UAV Remote Sensing Imagery,,FEB 2023,9,"Timely and accurate acquisition of crop type information is significant for irrigation scheduling, yield estimation, harvesting arrangement, etc. The unmanned aerial vehicle (UAV) has emerged as an effective way to obtain high resolution remote sensing images for crop type mapping. Convolutional neural network (CNN)-based methods have been widely used to predict crop types according to UAV remote sensing imagery, which has excellent local feature extraction capabilities. However, its receptive field limits the capture of global contextual information. To solve this issue, this study introduced the self-attention-based transformer that obtained long-term feature dependencies of remote sensing imagery as supplementary to local details for accurate crop-type segmentation in UAV remote sensing imagery and proposed an end-to-end CNN-transformer feature-fused network (CTFuseNet). The proposed CTFuseNet first provided a parallel structure of CNN and transformer branches in the encoder to extract both local and global semantic features from the imagery. A new feature-fusion module was designed to flexibly aggregate the multi-scale global and local features from the two branches. Finally, the FPNHead of feature pyramid network served as the decoder for the improved adaptation to the multi-scale fused features and output the crop-type segmentation results. Our comprehensive experiments indicated that the proposed CTFuseNet achieved a higher crop-type-segmentation accuracy, with a mean intersection over union of 85.33% and a pixel accuracy of 92.46% on the benchmark remote sensing dataset and outperformed the state-of-the-art networks, including U-Net, PSPNet, DeepLabV3+, DANet, OCRNet, SETR, and SegFormer. Therefore, the proposed CTFuseNet was beneficial for crop-type segmentation, revealing the advantage of fusing the features found by the CNN and the transformer. Further work is needed to promote accuracy and efficiency of this approach, as well as to assess the model transferability.",precision agriculture,UAV remote sensing,semantic segmentation,deep learning,CNN,"Deng, Chongjiu",,,,REMOTE SENSING,,transformer,feature fusion,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_729,"Qiao, Yicheng","Liu, Wei","Liang, Bin","Wang, Pengyun",SeMask-Mask2Former: A Semantic Segmentation Model for High Resolution Remote Sensing Images,2023 IEEE AEROSPACE CONFERENCE,2023,3,"With the development of remote sensing, semantic segmentation of high-resolution remote sensing images (RSIs) is increasingly essential. At the same time, the characteristics of objects in RSIs, such as large size, variation in object scales, and complex details, make it necessary to capture both long-range context and local information. There are some methods such as Fully Convolutional Networks (FCN) and Pyramid Scene Parsing Network (PSPNet) lack the ability to capture long-range dependencies, due to the limited receptive field of Convolutional Neural Network (CNN). However, the self-attention mechanism to capture the correlation between pixels in Transformer models has remarkable capability in capturing long-range context. One of the most outstanding Transformer models is the Masked-attention Mask Transformer (Mask2Former) which adopts the mask classification method. We propose a model SeMaskMask2Former with boundary loss. Semantically Masked (SeMask) is the model's backbone and Mask2Former is the decoder. Concretely, the mask classification that generates one or even more masks for specific categories to perform the elaborate segmentation is especially suitable for handling the characteristic of large within-class and small inter-class variance of RSIs.",,,,,,"Zhang, Haopeng","Yang, Junli",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_730,"Lang, Chunbo","Cheng, Gong","Tu, Binfei","Han, Junwei",Global Rectification and Decoupled Registration for Few-Shot Segmentation in Remote Sensing Imagery,,2023,25,"Few-shot segmentation (FSS), which aims to determine specific objects in the query image given only a handful of densely labeled samples, has received extensive academic attention in recent years. However, most existing FSS methods are designed for natural images, and few works have been done to investigate more realistic and challenging applications, e.g., remote sensing image understanding. In such a setup, the complex nature of the raw images would undoubtedly further increase the difficulty of the segmentation task. To couple with potential inference failures, we propose a novel and powerful remote sensing FSS framework with global rectification (GR) and decoupled registration (DR), termed R(2)Net. Specifically, a series of dynamically updated global prototypes are utilized to provide auxiliary nontarget segmentation cues and to prevent inaccurate prototype activation resulting from the variability between query-support image pairs. The foreground (FG) and background information flows are then decoupled for more targeted and tailored object localization, avoiding unnecessary confusion from information redundancy. Furthermore, we impose additional constraints to promote interclass separability and intraclass compactness. Extensive experiments on the standard benchmark iSAID-5(i) demonstrate the superiority of the proposed R(2)Net over state-of-the-art FSS models. The code is available at https://github.com/chunbolang/R2Net.",Few-shot learning,few-shot segmentation (FSS),meta-learning,remote sensing,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_731,"Gu, Haiyan","Li, Haitao","Yan, Li","Liu, Zhengjun",An Object-Based Semantic Classification Method for High Resolution Remote Sensing Imagery Using Ontology,,APR 2017,49,"Geographic Object-Based Image Analysis (GEOBIA) techniques have become increasingly popular in remote sensing. GEOBIA has been claimed to represent a paradigm shift in remote sensing interpretation. Still, GEOBIA-similar to other emerging paradigms-lacks formal expressions and objective modelling structures and in particular semantic classification methods using ontologies. This study has put forward an object-based semantic classification method for high resolution satellite imagery using an ontology that aims to fully exploit the advantages of ontology to GEOBIA. A three-step workflow has been introduced: ontology modelling, initial classification based on a data-driven machine learning method, and semantic classification based on knowledge-driven semantic rules. The classification part is based on data-driven machine learning, segmentation, feature selection, sample collection and an initial classification. Then, image objects are re-classified based on the ontological model whereby the semantic relations are expressed in the formal languages OWL and SWRL. The results show that the method with ontology-as compared to the decision tree classification without using the ontology-yielded minor statistical improvements in terms of accuracy for this particular image. However, this framework enhances existing GEOBIA methodologies: ontologies express and organize the whole structure of GEOBIA and allow establishing relations, particularly spatially explicit relations between objects as well as multi-scale/hierarchical relations.",geographic object-based image analysis,ontology,semantic network model,web ontology language,semantic web rule language,"Blaschke, Thomas","Soergel, Uwe",,,REMOTE SENSING,,machine learning,semantic rule,land-cover classification,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_732,"Bai, Hao","Bai, Tingzhu","Li, Wei","Liu, Xun",A Building Segmentation Network Based on Improved Spatial Pyramid in Remote Sensing Images,,JUN 2021,1,"Building segmentation is widely used in urban planning, disaster prevention, human flow monitoring and environmental monitoring. However, due to the complex landscapes and highdensity settlements, automatically characterizing building in the urban village or cities using remote sensing images is very challenging. Inspired by the rencent deep learning methods, this paper proposed a novel end-to-end building segmentation network for segmenting buildings from remote sensing images. The network includes two branches: one branch uses Widely Adaptive Spatial Pyramid (WASP) structure to extract multi-scale features, and the other branch uses a deep residual network combined with a sub-pixel up-sampling structure to enhance the detail of building boundaries. We compared our proposed method with three state-of-the-art networks: DeepLabv3+, ENet, ESPNet. Experiments were performed using the publicly available Inria Aerial Image Labelling dataset (Inria aerial dataset) and the Satellite dataset II(East Asia). The results showed that our method outperformed the other networks in the experiments, with Pixel Accuracy reaching 0.8421 and 0.8738, respectively and with mIoU reaching 0.9034 and 0.8936 respectively. Compared with the basic network, it has increased by about 25% or more. It can not only extract building footprints, but also especially small building objects.",CNN,semantic segmentation,super resolution,remote sensing,spatial pyramid,,,,,APPLIED SCIENCES-BASEL,,ResNet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_733,"Wei, Guangyi","Xu, Jindong","Chong, Qianpeng","Huang, Jianjun",Prior-Guided Fuzzy-Aware Multibranch Network for Remote Sensing Image Segmentation,,2024,1,"In remote sensing images (RSIs), accurate semantic segmentation faces significant challenges due to the variation in object scales, uncertain category boundaries, and complex scenes. In view of the above challenges, we propose a prior-guided fuzzy-aware multibranch network for RSI segmentation. Specifically, a prior-feature extractor (PFE) is designed to take the local features extracted by convolution structure as prior knowledge of the network. Fuzzy-aware module (FAM) is presented to perceive and refine category boundaries with fuzzy learning, which transforms the uncertainty problem into a quantitative analysis problem by establishing fuzzy relationships between neighborhood pixels. Multibranch-feature extractor (MFE) is put forward to aggregate multiscale global context information by combining positional attention and transformer. The whole network learning process is supervised by multibranch loss. We validated our method on the Gaofen Image Dataset (GID), Potsdam, and LoveDA datasets, achieving 75.58%, 75.62%, and 53.10% mean intersection over union (mIoU), respectively, which demonstrated the superiority of our method. In addition, ablation studies further demonstrate the validity of each module in the proposed method.",Feature extraction,Transformers,Decoding,Data mining,Kernel,"Xing, Haihua",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Aggregates,Remote sensing,Fuzzy learning,remote sensing,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_734,"Miao, Zuohua","Liu, Likun","Ren, Lei","Tang, Yang",Semantic Segmentation of Remote Sensing Images Based on Multi-Model Fusion,5TH ANNUAL INTERNATIONAL CONFERENCE ON INFORMATION SYSTEM AND ARTIFICIAL INTELLIGENCE (ISAI2020),2020,1,"Convolutional neural networks have created a new field in the research of semantic segmentation of remote sensing images. However, different network structures have different effects on the semantic segmentation of different land types. In this paper, the original data set is expanded, and an improved U-Net model is used to train a model for each type of feature target. Then combined with conditional random field (CRF) and image overlapping strategy for optimization processing; Finally, the two binary classification models obtained by training are fused to obtain multi-classified semantic segmentation images. Solve the obvious problem of large-scale remote sensing image edge stitching. The experimental results show that this method has higher accuracy in solving the segmentation of large-scale remote sensing images.",,,,,,"Chen, Yong","Li, Jun",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_735,Tian Qinglin,Zhao Yingjun,Qin Kai,Li Yao,Dense feature pyramid fusion deep network for building segmentation in remote sensing image,SEVENTH SYMPOSIUM ON NOVEL PHOTOELECTRONIC DETECTION TECHNOLOGY AND APPLICATIONS,2021,3,"It is difficult to achieve detailed segmentation since the building size varies in high-resolution remote sensing images, especially for small buildings. To address these problems, a dense feature pyramid fusion deep network is proposed in this study. First, we built an encoder-decoder structure, and combine attention mechanism and atrous convolution to improve the feature extraction results in the encoder. Second, the pyramid pooling module is selected to extract the multi-scale features from different levels. Finally, dense feature pyramid is adopted in the decoder to fuse multi-level and multi-scale features to obtain the final segmentation results. Experiments on Inria Aerial Image Labeling Dataset show that our method achieves competitive performance compared with other classical semantic segmentation networks.",remote sensing image,building segmentation,attention mechanism,feature pyramid,,Chen Xuejiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_736,"Wang, Yufeng","Ding, Wenrui","Zhang, Ruiqian","Li, Hongguang",Boundary-Aware Multitask Learning for Remote Sensing Imagery,,2021,30,"Semantic segmentation and height estimation play fundamental roles in the scene understanding of remote sensing images with their wide variety of aerial applications. Recently, deep convolutional neural networks (DCNNs) have achieved state-of-the-art performance in both tasks. However, DCNN-based methods learn to accumulate contextual information over large receptive fields while lose the local detailed information, resulting in blurry object boundaries. The complicated ground object distribution and low interclass variance further aggravate the difficulty in generating accurate predictions. To address the above-mentioned issues, we propose a novel boundary-aware multitask learning (BAMTL) framework to perform three tasks, semantic segmentation, height estimation, and boundary detection, within a unified model. The boundary detection is employed as an auxiliary task to regularize the other two master tasks at both the feature space and output space. We present a boundary attentive module to build the cross-task interaction for master tasks, which enforce the networks to filter out the confident area and focus on learning the high-frequency details. We then introduce a boundary regularized loss term to further refine the prediction maps to be locally consistent while preserving boundary structures. With these formulations, our model improves the performance of both segmentation and height tasks, especially along the boundaries. Experimental results on two publicly available remote sensing datasets demonstrate that the proposed approach performs favorably against the state-of-the-art methods.",Task analysis,Remote sensing,Semantics,Image segmentation,Estimation,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Feature extraction,Earth,Boundary regularization (BR),convolutional neural network (CNN),height estimation,multitask learning (MTL),remote sensing imagery (RSI),scene understanding,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_737,"Li, Xiang","Wen, Congcong","Wang, Lingjing","Fang, Yi",Geometry-Aware Segmentation of Remote Sensing Images via Joint Height Estimation,,2022,26,"Recent studies have shown the benefits of using additional elevation data [e.g., digital surface model (DSM) or normalized DSM (nDSM)] for enhancing the performance of the semantic labeling of aerial images. However, previous methods mostly adopt 3-D elevation information as additional inputs, while, in many real-world applications, one does not have the corresponding DSM images at hand, and the spatial resolution of acquired DSM images usually does not match the aerial images. To alleviate this data constraint and also take advantage of 3-D elevation information, in this letter, a geometry-aware segmentation model is introduced to achieve accurate semantic labeling of aerial images via joint height estimation. Instead of using a single-stream encoder-decoder network for semantic labeling, we design a separate decoder branch to predict the height map and use the DSM images as side supervision to train this newly designed decoder branch. With the newly designed decoder branch, our model can distill the 3-D geometric features from 2-D appearance features under the supervision of ground-truth DSM images. Moreover, we develop a new geometry-aware convolution module that fuses the 3-D geometric features from the height decoder branch and the 2-D contextual features from the semantic segmentation branch. The fused feature embeddings can produce geometry-aware segmentation maps with enhanced performance. Our model is trained with DSM images as side supervision, while, in the inference stage, it does not require DSM data and directly predicts the semantic labels. Experiments on International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam data sets demonstrate the effectiveness of the proposed method for the semantic segmentation of aerial images.",Semantics,Data models,Image segmentation,Labeling,Decoding,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Estimation,Convolution,Feature fusion,geometry-aware convolution (GAC),height estimation,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_738,"Dey, Madhumita","Prakash, P. S.","Aithal, Bharath Haridas",,UnetEdge: A transfer learning-based framework for road feature segmentation from high-resolution remote sensing images,,APR 2024,3,"Topological information is a crucial factor affecting road feature extraction using semantic segmentation. Many segmentation models have recently been developed for road feature extraction from high-resolution remote sensing imagery but have yet to achieve accurate predictions when occluded by shadows. This study proposes a transfer learning-based framework, called UnetEdge, which is designed to effectively propagate topological information into the feature map. The novel Edge module transmits edge level topological information along with contextual spatial information through the decoder layer to the final feature map. This essentially enhances the continuous flow of semantic road pixel information into the network. Moreover, to integrate heterogeneous road structure information into the network, we have leveraged the transfer learning approach to produce accurate road segmentation maps. Extensive experiments on five standard public datasets and comparative analysis against the state -of -the -art networks establish the robustness and efficiency of the model. Furthermore, the experimental results on our acquired Indian drone dataset achieved an IoU score of 70.22 and a mIoU score of 82.45% with an overall accuracy of 95.27%, validating the model's effectiveness for real -world applications.",Remote sensing,Semantic segmentation,Convolutional neural network (CNN),Indian drone dataset,,,,,,REMOTE SENSING APPLICATIONS-SOCIETY AND ENVIRONMENT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_739,"Liu, Huan","Li, Wei","Jia, Wen","Sun, Hong",Clusterformer for Pine Tree Disease Identification Based on UAV Remote Sensing Image Segmentation,,2024,7,"Pine wilt disease (PWD) is one of the most prevalent pine tree diseases, resulting in both ecological and economic havoc. Unmanned aerial vehicle (UAV) remote sensing segmentation plays a crucial role in early identifying and preventing PWD. However, deep learning segmentation models customized for PWD identification in scenarios with complex backgrounds have not received extensive exploration. In this article, we propose a novel UAV remote sensing segmentation model called Clusterformer with a conventional encoder-decoder structure. The encoder is comprised of the specially designed cluster transformer, which includes a cluster token mixer and a spatial-channel feed-forward network (SC-FFN). The cluster token mixer utilizes constructed clusters from the feature maps to represent pixels, thereby reducing redundant and interfering information. The SC-FFN extracts multiscale spatial information through depth-wise convolutions and channel information through a multilayer perceptron (MLP) in sequence. The decoder primarily consists of the specially designed D-cluster transformer. The token mixer of the D-cluster transformer employs constructed clusters from high-level decoded tokens to represent low-level encoded tokens without relying on traditional upsampling methods such as interpolation, transpose convolution, or patch expansion. Consequently, more robust and less redundant features from high-level decoded feature maps are transferred to low-level encoded feature maps. Experimental results on two PWD datasets demonstrate that Clusterformer outperforms existing state-of-the-art segmentation models. This confirms the effectiveness and efficiency of Clusterformer in PWD identification. The code is available at https://github.com/huanliu233/Clusterformer.",Cluster transformer,pine wilt identification,semantic segmentation,unmanned aerial vehicle (UAV) remote sensing,,"Zhang, Mengmeng","Song, Lujie","Gui, Yuanyuan",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_740,"Du, Shuang","Liu, Maohua",,,Class-Guidance Network Based on the Pyramid Vision Transformer for Efficient Semantic Segmentation of High-Resolution Remote Sensing Images,,2023,3,"Small differences between classes and big variations within classes in multicategory semantic segmentation are problems that are not completely solved by the ""encoder-decoder"" structure of the fully convolutional neural network, leading to the imprecise perception of easily confused categories. To address this issue, in this article, we believe that sufficient contextual information can provide more interpretation clues to the model. Additionally, if we can mine the class-specific perceptual information for each semantic class, we can enhance the information belonging to the corresponding class in the decoding process. Therefore, we propose the class-guidance network based on the pyramid vision transformer (PVT). In detail, with the PVT as the encoder network, the following decoding process is composed of three stages. First, we design a receptive field block to expand the receptive field to different degrees using parallel branching processing and different dilatation rates. Second, we put forward a semantic guidance block to utilize the high-level features to guide the channel enhancement of low-level features. Third, we propose the class guidance block to achieve the class-aware guidance of adjacent features and achieve the refined segmentation by a progressive approach. The overall accuracy of the method is 88.91% and 88.87%, respectively, according to experimental findings on the Potsdam and Vaihingen datasets.",Class-guidance network,remote sensing images,semantic segmentation,transformer,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_741,"Zheng, Yilin","He, Lingmin","Wu, Xiangping","Pan, Chen",Self-training and Multi-level Adversarial Network for Domain Adaptive Remote Sensing Image Segmentation,,DEC 2023,0,"Unsupervised domain adaptive (UDA) image segmentation has received more and more attention in recent years. Domain adaptive methods can align the features of data in different domains, so that segmentation models can be migrated to data in other domains without incurring additional labeling costs. The traditional adversarial training uses the global alignment strategy to align the feature space, which may cause some categories to be incorrectly mapped. At the same time, in high-spatial resolution remote sensing images (RSI), the same category from different scenes (such as urban and rural areas) may have completely different distributions, which severely limits the accuracy of UDA. In order to solve these problems, in this paper: (1) a multi-level adversarial network at category-level is proposed, aiming at integrating feature information in different dimensions, studying the joint distribution at category-level, and aligning each category with adaptive adversarial loss in different dimensional spaces. (2) Use covariance regularization to optimize self-training. A method combining self-training with adversarial training is proposed, optimizes the domain adaptation effect, reduces the negative impact of false pseudo-label iteration caused by self-training. We demonstrated the latest performance of semantic segmentation on challenging LoveDA datasets. Experiments on ""urban-to-rural"" and ""rural-to-urban"" show that our method has better performance than the most advanced methods.",Domain adaptation,Semantic segmentation,Remote sensing images,Self-training,Adversarial network,,,,,NEURAL PROCESSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_742,"Lu, Zili","Peng, Yuexing","Li, Wei","Yu, Junchuan",An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,,2023,12,"The geological characteristics of old landslides can provide crucial information for the task of landslide protection. However, detecting old landslides from high-resolution remote sensing images (HRSIs) is of great challenge due to their partially or strongly transformed morphology over a long time and thus the limited difference with their surroundings. Additionally, small-sized datasets can restrict in-depth learning. To address these challenges, this article proposes a new iterative classification and semantic segmentation network (ICSSN), which can significantly improve both object-level and pixel-level classification performance by iteratively upgrading the feature extraction module shared by the object classification and semantic segmentation networks. To improve the detection performance on small-sized datasets, object-level contrastive learning is employed in the object classification network featuring a siamese network to realize global features extraction, and a subobject-level contrastive learning (SOCL) method is designed in the semantic segmentation network to efficiently extract salient features from boundaries of landslides. An iterative training strategy is also proposed to fuse features in the semantic space, further improving both the object-level and pixel-level classification performances. The proposed ICSSN is evaluated on a real-world landslide dataset, and experimental results show that it greatly improves both the classification and segmentation accuracy of old landslides. For the semantic segmentation task, compared to the baseline, the F1 score increases from 0.5054 to 0.5448, the mean intersection over union (mIoU) improves from 0.6405 to 0.6610, the landslide IoU grows from 0.3381 to 0.3743, the pixel accuracy (PA) is improved from 0.945 to 0.949, and the object-level detection accuracy of old landslides surges from 0.55 to 0.90. For the object classification task, the F1 score increases from 0.8846 to 0.9230, and the accuracy score is up from 0.8375 to 0.8875.",Terrain factors,Feature extraction,Semantic segmentation,Semantics,Reliability,"Ge, Daqing","Han, Lingyi","Xiang, Wei",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Task analysis,Iterative methods,Contrastive learning,landslide detection,multitask learning,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_743,"Yang, Ruiqi","Zheng, Chen","Wang, Leiguang","Zhao, Yili",MAE-BG: dual-stream boundary optimization for remote sensing image semantic segmentation,,DEC 31 2023,4,"Deep learning has achieved remarkable performance in semantically segmenting remotely sensed images. However, the high-frequency detail loss caused by continuous convolution and pooling operations and the uncertainty introduced when annotating low-contrast objects with weak boundaries induce blurred object boundaries. Therefore, a dual-stream network MAE-BG, consisting of an edge detection (ED) branch and a smooth branch with boundary guidance (BG), is proposed. The ED branch is designed to enhance the weak edges that need to be preserved, simultaneously suppressing false responses caused by local texture. This mechanism is achieved by introducing improved multiple-attention edge detection blocks (MAE). Furthermore, two specific ED branches with MAE are designed to combine with typical deep convolutional (DC) and Codec infrastructures and result in two configurations of MAE-A and MAE-B. Meanwhile, multiscale edge information extracted by MAE networks is fed into the backbone networks to complement the detail loss caused by convolution and pooling operations. This results in smooth networks with BG. After that, the segmentation results with improved boundaries are obtained by stacking the output of the ED and smooth branches. The proposed algorithms were evaluated on the ISPRS Potsdam and Inria Aerial Image Labelling datasets. Comprehensive experiments show that the proposed method can precisely locate object boundaries and improve segmentation performance. The MAE-A branch leads to an overall accuracy (OA) of 89.16%, a mean intersection over union (MIOU) of 80.25% for Potsdam, and an OA of 96.61% and MIOU of 86.63% for Inria. Compared with the results without the proposed edge optimization blocks, the OAs from the Potsdam and Inria datasets increase by 5.49% and 7.64%, respectively.",Deep learning,Boundary optimization,Squeeze and excitation,Semantic segmentation,Remote sensing,"Fu, Zhitao","Dai, Qinling",,,GEOCARTO INTERNATIONAL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_744,"Ayala, Christian","Aranda, Carlos","Galar, Mikel",,Guidelines to Compare Semantic Segmentation Maps at Different Resolutions,,2024,0,"Choosing the proper ground sampling distance (GSD) is a vital decision in remote sensing, which can determine the success or failure of a project. Higher resolutions may be more suitable for accurately detecting objects, but they also come with higher costs and require more computing power. Semantic segmentation is a common task in remote sensing where GSD plays a crucial role. In semantic segmentation, each pixel of an image is classified into a predefined set of classes, resulting in a semantic segmentation map. However, comparing the results of semantic segmentation at different GSDs is not straightforward. Unlike scene classification and object detection tasks, which are evaluated at scene and object level, respectively, semantic segmentation is typically evaluated at pixel level. This makes it difficult to match elements across different GSDs, resulting in a range of methods for computing metrics, some of which may not be adequate. For this reason, the purpose of this work is to set out a clear set of guidelines for fairly comparing semantic segmentation results obtained at various spatial resolutions. Additionally, we propose to complement the commonly used scene-based pixel-wise metrics with region-based pixel-wise metrics, allowing for a more detailed analysis of the model performance. The set of guidelines together with the proposed region-based metrics are illustrated with building and swimming pool detection problems. The experimental study demonstrates that by following the proposed guidelines and the proposed region-based pixel-wise metrics, it is possible to fairly compare segmentation maps at different spatial resolutions and gain a better understanding of the model's performance.",Measurement,Guidelines,Semantic segmentation,Spatial resolution,Buildings,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Task analysis,Remote sensing,Error metrics,quality assessment,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_745,"Liu, Wei","Su, Fulin",,,Unsupervised Adversarial Domain Adaptation Network for Semantic Segmentation,,NOV 2020,26,"With the rapid development of deep learning technology, semantic segmentation methods have been widely used in remote sensing data. A pretrained semantic segmentation model usually cannot perform well when the testing images (target domain) have an obvious difference from the training data set (source domain), while a large enough labeled data set is almost impossible to be acquired for each scenario. Unsupervised domain adaptation (DA) techniques aim to transfer knowledge learned from the source domain to a totally unlabeled target domain. By reducing the domain shift, DA methods have shown the ability to improve the classification accuracy for the target domain. Hence, in this letter, we propose an unsupervised adversarial DA network that converts deep features into 2-D feature curves and reduces the discrepancy between curves from the source domain and curves from the target domain based on a conditional generative adversarial networks (cGANs) model. Our proposed DA network is able to improve the semantic labeling accuracy when we apply a pretrained semantic segmentation model to the target domain. To test the effectiveness of the proposed method, experiments are conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) 2-D Semantic Labeling data set. Results show that our proposed network is able to stably improve overall accuracy not only when the source and target domains are from the same city but with different building styles but also when the source and target domains are from different cities and acquired by different sensors. By comparing with a few state-of-the-art DA methods, we demonstrate that our proposed method achieves the best cross-domain semantic segmentation performance.",Feature extraction,Semantics,Image segmentation,Data models,Remote sensing,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Labeling,Training,Domain adaptation (DA),generative adversarial networks (GANs),remote sensing image,semantic segmentation,transfer learning,,,,,,,,,,,,,,,,,,,,,,,
Row_746,"Ye, Xiaoling","Dong, Shimiao","Hu, Kai","Zhang, Yingchao",Classification and extraction method of hidden dangers along railway lines based on semantic segmentation network,,OCT 2024,0,"Foreign objects invading high-speed railway lines can cause danger. One existing solution is to use remote sensing images to analyse the dangerous areas along the railway line, thereby providing a certain amount of investigation time. Considering the spatial and temporal resolution characteristics of existing remote sensing technologies in identifying floating objects and the reality of rapid land use changes, this paper identifies areas on the ground where floating objects may be generated by using semantic segmentation techniques oriented to remotely sensed imagery and provides early warnings to staff along the route. However, these regions that need to be analysed have different semantics and scales. To address these challenges, this paper proposes a Dual-branch Parallel Fusion Network (DPFNet) based on Transformer, aimed at enhancing multi-class semantic segmentation in remote sensing images. To leverage global contextual information, we introduce a Swin Transformer-based backbone network, which employs self-attention to capture a comprehensive scene context, facilitating better segmentation by considering the entire scene's context. For multi-scale semantic features, we propose one approach that involves independent branching feature expression and a Multi-scale Feature Space Fusion Module (MFSFM). The former enriches multi-scale information, while the latter fuses features across different levels to capture diverse semantic features. Experimental results demonstrate that DPFNet can effectively identify the hidden danger area, and the fusion of multi-scale features makes the network more accurately identify and segment the risk area of different sizes, improving the segmentation accuracy and robustness, and is of great significance to the formation of the 'prevention' as the core of the railway safety operation.",Foreign object intrusion,Semantic segmentation,Transformer,Remote sensing imagery,,"Xiong, Xiong","Zhang, Yanchao","Sheng, Tao",,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_747,"Yuan, Wei","Xu, Wenbo",,,GapLoss: A Loss Function for Semantic Segmentation of Roads in Remote Sensing Images,,MAY 2022,7,"At present, road continuity is a major challenge, and it is difficult to extract the centerline vector of roads, especially when the road view is obstructed by trees or other structures. Most of the existing research has focused on optimizing the available deep-learning networks. However, the segmentation accuracy is also affected by the loss function. Currently, little research has been published on road segmentation loss functions. To resolve this problem, an attention loss function named GapLoss that can be combined with any segmentation network was proposed. Firstly, a deep-learning network was used to obtain a binary prediction mask. Secondly, a vector skeleton was extracted from the prediction mask. Thirdly, for each pixel, eight neighboring pixels with the same value of the pixel were calculated. If the value was 1, then the pixel was identified as the endpoint. Fourth, according to the number of endpoints within a buffered range, each pixel in the prediction image was given a corresponding weight. Finally, the weighted average value of the cross-entropy of all the pixels in the batch was used as the final loss function value. We employed four well-known semantic segmentation networks to conduct comparative experiments on three large datasets. The results showed that, compared to other loss functions, the evaluation metrics after using GapLoss were nearly all improved. From the predicted image, the road prediction by GapLoss was more continuous, especially at intersections and when the road was obscured from view, and the road segmentation accuracy was improved.",deep learning,loss function,road extraction,remote sensing image,sematic segmentation,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_748,"Ding, Lei","Zhang, Jing","Guo, Haitao","Zhang, Kai",Joint Spatio-Temporal Modeling for Semantic Change Detection in Remote Sensing Images,,2024,24,"Semantic change detection (SCD) refers to the task of simultaneously extracting changed areas and their semantic categories (before and after the changes) in remote sensing images (RSIs). This is more meaningful than binary change detection (BCD) since it enables detailed change analysis in the observed areas. Previous works established triple-branch convolutional neural network (CNN) architectures as the paradigm for SCD. However, it remains challenging to exploit semantic information with a limited amount of change samples. In this work, we investigate to jointly consider the spatio-temporal dependencies to improve the accuracy of SCD. First, we propose a semantic change transformer (SCanFormer) to explicitly model the ""from-to"" semantic transitions between the bitemporal RSIs. Then, we introduce a semantic learning scheme to leverage the spatio-temporal constraints, which are coherent to the SCD task, to guide the learning of semantic changes. The resulting network semantic change network (SCanNet) significantly outperforms the baseline method in terms of both detection of critical semantic changes and semantic consistency in the obtained bitemporal results. It achieves the state-of-the-art (SOTA) accuracy on two benchmark datasets for the SCD.",Convolutional neural network (CNN),remote sensing (RS),semantic change detection (SCD),semantic segmentation,vision transformer,"Liu, Bing","Bruzzone, Lorenzo",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_749,"Mei, Liye","Ye, Zhaoyi","Xu, Chuan","Wang, Hongzhu",SCD-SAM: Adapting Segment Anything Model for Semantic Change Detection in Remote Sensing Imagery,,2024,7,"Semantic change detection (SCD) has gradually emerged as a prominent research focus in remote sensing image processing due to its critical role in Earth observation applications. In view of its powerful semantic-driven feature extraction capability, the segment anything model (SAM) has demonstrated its suitability across various visual scenes. However, it suffers from significant performance degradation when confronted with remote sensing images, especially those containing various ground objects that possess significant interclass similarity and substantial intraclass variations. To address the above issues, we propose SCD-SAM, aiming to leverage the potent visual recognition capabilities of SAM for enhanced accuracy and robustness in SCD. Specifically, we introduce a contextual semantic change-aware dual encoder that combines MobileSAM and CNN to extract progressive semantic change features in parallel and inject local features into the MobileSAM encoder through depth feature interaction (DFI) to compensate for the Transformer's limitations in perceiving local semantic details. In addition, in order to utilize the strong visual feature extraction capability of MobileSAM in remote sensing images, we propose a semantic adaptor that aggregates semantic-oriented information about changing objects. To better integrate the extracted contextual semantic information, we devise a progressive feature aggregation dual decoder that aggregates binary change features and semantic change features, respectively, alleviating the semantic gap across different scales. The quantitative and visual results show that SCD-SAM outperforms the state-of-the-art SCD methods on publicly open SCD datasets (e.g., SECOND-CD and Landsat-CD). The code will be available at: https://github.com/yzygit1230/SCD-SAM.",Semantics,Feature extraction,Remote sensing,Image segmentation,Decoding,"Wang, Ying","Lei, Cheng","Yang, Wei","Li, Yansheng",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Adaptation models,Transformers,Progressive feature aggregation,segment anything model (SAM),semantic adaptor,semantic change detection (SCD),,,,,,,,,,,,,,,,,,,,,,,,
Row_750,"Wang, Baoguo","Yao, Yonghui",,,Mountain Vegetation Classification Method Based on Multi-Channel Semantic Segmentation Model,,JAN 2024,5,"With the development of satellite remote sensing technology, a substantial quantity of remote sensing data can be obtained every day, but the ability to extract information from these data remains poor, especially regarding intelligent extraction models for vegetation information in mountainous areas. Because the features of remote sensing images (such as spectral, textural and geometric features) change with changes in illumination, viewing angle, scale and spectrum, it is difficult for a remote sensing intelligent interpretation model with a single data source as input to meet the requirements of engineering or large-scale vegetation information extraction and updating. The effective use multi-source, multi-resolution and multi-type data for remote sensing classification is still a challenge. The objective of this study is to develop a highly intelligent and generalizable classification model of mountain vegetation utilizing multi-source remote sensing data to achieve accurate vegetation extraction. Therefore, a multi-channel semantic segmentation model based on deep learning, FCN-ResNet, is proposed to integrate the features and textures of multi-source, multi-resolution and multi-temporal remote sensing data, thereby enhancing the differentiation of different mountain vegetation types by capturing their characteristics and dynamic changes. In addition, several sets of ablation experiments are designed to investigate the effectiveness of the model. The method is validated on Mt. Taibai (part of the Qinling-Daba Mountains), and the pixel accuracy (PA) of vegetation classification reaches 85.8%. The results show that the proposed multi-channel semantic segmentation model can effectively discriminate different vegetation types and has good intelligence and generalization ability in different mountainous areas with similar vegetation distributions. The multi-channel semantic segmentation model can be used for the rapid updating of vegetation type maps in mountainous areas.",vegetation classification,multi-source image,remote sensing,semantic segmentation,multi-channel model,,,,,REMOTE SENSING,,deep learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_751,"Ding, Cheng","Weng, Liguo","Xia, Min","Lin, Haifeng",Non-Local Feature Search Network for Building and Road Segmentation of Remote Sensing Image,,APR 2021,17,"Building and road extraction from remote sensing images is of great significance to urban planning. At present, most of building and road extraction models adopt deep learning semantic segmentation method. However, the existing semantic segmentation methods did not pay enough attention to the feature information between hidden layers, which led to the neglect of the category of context pixels in pixel classification, resulting in these two problems of large-scale misjudgment of buildings and disconnection of road extraction. In order to solve these problem, this paper proposes a Non-Local Feature Search Network (NFSNet) that can improve the segmentation accuracy of remote sensing images of buildings and roads, and to help achieve accurate urban planning. By strengthening the exploration of hidden layer feature information, it can effectively reduce the large area misclassification of buildings and road disconnection in the process of segmentation. Firstly, a Self-Attention Feature Transfer (SAFT) module is proposed, which searches the importance of hidden layer on channel dimension, it can obtain the correlation between channels. Secondly, the Global Feature Refinement (GFR) module is introduced to integrate the features extracted from the backbone network and SAFT module, it enhances the semantic information of the feature map and obtains more detailed segmentation output. The comparative experiments demonstrate that the proposed method outperforms state-of-the-art methods, and the model complexity is the lowest.",semantic segmentation,building and road segmentation,self-attention,deep learning,,,,,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_752,"Zhao, Rui","Shi, Zhenwei","Zou, Zhengxia",,High-Resolution Remote Sensing Image Captioning Based on Structured Attention,,2022,76,"Automatically generating language descriptions of remote sensing images has become an emerging research hot spot in the remote sensing field. Attention-based captioning, as a representative group of recent deep learning-based captioning methods, shares the advantage of generating the words while highlighting corresponding object locations in the image. Standard attention-based methods generate captions based on coarse-grained and unstructured attention units, which fails to exploit structured spatial relations of semantic contents in remote sensing images. Although the structure characteristic makes remote sensing images widely divergent to natural images and poses a greater challenge for the remote sensing image captioning task, the key of most remote sensing captioning methods is usually borrowed from the computer vision community without considering the domain knowledge behind. To overcome this problem, a fine-grained, structured attention-based method is proposed to utilize the structural characteristics of semantic contents in high-resolution remote sensing images. Our method learns better descriptions and can generate pixelwise segmentation masks of semantic contents. The segmentation can be jointly trained with the captioning in a unified framework without requiring any pixelwise annotations. Evaluations are conducted on three remote sensing image captioning benchmark data sets with detailed ablation studies and parameter analysis. Compared with the state-of-the-art methods, our method achieves higher captioning accuracy and can generate high-resolution and meaningful segmentation masks of semantic contents at the same time.",Remote sensing,Semantics,Image segmentation,Proposals,Decoding,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Task analysis,Feature extraction,Image captioning,image segmentation,remote sensing image,structured attention,,,,,,,,,,,,,,,,,,,,,,,,
Row_753,"Hu, Kai","Zhang, Enwei","Xia, Min","Weng, Liguo",MCANet: A Multi-Branch Network for Cloud/Snow Segmentation in High-Resolution Remote Sensing Images,,FEB 2023,33,"Because clouds and snow block the underlying surface and interfere with the information extracted from an image, the accurate segmentation of cloud/snow regions is essential for imagery preprocessing for remote sensing. Nearly all remote sensing images have a high resolution and contain complex and diverse content, which makes the task of cloud/snow segmentation more difficult. A multi-branch convolutional attention network (MCANet) is suggested in this study. A double-branch structure is adopted, and the spatial information and semantic information in the image are extracted. In this way, the model's feature extraction ability is improved. Then, a fusion module is suggested to correctly fuse the feature information gathered from several branches. Finally, to address the issue of information loss in the upsampling process, a new decoder module is constructed by combining convolution with a transformer to enhance the recovery ability of image information; meanwhile, the segmentation boundary is repaired to refine the edge information. This paper conducts experiments on the high-resolution remote sensing image cloud/snow detection dataset (CSWV), and conducts generalization experiments on two publicly available datasets (HRC_WHU and L8 SPARCS), and the self-built cloud and cloud shadow dataset. The MIOU scores on the four datasets are 92.736%, 91.649%, 80.253%, and 94.894%, respectively. The experimental findings demonstrate that whether it is for cloud/snow detection or more complex multi-category detection tasks, the network proposed in this paper can completely restore the target details, and it provides a stronger degree of robustness and superior segmentation capabilities.",multi-branch,segmentation,deep learning,remote sensing image,,"Lin, Haifeng",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_754,"Ling, Min","Cheng, Qun","Peng, Jun","Zhao, Chenyi",Image Semantic Segmentation Method Based on Deep Learning in UAV Aerial Remote Sensing Image,,APR 26 2022,2,"The existing semantic segmentation methods have some shortcomings in feature extraction of remote sensing images. Therefore, an image semantic segmentation method based on deep learning in UAV aerial remote sensing images is proposed. First, original remote sensing images obtained by S185 multirotor UAV are divided into smaller image blocks through sliding window and normalized to provide high-quality image set for subsequent operations. Then, the symmetric encoding-decoding network structure is improved. Bottleneck layer with 1x1 convolution is introduced to build ISegNet network model, and pooling index and convolution are used to fuse semantic information and image features. The improved encoding-decoding network gradually strengthens the extraction of details and reduces the number of parameters. Finally, based on ISegNet network, five-classification problem is transformed into five binary classification problems for network training, so as to obtain high-precision image semantic segmentation results. The experimental analysis of the proposed method based on TensorFlow framework shows that the accuracy value reaches 0.901, and the F1 value is not less than 0.83. The overall segmentation effect is better than those of other comparison methods.",,,,,,"Jiang, Ling",,,,MATHEMATICAL PROBLEMS IN ENGINEERING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_755,"Wen, Bin","Pan, Rumeng","Zeng, Yahui","Cheng, Haibo",Crop classification in time-series remote sensing images based on multi-feature extraction and attention connection semantic segmentation model,"2024 12TH INTERNATIONAL CONFERENCE ON AGRO-GEOINFORMATICS, AGRO-GEOINFORMATICS 2024",2024,0,"Time-series multispectral remote sensing imagery provides a dynamic representation of crop variations over time, highlighting the disparities in growth conditions among diverse crop types. This method offers a superior capability to differentiate between crop types compared to single temporal phase imagery. However, when applied to practical issues of crop segmentation, challenges persist, including low segmentation accuracy and underutilization of features. To comprehensively address these challenges, we design a new multidimensional multi-attention semantic segmentation network suitable for crop extraction from timeseries multispectral remote sensing images (TSRSnet). Specifically, we propose a multidimensional feature extraction module (MFE) for efficiently mining spatial, temporal and spectral dimensional features in multi-temporal remote sensing images. The module consists of a spatial convolutional layer with temporal modeling capabilities, and a pairwise attention mechanism for joint spatial and channel. Furthermore, we have introduced a difference skip connection module based on a multi-attention mechanism (ADC). This module can accomplish superior performance during the feature fusion phase, contingent on the significance of intra-feature and inter-feature relationships. We experimentally compare the proposed network with a typical semantic segmentation network on the dataset. The results show that the network proposed in this paper has the best segmentation performance and is well suited for the task of crop segmentation in time-series multispectral remote sensing images. The Mean Intersection over Union reached 86.37% and overall accuracy over 93.24%. Through the combination of meticulous feature extraction and multiattention mechanism, our approach improves the segmentation accuracy while better utilizing the multidimensional information of the image, thus realizing the accurate recognition and segmentation of different crops.",time-series,remote sensing images,multidimensional feature,attention connection,crop classification,"Zou, Jialuo","Cao, Yungang","Li, Xuqing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_756,"Liang, Han","Seo, Suyoung",,,LPASS-Net: Lightweight Progressive Attention Semantic Segmentation Network for Automatic Segmentation of Remote Sensing Images,,DEC 2022,2,"Semantic segmentation of remote sensing images plays a crucial role in urban planning and development. How to perform automatic, fast, and effective semantic segmentation of considerable size and high-resolution remote sensing images has become the key to research. However, the existing segmentation methods based on deep learning are complex and often difficult to apply practically due to the high computational cost of the excessive parameters. In this paper, we propose an end-to-end lightweight progressive attention semantic segmentation network (LPASS-Net), which aims to solve the problem of reducing computational costs without losing accuracy. Firstly, its backbone features are based on a lightweight network, MobileNetv3, and a feature fusion network composed of a reverse progressive attentional feature fusion network work. Additionally, a lightweight non-local convolutional attention network (LNCA-Net) is proposed to effectively integrate global information of attention mechanisms in the spatial dimension. Secondly, an edge padding cut prediction (EPCP) method is proposed to solve the problem of splicing traces in the prediction results. Finally, evaluated on the public datasets BDCI 2017 and ISPRS Potsdam, the mIoU reaches 83.17% and 88.86%, respectively, with an inference time of 0.0271 s.",lightweight network,attention mechanism,very high resolution,deep learning,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_757,"Zhou, Xichuan","Liang, Fu","Chen, Lihui","Liu, Haijun",MeSAM: Multiscale Enhanced Segment Anything Model for Optical Remote Sensing Images,,2024,1,"Segment anything model (SAM) has been widely applied to various downstream tasks for its excellent performance and generalization capability. However, SAM exhibits three limitations related to remote sensing (RS) semantic segmentation task: 1) the image encoders excessively lose high-frequency information, such as object boundaries and textures, resulting in rough segmentation masks; 2) due to being trained on natural images, SAM faces difficulty in accurately recognizing objects with large-scale variations and uneven distribution in RS images; and 3) the output tokens used for mask prediction are trained on natural images and not applicable to RS image segmentation. In this article, we explore an efficient paradigm for applying SAM to the semantic segmentation of RS images. Furthermore, we propose multiscale enhanced SAM (MeSAM), a new SAM fine-tuning method more suitable for RS images to adapt it to semantic segmentation tasks. Our method first introduces an inception mixer into the image encoder to effectively preserve high-frequency features. Second, by designing a mask decoder with RS correction and incorporating multiscale connections, we make up the difference in SAM from natural images to RS images. Experimental results demonstrated that our method significantly improves the segmentation accuracy of SAM for RS images, outperforming some state-of-the-art (SOTA) methods. The code will be available at https://github.com/Magic-lem/MeSAM.",High-frequency,multiscale,remote sensing (RS),segment anything model (SAM),semantic segmentation,"Song, Qianqian","Vivone, Gemine","Chanussot, Jocelyn",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_758,"Shankar, Siddharth","Stearns, Leigh A.","van der Veen, C. J.",,Semantic segmentation of glaciological features across multiple remote sensing platforms with the Segment Anything Model (SAM),,NOV 2023,4,"Semantic segmentation is a critical part of observation-driven research in glaciology. Using remote sensing to quantify how features change (e.g. glacier termini, supraglacial lakes, icebergs, crevasses) is particularly important in polar regions, where glaciological features may be spatially small but reflect important shifts in boundary conditions. In this study, we assess the utility of the Segment Anything Model (SAM), released by Meta AI Research, for cryosphere research. SAM is a foundational AI model that generates segmentation masks without additional training data. This is highly beneficial in polar science because pre-existing training data rarely exist. Widely-used conventional deep learning models such as UNet require tens of thousands of training labels to perform effectively. We show that the Segment Anything Model performs well for different features (icebergs, glacier termini, supra-glacial lakes, crevasses), in different environmental settings (open water, melange, and sea ice), with different sensors (Sentinel-1, Sentinel-2, Planet, timelapse photographs) and different spatial resolutions. Due to the performance, versatility, and cross-platform adaptability of SAM, we conclude that it is a powerful and robust model for cryosphere research.",Crevasses,glacier mapping,iceberg calving,remote sensing,sea ice,,,,,JOURNAL OF GLACIOLOGY,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_759,"Rao, Zhibo","He, Mingyi","Dai, Yuchao",,Class Attention Network for Semantic Segmentation of Remote Sensing Images,2020 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL SUMMIT AND CONFERENCE (APSIPA ASC),2020,3,"Semantic segmentation in remote sensing images is beneficial to detect objects and understand the scene in earth observation. However, classical networks always failed to obtain an accuracy segmentation map in remote sensing images due to the imbalanced labels. In this paper, we proposed a novel class attention module and decomposition-fusion strategy to cope with imbalanced labels. Based on this motivation, we investigate related architecture and strategy by follows. (1) we build a class attention module to generate multi-class attention maps, which forces the network to keep attention to small sample categories instead of being flooded by large sample data. (2) we introduce salient detection, which decomposes semantic segmentation into multi-class salient detection and then fuses them to produce a segmentation map. Extensive experiments on popular benchmarks (e.g., US3D dataset) show that our approach can serve as an efficient plug-and-play module or strategy in the previous scene parsing networks to help them cope with the problem of imbalance labels in remote sensing images.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_760,"Fan, Zhenyu","Zhan, Tao","Gao, Zhichao","Li, Rui",Land Cover Classification of Resources Survey Remote Sensing Images Based on Segmentation Model,,2022,10,"Land type survey is an important task of land resources survey and the basis of scientific management of land resources. With the increasingly prominent problems of population, resources, and environment, there is an urgent need for a fast and accurate classification method of large-scale land use and land cover based on remote sensing data. Traditional machine learning classification methods based on pixel classification achieved sufficient results and are widely used, such as maximum likelihood classification and random forests method. However, with the development of the novel technology of deep learning, in practical application, for multi-classified land resources, how to use the fast and effective classification method of low and medium resolution RS images needs further research. This paper takes the land resource classification of the Tonghe medium resolution RS dataset of the third land survey in China as an example to screen and compare traditional machine learning classification methods and semantic segmentation models FC-DenseNet56, GCN, BiSeNet, U-Net, DeepLabV3, AdapNet, and PSPNet, which aim to select the optimal feature extraction model. The results show that the classification accuracy of the U-Net model can reach 93.62%, which is more accurate and effective than traditional machine learning methods and other semantic segmentation models. It is suitable for multi-classification tasks of land cover resources in low and medium resolution RS images and shows a superior effect in practical application. Besides, the conclusion of this study can provide a demonstration for large-scale land cover resources investigation using low and medium resolution RS images.",Image segmentation,Semantics,Machine learning,Feature extraction,Radio frequency,"Liu, Yao","Zhang, Lianzhi","Jin, Zixiang","Xu, Supeng",IEEE ACCESS,,Classification algorithms,Spatial resolution,Land use and land cover,semantic segmentation,multi-classification,deep learning,U-Net,,,,,,,,,,,,,,,,,,,,,,,
Row_761,"Zhang, Yinsheng","Ji, Ru","Hu, Yuxiang","Yang, Yulong",Real-Time Semantic Segmentation of Remote Sensing Images for Land Management,,JUN 2024,0,"Remote sensing image segmentation is a crucial technique in the field of land management. However, existing semantic segmentation networks require a large number of floating-point operations (FLOPs) and have long run times. In this paper, we propose a dual -path feature aggregation network (DPFANet) specifically designed for the low -latency operations required in land management applications. Firstly, we use four sets of spatially separable convolutions with varying dilation rates to extract spatial features. Additionally, we use an improved version of MobileNetV2 to extract semantic features. Furthermore, we use an asymmetric multi -scale fusion module and dual -path feature aggregation module to enhance feature extraction and fusion. Finally, a decoder is constructed to enable progressive up -sampling. Experimental results on the Potsdam data set and the Gaofen image data set (GID) demonstrate that DPFANet achieves overall accuracy of 92.2% and 89.3%, respectively. The FLOPs are 6.72 giga and the number of parameters is 2.067 million.",,,,,,"Chen, Xin","Duan, Xiuxian","Shan, Huilin",,PHOTOGRAMMETRIC ENGINEERING AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_762,"Zhang, Junjie","Zhang, Qiming","Gong, Yongshun","Zhang, Jian",Weakly Supervised Semantic Segmentation With Consistency-Constrained Multiclass Attention for Remote Sensing Scenes,,2024,0,"Obtaining image-level class labels for remote sensing (RS) images is a relatively straightforward process, sparking significant interest in weakly supervised semantic segmentation (WSSS). However, RS images present challenges beyond those encountered in generic WSSS, including complex backgrounds, densely distributed small objects, and considerable scale variations. To address the above issues, we introduce a consistency-constrained multiclass attention model, noted as CocoaNet. Specifically, CocoaNet endeavors to capture both semantic correlation and class distinctiveness using a global-local adaptive attention mechanism, which integrates the self-attention to model global correlation, complemented by a local perception branch that intensifies focus on local regions. The resulting class-specific attention weights and the patch-level pairwise affinity weights are employed to optimize the initial class activation maps (CAMs). This mechanism proves highly effective in mitigating interclass interference and managing the distribution of densely clustered small objects. Moreover, we invoke a consistency constraint to rectify activation inaccuracy. By utilizing a Siamese structure for the mutual supervision of features extracted from images at different scales, we address substantial scale variations in RS scenes. Simultaneously, a class contrast loss is adopted to enhance the discriminativeness of class-specific features. Departing from the conventional CAM optimization, which is rather complex and time-consuming, we harness the prior knowledge from the generic segment anything model (SAM) to design a joint optimization strategy (JOS) that refines target boundaries and further promotes discriminative visual features. We validate the effectiveness of our proposed approach on three benchmark datasets in multiclass RS scenarios, and the experimental results demonstrate that our model yields promising advancements compared to state-of-the-art methods.",Consistency constraint,global-local adaptive attention,weakly supervised semantic segmentation (WSSS),,,"Chen, Liang","Zeng, Dan",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_763,"Gao, Han","Zhao, Yang","Guo, Peng","Sun, Zihao",Cycle and Self-Supervised Consistency Training for Adapting Semantic Segmentation of Aerial Images,,APR 2022,11,"Semantic segmentation is a critical problem for many remote sensing (RS) image applications. Benefiting from large-scale pixel-level labeled data and the continuous evolution of deep neural network architectures, the performance of semantic segmentation approaches has been constantly improved. However, deploying a well-trained model on unseen and diverse testing environments remains a major challenge: a large gap between data distributions in train and test domains results in severe performance loss, while manual dense labeling is costly and not scalable. To this end, we proposed an unsupervised domain adaptation framework for RS image semantic segmentation that is both practical and effective. The framework is supported by the consistency principle, including the cycle consistency in the input space and self-supervised consistency in the training stage. Specifically, we introduce cycle-consistent generative adversarial networks to reduce the discrepancy between source and target distributions by translating one into the other. The translated source data then drive a pipeline of supervised semantic segmentation model training. We enforce consistency of model predictions across target image transformations in order to provide self-supervision for the unlabeled target data. Experiments and extensive ablation studies demonstrate the effectiveness of the proposed approach on two challenging benchmarks, on which we achieve up to 9.95% and 7.53% improvements in accuracy over the state-of-the-art methods, respectively.",unsupervised domain adaptation,semantic segmentation,self-supervision,remote sensing image,,"Chen, Xiuwan","Tang, Yunwei",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_764,"Liang, Chenbin","Cheng, Bo","Xiao, Baihua","Dong, Yunyun",Multilevel Heterogeneous Domain Adaptation Method for Remote Sensing Image Segmentation,,2023,12,"Due to more abundant data sources, more various objects of interest, and more time-consuming annotations, there is a large amount of out-of-distribution (OOD) data in the remote sensing field, on which the performance of high-accuracy image segmentation models trained under ideal experimental conditions generally degrades dramatically. Domain adaptation (DA) consequently comes into being, which aims to learn the predictor for the label-scarce target domain of interest with the help of the label-sufficient source domain in the presence of the distribution difference, namely, domain shift, between the two domains. However, the off-the-shelf DA methods for image segmentation not only struggle to cope with the more complex domain shift problems in remote sensing imagery but also almost cannot process heterogeneous data directly without information loss. While the current heterogeneous DA methods mostly still rely on some supervision information from the target domain, which is typically inaccessible in the real world. To overcome these drawbacks, we propose the multilevel heterogeneous unsupervised DA (UDA) method, termed MHDA, which unifies the instance-level DA based on cycle consistency, the feature-level DA based on contrastive learning, and the decision-level DA based on task consistency into a framework to more effectively handle the complex domain shift and heterogeneous data. After that, extensive DA experiments are conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) dataset, the BigCity dataset constructed by ourselves, and the Wuhan University (WHU) dataset, to explore the effect of each module in MHDA, the necessity of heterogeneous DA, and the effectiveness of multilevel DA. And the results demonstrate that MHDA can achieve superior performance on the remote sensing image segmentation task, compared with several state-of-the-art DA methods.",Remote sensing,Image segmentation,Task analysis,Sensors,Adaptation models,"Chen, Jinfen",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Spatial resolution,Contrastive learning,cycle consistency,domain adaptation (DA),semantic segmentation,task consistency,,,,,,,,,,,,,,,,,,,,,,,
Row_765,"Pang, Shiyan","Li, Xinyu","Chen, Jia","Zuo, Zhiqi",Prior Semantic Information Guided Change Detection Method for Bi-temporal High-Resolution Remote Sensing Images,,MAR 2023,5,"High-resolution remote sensing image change detection technology compares and analyzes bi-temporal or multitemporal high-resolution remote sensing images to determine the change areas. It plays an important role in land cover/use monitoring, natural disaster monitoring, illegal building investigation, military target strike effect analysis, and land and resource investigation. The change detection of high-resolution remote sensing images has developed rapidly from data accumulation to algorithm models because of the rapid development of technologies such as deep learning and earth observation in recent years. However, the current deep learning-based change detection methods are strongly dependent on large sample data, and the training model has insufficient cross-domain generalization ability. As a result, a prior semantic information-guided change detection framework (PSI-CD), which alleviates the change detection model's dependence on datasets by making full use of prior semantic information, is proposed in this paper. The proposed method mainly includes two parts: one is a prior semantic information generation network that uses the semantic segmentation dataset to extract robust and reliable prior semantic information; the other is the prior semantic information guided change detection network that makes full use of prior semantic information to reduce the sample size of the change detection. To verify the effectiveness of the proposed method, we produced pixel-level semantic labels for the bi-temporal images of the public change detection dataset (LEVIR-CD). Then, we performed extensive experiments on the WHU and LEVIR-CD datasets, including comparisons with existing methods, experiments with different amounts of data, and ablation study, to show the effectiveness of the proposed method. Compared with other existing methods, our method has the highest IoU for all training samples and different amounts of training samples on WHU and LEVIR-CD, reaching a maximum of 83.25% and 83.80%, respectively.",change detection,prior semantic information,semantic segmentation,convolutional neural networks,high-resolution images,"Hu, Xiangyun",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_766,"Chen, Keyan","Liu, Chenyang","Chen, Hao","Zhang, Haotian",RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation Based on Visual Foundation Model,,2024,75,"Leveraging the extensive training data from SA-1B, the segment anything model (SAM) demonstrates remarkable generalization and zero-shot capabilities. However, as a category-agnostic instance segmentation method, SAM heavily relies on prior manual guidance, including points, boxes, and coarse-grained masks. Furthermore, its performance in remote sensing image segmentation tasks remains largely unexplored and unproven. In this article, we aim to develop an automated instance segmentation approach for remote sensing images based on the foundational SAM model and incorporating semantic category information. Drawing inspiration from prompt learning, we propose a method to learn the generation of appropriate prompts for SAM. This enables SAM to produce semantically discernible segmentation results for remote sensing images, a concept that we have termed RSPrompter. We also propose several ongoing derivatives for instance segmentation tasks, drawing on recent advancements within the SAM community, and compare their performance with RSPrompter. Extensive experimental results, derived from the WHU building dataset, the NWPU VHR-10 dataset, and the SAR Ship Detection Dataset (SSDD) dataset, validate the effectiveness of our proposed method. The code for our method is publicly available at https://kychen.me/RSPrompter.",Foundation model,instance segmentation,prompt learning,remote sensing images,segment anything model (SAM),"Li, Wenyuan","Zou, Zhengxia","Shi, Zhenwei",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_767,"Chen, Ziyi","Li, Dilong","Fan, Wentao","Guan, Haiyan",Self-Attention in Reconstruction Bias U-Net for Semantic Segmentation of Building Rooftops in Optical Remote Sensing Images,,JUL 2021,56,"Deep learning models have brought great breakthroughs in building extraction from high-resolution optical remote-sensing images. Among recent research, the self-attention module has called up a storm in many fields, including building extraction. However, most current deep learning models loading with the self-attention module still lose sight of the reconstruction bias's effectiveness. Through tipping the balance between the abilities of encoding and decoding, i.e., making the decoding network be much more complex than the encoding network, the semantic segmentation ability will be reinforced. To remedy the research weakness in combing self-attention and reconstruction-bias modules for building extraction, this paper presents a U-Net architecture that combines self-attention and reconstruction-bias modules. In the encoding part, a self-attention module is added to learn the attention weights of the inputs. Through the self-attention module, the network will pay more attention to positions where there may be salient regions. In the decoding part, multiple large convolutional up-sampling operations are used for increasing the reconstruction ability. We test our model on two open available datasets: the WHU and Massachusetts Building datasets. We achieve IoU scores of 89.39% and 73.49% for the WHU and Massachusetts Building datasets, respectively. Compared with several recently famous semantic segmentation methods and representative building extraction methods, our method's results are satisfactory.",building extraction,U-Net,remote sensing image,building footprint,,"Wang, Cheng","Li, Jonathan",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_768,"de Paulo, M. C. M.","Turnes, J. N.","Happ, P. N.","Ferreira, M. P.",HOWFAR SHOULD I LOOK? A NEURAL ARCHITECTURE SEARCH STRATEGY FOR SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES,"XXIV ISPRS CONGRESS: IMAGING TODAY, FORESEEING TOMORROW, COMMISSION III",2022,1,"Neural architecture search (NAS) is a subset of automated machine learning that tries to find the best neural network to perform a given task. In this article, a network search space is defined and applied to perform the semantic segmentation of satellite imagery. Due to the spatial nature of the data, the search space uses cells that group parallel operations with kernels of different sizes, providing options to accommodate the neighborhood information required to perform a better classification. The architecture search space follows a UNet-like network. The proposed approach uses scaled sigmoid gates, a strategy for network pruning that was adapted to search for the best operations on the cell search space. The architecture achieved by the proposed approach uses wider kernels on lower resolution feature maps, which leads to the interpretation that some pixels required information from pixels farther away than expected. The resulting network was compared to a very similar UNet-like network that only used 3x3 convolutions. The resulting network shows slightly better results on the test set.",Neural Architecture Search,Semantic Segmentation,Remote Sensing,Satellite imagery,Convolutional Neural Networks,"Marques, H. A.","Feitosa, R. Q.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_769,"Li, Huiping",,,,Multi-Scale Segmentation Method of Remote Sensing Big Data Image Using Deep Learning,,NOV 2024,0,"Remote sensing image (RSI) segmentation is an effective method to interpret remote sensing information and an important means of remote sensing data information processing. Traditional RSI segmentation methods have some problems such as poor segmentation accuracy and low similarity difference measurement. Therefore, we propose a multi-scale segmentation (MSS) method for remote sensing big data image. First, the segmentation scale of RSI is divided, and the quantitative value of histogram band is used to calculate the similarity index between different objects; Second, the parameters in the same spot are improved based on the maximum area method to determine the shape factor of RSI; Finally, the object closure model is established to clarify the region conversion cost, and the RSI is dynamically segmented based on Multi-scale convolutional neural networks; The MSS algorithm of RSI is designed, and the MSS method of RSI is obtained. The results show that the maximum similarity difference measure of the proposed method is 0.648, and the similarity difference measure always remains the largest. The maximum recall of RSI is 0.954, and the highest recall is 0.988, indicating that the RSI segmentation accuracy of the proposed method is good.",Deep learning,remote sensing big data images,multi-scale segmentation,feature space,segmentation accuracy,,,,,JOURNAL OF INTERCONNECTION NETWORKS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_770,"Yang, Yang","Wang, Yanhui","Dong, Junwu","Yu, Bibo",A Knowledge Distillation-Based Ground Feature Classification Network With Multiscale Feature Fusion in Remote-Sensing Images,,2024,2,"As a fundamental task in remote-sensing interpretation, semantic segmentation of remote-sensing images intends to allocate a definite class to each pixel in the image. Fast and efficient semantic segmentation of high-resolution remote-sensing images provides help to capture the real surface covering and plays an essential role in urban planning and dynamic monitoring. However, there are still some limitations in the previous remote-sensing image semantic segmentation model for urban scenes, such as the low weight of small target pixels and the tiny target size leading to the unsatisfactory recognition and segmentation results of the model for small target features. Meanwhile, the deeper and broader feature extraction module in the semantic segmentation network usually leads to more redundant parameters, which takes a lot of computation time. Thus, we propose a lightweight semantic segmentation network based on the knowledge distillation combined with a multiscale pyramidal pooling module and attention mechanism named KD-MSANet, which enhanced the ability to fuse and focus on shallow features. Then, we trained teacher-student models to obtain lightweight network models through a model pruning and distillation framework. Experiments on Vaihingen and Potsdam datasets demonstrated that the network we designed significantly reduces the number of parameters while ensuring almost constant accuracy. Compared with the precompression model, the student model reduced in size by 43.6% and the training efficiency was improved by 22.3%, while the accuracy reached 99.30% of the teacher model.",Feature extraction,Semantic segmentation,Semantics,Remote sensing,Computational modeling,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Knowledge engineering,Task analysis,Knowledge distillation (KD),multiscale pyramidal pooling,remote-sensing image (RSI),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_771,"Jaber, Mustafa Musa","Ali, Mohammed Hasan","Abd, Sura Khalil","Jassim, Mustafa Mohammed",A Machine Learning-Based Semantic Pattern Matching Model for Remote Sensing Data Registration,,DEC 2022,2,"Remote sensing image registration can benefit from a machine learning method based on the likelihood of predicting semantic spatial position distributions. Semantic segmentation of images has been revolutionized due to the accessibility of high-resolution remote sensing images and the advancement of machine learning techniques. This system captures the semantic distribution location of the matching reference picture, which ML mapped using learning-based algorithms. The affine invariant is utilized to determine the semantic template's barycenter position and the pixel's center, which changes the semantic border alignment problem into a point-to-point matching issue for the machine learning-based semantic pattern matching (ML-SPM) model. The first step examines how various factors such as template radius, training label filling form, or loss function combination affect matching accuracy. In this second step, the matching of sub-images (MSI) images is compared using heatmaps created from the expected similarity between the images' cropped sub-images. Images having radiometric discrepancies are matched with excellent accuracy by the approach. SAR-optical image matching has never been easier, and now even large-scale sceneries can be registered using this approach, which is a significant advance over previous methods. Optical satellite imaging or multi-sensor stereogrammetry can be combined with both forms of data to enhance geolocation.",Remote sensing image,Machine learning,Semantic pattern matching,Matching of sub-images,Loss function,"Alkhayyat, Ahmed","Alreda, Baraa A.","Alkhuwaylidee, Ahmed Rashid","Alyousif, Shahad",JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING,,Synthetic aperture radar (SAR),,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_772,"Xiu, Xiaochen","Ma, Xianping","Pun, Man-On","Liu, Ming",MDAFNET: MONOCULAR DEPTH-ASSISTED FUSION NETWORKS FOR SEMANTIC SEGMENTATION OF COMPLEX URBAN REMOTE SENSING DATA,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"This work proposes an end-to-end Monocular Depth-Assisted Fusion Network (MDAFNet) for semantic segmentation of complex urban remote sensing data. The proposed MDAFNet consists of a Monocular Depth Estimation Network (MDENet) and a Crossmodal Fusion Network (CFNet). More specifically, the MDENet first generates the earth surface depth information while the CFNet fuses the generated depth information and RGB images to address the segmentation task. In particular, the MDENet is capable of effectively extracting features of the ground surface while overcoming artifacts such as building shadows. Furthermore, the CFNet is designed to perform segmentation by extracting and fusing semantic information from generated depth information and Red-Green-Blue (RGB) images. Extensive experiments performed on a large-scale fine-resolution remote sensing dataset named the ISPRS Vaihingen confirm that the proposed MDAFNet outperforms conventional crossmodal models equipped with Digital Surface Model information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_773,"Tian, Qing","Zhao, Fuhui","Zhang, Zheng","Qu, Hongquan",GLFFNet: A Global and Local Features Fusion Network with Biencoder for Remote Sensing Image Segmentation,,AUG 2023,2,"In recent years, semantic segmentation of high-resolution remote sensing images has been gradually applied to many important scenes. However, with the rapid development of remote sensing data acquisition technology, the existing image data processing methods are facing major challenges. Especially in the accuracy of extraction and the integrity of the edges of objects, there are often problems such as small objects being assimilated by large objects. In order to solve the above problems, based on the excellent performance of Transformer, convolution and its variants, and feature pyramids in the field of deep learning image segmentation, we designed two encoders with excellent performance to extract global high-order interactive features and low-order local feature information. These encoders are then used as the backbone to construct a global and local feature fusion network with a dual encoder (GLFFNet) to effectively complete the segmentation of remote sensing images. Furthermore, a new auxiliary training module is proposed that uses the semantic attention layer to process the extracted feature maps separately, adjust the losses, and more specifically optimize each encoder of the backbone, thus optimizing the training process of the entire network. A large number of experiments show that our model achieves 87.96% mIoU on the Potsdam dataset and 80.42% mIoU on the GID dataset, and it has superior performance compared with some state-of-the-art methods on semantic segmentation tasks in the field of remote sensing.",remote sensing image,gated convolution,transformer,atrous convolution,,,,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_774,"Kim, Minho","Dronova, Iryna","Radke, John",,SEMANTIC SEGMENTATION OF ENHANCED LANDFORM MAPS USING HIGH RESOLUTION SATELLITE IMAGES,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"High resolution fuel maps are useful for high resolution wildfire simulations and detection of hazards on the landscape. In general, high resolution Enhanced Lifeform Maps (ELMs) are used in conjunction with other data layers to create these fuel maps. However, ELMs are costly to make with substantial manual editing involved. In response, this study uses deep learning-based semantic segmentation models to generate 5-m resolution ELMs (14 classes) in Marin and San Mateo, California using high resolution remote sensing datasets. ELM classes were found to be severely imbalanced, leading to model overfitting. Sample weighted loss functions helped alleviate this issue to an extent. High resolution ELMs are bound to be more valuable with the growing fire risk and landscape heterogeneity, particularly near the wildland urban interface. All codes, future updates, and further details can be found at https://github.com/minhokim93/elm_mapping.",Enhanced lifeform map,semantic segmentation,deep learning,remote sensing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_775,"Li, Hanlu","Li, Lei","Zhao, Liangyu","Liu, Fuxiang",ResU-Former: Advancing Remote Sensing Image Segmentation with Swin Residual Transformer for Precise Global-Local Feature Recognition and Visual-Semantic Space Learning,,JAN 2024,4,"In the field of remote sensing image segmentation, achieving high accuracy and efficiency in diverse and complex environments remains a challenge. Additionally, there is a notable imbalance between the underlying features and the high-level semantic information embedded within remote sensing images, and both global and local recognition improvements are also limited by the multi-scale remote sensing scenery and imbalanced class distribution. These challenges are further compounded by inaccurate local localization segmentation and the oversight of small-scale features. To achieve balance between visual space and semantic space, to increase both global and local recognition accuracy, and to enhance the flexibility of input scale features while supplementing global contextual information, in this paper, we propose a U-shaped hierarchical structure called ResU-Former. The incorporation of the Swin Residual Transformer block allows for the efficient segmentation of objects of varying sizes against complex backgrounds, a common scenario in remote sensing datasets. With the specially designed Swin Residual Transformer block as its fundamental unit, ResU-Former accomplishes the full utilization and evolution of information, and the maximum optimization of semantic segmentation in complex remote sensing scenarios. The standard experimental results on benchmark datasets such as Vaihingen, Overall Accuracy of 81.5%, etc., show the ResU-Former's potential to improve segmentation tasks across various remote sensing applications.",semantic segmentation,transformer,balance between visual and semantic space,enhancement of both global and local aspects,,,,,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_776,"Zhang, Yanan","Lu, Chen","Wang, Jiao","Du, Fuguang",A large-scale extraction framework for mapping urban in-formal settlements using remote sensing and semantic segmentation,,JAN 1 2024,2,"Urban informal settlements (UISs) are densely populated and poorly developed residential areas in urban areas. The mapping of UISs using remote sensing is crucial for urban planning and management. However, the large-scale extraction of UISs is impeded by the labor-intensive task of collecting numerous training samples and the lack of automatic and effective city partition. To overcome these challenges, we proposed a large-scale extraction framework for UISs based on semantic segmentation of high-resolution remote sensing images. Utilizing Deeplab V3 Plus as the foundational extraction model, the proposed framework introduces fast sample collection based on GLCM features. Besides, an automatic city partition approach combined with clustering and fine-tuning was proposed to enhance the performance on extracting a specific category of UISs. The results of the case study conducted in 36 major Chinese cities show that the proposed framework achieved good performance, with an overall F1 score of 85.76%. Furthermore, comparative assessments were performed to demonstrate the effectiveness of automatic city partition. The proposed framework offers a practical approach for the large-scale extraction of UISs, which holds great significance for sustainable development, poverty estimation, infrastructure construction, and urban planning.",Urban informal settlement,large scale,remote sensing,China,,,,,,GEOCARTO INTERNATIONAL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_777,"Xiao, Dong","Kang, Zhihao","Fu, Yanhua","Li, Zhenni",Csswin-unet: a Swin-unet network for semantic segmentation of remote sensing images by aggregating contextual information and extracting spatial information,,DEC 2 2023,4,"Image interpretation algorithms based on deep learning are becoming increasingly important in land cover information acquisition. We propose CSSwin-unet, a network designed for the semantic segmentation of remote sensing images. CSSwin-unet is based on Swin-unet, follows the U-shaped codec structure of U-Net, but utilizes Swin transformer blocks with superior global modelling capabilities to constitute the codec. In addition, we design a parallel branch in the encoder with a context aggregation module (CAM) to enhance contextual information extraction and alleviate the semantic ambiguity problem resulting from occlusion. To address the problem of semantic information mismatch between codecs and improve the model's ability to extract spatial information, we constructed a space extraction module (SEM) in the skip connections, which replaces the direct copying of encoder features in Swin-unet. To reduce information loss during the downsampling process and strengthen the segmentation capacity of the network, we designed a feature shrinkage module (FSM) in the downsampling session. We conducted comprehensive ablation experiments on a dataset we produced ourselves and compared the results with other advanced methods. The test results showed significant improvement, with mIoU, mF1, and OA values improving by 2.83%, 2.47%, and 2.05%, respectively, compared to the second best performing model, Swin-unet. The above results prove the excellent performance of CSSwin-unet.",Deep learning,remote sensing,segmentation,contextual information aggregation,spatial information extraction,"Ran, Mengying",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_778,"Chakravorty, Anisha","Chakraborty, Shounak",,,A novel semi-supervised approach for semantic segmentation of aerial remote sensing images under limited ground-truth availability,,DEC 2024,0,"Conventional semantic segmentation techniques rely heavily on the availability of substantial ground-truth data. However, this prerequisite often proves infeasible in real-world scenarios, particularly with the labeling complexities inherent in remote sensing images. In this manuscript, a semi-supervised approach has been investigated towards semantic segmentation of remotely sensed images by addressing the challenge of limited availability of ground-truth information. For this purpose, a hybrid integration of a standard semantic segmentation model and an adversarial model has been proposed under semi-supervised setting. The former predict the masks for the unlabelled images when fine-tuned with the available labelled training images (however limited they may be); whereas the latter aids the reconstruction of original input images from the predicted soft(masks) through an adversarial mechanism. This reconstruction, further validated through a reconstruction score, assist in the identification of 'most-confident' image-mask pairs to be strategically integrated into the training set. The contribution ultimately is to utilise the unannotated images to meaningfully augment the limited training set to obtain an enhanced one. The proposed technique showcases a significant improvement, with an 11-34% enhancement over existing approaches in terms of mean intersection over union, precision, and F1-score across both the minifrance and dense labeling remote sensing dataset datasets.",Semantic segmentation,Semi-supervision,Adversarial learning,Aerial imagery,Limited ground-truth,,,,,SIGNAL IMAGE AND VIDEO PROCESSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_779,"Zhang, Xiuwei","Zhao, Zixu","Ran, Lingyan","Xing, Yinghui",FastICENet: A real-time and accurate semantic segmentation model for aerial remote sensing river ice image,,NOV 2023,5,"River ice semantic segmentation is a crucial task, which can provide us with information for river monitoring, disaster forecasting, and transportation management. Previous works mainly focus on higher accuracy acquirement, while efficiency is also important for reality usage. In this paper, a real-time and accurate river ice semantic segmentation network is proposed, named FastICENet. The general architecture consists of two branches, i.e., a shallow high-resolution spatial branch and a deep context semantic branch, which are carefully designed for the scale diversity and irregular shape of river ice in remote sensing images. Then, a novel Downsampling module and a dense connection block based on a lightweight Ghost module are adopted in the context branch to reduce the computation cost. Furthermore, a learnable upsampling strategy DUpsampling is utilized to replace the commonly used bilinear interpolation to improve the segmentation accuracy. We deploy detailed experiments on three publicly available datasets, named NWPU_YRCC_EX, NWPU_YRCC2, and Alberta River Ice Segmentation Dataset. The experimental results demonstrate that our method achieves state-of-the-art performance with competing methods, on the NWPU_YRCC_EX dataset, we can achieve the segmentation speed as 90.84FPS and the segmentation accuracy as 90.770 % mIoU, which also illustrates the good leverage between accuracy and speed. Our code is available at https://github.com/nwpulab113/FastICENet & COPY; 2023 Elsevier B.V. All rights reserved.",River ice semantic segmentation,Deep learning,Ghost module,DUpsampling,,"Wang, Wenna","Lan, Zeze","Yin, Hanlin","He, Houjun",SIGNAL PROCESSING,"Liu, Qixing",,,,,,,,,,"Zhang, Baosen","Zhang, Yanning",,,,,,,,,,,,,,,,,,,
Row_780,"Wu, Linshan","Lu, Ming","Fang, Leyuan",,Deep Covariance Alignment for Domain Adaptive Remote Sensing Image Segmentation,,2022,25,"Unsupervised domain adaptive (UDA) image segmentation has recently gained increasing attention, aiming to improve the generalization capability for transferring knowledge from the source domain to the target domain. However, in high spatial resolution remote sensing image (RSI), the same category from different domains (e.g., urban and rural) can appear to be totally different with extremely inconsistent distributions, which heavily limits the UDA accuracy. To address this problem, in this article, we propose a novel deep covariance alignment (DCA) model for UDA RSI segmentation. The DCA can explicitly align category features to learn shared domain-invariant discriminative feature representations, which enhance the ability of model generalization. Specifically, a category feature pooling (CFP) module is first used to extract category features by combining coarse outputs and deep features. Then, we leverage a novel covariance regularization (CR) to enforce the intracategory features to be closer and the intercategory features to be further separate. Compared with the existing category alignment methods, our CR aims to regularize the correlation between different dimensions of the features, and thus performs more robustly when dealing with divergent category features of imbalanced and inconsistent distributions. Finally, we propose a stagewise procedure to train the DCA to alleviate error accumulation. Experiments on both rural-to-urban and urban-to-rural scenarios of the LoveDA dataset demonstrate the superiority of our proposed DCA over other state-of-the-art UDA segmentation methods. Code is available at https://github.com/Luffy03/DCA.",Feature extraction,Training,Image segmentation,Generative adversarial networks,Task analysis,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantics,Adaptation models,Deep covariance alignment (DCA),remote sensing image (RSI),semantic segmentation,unsupervised domain adaptive (UDA),,,,,,,,,,,,,,,,,,,,,,,,
Row_781,"Cui, Jian","Liu, Jiahang","Wang, Jinjin","Ni, Yue",Global Context Dependencies Aware Network for Efficient Semantic Segmentation of Fine-Resolution Remoted Sensing Images,,2023,5,"Geospatial object segmentation is a fundamental task in remote sensing image interpretation. Although deep learning has shown great potential for this task, it often suffers from limited receptive fields, insufficient global feature extraction ability, and inaccurate edge positioning, resulting in low accuracy and errors in the results. In this letter, we propose a novel global context dependency aware network (GCDNet) to achieve high-accuracy segmentation results. To overcome the limited receptive fields and promote feature extraction ability, we propose a new dot-product attention (DPA) mechanism to establish long-distance dependencies between different receptive field feature maps. To achieve more accurate object edges, we design an edge-aware optimization (EAO) module to guide the operation to directly optimize the edge details from the prediction result at the pixel level. Extensive experiments on two well-known public high-resolution remote sensing image datasets have been conducted to verify the performance of the proposed method, and the results show that the proposed method has significant advantages and maintains its robustness in different cases. Code will be available at: https://github.com/Cuiadd/GCDNet.",Edge refinement,global context,remote sensing,semantic segmentation,urban scene,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_782,"Tian, Liang","Zhong, Xiaorou","Chen, Ming",,Semantic Segmentation of Remote Sensing Image Based on GAN and FCN Network Model,,NOV 3 2021,12,"Accurate remote sensing image segmentation can guide human activities well, but current image semantic segmentation methods cannot meet the high-precision semantic recognition requirements of complex images. In order to further improve the accuracy of remote sensing image semantic segmentation, this paper proposes a new image semantic segmentation method based on Generative Adversarial Network (GAN) and Fully Convolutional Neural Network (FCN). This method constructs a deep semantic segmentation network based on FCN, which can enhance the receptive field of the model. GAN is integrated into FCN semantic segmentation network to synthesize the global image feature information and then accurately segment the complex remote sensing image. Through experiments on a variety of datasets, it can be seen that the proposed method can meet the high-efficiency requirements of complex image semantic segmentation and has good semantic segmentation capabilities.",,,,,,,,,,SCIENTIFIC PROGRAMMING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_783,"Zheng, Shangdong","Wu, Zebin","Du, Qian","Xu, Yang",Oriented Object Detection for Remote Sensing Images via Object-Wise Rotation-Invariant Semantic Representation,,2024,1,"Oriented object detection (OOD) in remote sensing images (RSIs) remains a challenging work due to an arbitrary orientation of instances. Learning rotation-invariant features is critical in modeling a fixed descriptor for instances with its rotated variants. However, most existing methods construct the descriptor from the perspectives of data or feature augmentation, but ignore the exploration of potentially useful supervision information inside the detection algorithm. In this article, we propose an object-wise rotation-invariant semantic representation (ORSR) framework, which synergizes the exploration of latent supervision, rotation-invariant learning, and guided attention mechanism into a unified network to boost the performance of OOD in RSIs. First, supervised by our constructed pseudo-ground truth of segmentation masks, a semantic segmentation branch is built along with the detection algorithm to refine the representation of backbone features. Moreover, a consistency loss function is proposed to encourage the segmentation branch to make fixed predictions for backbone features with its rotated variants. Considering that segmentation predictions remain the same affine transformations before and after rotating, we further construct a Kullback-Leibler (KL) divergence-based similarity loss function to encourage the network to model the rotation-invariant features. Finally, we separate the ""object"" descriptor from the segmentation predictions to extend the implicit constraint in our proposed semantic segmentation branch. The separated ""object"" descriptor not only involves the spatial regularizer to emphasize the high-responsive regions in the image but also can be guided by the constructed consistency loss function. We evaluate our proposed ORSR on the challenging DOTA, DIOR-R, and HRSC2016 datasets. Extensive experiments demonstrate that the proposed ORSR achieves competitive performance compared to other single-scale and multiscale (MS) detection methods.",Image segmentation,Feature extraction,Task analysis,Semantics,Object detection,"Wei, Zhihui",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Remote sensing,Proposals,Oriented object detection (OOD),remote sensing images (RSIs),rotation-invariant learning,semantic representation,,,,,,,,,,,,,,,,,,,,,,,,
Row_784,"Yang, Zimeng","Wu, Qiulan","Zhang, Feng","Chen, Xuefei",Optimizing Spatial Relationships in GCN to Improve the Classification Accuracy of Remote Sensing Images,,2023,2,"Semantic segmentation of remote sensing images is one of the core tasks of remote sensing image interpretation. With the continuous develop-ment of artificial intelligence technology, the use of deep learning methods for interpreting remote-sensing images has matured. Existing neural networks disregard the spatial relationship between two targets in remote sensing images. Semantic segmentation models that combine convolutional neural networks (CNNs) and graph convolutional neural networks (GCNs) cause a lack of feature boundaries, which leads to the unsatisfactory segmentation of various target feature boundaries. In this paper, we propose a new semantic segmentation model for remote sensing images (called DGCN hereinafter), which combines deep semantic segmentation networks (DSSN) and GCNs. In the GCN module, a loss function for boundary information is employed to optimize the learning of spatial relationship features between the target features and their relationships. A hierarchical fusion method is utilized for feature fusion and classification to optimize the spatial relationship informa-tion in the original feature information. Extensive experiments on ISPRS 2D and DeepGlobe semantic segmentation datasets show that compared with the existing semantic segmentation models of remote sensing images, the DGCN significantly optimizes the segmentation effect of feature boundaries, effectively reduces the noise in the segmentation results and improves the segmentation accuracy, which demonstrates the advancements of our model.",Remote sensing image,semantic segmentation,GCN,spatial relationship,feature fusion,"Wang, Weiqiang","Zhang, XueShen",,,INTELLIGENT AUTOMATION AND SOFT COMPUTING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_785,Yuan Wei,Xu Wenbo,Zhou Tian,,Remote sensing image segmentation based on PSPNet with neighborhood color difference,,FEB 25 2022,0,"Traditional semantic segmentation of remote sensing image is to classify the pixels with similar values by using the spectral characteristics of images, but it is unable to distinguish the same kind of objects with different spectra. Aiming at this problem, a method was proposed in which the color difference information of neighborhood is integrated into the original image as input to PSPNet. Firstly, RGB was transformed into LAB. Then CIELAB formula was used to calculate the color difference value between each pixel and eight neighboring pixels, and the average value was taken as the neighborhood color difference value of the pixel. Experiment was done by using PSPNct on WHU building dataset and Massachusetts building dataset. The results show that the MIoU, ACC and F1-score with neighborhood color difference arc better than without. Therefore, the proposed method of merging neighborhood color difference is an effective way to improve the segmentation accuracy of PSPNet.",remote sensing image,semantic segmentation,deep learning,neighborhood color difference,convolutional neural network,,,,,CHINESE SPACE SCIENCE AND TECHNOLOGY,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_786,"Jaber, Mustafa Musa","Ali, Mohammed Hasan","Abd, Sura Khalil","Jassim, Mustafa Mohammed",A Machine Learning-Based Semantic Pattern Matching Model for Remote Sensing Data Registration,,SEP 2023,1,"Remote sensing image registration can benefit from a machine learning method based on the likelihood of predicting semantic spatial position distributions. Semantic segmentation of images has been revolutionized due to the accessibility of high-resolution remote sensing images and the advancement of machine learning techniques. This system captures the semantic distribution location of the matching reference picture, which ML mapped using learning-based algorithms. The affine invariant is utilized to determine the semantic template's barycenter position and the pixel's center, which changes the semantic border alignment problem into a point-to-point matching issue for the machine learning-based semantic pattern matching (ML-SPM) model. The first step examines how various factors such as template radius, training label filling form, or loss function combination affect matching accuracy. In this second step, the matching of sub-images (MSI) images is compared using heatmaps created from the expected similarity between the images' cropped sub-images. Images having radiometric discrepancies are matched with excellent accuracy by the approach. SAR-optical image matching has never been easier, and now even large-scale sceneries can be registered using this approach, which is a significant advance over previous methods. Optical satellite imaging or multi-sensor stereogrammetry can be combined with both forms of data to enhance geolocation.",Remote sensing image,Machine learning,Semantic pattern matching,Matching of sub-images,Loss function,"Alkhayyat, Ahmed","Alreda, Baraa A.","Alkhuwaylidee, Ahmed Rashid","Alyousif, Shahad",JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING,,Synthetic aperture radar (SAR),,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_787,"Zhu, Peng","Zhang, Xiangrong","Han, Xiao","Chen, Puhua",High-Resolution Remote Sensing Image Segmentation With Global-Guided Normalization and Local Affinity Distillation,,2024,0,"In recent years, high-resolution (HR) remote sensing images (RSIs) segmentation has received growing attention. The huge number of pixels poses a challenge to the semantic segmentation algorithm, which is limited by the storage of GPUs, so the current methods for processing HR RSIs are categorized into two main categories, i.e., global methods and local methods. The former downsamples the original image and loses a lot of feature details. The latter crops the original image and fails to obtain global contextual information. Both types of methods lead to limited segmentation accuracy. In this article, we propose an end-to-end framework, called global injection network (GINet), which explores two levels of feature distribution and feature relationship to achieve tradeoff between global context and local details. In concrete terms, we propose the global-guided normalization (GGN) module, which injects global context information into local branch and modulates local features using global features to enhance the global perception of local branch. In addition, to constrain the spatial consistency of two branches, inspired by the knowledge distillation technique, we propose local affinity distillation (LAD) loss, which distills the relations in local features into global features to keep the similarity of the relationships corresponding to patches in the two branches. The comprehensive experimental results on three large-scale land-cover classification datasets, DeepGlobe (2448 x 2448), Inria Aerial (5000 x 5000), and GID-15 (7200 x 6800), confirm the effectiveness and superiority of our method in HR semantic segmentation tasks.",Remote sensing,Semantic segmentation,Semantics,Image resolution,Context modeling,"Tang, Xu","Cheng, Xina","Jiao, Licheng",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Accuracy,Transformers,Correlation,Convolution,Training,Distillation,high-resolution (HR) remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,
Row_788,"Wu, Yingxin","Liu, Yinhe","Shi, Sunan","Zhong, Yanfei",HIGH-RESOLUTION FINE-GRAINED WETLAND MAPPING BASED ON CLASS-BALANCED DEEP SEMANTIC SEGMENTATION NETWORKS,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,2,"Wetlands are among the most valuable environmental resources and are crucial to achieving sustainable development strategies. However, the number, distribution, and types of wetlands are poorly understood. Widely used medium-resolution wetland products do not provide enough surface feature for fine-grained wetland mapping. To fill this gap, a high-resolution wetland remote sensing mapping dataset covering the contiguous United States was constructed. On this dataset, the performance of deep semantic segmentation networks with various backbones and architecture for wetland mapping was evaluated. Several loss functions were implemented to address the issue of class imbalance of this dataset, resulting in an improvement in classification accuracy. High spatial resolution images offer an abundance of surface texture, shape, structure, and neighborhood relationship data that can be applied to the classification of large-scale wetlands. The combination of high-resolution imagery and deep semantic segmentation models enables the automatic classification of wetlands to be refined.",Wetland mapping,high-resolution remote sensing,deep learning,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_789,"Zheng, Zhuo","Zhong, Yanfei","Wang, Junjue","Ma, Ailong",FarSeg plus plus : Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery,,NOV 1 2023,17,"Geospatial object segmentation, a fundamental Earth vision task, always suffers from scale variation, the larger intra-class variance of background, and foreground-background imbalance in high spatial resolution (HSR) remote sensing imagery. Generic semantic segmentation methods mainly focus on the scale variation in natural scenarios. However, the other two problems are insufficiently considered in large area Earth observation scenarios. In this paper, we propose a foreground-aware relation network (FarSeg++) from the perspectives of relation-based, optimization-based, and objectness-based foreground modeling, alleviating the above two problems. From the perspective of the relations, the foreground-scene relation module improves the discrimination of the foreground features via the foreground-correlated contexts associated with the object-scene relation. From the perspective of optimization, foreground-aware optimization is proposed to focus on foreground examples and hard examples of the background during training to achieve a balanced optimization. Besides, from the perspective of objectness, a foreground-aware decoder is proposed to improve the objectness representation, alleviating the objectness prediction problem that is the main bottleneck revealed by an empirical upper bound analysis. We also introduce a new large-scale high-resolution urban vehicle segmentation dataset to verify the effectiveness of the proposed method and push the development of objectness prediction further forward. The experimental results suggest that FarSeg++ is superior to the state-of-the-art generic semantic segmentation methods and can achieve a better trade-off between speed and accuracy.",Remote sensing,Feature extraction,Object segmentation,Decoding,Semantic segmentation,"Zhang, Liangpei",,,,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,,Optimization,Semantics,Foreground modeling,object segmentation,semantic segmentation,remote sensing,deep learning,,,,,,,,,,,,,,,,,,,,,,,
Row_790,"Wei, Yao","Ji, Shunping",,,Scribble-Based Weakly Supervised Deep Learning for Road Surface Extraction From Remote Sensing Images,,2022,79,"Road surface extraction from remote sensing images using deep learning methods has achieved good performance, while most of the existing methods are based on fully supervised learning, which requires a large amount of training data with laborious per-pixel annotation. In this article, we propose a scribble-based weakly supervised road surface extraction method named ScRoadExtractor, which learns from easily accessible scribbles such as centerlines instead of densely annotated road surface ground truths. To propagate semantic information from sparse scribbles to unlabeled pixels, we introduce a road label propagation algorithm, which considers both the buffer-based properties of road networks and the color and spatial information of super-pixels, to produce a proposal mask with categories road, nonroad, and unknown. The proposal mask, along with the auxiliary boundary prior information detected from images, is utilized to train a dual-branch encoderx2013;decoder network which we designed for precise road surface segmentation. We perform experiments on three diverse road data sets that are comprised of high-resolution remote sensing satellite and aerial images across the world. The results demonstrate that ScRoadExtractor exceeds the classic scribble-supervised segmentation method by 20x0025; for the intersection over union (IoU) indicator and outperforms the state-of-the-art scribble-based weakly supervised methods at least 4x0025;.",Roads,Proposals,Annotations,Training,Remote sensing,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Image segmentation,Semantics,Remote sensing image,road surface extraction,semantic segmentation,scribble,weakly supervised learning,,,,,,,,,,,,,,,,,,,,,,,
Row_791,"Liu, Guohong","Liu, Cong","Wu, Xianyun","Li, Yunsong",Optimization of Remote-Sensing Image-Segmentation Decoder Based on Multi-Dilation and Large-Kernel Convolution,,AUG 2024,0,"Land-cover segmentation, a fundamental task within the domain of remote sensing, boasts a broad spectrum of application potential. We address the challenges in land-cover segmentation of remote-sensing imagery and complete the following work. Firstly, to tackle the issues of foreground-background imbalance and scale variation, a module based on multi-dilated rate convolution fusion was integrated into a decoder. This module extended the receptive field through multi-dilated convolution, enhancing the model's capability to capture global features. Secondly, to address the diversity of scenes and background interference, a hybrid attention module based on large-kernel convolution was employed to improve the performance of the decoder. This module, based on a combination of spatial and channel attention mechanisms, enhanced the extraction of contextual information through large-kernel convolution. A convolution kernel selection mechanism was also introduced to dynamically select the convolution kernel of the appropriate receptive field, suppress irrelevant background information, and improve segmentation accuracy. Ablation studies on the Vaihingen and Potsdam datasets demonstrate that our decoder significantly outperforms the baseline in terms of mean intersection over union and mean F1 score, achieving an increase of up to 1.73% and 1.17%, respectively, compared with the baseline. In quantitative comparisons, the accuracy of our improved decoder also surpasses other algorithms in the majority of categories. The results of this paper indicate that our improved decoder achieves significant performance improvement compared with the old decoder in remote-sensing image-segmentation tasks, which verifies its application potential in the field of land-cover segmentation.",remote-sensing images,land-cover segmentation,dilated convolution,attention mechanism,large-kernel convolution,"Zhang, Xiao","Xu, Junjie",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_792,"Lin, Zhiyuan","Zhu, Feng","Kong, Yanzi","Wang, Qun",SRSG and S2SG: A Model and a Dataset for Scene Graph Generation of Remote Sensing Images From Segmentation Results,,2022,2,"Remote sensing image analysis has drawn more attentions in the field of computer vision. At present, the methods commonly used in remote sensing image analysis are mainly at a low level semantically. The scene graph is an abstraction of objects and their relationships, which is a high-level image understanding task. To fully comprehend the meanings of remote sensing images, in this article, we propose a novel segmentation-based model to generate remote sensing image scene graphs (SRSG). In the SRSG model, a more complete and accurate scene graph is generated with the segmentation results as inputs, while the shapes of objects are reasonably coded. The morphological features of object pairs are embedded together by different branches of the SRSG model and then mapped to semantic space to predicate their relationships. Furthermore, a new dataset for scene graph generation of remote sensing images, namely, segmentation results to scene graphs (S2SG), is constructed based on pixel-level segmentation results. Experimental results demonstrate that the performance of the SRSG model is far superior to the previous methods in the task of generating remote sensing image scene graphs. The proposed SRSG model opens up new possibilities for remote sensing image analysis at a high level. Moreover, the S2SG dataset further allows for the evaluation of different approaches and is provided for the benefit of the research community.",Remote sensing,Image segmentation,Semantics,Visualization,Task analysis,"Wang, Jianyu",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Sensors,Vegetation mapping,Dataset,relationship prediction,remote sensing image,scene graph generation,segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_793,"Ji, Xun","Tang, Longbin","Lu, Tongwei","Cai, Chengtao",DBENet: Dual-Branch Ensemble Network for Sea-Land Segmentation of Remote-Sensing Images,,2023,12,"Sea-land segmentation of optical remote-sensing images holds great importance for military and civilian applications, such as coastal monitoring, target detection, and resource management. Although convolutional neural networks (CNNs) have achieved significant improvements in semantic segmentation, the challenges of efficient feature extraction, representation, fusion, and information transmission remain unsolved, which especially impacts the segmentation effectiveness of existing CNN-based models in extracting irregular and refined sea-land boundaries. In this article, a novel dual-branch ensemble network (DBENet) is proposed for pixel-level sea-land segmentation. The salient properties of the DBENet are: 1) a novel dual-branch network architecture is developed to achieve sufficient feature extraction and representation and 2) an efficient ensemble attention learning strategy suitable for the DBENet is designed to strengthen the correlation between dual branches to further facilitate feature fusion and information transmission. The comparative study with state-of-the-art methods reveals the superior performance of our approach, and the ablation study demonstrates the effectiveness of each component in the proposed network. The source code is available at https://github.com/RobertTang0/DBENet.",Convolutional neural networks (CNNs),deep learning,remote-sensing image,sea-land segmentation,semantic segmentation,,,,,IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_794,"Liu, Qi","Li, Yang","Bilal, Muhammad","Liu, Xiaodong",CFNet: An Eigenvalue Preserved Approach to Multiscale Building Segmentation in High-Resolution Remote Sensing Images,,2023,2,"In recent years, AI and deep learning (DL) methods have been widely used for object classification, recognition, and segmentation of high-resolution multispectral remote sensing images. These DL-based solutions perform better compared with the traditional spectral algorithms but still suffer from insufficient optimization of global and local features of object context. In addition, failure of code-data isolation and/or disclosure of detailed eigenvalues cause serious privacy and even secret leakage due to the sensitivity of high-resolution remote sensing data and their processing mechanisms. In this article, class feature modules have been presented in the decoder part of an attention-based CNN network to distinguish between building and nonbuilding (background) area. In this way, context features of a focused object can be extracted with more details being processed while the resolution of images is maintained. The reconstructed local and global feature values and dependencies in the proposed model are maintained by reconfiguring multiple effective attention modules with contextual dependencies to achieve better results for the eigenvalue. According to quantitative results and their visualization, the proposed model has depicted better performance over others' work using two large-scale building remote sensing datasets. The F1-score of this model reached 87.91 and 89.58 on WHU Buildings Dataset and Massachusetts Buildings Dataset, respectively, which exceeded the other semantic segmentation models.",Building extraction,class feature (CF),semantic segmentation,,,"Zhang, Yonghong","Wang, Huihui","Xu, Xiaolong","Lu, Hui",IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_795,"Neupane, Bipul","Horanont, Teerayut","Aryal, Jagannath",,Deep Learning-Based Semantic Segmentation of Urban Features in Satellite Images: A Review and Meta-Analysis,,FEB 2021,119,"Availability of very high-resolution remote sensing images and advancement of deep learning methods have shifted the paradigm of image classification from pixel-based and object-based methods to deep learning-based semantic segmentation. This shift demands a structured analysis and revision of the current status on the research domain of deep learning-based semantic segmentation. The focus of this paper is on urban remote sensing images. We review and perform a meta-analysis to juxtapose recent papers in terms of research problems, data source, data preparation methods including pre-processing and augmentation techniques, training details on architectures, backbones, frameworks, optimizers, loss functions and other hyper-parameters and performance comparison. Our detailed review and meta-analysis show that deep learning not only outperforms traditional methods in terms of accuracy, but also addresses several challenges previously faced. Further, we provide future directions of research in this domain.",deep learning,remote sensing,review,semantic segmentation,urban image classification,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_796,"Pereira, Matheus Barros","dos Santos, Jefersson Alex",,,ChessMix: Spatial Context Data Augmentation for Remote Sensing Semantic Segmentation,"2021 34TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI 2021)",2021,2,"Labeling semantic segmentation datasets is a costly and laborious process if compared with tasks like image classification and object detection. This is especially true for remote sensing applications that not only work with extremely high spatial resolution data but also commonly require the knowledge of experts of the area to perform the manual labeling. Data augmentation techniques help to improve deep learning models under the circumstance of few and imbalanced labeled samples. In this work, we propose a novel data augmentation method focused on exploring the spatial context of remote sensing semantic segmentation. This method, ChessMix, creates new synthetic images from the existing training set by mixing transformed mini-patches across the dataset in a chessboard-like grid. ChessMix prioritizes patches with more examples of the rarest classes to alleviate the imbalance problems. The results in three diverse well-known remote sensing datasets show that this is a promising approach that helps to improve the networks' performance, working especially well in datasets with few available data. The results also show that ChessMix is capable of improving the segmentation of objects with few labeled pixels when compared to the most common data augmentation methods widely used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_797,"Luo, Zheng","Pan, Jianping","Hu, Yong","Deng, Lin",RS-Dseg: semantic segmentation of high-resolution remote sensing images based on a diffusion model component with unsupervised pretraining,,AUG 10 2024,2,"Semantic segmentation plays a crucial role in interpreting remote sensing images, especially in high-resolution scenarios where finer object details, complex spatial information and texture structures exist. To address the challenge of better extracting semantic information and ad-dressing class imbalance in multiclass segmentation, we propose utilizing diffusion models for remote sensing image semantic segmentation, along with a lightweight classification module based on a spatial-channel attention mechanism. Our approach incorporates unsupervised pretrained components with a classification module to accelerate model convergence. The diffusion model component, built on the UNet architecture, effectively captures multiscale features with rich contextual and edge information from images. The lightweight classification module, which leverages spatial-channel attention, focuses more efficiently on spatial-channel regions with significant feature information. We evaluated our approach using three publicly available datasets: Postdam, GID, and Five Billion Pixels. In the test of three datasets, our method achieved the best results. On the GID dataset, the overall accuracy was 96.99%, the mean IoU was 92.17%, and the mean F1 score was 95.83%. In the training phase, our model achieved good performance after only 30 training cycles. Compared with other models, our method reduces the number of parameters, improves the training speed, and has obvious performance advantages.",Diffusion models,Multiscale,Pretraining,Attention mechanism,Semantic segmentation,"Li, Yimeng","Qi, Chen","Wang, Xunxun",,SCIENTIFIC REPORTS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_798,"Cai, Jiali","Liu, Chunjuan","Yan, Haowen","Wu, Xiaosuo",Real-Time Semantic Segmentation of Remote Sensing Images Based on Bilateral Attention Refined Network,,2021,8,"The trade-off between feature representation capability and spatial positioning accuracy is crucial to dense classification or semantic segmentation of remote sensing images. In order to better balance the low-level spatial details in the shallow network and the high-level abstract semantics in the deep network, the bilateral attention refinement lightweight network BARNet is introduced. In this way, we can use the fine-grained features in the shallow layer to further supplement and capture the deeper information of the high-level semantic features. The network employs an asymmetric encoder decoder architecture for the task of real-time semantic segmentation. Encoder part proposes a lightweight network residual unit with the split, concatenate and split bottleneck structure to achieve more light weighted, effificient and powerful feature extraction. In the decoding section, we propose an adaptive method to enhance feature representation in local attention enhancement module. In addition, the global context embedding module is introduced to divide the high-level features into two branches. One branch gets the weight vector to guide the low-level learning, and the other branch will get a semantic vector, which is used to calculate the multi-label category loss and further introduce into the overall loss function to regulate the training process better. The effectiveness and efficiency of the network are verified on ISPRS Potsdam data set and CCF data set, respectively. The results show that the models using these strategies outperform the baseline network on MIoU, PA and F1, which increase by 18.86%, 16.21% and 15.64% on the Potsdam dataset; 10.51%, 6.53% and 8.19% on the CCF dataset.",Semantics,Feature extraction,Convolution,Licenses,Convolutional codes,"Lu, Wanzhen","Wang, Xiaoyu","Sang, Changlin",,IEEE ACCESS,,Task analysis,Spatial resolution,Remote sensing image,real-time semantic segmentation,local attention enhancement module,global context embedding module,multi-label category loss,,,,,,,,,,,,,,,,,,,,,,,
Row_799,"Li, Jiaojiao","Zi, Shunyao","Song, Rui","Li, Yunsong",A Stepwise Domain Adaptive Segmentation Network With Covariate Shift Alleviation for Remote Sensing Imagery,,2022,33,"Semantic segmentation for remote sensing images (RSI) is critical for the Earth monitoring system. However, the covariate shift between RSI datasets under different capture conditions cannot be alleviated by directly using the unsupervised domain adaptation (UDA) method, which negatively affects the segmentation accuracy in RSI. We propose a stepwise domain adaptive segmentation network with covariate shift alleviation (Cov-DA) for RSI parsing to solve this issue. Specifically, to alleviate domain shift generated by different sensors, both the source and target domains are projected into a colorspace with normalized distribution through an elaborate colorspace mapping unified module (CMUM). The color distributions of these two domains tend to be more uniform. Furthermore, in the target domain, the multistatistics joint evaluation module (MJEM) is proposed to capture different statistical characteristics of subscenarios for selecting plain scenarios regarded as high-confidence segmentation results to assist the further improvement of segmentation performance. In addition, a pyramid perceptual attention module (PPAM) containing omnidirectional features without computational burdens is added to our network for effectively enhancing the multiscale feature capture ability. In the cross-city DA experiments based on the International Society for Photogrammetry and Remote Sensing (ISPRS) and aerial benchmarks, the superiority of our algorithm is significantly demonstrated. Furthermore, we release a large-scale Martian terrain dataset noted as ""Mars-Seg"" containing 5 K images with pixel-level accurate annotations regarding issues, such as the lack of semantic segmentation datasets for unknown scenes.",Image segmentation,Semantics,Feature extraction,Training,Complexity theory,"Hu, Yinlin","Du, Qian",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Adaptive systems,Generative adversarial networks,Covariate shift alleviation,semantic segmentation,stepwise,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,,,
Row_800,"Sun, Yangjie","Fu, Zhongliang","Sun, Chuanxia","Hu, Yinglei",Deep Multimodal Fusion Network for Semantic Segmentation Using Remote Sensing Image and LiDAR Data,,2022,47,"Extracting semantic information from very-high-resolution (VHR) aerial images is a prominent topic in the Earth observation research. An increasing number of different sensor platforms are appearing in remote sensing, each of which can provide corresponding multimodal supplemental or enhanced information, such as optical images, light detection and ranging (LiDAR) point clouds, infrared images, or inertial measurement unit (IMU) data. However, these current deep networks for LiDAR and VHR images have not fully utilized the complete potential of multimodal data. The stacked multimodal fusion network (MFNet) ignores the structural differences between the modalities and the manual statistical characteristics within the modalities. For multimodal remote sensing data and its corresponding carefully designed handcrafted features, we designed a novel deep MFNet that can use multimodal VHR aerial images and LiDAR data and the corresponding intramodal features, such as LiDAR-derived features [slope and normalized digital surface model (NDSM)] and imagery-derived features [infrared-red-green (IRRG), normalized difference vegetation index (NDVI), and difference of Gaussian (DoG)]. Technically, we introduce the attention mechanism and multimodal learning to adaptively fuse intermodal and intramodal features. Specifically, we designed a multimodal fusion mechanism, pyramid dilation blocks, and a multilevel feature fusion module. Through these modules, our network realized the adaptive fusion of multimodal features, improved the receptive field, and enhanced the global-to-local contextual fusion effect. Moreover, we used a multiscale supervision training scheme to optimize the network. Extensive experimental results and ablation studies on the ISPRS semantic dataset and IEEE GRSS DFC Zeebrugge dataset show the effectiveness of our proposed MFNet.",Semantics,Image segmentation,Laser radar,Sensors,Task analysis,"Zhang, Shengyuan",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Sun,Feature extraction,Aerial images,attention mechanism,convolutional neural network (CNN),multimodal fusion,semantic labeling,,,,,,,,,,,,,,,,,,,,,,,
