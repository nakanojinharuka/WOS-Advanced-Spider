,author1,author2,author3,author4,author5,author6,title,journal/book,publish time,citation,abstract,keyword1,keyword2,keyword3,keyword4,conference,keyword5,keyword6,keyword7,keyword8,keyword9,keyword10,keyword11,author7,author8,keyword12,keyword13,keyword14,author9,author10,author11,author12
Row_1551,"Chen, Lijia","Chen, Honghui","Xie, Yanqiu","He, Tianyou","Ye, Jing","Zheng, Yushan",An Efficient and Light Transformer-Based Segmentation Network for Remote Sensing Images of Landscapes,FORESTS,NOV 2023,0,"High-resolution image segmentation for landscape applications has garnered significant attention, particularly in the context of ultra-high-resolution (UHR) imagery. Current segmentation methodologies partition UHR images into standard patches for multiscale local segmentation and hierarchical reasoning. This creates a pressing dilemma, where the trade-off between memory efficiency and segmentation quality becomes increasingly evident. This paper introduces the Multilevel Contexts Weighted Coupling Transformer (WCTNet) for UHR segmentation. This framework comprises the Mult-level Feature Weighting (MFW) module and Token-based Transformer (TT) designed to weigh and couple multilevel semantic contexts. First, we analyze the multilevel semantics within a local patch without image-level contextual reasoning. It avoids complex image-level contextual associations and eliminates the misleading information carried. Second, MFW is developed to weigh shallow and deep features for enhancing object-related attention at different grain sizes from multilevel semantics. Third, the TT module is introduced to couple multilevel semantic contexts and transform them into semantic tokens using spatial attention. Then, we can capture token interactions and obtain clearer local representations. The suggested contextual weighting and coupling of single-scale patches empower WCTNet to maintain a well-balanced relationship between accuracy and computational overhead. Experimental results show that WCTNet achieves state-of-the-art performance on two UHR datasets of DeepGlobe and Inria Aerial.",ultra-high-resolution image,segmentation quality,multilevel semantic contexts,transformer,,,,,,,,,,,,,,,,,
Row_1552,"Chong, Yanwen","Chen, Xiaoshu","Tao, Yulong","Pan, Shaoming",,,Erase then grow: Generating correct class activation maps for weakly-supervised semantic segmentation,NEUROCOMPUTING,SEP 17 2021,12,"In spite of extremely challenging, the weakly-supervised semantic segmentation using image-level labels has made encouraging progress in the recent phase. The existing methods mainly adopt two-stage training procedures: a) optimizing class activation map (CAM) produced by the multi-label classification network to generate pseudo ground truth; b) training a conventional fully supervised semantic segmentation network through pseudo ground truth. When optimizing CAM, most advanced methods just consider the problem that CAM can only activate the sparse and discriminative regions for each class. However, since the loss function of the classification task is image-level supervision, classification network is weak in capturing intricate contextual information, which results in another problem that many misclassified regions are activated in CAM. Compared with classification networks, the loss function of semantic segmentation tasks is pixel-level supervision, which makes it better at capturing intricate contextual information. Thus, based on this ability of the segmentation network, we propose an erasing module to erase the misclassified regions in the CAM. Furthermore, to transform the sparse CAM into high quality dense pseudo ground truth, we apply the proposed hierarchical deep seeded region growing (H-DSRG) on the erased CAM. Finally, we conduct extensive analysis to validate the proposed method. The proposed method achieves 66.8 of mIoU for Pascal voc 2012 val dataset and 67.6 of mIoU for Pascal voc 2012 test dataset, harvesting new state-of-the-art results.(c) 2021 Elsevier B.V. All rights reserved.",Semantic segmentation,Weakly-supervised learning,Context,Seeded region growing,,,,,,,,,,,,,,,,,
Row_1553,"Huang, Ziyue","Zhang, Mingming","Gong, Yuan","Liu, Qingjie","Wang, Yunhong",,Generic Knowledge Boosted Pretraining for Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,3,"Deep learning models are essential for scene classification, change detection, land cover segmentation, and other remote sensing (RS) image understanding tasks. Most backbones of existing RS deep learning models are typically initialized by pretrained weights obtained from ImageNet pretraining (IMP). However, domain gaps exist between RS images and natural images (e.g., ImageNet), making deep learning models initialized by pretrained weights of IMP perform poorly for RS image understanding. Although some pretraining methods are studied in the RS community, current RS pretraining (RSP) methods face the problem of vague generalization by only using RS images. In this article, we propose a novel RSP framework, generic knowledge boosted RSP (GeRSP), to learn robust representations from RS and natural images for RS understanding tasks. GeRSP contains two pretraining branches: 1) a self-supervised pretraining branch is adopted to learn domain-related representations from unlabeled RS images and 2) a supervised pretraining branch is integrated into GeRSP for general knowledge learning from labeled natural images. Moreover, GeRSP combines two pretraining branches using a teacher-student architecture to simultaneously learn representations with general and special knowledge, which generates a powerful pretrained model for deep learning model initialization. Finally, we evaluate GeRSP and other RSP methods on three downstream tasks, i.e., object detection, semantic segmentation, and scene classification. The extensive experimental results consistently demonstrate that GeRSP can effectively learn robust representations in a unified manner, improving the performance of RS downstream tasks. Code and pretrained models: https://github.com/floatingstarZ/GeRSP.",Pretraining,remote sensing (RS) image,self-supervised learning,,,,,,,,,,,,,,,,,,
Row_1554,"Maiti, A.","Elberink, S. J. Oude","Vosselman, G.",,,,EFFECT OF LABEL NOISE IN SEMANTIC SEGMENTATION OF HIGH RESOLUTION AERIAL IMAGES AND HEIGHT DATA,,2022,3,"The performance of deep learning models in semantic segmentation is dependent on the availability of a large amount of labeled data. However, the influence of label noise, in the form of incorrect annotations, on the performance is significant and mostly ignored. This is a big concern in remote sensing applications, wherein acquired datasets are spatially limited, labeling is done by domain experts with possible sources of high inter-and intra-observer variability leading to erroneous predictions. In this paper, we first simulate the label noise while conducting experiments on two different datasets with very high-resolution aerial images, height data, and inaccurate labels, responsible for the training of deep learning models. We then focus on the effect of these noises on the model performance. Different classes respond differently to the label noise. The typical size of an object belonging to a class is a crucial factor regarding the class-specific performance of the model trained with erroneous labels. Errors caused by relative shifts of labels are the most influential label errors. The model is generally more tolerant of the random label noise than other label errors. It has been observed that the accuracy gets reduced by at least 3% while 5% of label pixels are erroneous. In this regard, our study provides a new perspective of evaluating and quantifying the propagation of label noise in the model performance that is indeed important for adopting reliable semantic segmentation practices.",Deep Learning,Semantic Segmentation,Label Noise,Very High Resolution,"XXIV ISPRS CONGRESS IMAGING TODAY, FORESEEING TOMORROW, COMMISSION II",,,,,,,,,,,,,,,,
Row_1555,"Gonzales, Cindy","Sakla, Wesam",,,,,Semantic Segmentation of Clouds in Satellite Imagery Using Deep Pre-trained U-Nets,REMOTE SENSING OF ENVIRONMENT,2019,8,"Earth observation and remote sensing technologies are widely used in various application areas. Because the abundance of collected data requires automated analytics, many communities are utilizing deep convolutional neural networks for such tasks. Automating cloud detection in remote sensing and earth observation imagery is a useful prerequisite for providing quality imagery for further analysis. In this paper, we train a model that uses a deep convolutional U-Net architecture, utilizing transfer learning to perform semantic segmentation of clouds in satellite imagery. Our proposed model outperforms state-of-the-art networks on a benchmark dataset based on several relevant segmentation metrics, including Jaccard Index (+ 7.69%), precision (+ 6.21%), and specificity (+ 0.37%). Moreover, we demonstrate that transfer learning utilizing a 4-channel input into a U-Net architecture is possible and highly performant by using a deep ResNet-style architecture pre-trained on ImageNet for the initialization of weights in three channels (red, green, and blue bands) and random initialization of weights in the fourth channel (near infrared band) of the first convolutional layer of the network.",Remote sensing,cloud detection,satellite imagery,image segmentation,,U-Net,convolutional neural network (CNN),deep learning,transfer learning,,,,,,,,,,,,
Row_1556,Lu Huanhuan,Liu Tao,Zhang Hui,Peng Guofeng,Zhang Juntong,,High-Resolution Remote Sensing Scene Classification Based on Salient Features and DCNN,LASER & OPTOELECTRONICS PROGRESS,OCT 2021,0,"Scene classification of high-resolution remote sensing image is one of the important tasks in interpreting remote sensing image information. In order to extract the target information accurately, we propose a high-resolution remote sensing image scene classification method based on salient features combined with deep convolutional neural network (DCNN) to solve the problems of complex background, diverse targets, and difficult to distinguish between target information and background information in the classification of high-scoring remote sensing image scenes. First, K-means clustering algorithm and super-pixel segmentation algorithm are used to generate the color spatial distribution map and color contrast map of the image, and the different contrast maps are fused to get the saliency map. Then, the features in the saliency map are enhanced through logarithmic transformation, and the adaptive threshold segmentation method is used to improve the discrimination of the target and divide the target area and the background area, and extract the area of interest. Finally, a DCNN model is constructed to extract deep semantic features and classification, and the obtained features are input into the network model for training and classification. Experimental results show that the method can effectively distinguish the main target information from the background information and reduce the interference of irrelevant information. The classification accuracy of the method on the UC-Merced data set and WHU-RS data set are 96.10% and 95. 84%, respectively.",atmospheric optics,high-resolution remote sensing image,scene classification,saliency detection,,convolutional neural network,deep semantic feature,,,,,,,,,,,,,,
Row_1557,"Yang, Michael Ying","Kumaar, Saumya","Lyu, Ye","Nex, Francesco",,,Real-time Semantic Segmentation with Context Aggregation Network,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,AUG 2021,57,"With the increasing demand of autonomous systems, pixelwise semantic segmentation for visual scene understanding needs to be not only accurate but also efficient for potential real-time applications. In this paper, we propose Context Aggregation Network, a dual branch convolutional neural network, with significantly lower computational costs as compared to the state-of-the-art, while maintaining a competitive prediction accuracy. Building upon the existing dual branch architectures for high-speed semantic segmentation, we design a high resolution branch for effective spatial detailing and a context branch with light-weight versions of global aggregation and local distribution blocks, potent to capture both long-range and local contextual dependencies required for accurate semantic segmentation, with low computational overheads. We evaluate our method on two semantic segmentation datasets, namely Cityscapes dataset and UAVid dataset. For Cityscapes test set, our model achieves state-of-the-art results with mIOU of 75.9%, at 76 FPS on an NVIDIA RTX 2080Ti and 8 FPS on a Jetson Xavier NX. With regards to UAVid dataset, our proposed network achieves mIOU score of 63.5% with high execution speed (15 FPS).",Semantic segmentation,Real-time,Convolutional neural network,Context aggregation network,,,,,,,,,,,,,,,,,
Row_1558,"Zhou, Wujie","Li, Yangzhen","Huang, Juan","Yan, Weiqing","Fang, Meixin","Jiang, Qiuping",GSGNet-S∗: Graph Semantic Guidance Network via Knowledge Distillation for Optical Remote Sensing Image Scene Analysis,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,0,"In recent years, optical remote sensing image (ORSI) scene analysis has attracted increasing interest. However, existing networks show a trend of bifurcation. Lightweight networks have very high inference speed but poor inference of contextual information in highly complex backgrounds. In contrast, networks with high-performance contextual information reasoning capability require many parameters and are computationally expensive. Since the knowledge distillation (KD) method can greatly lighten the model, we propose a graph semantic guided network (GSGNet) that utilizes knowledge refinement for ORSI scenario analysis, which has a high inference speed while maintaining practical contextual inference capability. Rich semantic and detailed information facilitates the semantic segmentation of ORSIs. We design adjacent dynamic capture (ADC) and local-global map inference modules that can effectively extract low-level spatial details and high-level contextual semantics. To improve the attention map relearning (AR) performance of the distillation method, we designed semantically guided fusion modules (SGFMs) to locate spatial information and refine edge information. We also employed a structural relationship transfer (SRT) distillation method in which the structural relationship knowledge of the teacher model (GSGNet-T) was used to guide the student model (GSGNet-S). We compared the performances of GSGNet-T and the GSGNet-S with KD (GSGNet-S*) with those of several state-of-the-art (SOAT) methods on the Vaihingen and Potsdam datasets. Extensive experiments showed that GSGNet-S* outperformed most advanced methods with only 19.61 M parameters and a computation cost of 2.9 GFLOPs. The experimental results and code of our network can be accessed at the following URL: https://github.com/LYZ00918/GSGNet-KD.",Feature extraction,Convolution,Semantics,Task analysis,,Data mining,Knowledge engineering,Image analysis,Graph convolution,knowledge distillation (KD),optical remote sensing images (ORSIs),scene analysis,,,,,,,,,
Row_1559,"Wang, Haoran","Xue, Tianyu","Wang, Zhaoran","Bai, Xiangyu",,,Comparison of regional monitoring methods for grassland degradation based on remote sensing images,,2023,0,"As an integral part of the ecosystem, grassland plays an important role in protecting water and soil, preventing wind and fixing sand and protecting biodiversity. However, some grasslands are degraded at this stage, so a grassland monitoring method is urgently needed to prevent desertification from spreading. With the rapid rise of deep learning, it is more and more popular to apply artificial intelligence methods to grassland degradation monitoring. This paper systematically and comprehensively analyzes that almost all semantic segmentation methods have been applied to relevant research on grassland degradation areas since semantic segmentation methods were applied to grassland monitoring. Then, according to the different algorithm structures of grassland extraction methods, the principles of representative algorithms are introduced in turn. Then we made a statistical analysis of the publication status, research space distribution and the number of citations of papers in this field. Finally, the analysis results are discussed, and the possible research hotspots in the future are discussed.",Grassland monitoring,Remote sensing images,Deep learning,Semantic segmentation,"2023 2ND ASIA CONFERENCE ON ALGORITHMS, COMPUTING AND MACHINE LEARNING, CACML 2023",,,,,,,,,,,,,,,,
Row_1560,"Hui, He","Ya-Dong, Sun","Bo-Xiong, Yang","Mu-Xi, Xie","She-Lei, Li","Bo, Zhou",Building extraction based on hyperspectral remote sensing images and semisupervised deep learning with limited training samples,COMPUTERS & ELECTRICAL ENGINEERING,SEP 2023,1,"Hyperspectral remote sensing imaging technology provides assistance in various aspects of daily life through applications such as urban building information statistics and green vegetation estimation. Ensuring the accuracy of automatic thematic information extraction under limited samples is a challenge. In this manuscript, a lightweight semantic segmentation model based on the ""encoder-decoder"" structure is proposed for extracting buildings from hyperspectral remote sensing images. The proposed model employs the lightweight MobileNet combined with multi scale feature fusion and a group dilated convolution for modelling both shallow and deep spatial and spectral features as the encoder and an efficient combined standardized attention mechanism for selecting the most valuable bands and local information. Extensive experiments reveal that our method produces greater accuracy than state-of-the-art lightweight models in building extraction tasks. We also demonstrated the superiority of our method for insufficient training sample sizes. When only 50% of the samples of the initial training set were used, the mean intersection over union (mIOU) reached 91.90%, 4.5% higher than that of the next best method. For training sets composed of only 16 and 8 images, the mIOU values were 89.42 and 77.11%, respectively, 13.6 and 18 percentage points higher than that of the next best method. According to the visualization of the results, the proposed method obviously outperformed the compared methods. The model proposed in this paper is suitable for accurately extracting buildings from hyperspectral images in situations involving limited training samples.",Building extraction,Limited training samples,Semantic segmentation model,Attention mechanism,,Hyperspectral remote sensing,,,,,,,"Kai-Cun, Zhang",,,,,,,,
Row_1561,"Zhang, Xiuwei","Jin, Jiaojiao","Lan, Zeze","Li, Chunjiang","Fan, Minhao","Wang, Yafei",ICENET: A Semantic Segmentation Deep Network for River Ice by Fusing Positional and Channel-Wise Attentive Features,REMOTE SENSING,JAN 2020,41,"River ice monitoring is of great significance for river management, ship navigation and ice hazard forecasting in cold-regions. Accurate ice segmentation is one most important pieces of technology in ice monitoring research. It can provide the prerequisite information for the calculation of ice cover density, drift ice speed, ice cover distribution, change detection and so on. Unmanned aerial vehicle (UAV) aerial photography has the advantages of higher spatial and temporal resolution. As UAV technology has become more popular and cheaper, it has been widely used in ice monitoring. So, we focused on river ice segmentation based on UAV remote sensing images. In this study, the NWPU_YRCC dataset was built for river ice segmentation, in which all images were captured by different UAVs in the region of the Yellow River, the most difficult river to manage in the world. To the best of our knowledge, this is the first public UAV image dataset for river ice segmentation. Meanwhile, a semantic segmentation deep convolution neural network by fusing positional and channel-wise attentive features is proposed for river ice semantic segmentation, named ICENET. Experiments demonstrated that the proposed ICENET outperforms the state-of-the-art methods, achieving a superior result on the NWPU_YRCC dataset.",river ice,position attention,channel-wise attention,deep convolutional neural network,,semantic segmentation,,,,,,,"Yu, Xin","Zhang, Yanning",,,,,,,
Row_1562,"Alexakis, E. Bousias","Armenakis, C.",,,,,IMPROVING CNN-BASED BUILDING SEMANTIC SEGMENTATION USING OBJECT BOUNDARIES,,2022,0,"Semantic segmentation is an active area of research with a wide range of applications including autonomous driving, digital mapping, urban monitoring, land use analysis and disaster management. For the past few years approaches based on Convolutional Neural Networks, especially end-to-end approaches based on architectures like the Fully Convolutional Networks (FCN) and UNet, have made great progress and are considered the current state-of-the-art. Nevertheless, there is still room for improvement as CNN-based supervised-learning models require a very large amount of labelled data in order to generalize effectively to new data and the segmentation results often lack detail, mostly in areas near the boundaries between objects. In this work we leverage the semantic information provided by the objects' boundaries to improve the quality and detail of an encoder-decoder model's semantic segmentation output. We use a UNet-based model with ResNet as an encoder for our backbone architecture in which we incorporate a decoupling module that separates the boundaries from the main body of the objects and thus learns explicit representations for both body and edges of each object. We evaluate our proposed approach on the Inria Aerial Image Labelling dataset and compare the results to a more traditional Unet-based architecture. We show that the proposed approach marginally outperforms the baseline on the mean precision, F1-score and IoU metrics by 1.1 to 1.6%. Finally, we examine certain cases of misclassification in the ground truth data and discuss how the trained models perform in such cases.",Building Extraction,CNN,Building Boundaries,Semantic Segmentation,"XXIV ISPRS CONGRESS: IMAGING TODAY, FORESEEING TOMORROW, COMMISSION III",Decoupled Body and Edge Segmentation,,,,,,,,,,,,,,,
Row_1563,"Li, Qiusheng","Yuan, Hang","Fu, Tianning","Yu, Zhibin","Zheng, Bing","Chen, Shuguo",Multispectral Semantic Segmentation for UAVs: A Benchmark Dataset and Baseline,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Solidago canadensis L. is a typical invasive plant that has become a significant threat worldwide and profoundly impacts local ecosystems. An unmanned aerial vehicle (UAV)-based semantic segmentation (SS) system can help in monitoring the spread and location of Solidago canadensis L. To identify the growth range of this species with greater efficiency, we employ a high-speed multispectral camera, which provides richer color information and features with limited resolution, in conjunction with a high-quality RGB camera to construct a segmentation dataset. We construct a validated UAV multispectral (UAVM) dataset comprising 3260 pairs of calibrated RGB and multispectral images. All the images in the dataset underwent semantic annotation at a fine-grained pixel level, with 12 categories being covered. In addition, other plant categories can be employed in precision agriculture and ecological conservation. Moreover, we propose a benchmark model, UAVM semantic segmentation network (UAVMNet). With the aid of the feature alignment module and the UAVMFuse module, UAVMNet efficiently integrates multispectral and high-quality RGB image information, enhancing its ability to perform semantic segmentation tasks effectively. To the best of our knowledge, this is the first model to colearn semantic representations via high-quality RGB and paired multispectral information on a UAV platform. We conduct comprehensive experiments on the proposed UAVM dataset.",Semantic segmentation,Autonomous aerial vehicles,Semantics,Vegetation mapping,,Oceans,Cameras,Monitoring,Multispectral images,semantic segmentation (SS),Solidago canadensis L.,UAV multispectral (UAVM) dataset,,,UAVM semantic segmentation network (UAVMNet),unmanned aerial vehicle (UAV),,,,,
Row_1564,"Bo, Weihao","Liu, Jie","Fan, Xijian","Tjahjadi, Tardi","Ye, Qiaolin","Fu, Liyong",BASNet: Burned Area Segmentation Network for Real-Time Detection of Damage Maps in Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,34,"Since remote sensing images of post-fire vegetation are characterized by high resolution, multiple interferences, and high similarities between the background and the target area, it is difficult for existing methods to detect and segment the burned area in these images with sufficient speed and accuracy. In this article, we apply salient object detection (SOD) to burned area segmentation (BAS), the first time this has been done, and propose an efficient burned area segmentation network (BASNet) to improve the performance of unmanned aerial vehicle (UAV) high-resolution image segmentation. BASNet comprises positioning module and refinement module. The positioning module efficiently extracts high-level semantic features and general contextual information via global average pooling layer and convolutional block (CB) to determine the coarse location of the salient region. The refinement module adopts the CB attention module to effectively discriminate the spatial location of objects. In addition, to effectively combine edge information with spatial location information in the lower layer of the network and the high-level semantic information in the deeper layer, we design the residual fusion module to perform feature fusion by level to obtain the prediction results of the network. Extensive experiments on two UAV datasets collected from Chongli in China and Andong in South Korea, demonstrate that our proposed BASNet significantly outperforms the state-of-the-art SOD methods quantitatively and qualitatively. BASNet also achieves a promising prediction speed for processing high-resolution UAV images, thus providing wide-ranging applicability in post-disaster monitoring and management.",Image segmentation,Semantics,Vegetation mapping,Optical imaging,,Feature extraction,Autonomous aerial vehicles,Satellites,Burned area segmentation (BAS),convolutional neural network (CNN),forest fire monitoring,salient object detection (SOD),,,,,,,,,
Row_1565,"Lian, Renbao","Huang, Liqin",,,,,DeepWindow: Sliding Window Based on Deep Learning for Road Extraction From Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2020,38,"The road centerline extraction is the key step of the road network extraction and modeling. The hand-craft feature engineering in the traditional road extraction methods is unstable, which makes the extracted road centerline deviated from the road center in complex cases and even results in overall extracting errors. Recently, the road centerline extraction methods based on semantic segmentation employing deep neural network greatly outperformed the traditional methods. Nevertheless, the pixel-wise labels for training deep learning models are expensive and the postprocess of road segmentation is error-prone. Inspired by the work of human pose estimation, we propose DeepWindow, a novel method to automatically extract the road network from remote sensing images. DeepWindow uses a sliding window guided by a CNN-based decision function to track the road network directly from the images without the prior of road segmentation. First of all, we design and train a CNN model to estimate the road center points inside a patch. Then, the road seeds are automatically searched patch by patch employing the CNN model. Finally, starting from seeds, our method first estimates the road direction using a Fourier spectrum analysis algorithm and then iteratively tracks the road center-line along the road direction guided by the CNN model. In our method, the CNN model is trained by point annotations, which greatly reduces the training costs comparing to those in semantic model training. Our method achieves comparable performance with the state-of-the-art road extraction methods, and extensive experiments indicate that our method is robust to the point deviation.",Roads,Image segmentation,Remote sensing,Feature extraction,,Training,Network topology,Topology,Deep learning,remote sensing images,road extraction,sliding window,,,,,,,,,
Row_1566,"Li, Xungen","Zhang, Zhan","Lv, Shuaishuai","Pan, Mian","Ma, Qi","Yu, Haibin",Road Extraction From High Spatial Resolution Remote Sensing Image Based on Multi-Task Key Point Constraints,IEEE ACCESS,2021,8,"To solve some problems of high spatial resolution remote sensing images caused by land coverage, building coverage and shading of trees, such as difficult road extraction and low precision, a road extraction method based on multi-task key point constraints is put forward in this article based on Linknet. At the preprocessing stage, an auxiliary constraint task is designed to solve the connectivity problem caused by shading during road extraction from remote sensing images. At the encoding & decoding stage, first, a position attention (PA) mechanism module and channel attention (CA) mechanism module are applied to realize the effective fusion of semantic information in the context during road extraction. Second, a multi-branch cascade dilated spatial pyramid (CDSP) is established with dilated convolution, by which the problem of loss of partial information during information extraction from remote sensing road image is solved and the detection accuracy is further improved. The method put forward in this article is verified through the experiment with public datasets and private datasets, revealing that the proposed method provides better performance than several state of the art techniques in terms of detection accuracy, recall, precision, and F1-score.",Roads,Feature extraction,Decoding,Image segmentation,,Data mining,Semantics,Remote sensing,Road extraction,remote sensing image,semantic segmentation,high-resolution imagery,,,,,,,,,
Row_1567,"Feng, Lin","Xu, Penglei","Tang, Hong","Liu, Zeping","Hou, Peng",,National-scale mapping of building footprints using feature super-resolution semantic segmentation of Sentinel-2 images,GISCIENCE & REMOTE SENSING,DEC 31 2023,3,"Since buildings are closely related to human activities, large-scale mapping of individual buildings has become a hot research topic. High-resolution images with sub-meter or meter resolution are common choices to produce maps of building footprints. However, high-resolution images are both infrequently collected and expensive to obtain and process, making it very difficult to produce large-scale maps of individual buildings timely. This paper presents a simple but effective way to produce a national-scale map of building footprints using feature super-resolution semantic segmentation of sentinel-2 images. Specifically, we proposed a super-resolution semantic segmentation network named EDSR_NASUnet, which is an end-to-end network to generate semantic maps with a spatial resolution of 2.5 m from real remote sensing images with a spatial resolution of 10 m. Based on the dataset consisting of images from 35 cities in China, we quantitatively compared the proposed method with three methods under the same framework and qualitatively evaluated the identification results of individual buildings. In addition, we mapped building footprints within the entire China at 2.5 m-resolution using Sentinel-2 images of 10 m resolution. The density of building footprints varies considerably across China, with a gradual increase in building footprints from west to east, i.e. from the first step of China's terrain to the third one. We detected over 86.3 million individual buildings with a total rooftop area of approximately 58,719.43 km(2). The number of buildings increased from 5.73 million in the first step of China's terrain, through 23.41 million in the second step of China's terrain, to 57.16 million in the third step of China's terrain. The area of buildings also increased from 3318.02 km(2) through 13,844.29 to 41,557.12 km(2). The Aihui-Tengchong line, a dividing line representing the regional distribution of China's population, also divides the regional distribution of Chinese buildings. Our approach has a more open and practical application because of the medium-resolution images and platform with open access. Results are available to the community (https://code.earthengine.google.com/?asset=users/flower/2019_China)).",Building footprints,super-resolution semantic segmentation,single building,national-scale,,Sentinel-2,,,,,,,,,,,,,,,
Row_1568,"Yuan, Ming","Chai, Zhilei","Zhao, Wenlai",,,,Severe Convective Weather Classification in Remote Sensing Images by Semantic Segmentation,"ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2019: IMAGE PROCESSING, PT III",2019,0,"Severe convective weather is a catastrophic weather that can cause great harm to the public. One of the key studies for meteorological practitioners is how to recognize severe convection weather accurately and effectively, and it is also an important issue in government climate risk management. However, most existing methods extract features from satellite data by classifying individual pixels instead of using tightly integrated spatial information, ignoring the fact the clouds are highly dynamic. In this paper, we propose a new classification model, which is based on image segmentation of deep learning. And it uses U-net architecture as the technology platform to identify all weather conditions in the datasets accurately. As heavy rainfall is one of the most frequent and widespread server weather hazards, when the storms come ashore with high speed of wind, it makes the precipitation time longer and causes serious damage in turn. Therefore, we suggest a new evaluation metric to evaluate the performance of detecting heavy rainfall. Compared with existing methods, the model based on Himawari-8 dataset has a better performance. Further, we explore the representations learned by our model in order to better understand this important dataset. The results play a crucial role in the prediction of climate change risks and the formulation of government policies on climate change.",Severe convective weather,Segmentation,Evaluation metric,,,,,,,,,,,,,,,,,,
Row_1569,"Pu, Weihua","Wang, Zhipan","Liu, Di","Zhang, Qingling",,,Optical Remote Sensing Image Cloud Detection with Self-Attention and Spatial Pyramid Pooling Fusion,REMOTE SENSING,SEP 2022,11,"Cloud detection is a key step in optical remote sensing image processing, and the cloud-free image is of great significance for land use classification, change detection, and long time-series landcover monitoring. Traditional cloud detection methods based on spectral and texture features have acquired certain effects in complex scenarios, such as cloud-snow mixing, but there is still a large room for improvement in terms of generation ability. In recent years, cloud detection with deep-learning methods has significantly improved the accuracy in complex regions such as high-brightness feature mixing areas. However, the existing deep learning-based cloud detection methods still have certain limitations. For instance, a few omission alarms and commission alarms still exist in cloud edge regions. At present, the cloud detection methods based on deep learning are gradually converted from a pure convolutional structure to a global feature extraction perspective, such as attention modules, but the computational burden is also increased, which is difficult to meet for the rapidly developing time-sensitive tasks, such as onboard real-time cloud detection in optical remote sensing imagery. To address the above problems, this manuscript proposes a high-precision cloud detection network fusing a self-attention module and spatial pyramidal pooling. Firstly, we use the DenseNet network as the backbone, then the deep semantic features are extracted by combining a global self-attention module and spatial pyramid pooling module. Secondly, to solve the problem of unbalanced training samples, we design a weighted cross-entropy loss function to optimize it. Finally, cloud detection accuracy is assessed. With the quantitative comparison experiments on different images, such as Landsat8, Landsat9, GF-2, and Beijing-2, the results indicate that, compared with the feature-based methods, the deep learning network can effectively distinguish in the cloud-snow confusion-prone region using only visible three-channel images, which significantly reduces the number of required image bands. Compared with other deep learning methods, the accuracy at the edge of the cloud region is higher and the overall computational efficiency is relatively optimal.",cloud detection,self-attention,pyramid pooling module,semantic segmentation,,optical remote sensing image,,,,,,,,,,,,,,,
Row_1570,"Ghosh, Arthita","Ehrlich, Max","Shah, Sohil","Davis, Larry","Chellappa, Rama",,Stacked U-Nets for Ground Material Segmentation in Remote Sensing Imagery,,2018,43,We present a semantic segmentation algorithm for RGB remote sensing images. Our method is based on the Dilated Stacked U-Nets architecture. This state-of-the-art method has been shown to have good performance in other applications. We perform additional post-processing by blending image tiles and degridding the result. Our method gives competitive results on the DeepGlobe dataset.,,,,,PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPRW),,,,,,,,,,,,,,,,
Row_1571,"Wang, Li","Duan, Wensheng","Yu, Bo","Ying, Qing","Yang, Hanbing","Lei, Yahui",Exploring the Response Mechanism of Remote Sensing Images in Monitoring Fixed Assets Investment Project in Terms of Building Detection,IEEE ACCESS,2019,0,"Fixed assets investment is a driving factor in facilitating urbanization and economic growth. The local governments have spent a lot of budgets on the construction of fixed assets. However, such investment lacks a scientific and objective mechanism to supervise the construction process continuously to guarantee on-time delivery. Owing to the development of remote sensing technology, the availability of high spatial resolution images makes it possible to visualize the construction process continuously. By synthesizing the amount of fixed assets investment, we can build a reasonable monitoring system to supervise the fixed assets projects mutually in both terms of visual construction and statistical money spent. However, as far as we know, there is not much work exploring the methods in monitoring fixed assets investment yet. We collected the continuous investment records of fourteen fixed assets projects from the year 2015 to 2017 and the corresponding GaoFen satellite images in nine-time nodes. Semantic segmentation deep learning technology is applied to detect buildings from the high spatial resolution images. The monitoring system is built by regression between the ratio of investment and the ratio of building area at nine-time nodes. Compared with the regression model from the ratio of investment and that of ground truth building area, our model achieves an RMSE of 0.0136 in the test samples. It indicates the strong potential applicability of remote sensing images in supervising the reasonability of the construction process of fixed assets and the investment allocation.",Fixed assets investment project,semantic segmentation,high spatial resolution remote sensing,deep learning,,continuous monitoring,supervise construction process,,,,,,,,,,,,,,
Row_1572,"Chen, Weitao","Zhou, Gaodian","Liu, Zhuoyue","Li, Xianju","Zheng, Xiongwei","Wang, Lizhe",NIGAN: A Framework for Mountain Road Extraction Integrating Remote Sensing Road-Scene Neighborhood Probability Enhancements and Improved Conditional Generative Adversarial Network,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,27,"Mountain roads are a source of important basic geographic data used in various fields. The automatic extraction of road images through high-resolution remote sensing imagery using deep learning has attracted considerable attention. But the interference of context information limited extraction accuracy, especially for roads in mountain area. Furthermore, when pursuing research in a new district, many algorithms are difficult to train due to a lack of data. To address these issues, a framework based on remote sensing road-scene neighborhood probability enhancement and improved conditional generative adversarial network (NIGAN) is proposed in this article. This framework can be divided into two sections: 1) road scenes classification section. A remote sensing road-scene neighborhood confidence enhancement method was designed for classifying road scenes of the study area to reduce the impact of nonroad information on subsequent fine-road segmentation and 2) fine-road segmentation section. An improved dilated convolution module, which is helpful in extracting small objects such as road, was added into the conditional generative adversarial network (CGAN) to increase the receptive field and pay attention to global information, and segment roads from the results of road scenes classification section. To validate the NIGAN framework, new mountain road-scene and label datasets were constructed, and diverse comparison experiments were performed. The results indicate that the NIGAN framework can improve the integrity and accuracy of mountain road-scene extraction in diverse and complex conditions. The results further confirm the validity of the NIGAN framework in small samples. In addition, the mountain road-scene datasets can serve as benchmark datasets for studying mountain road extraction.",Roads,Feature extraction,Remote sensing,Data mining,,Convolution,Generative adversarial networks,Image segmentation,Deep learning,generative adversarial network (GAN),remote sensing,road extraction,,,scene classification,semantic segmentation,ZiYuan-3,,,,
Row_1573,"Randrianasoa, Jimmy Francky","Kurtz, Camille","Gancarski, Pierre","Desjardin, Eric","Passat, Nicolas",,EVALUATING THE QUALITY OF BINARY PARTITION TREES BASED ON UNCERTAIN SEMANTIC GROUND-TRUTH FOR IMAGE SEGMENTATION,,2017,7,"The binary partition tree (BPT) is a hierarchical data-structure that models the content of an image in a multiscale way. In particular, a cut of the BPT of an image provides a segmentation, as a partition of the image support. Actually, building a BPT allows for dramatically reducing the search space for segmentation purposes, based on intrinsic (image signal) and extrinsic (construction metric) information. A large literature has been devoted to the construction on such metrics, and the associated choice of criteria (spectral, spatial, geometric, etc.) for building relevant BPTs, in particular in the challenging context of remote sensing. But, surprisingly, there exists few works dedicated to evaluate the quality of BPTs, i.e. their ability to further provide a satisfactory segmentation. In this paper, we propose a framework for BPT quality evaluation, in a supervised paradigm. Indeed, we assume that ground-truth segments are provided by an expert, possibly with a semantic labelling and a given uncertainty. Then, we describe local evaluation metrics, BPT nodes / ground-truth segments fitting strategies, and global quality score computation considering semantic information, leading to a complete evaluation framework. This framework is illustrated in the context of BPT segmentation of multispectral satellite images.",Binary partition tree,supervised evaluation,uncertainty,semantics,2017 24TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP),segmentation,mathematical morphology,remote sensing,,,,,,,,,,,,,
Row_1574,"Cao, Qinglong","Chen, Yuntian","Ma, Chao","Yang, Xiaokang",,,Few-Shot Rotation-Invariant Aerial Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,4,"Few-shot aerial image semantic segmentation is a challenging task that requires precisely parsing unseen-category objects in query aerial images with limited annotated support aerial images. Formally, category prototypes would be extracted from support samples to segment query images in a pixel-to-pixel matching manner. However, aerial objects in aerial images are often distributed with arbitrary orientations, and varying orientations could cause a dramatic feature change. This unique property of aerial images renders conventional matching manner without consideration of orientations fails to activate same-category objects with different orientations. Furthermore, the oscillation of the confidence scores in existing rotation-insensitive algorithms, engendered by the striking changes of object orientations, often leads to false recognition of lower scored rotated semantic objects. To tackle these challenges, inspired by the intrinsic rotation invariance in aerial images, we propose a novel few-shot rotation-invariant aerial semantic segmentation network (FRINet) to efficiently segment aerial semantic objects with diverse orientations. Specifically, through extracting orientation-varying yet category-consistent support information, FRINet provides rotation-adaptive matching for each query feature in a feature-aggregation manner. Meanwhile, to encourage consistent predictions for aerial objects with arbitrary orientations, segmentation predictions from different orientations are supervised by the same label and further fused to obtain the final rotation-invariant prediction in a complementary manner. Moreover, aiming at providing a better solution searching space, the backbones are newly pretrained in the base category to basically boost the segmentation performance. Extensive experiments on the few-shot aerial image semantic segmentation benchmark demonstrate that the proposed FRINet achieves a new state-of-the-art performance. The code is available at https://github.com/caoql98/FRINet.",Consistent prediction,few-shot aerial semantic segmentation,rotation invariance,rotation-adaptive matching,,,,,,,,,,,,,,,,,
Row_1575,"Xu, Weicheng","Yang, Weiguang","Chen, Pengchao","Zhan, Yilong","Zhang, Lei","Lan, Yubin",Cotton Fiber Quality Estimation Based on Machine Learning Using Time Series UAV Remote Sensing Data,REMOTE SENSING,FEB 2023,5,"As an important factor determining the competitiveness of raw cotton, cotton fiber quality has received more and more attention. The results of traditional detection methods are accurate, but the sampling cost is high and has a hysteresis, which makes it difficult to measure cotton fiber quality parameters in real time and at a large scale. The purpose of this study is to use time-series UAV (Unmanned Aerial Vehicle) multispectral and RGB remote sensing images combined with machine learning to model four main quality indicators of cotton fibers. A deep learning algorithm is used to identify and extract cotton boll pixels in remote sensing images and improve the accuracy of quantitative extraction of spectral features. In order to simplify the input parameters of the model, the stepwise sensitivity analysis method is used to eliminate redundant variables and obtain the optimal input feature set. The results of this study show that the R-2 of the prediction model established by a neural network is improved by 29.67% compared with the model established by linear regression. When the spectral index is calculated after removing the soil pixels used for prediction, R-2 is improved by 4.01% compared with the ordinary method. The prediction model can well predict the average length, uniformity index, and micronaire value of the upper half. R-2 is 0.8250, 0.8014, and 0.7722, respectively. This study provides a method to predict the cotton fiber quality in a large area without manual sampling, which provides a new idea for variety breeding and commercial decision-making in the cotton industry.",UAV remote sensing,cotton fiber quality,inversion,semantic segmentation,,,,,,,,,,,,,,,,,
Row_1576,"Chowdhury, Tashnim","Rahnemoonfar, Maryam","Murphy, Robin","Fernandes, Odair",,,Comprehensive Semantic Segmentation on High Resolution UAV Imagery for Natural Disaster Damage Assessment,,2020,26,"In this paper, we present a large-scale hurricane Michael dataset for visual perception in disaster scenarios, and analyze state-of-the-art deep neural network models for semantic segmentation. The dataset consists of around 2000 high-resolution aerial images, with annotated ground-truth data for semantic segmentation. We discuss the challenges of the dataset and train the state-of-the-art methods on this dataset to evaluate how well these methods can recognize the disaster situations. Finally, we discuss challenges for future research.",Natural disaster,semantic segmentation,aerial,,2020 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA),,,,,,,,,,,,,,,,
Row_1577,"Anilkumar, P.","Venugopal, P.",,,,,An Enhanced Multi-Objective-Derived Adaptive DeepLabv3 Using G-RDA for Semantic Segmentation of Aerial Images,ARABIAN JOURNAL FOR SCIENCE AND ENGINEERING,AUG 2023,3,"Semantic segmentation acts as a major role in classifying the remote sensing images into oceanic ice, vegetation, roads, vehicles, houses, and more for offering high precision at the pixel level. In recent studies, convolutional neural network (CNN) has accomplished superior efficiency in the semantic segmentation of images. Even though various deep techniques and architectures have been utilized for enhancing the accuracy, it suffers from classifying the confused classes. Due to the optical conditions and remote sensing information, the sub-decimeter aerial imagery segmentation is challenging while achieving fine-grained semantic segmentation outcomes. The core goal of this task is to adopt the latest Adaptive DeepLabv3 strategy for enhanced semantic segmentation of aerial images. In Adaptive Deeplabv3, the involvement of both the encoder-decoder structure and spatial pyramid pooling module with adaptiveness by a hybrid meta-heuristic algorithm makes faster and stronger segmentation performance within less search space and reduced computation time. The relevant parameters of DeepLabv3 are tuned or optimized by the hybrid meta-heuristic algorithm based on Genetic Inspired Red Deer Algorithm (G-RDA). The enhanced segmentation is employed concerning a fitness function with precision and accuracy. Finally, the experimental analysis of the suggested Adaptive DeepLabv3 strategy for semantic segmentation of aerial images proves its competitive solution when evaluated over conventional approaches.",Semantic segmentation,Aerial images,Adaptive DeepLabv3,Genetic inspired red deer algorithm,,Multi-objective function,Adaptive deep learning,,,,,,,,,,,,,,
Row_1578,"Martins, Jose Augusto Correa","Nogueira, Keiller","Osco, Lucas Prado","Gomes, Felipe David Georges","Furuya, Danielle Elis Garcia","Goncalves, Wesley Nunes",Semantic Segmentation of Tree-Canopy in Urban Environment with Pixel-Wise Deep Learning,REMOTE SENSING,AUG 2021,40,"Urban forests are an important part of any city, given that they provide several environmental benefits, such as improving urban drainage, climate regulation, public health, biodiversity, and others. However, tree detection in cities is challenging, given the irregular shape, size, occlusion, and complexity of urban areas. With the advance of environmental technologies, deep learning segmentation mapping methods can map urban forests accurately. We applied a region-based CNN object instance segmentation algorithm for the semantic segmentation of tree canopies in urban environments based on aerial RGB imagery. To the best of our knowledge, no study investigated the performance of deep learning-based methods for segmentation tasks inside the Cerrado biome, specifically for urban tree segmentation. Five state-of-the-art architectures were evaluated, namely: Fully Convolutional Network; U-Net; SegNet; Dynamic Dilated Convolution Network and DeepLabV3+. The experimental analysis showed the effectiveness of these methods reporting results such as pixel accuracy of 96,35%, an average accuracy of 91.25%, F1-score of 91.40%, Kappa of 82.80% and IoU of 73.89%. We also determined the inference time needed per area, and the deep learning methods investigated after the training proved to be suitable to solve this task, providing fast and effective solutions with inference time varying from 0.042 to 0.153 minutes per hectare. We conclude that the semantic segmentation of trees inside urban environments is highly achievable with deep neural networks. This information could be of high importance to decision-making and may contribute to the management of urban systems. It should be also important to mention that the dataset used in this work is available on our website.",remote sensing,image segmentation,sustainability,convolutional neural network,,urban environment,,,,,,,"Sant'Ana, Diego Andre","Ramos, Ana Paula Marques",,,,"Liesenberg, Veraldo","dos Santos, Jefersson Alex","de Oliveira, Paulo Tarso Sanches","Marcato Junior, Jose"
Row_1579,Han Xing,Han Ling,Li Liangzhi,Li Huihui,,,Building Change Detection in High-Resolution Remote-Sensing Images Based on Deep Learning,LASER & OPTOELECTRONICS PROGRESS,MAY 2022,4,"To overcome low detection accuracy, false and leak detections for medium- and small-scale targets, rough segmentation for building boundary of traditional semantic segmentation network, we propose a high-resolution remote-sensing image building change detection method based on deep learning. The proposed method adopts the coding-decoding structure. At the coding stage, the residual network is used to extract the image features. The dilated convolution and pyramid pooling module are introduced in the deepest features of the encoder to enlarge the receptive field and extract the multiscale image features. At the decoding stage, the attention module highlights the useful features, and the features with different scales and resolutions are aggregated. We performed experiments on large-scale remote-sensing building change detection datasets. The results show that the proposed method can obtain deep-layer semantic information and pay attention to detailed information. It also has a considerable improvement in precision, recall, and F1 score. Additionally, the proposed method performs better than other semantic segmentation networks in multiscale target detection and building boundary extraction.",remote sensing image,change detection,ResNet50,attention mechanism,,feature pyramid,,,,,,,,,,,,,,,
Row_1580,"Zhang, Feifei","Wang, Yong","Du, Yawen","Zhu, Yijia",,,A Spatio-Temporal Encoding Neural Network for Semantic Segmentation of Satellite Image Time Series,APPLIED SCIENCES-BASEL,DEC 2023,0,"Remote sensing image semantic segmentation plays a crucial role in various fields, such as environmental monitoring, urban planning, and agricultural land classification. However, most current research primarily focuses on utilizing the spatial and spectral information of single-temporal remote sensing images, neglecting the valuable temporal information present in historical image sequences. In fact, historical images often contain valuable phenological variations in land features, which exhibit diverse patterns and can significantly benefit from semantic segmentation tasks. This paper introduces a semantic segmentation framework for satellite image time series (SITS) based on dilated convolution and a Transformer encoder. The framework includes spatial encoding and temporal encoding. Spatial encoding, utilizing dilated convolutions exclusively, mitigates the loss of spatial accuracy and the need for up-sampling, while allowing for the extraction of rich multi-scale features through a combination of different dilation rates and dense connections. Temporal encoding leverages a Transformer encoder to extract temporal features for each pixel in the image. To better capture the annual periodic patterns of phenological phenomena in land features, position encoding is calculated based on the image's acquisition date within the year. To assess the performance of this framework, comparative and ablation experiments were conducted using the PASTIS dataset. The experiments indicate that this framework achieves highly competitive performance with relatively low optimization parameters, resulting in an improvement of 8 percentage points in the mean Intersection over Union (mIoU).",semantic segmentation,phenology,spatial encoding,temporal encoding,,satellite image time series,,,,,,,,,,,,,,,
Row_1581,"Yu, Lingjuan","Zeng, Zhaoxin","Liu, Ao","Xie, Xiaochun","Wang, Haipeng","Xu, Feng",A Lightweight Complex-Valued DeepLabv3+for Semantic Segmentation of PolSAR Image,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,38,"Semantic image segmentation is one kindof end-to-end segmentation method which can classify the target region pixel by pixel. As a classic semantic segmentation network in optical images, DeepLabv3+ can achieve a good segmentation performance. However, when this network is directly used in the semantic segmentation of polarimetric synthetic aperture radar (PolSAR) image, it is hard to obtain the ideal segmentation results. The reason is that it is easy to yield overfitting due to the small PolSAR dataset. In this article, a lightweight complex-valued DeepLabv3+ (L-CV-DeepLabv3+) is proposed for semantic segmentation of PolSAR data. It has two significant advantages when compared with the original DeepLabv3+. First, the proposed network with the simplified structure and parameters can be suitable for the small PolSAR data, and thus, it can effectively avoid the overfitting. Second, the proposed complex-valued (CV) network can make full use of both amplitude and phase information of PolSAR data, which brings better segmentation performance than the real-valued (RV) network, and the related CV operations are strictly true in the mathematical sense. Experimental results about two Flevoland datasets and one San Francisco dataset show that the proposed network can obtain better overall average, mean intersection over union, and mean pixel accuracy than the original DeepLabv3+ and some other RV semantic segmentation networks.",Image segmentation,Convolution,Semantics,Kernel,,Scattering,Feature extraction,Decoding,Lightweight complex-valued DeepLabv3+(L-CV-DeepLabv3+),polarimteric synthetic aperture radar (SAR),segmentation performance,semantic image segmentation,"Hong, Wen",,,,,,,,
Row_1582,"Zhou, Yicheng","Yang, Lingbo","Yuan, Lin","Li, Xin","Mao, Yihu","Dong, Jiancong",High-Precision Tea Plantation Mapping with Multi-Source Remote Sensing and Deep Learning,AGRONOMY-BASEL,DEC 2024,0,"Accurate mapping of tea plantations is crucial for agricultural management and economic planning, yet it poses a significant challenge due to the complex and variable nature of tea cultivation landscapes. This study presents a high-precision approach to mapping tea plantations in Anji County, Zhejiang Province, China, utilizing multi-source remote sensing data and advanced deep learning models. We employed a combination of Sentinel-2 optical imagery, Sentinel-1 synthetic aperture radar imagery, and digital elevation models to capture the rich spatial, spectral, and temporal characteristics of tea plantations. Three deep learning models, namely U-Net, SE-UNet, and Swin-UNet, were constructed and trained for the semantic segmentation of tea plantations. Cross-validation and point-based accuracy assessment methods were used to evaluate the performance of the models. The results demonstrated that the Swin-UNet model, a transformer-based approach capturing long-range dependencies and global context for superior feature extraction, outperformed the others, achieving an overall accuracy of 0.993 and an F1-score of 0.977 when using multi-temporal Sentinel-2 data. The integration of Sentinel-1 data with optical data slightly improved the classification accuracy, particularly in areas affected by cloud cover, highlighting the complementary nature of Sentinel-1 imagery for all-weather monitoring. The study also analyzed the influence of terrain factors, such as elevation, slope, and aspect, on the accuracy of tea plantation mapping. It was found that tea plantations at higher altitudes or on north-facing slopes exhibited higher classification accuracy, and that accuracy improves with increasing slope, likely due to simpler land cover types and tea's preference for shade. The findings of this research not only provide valuable insights into the precision mapping of tea plantations but also contribute to the broader application of deep learning in remote sensing for agricultural monitoring.",tea plantation mapping,multi-source remote sensing,deep learning,semantic segmentation,,agricultural monitoring,DEM,,,,,,"Lin, Zhenyu","Zhou, Xianfeng",,,,,,,
Row_1583,"Li, Ruirui","Liu, Wenjie","Yang, Lei","Sun, Shihao","Hu, Wei","Zhang, Fan",DeepUNet: A Deep Fully Convolutional Network for Pixel-Level Sea-Land Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,NOV 2018,238,"Semantic segmentation is a fundamental research in optical remote sensing image processing. Because of the complex maritime environment, the sea-land segmentation is a challenging task. Although the neural network has achieved excellent performance in semantic segmentation in the last years, there were a few of works using CNN for sea-land segmentation and the results could be further improved. This paper proposes a novel deep convolution neural network named DeepUNet. Like the U-Net, its structure has a contracting path and an expansive path to get high-resolution optical output. But differently, the DeepUNet uses DownBlocks instead of convolution layers in the contracting path and uses UpBlock in the expansive path. The two novel blocks bring two new connections that are U-connection and Plus connection. They are promoted to get more precise segmentation results. To verify the network architecture, we construct a new challenging sea-land dataset and compare the DeepUNet on it with the U-Net, SegNet, and SeNet. Experimental results show that DeepUNet can improve 1-2% accuracy performance compared with other architectures, especially in high-resolution optical remote sensing imagery.",Fully convolutional network (FCN),optical remote sensing image,sea-land segmentation,SeNet,,U-Net,,,,,,,"Li, Wei",,,,,,,,
Row_1584,"Hu, Lei","Niu, Chuang","Ren, Shenghan","Dong, Minghao","Zheng, Changli","Zhang, Wei",Discriminative Context-Aware Network for Target Extraction in Remote Sensing Imagery,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,4,"Extracting objects of interest from remote sensing imagery is an essential part in various practical applications. The objects that people pay attention to in the remote sensing scene mainly include buildings, roads, vehicles, etc. In this article, extracting the aforementioned objects are collectively referred to as the target extraction task. Arising from object scale variation, appearance similarity between adjacent patches, diversity of imaging orientation, and complexity of background, it is difficult to extract complete objects from cluttered backgrounds. Deep neural network has made great achievement in dense prediction for target extraction. However, most of the previous works are still faced with a formidable challenge in discriminative context feature representation to extract targets of various categories and correctly classify pixels around the boundary. In this article, we propose a target extraction neural network, named discriminative context-aware network, to focus on discriminative high-level context features and preserve spatial information. First, a discriminative context-aware feature module is designed to generate the feature maps in the top layer, which not only captures the rich image context information but also aggregates the contrasted local information at multiple scales. Second, a refine decoder module is adopted to preserve spatial information from low-level layers and enhance the feature representation, leading to precise segmentation results. We conducted extensive experiments on building and road extraction benchmarks, including WHU building dataset and Massachusetts road dataset, together with a self-constructed dataset for vehicle extraction in SAR images. Our method achieves state-of-the-art results with fewer parameters and faster inference.",Feature extraction,Remote sensing,Task analysis,Roads,,Buildings,Semantics,Image segmentation,Deep learning,discriminative context-aware feature (DCF),refine decoder (RD),remote sensing imagery,"Liang, Jimin",,target extraction,,,,,,
Row_1585,"Liu, Zhengyu","Shi, Qian","Ou, Jinpei",,,,LCS: A Collaborative Optimization Framework of Vector Extraction and Semantic Segmentation for Building Extraction,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,20,"In the field of building extraction, many convolutional neural network (CNN)-based methods have been developed to solve the problem of the irregular boundaries in their predictions. The prevailing approach is to build an additional edge segmentation branch or obtain accurate vector components of buildings. However, pixel-based methods still cannot obtain an accurate location of the boundary, while vector extraction will bring the problem of sample imbalance and missing detection. In this work, we utilize the complementarity of the two types of methods and propose the line segment collaborate segmentation (LCS) framework. In the proposed LCS framework, semantic segmentation provides location guidance for vector extraction, while vector extraction provides precise positioning for semantic segmentation. In this way, the two tasks can leverage their respective strengths. The results on three datasets show that the performance of vector extraction and semantic segmentation is improved simultaneously using the LCS framework, which proves the effectiveness of our method. At the same time, our framework is flexible and can be embedded in other vector extraction methods to improve performance.",Feature extraction,Buildings,Task analysis,Data mining,,Remote sensing,Semantics,Image edge detection,Building extraction,multitask learning,semantic segmentation,vector extraction,,,,,,,,,
Row_1586,"Yin, Xiaoqing","Li, Xu","Ni, Peizhou","Xu, Qimin","Kong, Dong",,A Novel Real-Time Edge-Guided LiDAR Semantic Segmentation Network for Unstructured Environments,REMOTE SENSING,FEB 2023,4,"LiDAR-based semantic segmentation, particularly for unstructured environments, plays a crucial role in environment perception and driving decisions for unmanned ground vehicles. Unfortunately, chaotic unstructured environments, especially the high-proportion drivable areas and large-area static obstacles therein, inevitably suffer from the problem of blurred class edges. Existing published works are prone to inaccurate edge segmentation and have difficulties dealing with the above challenge. To this end, this paper proposes a real-time edge-guided LiDAR semantic segmentation network for unstructured environments. First, the main branch is a lightweight architecture that extracts multi-level point cloud semantic features; Second, the edge segmentation module is designed to extract high-resolution edge features using cascaded edge attention blocks, and the accuracy of extracted edge features and the consistency between predicted edge and semantic segmentation results are ensured by additional supervision; Third, the edge guided fusion module fuses edge features and main branch features in a multi-scale manner and recalibrates the channel feature using channel attention, realizing the edge guidance to semantic segmentation and further improving the segmentation accuracy and adaptability of the model. Experimental results on the SemanticKITTI dataset, the Rellis-3D dataset, and on our test dataset demonstrate the effectiveness and real-time performance of the proposed network in different unstructured environments. Especially, the network has state-of-the-art performance in segmentation of drivable areas and large-area static obstacles in unstructured environments.",LiDAR,unstructured environment,semantic segmentation,edge-guidance,,,,,,,,,,,,,,,,,
Row_1587,"Yang, Zhao","Guo, Peng","Gao, Han","Chen, Xiuwan",,,Depth-Assisted ResiDualGAN for Cross-Domain Aerial Images Semantic Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,2,"Unsupervised domain adaptation (UDA) is an approach to minimizing the domain gap. Generative methods are common approaches to minimizing the domain gap of aerial images, which improves the performance of the downstream tasks, for example, cross-domain semantic segmentation. For aerial images, the digital surface model (DSM) is usually available in both the source domain and the target domain. Depth information in DSM brings external information to generative models. However, little research utilizes it. In this letter, depth-assisted ResiDualGAN (DRDG) is proposed where depth supervised loss (DSL) and depth cycle consistency loss (DCCL) are used to bring depth information into the generative model. Experimental results show that DRDG reaches state-of-the-art accuracy between generative methods in cross-domain semantic segmentation tasks. Source code is available at https://github.com/miemieyanga/ResiDualGAN-DRDG.",DSL,Semantic segmentation,Task analysis,Computational modeling,,Generators,Semantics,Optical losses,Aerial images,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,
Row_1588,"Huang, Huaigang","Chen, Yiping","Wang, Ruisheng",,,,A Lightweight Network for Building Extraction From Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,30,"Building extraction is a fundamental research topic in remote sensing image interpretation. Convolutional neural network (CNN)-based building extraction algorithms have achieved high accuracy but require a large account of parameters and calculations, which hinders the practical application of these algorithms. To address the challenge, we propose a lightweight network (RSR-Net) for building extraction from remote sensing images. The network consists of three basic units with only a few parameters, and uses the idea of the fusion of shallow features and deep features, which is proposed by U-Net. Before features fusion, the squeeze-and-excitation (SE) module in RSR-Net assigned channel weights to these deep and shallow features. This operation can effectively reduce the influence of noise caused by shallow features in feature fusion, so as to improve the performance of the model. We estimated our network on datasets and achieved 88.32%, 71.58%, and 77.07% intersection-over-union (IoU) on datasets of aerial image and satellite image in Wuhan University (WHU) dataset, and the self-made building dataset of Guangzhou University Town, with only 2.81 M parameters and 6.91 G floating point operations (FLOPs). In addition, we propose a strategy combining target and background prediction, which makes RSR-Net achieve 0.37% improvement in IoU on WHU aerial image dataset. The effectiveness of RSR-Net is high. It showed that the proposed network is light and fast for the application of convolution neural network algorithm in practice.",Feature extraction,Buildings,Convolution,Remote sensing,,Decoding,Computational complexity,Image classification,Building extraction,lightweight convolutional neural network (CNN),remote sensing images,,,,,,,,,,
Row_1589,"Balti, Hanen","Mellouli, Nedra","Chebbi, Imen","Farah, Imed","Lamolle, Myriam",,Deep Semantic Feature Detection from Multispectral Satellite Images,,2019,2,"Recent progress in satellite technology has resulted in explosive growth in volume and quality of high-resolution remote sensing images. To solve the issues of retrieving high-resolution remote sensing (RS) data in both efficiency and precision, this paper proposes a distributed system architecture for object detection in satellite images using a fully connected neural network. On the one hand, to address the issue of higher computational complexity and storage ability, the Hadoop framework is used to handle satellite image data using parallel architecture. On the other hand, deep semantic features are extracted using Convolutional Neural Network (CNN),in order to identify objects and accurately locate them. Experiments are held out on several datasets to analyze the efficiency of the suggested distributed system. Experimental results indicate that our system architecture is simple and sustainable, both efficiency and precision can satisfy realistic requirements.",Big Data,Remote Sensing,Feature Detection,CNN,"KDIR: PROCEEDINGS OF THE 11TH INTERNATIONAL JOINT CONFERENCE ON KNOWLEDGE DISCOVERY, KNOWLEDGE ENGINEERING AND KNOWLEDGE MANAGEMENT - VOL 1: KDIR",Semantic Segmentation,,,,,,,,,,,,,,,
Row_1590,"Zhao, Danpei","Yuan, Bo","Chen, Ziqiang","Li, Tian","Liu, Zhuoran","Li, Wentao",Panoptic Perception: A Novel Task and Fine-Grained Dataset for Universal Remote Sensing Image Interpretation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,4,"Current remote-sensing interpretation models often focus on a single task, such as detection, segmentation, or caption. However, the task-specific designed models are unattainable to achieve the comprehensive multilevel interpretation of images. The field also lacks support for multitask joint interpretation datasets. In this article, we propose panoptic perception: a novel task and a new fine-grained panoptic perception (FineGrip) dataset to achieve a more thorough and universal interpretation for remote sensing images (RSIs). The new task: 1) integrates pixel-level, instance-level, and image-level information for universal image perception; 2) captures image information from coarse-to-fine granularity, achieving deeper scene understanding and description; and 3) enables various independent tasks to complement and enhance each other through multitask learning. By emphasizing multitask interactions and the consistency of perception results, this task enables the simultaneous processing of fine-grained foreground instance segmentation, background semantic segmentation, and global fine-grained image captioning. Concretely, the FineGrip dataset includes 2649 RSIs, 12054 fine-grained instance segmentation masks belonging to 20 foreground things categories, and 7599 background semantic masks for five stuff classes. Furthermore, we propose a joint optimization-based panoptic perception model. Experimental results on FineGrip demonstrate the feasibility of the panoptic perception task and the beneficial effect of multitask joint optimization on individual tasks. The project page is at: https://ybio.github.io/FineGrip.",Benchmark dataset,fine-grained interpretation,multitask learning,panoptic perception,,remote sensing images (RSIs),,,,,,,"Gao, Yue",,,,,,,,
Row_1591,"Zheng, Zhuo","Zhong, Yanfei","Wang, Junjue",,,,POP-NET: ENCODER-DUAL DECODER FOR SEMANTIC SEGMENTATION AND SINGLE-VIEW HEIGHT ESTIMATION,,2019,21,"The single-view semantic 3D challenge in 2019 Data Fusion Contest is to predict both semantic labels and normalized digital surface model (nDSM) for urban scenes from single-view satellite images. We propose a novel pyramid on pyramid network (Pop-Net) based on Encoder-Dual Decoder framework to end-to-end multi-task learning. The encoder is a deformable ResNet-101 backbone network. Two feature pyramid networks, as decoders, are responsible for semantic segmentation and height estimation, respectively. Semantic information is crucial to estimate height. Therefore, regression pyramid on the semantic pyramid is introduced to leverage semantic features to help height estimation. To deal with outliers in heights, we leverage anchor-based regression and smooth L1 loss for optimization to obtain more robust height estimation. Without bells and whistles, our single model entry achieves 77.78% mIoU and 53.40% mIoU-3 on test set, ranking 2nd in the Single-view Semantic 3D Challenge of the 2019 IEEE GRSS Data Fusion Contest. The code is available at https://github.com/Z-Zheng/PopNet.",single-view semantic 3D challenge,pyramid on pyramid,semantic segmentation,height estimation,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),,,,,,,,,,,,,,,,
Row_1592,"Zhang, Xueliang","Xiao, Pengfeng","Feng, Xuezhi",,,,Fast Hierarchical Segmentation of High-Resolution Remote Sensing Image with Adaptive Edge Penalty,PHOTOGRAMMETRIC ENGINEERING AND REMOTE SENSING,JAN 2014,12,"A fast hierarchical segmentation method (FHS) for high-resolution remote sensing (HR) image is proposed in the paper. FHS is completely unsupervised. It is characterized by two aspects. First, the hierarchical segmentation process is accelerated by the improved linear nearest neighbor graph (LNNG) model and the segment tree model. It runs faster than other existing hierarchical segmentation methods, and can produce multi-resolution segmentations in time linear to the image size. Second, an adaptive edge penalty function is introduced to formulate the merging criterion, serving as a semantic factor. A set of QuickBird, World View, and aerial images is used to test the proposed method. The experiments show that the multi-resolution segmentations produced by FHS can represent objects at different scales very well. Moreover, the adaptive edge penalty function helps to remove meaningless weak edges within objects, enclosing the relation between segments and real-world objects.",,,,,,,,,,,,,,,,,,,,,
Row_1593,"Saputra, Muhamad Risqi U.","Bhaswara, Irfan Dwiki","Nasution, Bahrul Ilmi","Ern, Michelle Ang Li","Husna, Nur Laily Romadhotul","Witra, Tahjudil",Multi-modal deep learning approaches to semantic segmentation of mining footprints with multispectral satellite imagery,REMOTE SENSING OF ENVIRONMENT,MAR 1 2025,0,"Existing remote sensing applications in mining are often of limited scope, typically mapping multiple mining land covers for a single mine or only mapping mining extents or a single feature (e.g., tailings dam) for multiple mines across a region. Many of these works have a narrow focus on specific mine land covers rather than encompassing the variety of mining and non-mining land use in a mine site. This study presents a pioneering effort in performing deep learning-based semantic segmentation of 37 mining locations worldwide, representing a range of commodities from gold to coal, using multispectral satellite imagery, to automate mapping of mining and non-mining land covers. Due to the absence of a dedicated training dataset, we crafted a customized multispectral dataset for training and testing deep learning models, leveraging and refining existing datasets in terms of boundaries, shapes, and class labels. We trained and tested multimodal semantic segmentation models, particularly based on U-Net, DeepLabV3+, Feature Pyramid Network (FPN), SegFormer, and IBM-NASA foundational geospatial model (Prithvi) architecture, with a focus on evaluating different model configurations, input band combinations, and the effectiveness of transfer learning. In terms of multimodality, we utilized various image bands, including Red, Green, Blue, and Near Infra-Red (NIR) and Normalized Difference Vegetation Index (NDVI), to determine which combination of inputs yields the most accurate segmentation. Results indicated that among different configurations, FPN with DenseNet-121 backbone, pre-trained on ImageNet, and trained using both RGB and NIR bands, performs the best. We concluded the study with a comprehensive assessment of the model's performance based on climate classification categories and diverse mining commodities. We believe that this work lays a robust foundation for further analysis of the complex relationship between mining projects, communities, and the environment.",Semantic segmentation,Global mining footprints,Multispectral,Deep learning,,,,,,,,,"Feliren, Vicky","Owen, John R.",,,,"Kemp, Deanna","Lechner, Alex M.",,
Row_1594,"Chong, Yanwen","Nie, Congchong","Tao, Yulong","Chen, Xiaoshu","Pan, Shaoming",,HCNet: Hierarchical Context Network for Semantic Segmentation,IEEE ACCESS,2020,1,"Global context information is vital in visual understanding problems, especially in pixel-level semantic segmentation. The mainstream methods adopt the self-attention mechanism to model global context information. However, pixels belonging to different classes usually have weak feature correlation. Modeling the global pixel-level correlation matrix indiscriminately is extremely redundant in the self-attention mechanism. In order to solve the above problem, we propose a hierarchical context network to differentially model homogeneous pixels with strong correlations and heterogeneous pixels with weak correlations. Specifically, we first propose a multi-scale guided pre-segmentation module to divide the entire feature map into different classed-based homogeneous regions. Within each homogeneous region, we design the pixel context module to capture pixel-level correlations. Subsequently, different from the self-attention mechanism that still models weak heterogeneous correlations in a dense pixel-level manner, the region context module is proposed to model sparse region-level dependencies using a unified representation of each region. Through aggregating fine-grained pixel context features and coarse-grained region context features, our proposed network can not only hierarchically model global context information but also harvest multi-granularity representations to more robustly identify multi-scale objects. We evaluate our approach on Cityscapes and the ISPRS Vaihingen dataset. Without Bells or Whistles, our approach realizes a mean IoU of 82.8% and overall accuracy of 91.4% on Cityscapes and ISPRS Vaihingen test set, achieving state-of-the-art results.",Correlation,Semantics,Feature extraction,Context modeling,,Convolution,Image segmentation,Computational modeling,Semantic segmentation,self-attention,global context,pixel context,,,region context,,,,,,
Row_1595,"Liu, Lili","Gao, Zhan","Luo, Pingping","Duan, Weili","Hu, Maochuan","Zainol, Mohd Remy Rozainy Mohd Arif",The Influence of Visual Landscapes on Road Traffic Safety: An Assessment Using Remote Sensing and Deep Learning,REMOTE SENSING,SEP 2023,10,"Rapid global economic development, population growth, and increased motorization have resulted in significant issues in urban traffic safety. This study explores the intrinsic connections between road environments and driving safety by integrating multiple visual landscape elements. High-resolution remote sensing and street-view images were used as primary data sources to obtain the visual landscape features of an urban expressway. Deep learning semantic segmentation was employed to calculate visual landscape features, and a trend surface fitting model of road landscape features and driver fatigue was established based on experimental data from 30 drivers who completed driving tasks in random order. There were significant spatial variations in the visual landscape of the expressway from the city center to the urban periphery. Heart rate values fluctuated within a range of 0.2% with every 10% change in driving speed and landscape complexity. Specifically, as landscape complexity changed between 5.28 and 8.30, the heart rate fluctuated between 91 and 96. This suggests that a higher degree of landscape richness effectively mitigates increases in driver fatigue and exerts a positive impact on traffic safety. This study provides a reference for quantitative assessment research that combines urban road landscape features and traffic safety using multiple data sources. It may guide the implementation of traffic safety measures during road planning and construction.",deep learning,semantic segmentation,traffic safety,driving performance,,remote sensing,street view image,visual landscape elements,,,,,"Zawawi, Mohd Hafiz",,,,,,,,
Row_1596,"Lv, Suna","Meng, Lingsheng","Edwing, Deanna","Xue, Sihan","Geng, Xupu","Yan, Xiao-Hai",High-Performance Segmentation for Flood Mapping of HISEA-1 SAR Remote Sensing Images,REMOTE SENSING,NOV 2022,19,"Floods are the among the most frequent and common natural disasters, causing numerous casualties and extensive property losses worldwide every year. Since flooding areas are often accompanied by cloudy and rainy weather, synthetic aperture radar (SAR) is one of the most powerful sensors for flood monitoring with capabilities of day-and-night and all-weather imaging. However, SAR images are prone to high speckle noise, shadows, and distortions, which affect the accuracy of water body segmentation. To address this issue, we propose a novel Modified DeepLabv3+ model based on the powerful extraction ability of convolutional neural networks for flood mapping from HISEA-1 SAR remote sensing images. Specifically, a lightweight encoder MobileNetv2 is used to improve floodwater detection efficiency, small jagged arrangement atrous convolutions are employed to capture features at small scales and improve pixel utilization, and more upsampling layers are utilized to refine the segmented boundaries of water bodies. The Modified DeepLabv3+ model is then used to analyze two severe flooding events in China and the United States. Results show that Modified DeepLabv3+ outperforms competing semantic segmentation models (SegNet, U-Net, and DeepLabv3+) with respect to the accuracy and efficiency of floodwater extraction. The modified model training resulted in average accuracy, F1, and mIoU scores of 95.74%, 89.31%, and 87.79%, respectively. Further analysis also revealed that Modified DeepLabv3+ is able to accurately distinguish water feature shape and boundary, despite complicated background conditions, while also retaining the highest efficiency by covering 1140 km(2) in 5 min. These results demonstrate that this model is a valuable tool for flood monitoring and emergency management.",flood mapping,water extraction,HISEA-1,synthetic aperture radar (SAR),,convolution neural network (CNN),,,,,,,,,,,,,,,
Row_1597,"Wang, Sheng","Huang, Xiaohui","Han, Wei","Zhang, Xiaohan","Li, Jun",,Geological remote sensing interpretation via a local-to-global sensitive feature fusion network,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,DEC 2024,0,"Interpreting surface geological elements (such as rocks, minerals, soils, and water bodies) is the main task of geological survey, which plays a crucial role in geological environment remote sensing (GERS). However, the characteristics of geological elements, including high variabilities, various morphology, complicated boundaries and imbalanced class distribution, make it still a challenge for deep learning methods to interpret GERS images. Considering the correlations of geological elements as the regionalized variables in geostatistics, the sensitive features of GERS interpretation mainly include three aspects: tonal, textural and structural characteristics within a singular-class elements, spatial and spectral correlations of adjacent elements, and their global tectonic or spatial distribution. Thus, to simulate the manual interpretation process of geologists from local to global and promote GERS interpretation performance, we propose a local-to-global multi-scale feature fusion network (LGMSFNet). A geological object context represents the intra-class semantic dependencies of pixel sets with the same class. And a local feature aggregation module models the channel and spatial association. Then discriminative features are integrated by a global feature fusion module. For the model optimization, we focus on hard examples during the training process to achieve the balanced optimization of various categories. Two research areas that include large-scale rocks, soils and water exposed on the surface are selected. Massive experiments demonstrate the superiority of the LGMSFNet in GERS interpretation.",Geological environment remote sensing,Deep learning,Semantic segmentation,,,,,,,,,,,,,,,,,,
Row_1598,"Liu, Xinni","Han, Fengrong","Ghazali, Kamarul Hawari","Mohamed, Izzeldin Ibrahim","Zhao, Yue",,A review of Convolutional Neural Networks in Remote Sensing Image,,2019,15,"Effectively analysis of remote-sensing images is very important in many practical applications, such as urban planning, geospatial object detection, military monitoring, vegetation mapping and precision agriculture. Recently, convolutional neural network based deep learning algorithm has achieved a series of breakthrough research results in the fields of objective detection, image semantic segmentation and image classification, etc. Their powerful feature learning capabilities have attracted more attention and have important research value. In this article, firstly we have summarized the basic structure and several classical convolutional neural network architectures. Secondly, the recent research problems on convolutional neural network are discussed. Later, we summarized the latest research results in convolutional neural network based remote sensing fields. Finally, the conclusion has made on the basis of current issue on convolutional neural networks and the future development direction.",Convolutional neural network,deep learning,remote-sensing images,,2019 8TH INTERNATIONAL CONFERENCE ON SOFTWARE AND COMPUTER APPLICATIONS (ICSCA 2019),,,,,,,,,,,,,,,,
Row_1599,"Rashidan, Hanis","Musliman, Ivin Amri","Rahman, Alias Abdul","Coors, Volker","Buyuksalih, Gurcan",,Semantic Segmentation of Building Models with Deep Learning in CityGML,,2024,0,"Semantic segmentation of 3D urban environments plays an important role in urban planning, management, and analysis. This paper presents an exploration of leveraging BuildingGNN, a deep learning framework for semantic segmentation of 3D building models, and the subsequent conversion of semantic labels into CityGML, the standardized format for 3D city models. The study begins with a methodology outlining the acquisition of a labelled dataset from BuildingNet and the necessary preprocessing steps for compatibility with BuildingGNN's architecture. The training process involves deep learning techniques tailored for 3D building structures, yielding insights into model performance metrics such as Intersection over Union (IoU) for several architectural components. Evaluation of the trained model highlights its accuracy and reliability, albeit with challenges observed, particularly in segmenting certain classes like doors. Moreover, the conversion of semantic labels into CityGML format is discussed, emphasizing the importance of data quality and meticulous annotation practices. The experiment as described in the methodology shows that outputs from the BuildingGNN for semantic segmentation can be utilized for the generation of CityGML building elements with some percentage of success. This particular work reveals several challenges such as the identification of individual architectural elements based on geometry groups. We believe that the improvement of the segmentation process could be further investigated in our near future work.",3D Buildings Models,Semantic Segmentation,CityGML,,"19TH 3D GEOINFO CONFERENCE 2024, VOL. 48-4",,,,,,,,,,,,,,,,
Row_1600,"He, Kang","Zhang, Zhijun","Dong, Yusen","Cai, Depan","Lu, Yue","Han, Wei",Improving Geological Remote Sensing Interpretation Via a Contextually Enhanced Multiscale Feature Fusion Network,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"Geological remote sensing interpretation plays a pivotal role in the field of regional geological mapping, encompassing the analysis of rock, soil, and water features. However, these geological elements can be obscured by the surrounding geographical environment and can undergo modifications caused by geological activities. The former hinders the effectiveness of satellite remote sensing data, resulting in the invisibility of element features, while the latter leads to the complex distribution of element features and significant spatial variations of geological elements. Consequently, existing deep learning-based models for interpreting geological elements often exhibit limited accuracy. To address these issues, this study proposes the contextually enhanced multiscale feature fusion network for the efficient interpretation of geological elements. First, the context enhancement module is employed to extract abundant feature information and reinforce contextual features, aiming to capture essential features and strengthen their interconnections. Second, the multiscale feature fusion module incorporates the SimAM attention mechanism to adaptively learn features from different channels, emphasizing the feature information that contributes to interpretation results and maximizing the comprehensive and crucial feature information for each element. Extensive experiments demonstrate the superior performance of both the context enhancement module and the multiscale feature fusion module compared to several representative deep learning networks in terms of overall interpretation accuracy on two datasets. The model demonstrated improvements in oPA and mIoU of 2.4% and 2.8%, respectively, on the Landsat 8 dataset, and 3.5% and 3.2%, respectively, on the Sentinel-2 dataset.",Deep learning,feature fusion,geological remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,
Row_1601,"Zhang, XM","Yokoya, N","Bruzzone, L",,,,Local-to-Global Cross-Modal Attention-Aware Fusion for HSI-X Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Hyperspectral image (HSI) classification has recently reached its performance bottleneck. Multimodal data fusion is emerging as a promising approach to overcome this bottleneck by providing rich complementary information from the supplementary modality (X-modality). However, achieving comprehensive cross-modal interaction and fusion that can be generalized across different sensing modalities is challenging due to the disparity in imaging sensors, resolution, and content of different modalities. In this study, we propose a local-to-global cross-modal attention-aware fusion (LoGoCAF) framework for HSI-X segmentation. LoGoCAF adopts a two-branch semantic segmentation architecture to learn information from HSI and X modalities. The pipeline of LoGoCAF consists of a local-to-global encoder and a lightweight all multilayer perceptron (ALL-MLP) decoder. In the encoder, convolutions are used to encode local and high-resolution fine details in shallow layers, while transformers are used to integrate global and low-resolution coarse features in deeper layers. The ALL-MLP decoder aggregates information from the encoder for feature fusion and prediction. In particular, two cross-modality modules, the feature enhancement module (FEM) and the feature interaction and fusion module (FIFM), are introduced in each encoder stage. The FEM is used to enhance complementary information by combining information from the other modality across direction-aware, position-sensitive, and channel-wise dimensions. With the enhanced features, the FIFM is designed to promote cross-modality information interaction and fusion for the final semantic prediction. Extensive experiments demonstrate that our LoGoCAF achieves superior performance and generalizes well on various multimodal datasets. Code is available at https://github.com/xumzhang.",,,,,,,,,,,,,,,,,,,,,
Row_1602,"Chen, Kaiqiang","Weinmann, Michael","Sun, Xian","Yan, Menglong","Hinz, Stefan","Jutzi, Boris",SEMANTIC SEGMENTATION OF AERIAL IMAGERY VIA MULTI-SCALE SHUFFLING CONVOLUTIONAL NEURAL NETWORKS WITH DEEP SUPERVISION,,2018,10,"In this paper, we address the semantic segmentation of aerial imagery based on the use of multi-modal data given in the form of true orthophotos and the corresponding Digital Surface Models (DSMs). We present the Deeply-supervised Shuffling Convolutional Neural Network (DSCNN) representing a multi-scale extension of the Shuffling Convolutional Neural Network (SCNN) with deep supervision. Thereby, we take the advantage of the SCNN involving the shuffling operator to effectively upsample feature maps and then fuse multiscale features derived from the intermediate layers of the SCNN, which results in the Multi-scale Shuffling Convolutional Neural Network (MSCNN). Based on the MSCNN, we derive the DSCNN by introducing additional losses into the intermediate layers of the MSCNN. In addition, we investigate the impact of using different sets of hand-crafted radiometric and geometric features derived from the true orthophotos and the DSMs on the semantic segmentation task. For performance evaluation, we use a commonly used benchmark dataset. The achieved results reveal that both multi-scale fusion and deep supervision contribute to an improvement in performance. Furthermore, the use of a diversity of hand-crafted radiometric and geometric features as input for the DSCNN does not provide the best numerical results, but smoother and improved detections for several objects.",Semantic Segmentation,Aerial Imagery,Multi-Modal Data,Multi-Scale,ISPRS TC I MID-TERM SYMPOSIUM INNOVATIVE SENSING - FROM SENSORS TO METHODS AND APPLICATIONS,CNN,Deep Supervision,,,,,,"Weinmann, Martin",,,,,,,,
Row_1603,"Yi, Zhiyu","Wang, Yuebin","Zhang, Liqiang",,,,Revolutionizing Remote Sensing Image Analysis With BESSL-Net: A Boundary-Enhanced Semi-Supervised Learning Network,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,1,"Deep learning (DL) has become increasingly popular in remote sensing (RS) change detection (CD), leading to the development of massive networks that surpass traditional methods in accuracy and automation. However, the need for enormous amounts of annotated data remains a major concern, and accurate boundary segmentation in RS images is challenging due to their complexity and heterogeneity. Moreover, properly aggregating the bitemporal feature pairs and creating highly discriminative change features are crucial for detection performance. This article proposes a boundary-enhanced semi-supervised network (BESSL-Net) to tackle these issues for CD tasks. The network adopts dual encoders and one decoder architecture for segmentation and incorporates pseudolabeling, contrastive learning (CL), along with a teacher-student scheme to leverage unlabeled data. A boundary extraction module (BEM) is used to conduct boundary segmentation, while a change segmentation feature learning module (SFLM) is used to create discriminative change features in both channel and spatial domains by integrating multilevel features. Three publicly available CD datasets are used to validate the proposed BESSL-Net. Compared with the current state-of-the-art networks, the semi-supervised network demonstrates advanced performance metrics, especially regarding IoU metric of change-class (IoUc), showing improvements ranging from 1% to 9%.",Feature extraction,Task analysis,Semantic segmentation,Semantics,,Data mining,Reliability,Decoding,Boundary information,change detection (CD),change features' generation,remote sensing (RS),,,semi-supervised deep learning (DL),,,,,,
Row_1604,"Lyu, Xin","Jiang, Wenxuan","Li, Xin","Fang, Yiwei","Xu, Zhennan","Wang, Xinyuan",MSAFNet: Multiscale Successive Attention Fusion Network for Water Body Extraction of Remote Sensing Images,REMOTE SENSING,JUN 2023,8,"Water body extraction is a typical task in the semantic segmentation of remote sensing images (RSIs). Deep convolutional neural networks (DCNNs) outperform traditional methods in mining visual features; however, due to the inherent convolutional mechanism of the network, spatial details and abstract semantic representations at different levels are difficult to capture accurately at the same time, and then the extraction results decline to become suboptimal, especially on narrow areas and boundaries. To address the above-mentioned problem, a multiscale successive attention fusion network, named MSAFNet, is proposed to efficiently aggregate the multiscale features from two aspects. A successive attention fusion module (SAFM) is first devised to extract multiscale and fine-grained features of water bodies, while a joint attention module (JAM) is proposed to further mine salient semantic information by jointly modeling contextual dependencies. Furthermore, the multi-level features extracted by the above-mentioned modules are aggregated by a feature fusion module (FFM) so that the edges of water bodies are well mapped, directly improving the segmentation of various water bodies. Extensive experiments were conducted on the Qinghai-Tibet Plateau Lake (QTPL) and the Land-cOVEr Domain Adaptive semantic segmentation (LoveDA) datasets. Numerically, MSAFNet reached the highest accuracy on both QTPL and LoveDA datasets, including Kappa, MIoU, FWIoU, F1, and OA, outperforming several mainstream methods. Regarding the QTPL dataset, MSAFNet peaked at 99.14% and 98.97% in terms of F1 and OA. Although the LoveDA dataset is more challenging, MSAFNet retained the best performance, with F1 and OA being 97.69% and 95.87%. Additionally, visual inspections exhibited consistency with numerical evaluations.",water body extraction,remote sensing images,convolutional neural network,attention mechanism,,successive attention fusion module,,,,,,,,,,,,,,,
Row_1605,"Fan, Xiangsuo","Yan, Chuan","Fan, Jinlong","Wang, Nayi",,,Improved U-Net Remote Sensing Classification Algorithm Fusing Attention and Multiscale Features,REMOTE SENSING,AUG 2022,21,"The selection and representation of classification features in remote sensing image play crucial roles in image classification accuracy. To effectively improve the features classification accuracy, an improved U-Net remote sensing classification algorithm fusing attention and multiscale features is proposed in this paper, called spatial attention-atrous spatial pyramid pooling U-Net (SA-UNet). This framework connects atrous spatial pyramid pooling (ASPP) with the convolutional units of the encoder of the original U-Net in the form of residuals. The ASPP module expands the receptive field, integrates multiscale features in the network, and enhances the ability to express shallow features. Through the fusion residual module, shallow and deep features are deeply fused, and the characteristics of shallow and deep features are further used. The spatial attention mechanism is used to combine spatial with semantic information so that the decoder can recover more spatial information. In this study, the crop distribution in central Guangxi province was analyzed, and experiments were conducted based on Landsat 8 multispectral remote sensing images. The experimental results showed that the improved algorithm increases the classification accuracy, with the accuracy increasing from 93.33% to 96.25%, The segmentation accuracy of sugarcane, rice, and other land increased from 96.42%, 63.37%, and 88.43% to 98.01%, 83.21%, and 95.71%, respectively. The agricultural planting area results obtained by the proposed algorithm can be used as input data for regional ecological models, which is conducive to the development of accurate and real-time crop growth change models.",multiscale features,U-Net,attention,remote sensing image classification,,,,,,,,,,,,,,,,,
Row_1606,"Hu, Leiyi","Lu, Wanxuan","Yu, Hongfeng","Yin, Dongshuo","Sun, Xian","Fu, Kun",TEA: A Training-Efficient Adapting Framework for Tuning Foundation Models in Remote Sensing,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"With well-pretrained foundation models (FMs), the performance of almost every remote sensing interpretation task has been boosted. The parameter volume of FMs increases with their continuously enhanced capabilities, leading to increased costs of fine-tuning. To apply FMs more effectively and efficiently, there are already some arts that introduce the parameter-efficient fine-tuning (PEFT) concept into remote sensing and achieve competitive performance with much lower parameter cost. However, the training efficiency of most PEFT frameworks may be not satisfactory. To make tuning FMs for remote sensing applications more efficient, we propose a training-efficient adapting (TEA) framework. Specifically, we attach a SIDE adapter network (SIDEAN) to the frozen powerful FMs and only update the SIDEAN to perform the downstream tasks. Moreover, to make TEA perceive remote sensing scenes from a macroscopic perspective and boost the performance, we propose a top-down guidance mechanism to inject macro scene information into the SIDEAN during adapting. TEA is also parameter-efficient, as SIDEAN is designed to be lightweight. We conduct extensive experiments to demonstrate the effectiveness and efficiency of TEA on ten widely adopted datasets covering four primary remote sensing tasks, e.g., object detection, orientated object detection, semantic segmentation, and scene classification. By training only 5.43% of the frozen FM parameters, TEA can save more than 57% of training memory footprint and up to 15% of time cost on average while achieving competitive performance on all datasets. Furthermore, TEA can surpass full fine-tuning on several datasets.",Foundation model (FM),parameter-efficient fine-tuning (PEFT),remote sensing,remote sensing,,remote sensing,,,,,,,,,,,,,,,
Row_1607,"Li, Yansheng","Li, Xinwei","Chen, Wei","Zhang, Yongjun",,,A disparity-aware Siamese network for building change detection in bi-temporal remote sensing images,PHOTOGRAMMETRIC RECORD,SEP 2024,1,"Building change detection has various applications, such as urban management and disaster assessment. Along with the exponential growth of remote sensing data and computing power, an increasing number of deep-learning-based remote sensing building change detection methods have been proposed in recent years. Objectively, the overwhelming majority of existing methods can perfectly deal with the change detection of low-rise buildings. By contrast, high-rise buildings often present a large disparity in multitemporal high-resolution remote sensing images, which degrades the performance of existing methods dramatically. To alleviate this problem, we propose a disparity-aware Siamese network for detecting building changes in bi-temporal high-resolution remote sensing images. The proposed network utilises a cycle-alignment module to address the disparity problem at both the image and feature levels. A multi-task learning framework with joint semantic segmentation and change detection loss is used to train the entire deep network, including the cycle-alignment module in an end-to-end manner. Extensive experiments on three publicly open building change detection datasets demonstrate that our method achieves significant improvements on datasets with severe building disparity and state-of-the-art performance on datasets with minimal building disparity simultaneously.",bi-temporal high-resolution remote sensing images,building change detection,disparity-aware Siamese network,,,,,,,,,,,,,,,,,,
