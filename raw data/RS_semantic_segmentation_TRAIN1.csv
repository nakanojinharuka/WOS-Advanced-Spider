,author1,author2,author3,author4,title,conference,publish time,citation,abstract,keyword1,keyword2,keyword3,keyword4,keyword5,author5,author6,author7,author8,journal/book,author9,keyword6,keyword7,keyword8,keyword9,keyword10,keyword11,keyword12,keyword13,keyword14,author10,author11,author12,author13,author14,author15,author16,author17,author18,author19,author20,author21,keyword15
Row_1,"Ayala, C.","Sesma, R.","Aranda, C.","Galar, M.",DIFFUSION MODELS FOR REMOTE SENSING IMAGERY SEMANTIC SEGMENTATION,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,6,"Denoising Diffusion Probabilistic Models have exhibited impressive performance for generative modelling of images. This paper aims to explore the potential of diffusion models for semantic segmentation tasks in the context of remote sensing. The major challenge of employing these models for semantic segmentation tasks is the generative nature of the model, which produces an arbitrary segmentation mask from a random noise input. Therefore, the diffusion process needs to be constrained to produce a segmentation mask that matches the target image. To address this issue, the denoising process is conditioned by utilizing the input image as a reference. In the experimental study, the proposed model is compared against other state-of-the-art semantic segmentation architectures using the Massachusetts Buildings Aerial dataset. The results of this study provide valuable insights into the potential of diffusion models for semantic segmentation tasks in the field of remote sensing.",Denoising Diffusion Probabilistic Models,Semantic Segmentation,Remote Sensing,Building Segmentation,Uncertainty Estimation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_2,"Tao, Chongxin","Meng, Yizhuo","Li, Junjie","Yang, Beibei",MSNet: multispectral semantic segmentation network for remote sensing images,,DEC 31 2022,19,"In the research of automatic interpretation of remote sensing images, semantic segmentation based on deep convolutional neural networks has been rapidly developed and applied, and the feature segmentation accuracy and network model generalization ability have been gradually improved. However, most of the network designs are mainly oriented to the three visible RGB bands of remote sensing images, aiming to be able to directly borrow the mature natural image semantic segmentation networks and pre-trained models, but simultaneously causing the waste and loss of spectral information in the invisible light bands such as near-infrared (NIR) of remote sensing images. Combining the advantages of multispectral data in distinguishing typical features such as water and vegetation, we propose a novel deep neural network structure called the multispectral semantic segmentation network (MSNet) for semantic segmentation of multi-classified feature scenes. The multispectral remote sensing image bands are split into two groups, visible and invisible, and ResNet-50 is used for feature extraction in both coding stages, and cascaded upsampling is used to recover feature map resolution in the decoding stage, and the multi-scale image features and spectral features from the upsampling process are fused layer by layer using the feature pyramid structure to finally obtain semantic segmentation results. The training and validation results on two publicly available datasets show that MSNet has competitive performance. The code is available: https://github.com/taochx/MSNet.",Multispectral remote sensing images,spectral feature,feature fusion,semantic segmentation,,"Hu, Fengmin","Li, Yuanxi","Cui, Changlu","Zhang, Wen",GISCIENCE & REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_3,"Hua, Wenyi","Liu, Jia","Liu, Fang","Zhang, Wenhua",STAIR FUSION NETWORK FOR REMOTE SENSING IMAGE SEMANTIC SEGMENTATION,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"Semantic segmentation of very high spatial resolution remote sensing images plays a vital role in many fields, such as land resource management, urban planning, and biosphere monitoring. Due to the scare variance between different types of regions, it is important to fully utilize multi-scale features. Moreover, with the complexity of some ground objects, global semantic information should be specially considered. As a consequence, in this paper, we propose a stair fusion network to further refine and fuse low-level and high-level features. In addition, we propose a global information enhancement module (GIEM) to extract global semantic information from the high-level features and reduce the length of delivery chain from them to the final results via a skip connection. Experimental results demonstrate the effectiveness of our model.",semantic segmentation,feature fusion,remote sensing,,,"An, Jiaqi",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_4,"Gao, Liang","Qian, Yurong","Liu, Hui","Zhong, Xiwu",SRANet: semantic relation aware network for semantic segmentation of remote sensing images,,JAN 1 2022,5,"Remote sensing images contain complex feature information, and traditional convolutional networks cannot effectively model the contextual relationships. To address this problem, we propose a semantic segmentation network for remote sensing images based on semantic relationship aware. We construct the semantic relationship aware module to obtain the global semantic information of remote sensing images by self-attention. In addition, the separable space convergence pyramid module was constructed to effectively utilize the feature information in the high-level feature maps. By separable convolution with different dilation rates, the network can acquire multiscale semantic information. Our semantic relation aware network (SRANet) improves the overall accuracy by 0.33% over the benchmark network in the Vaihingen dataset and by 0.42% in the Potsdam dataset. The class activation maps show that the SRANet has ideal activation responses for targets at different scales in images. Furthermore, our SRANet can produce competitive segmentation performance compared with other state-of-the-art segmentation networks. (C) 2022 Society of Photo-Optical Instrumentation Engineers (SPIE)",self attention,remote sensing image,semantic segmentation,context information,,"Xiao, Zhengqing",,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_5,"Wei, Xin","Guo, Yajing","Gao, Xin","Yan, Menglong",A NEW SEMANTIC SEGMENTATION MODEL FOR REMOTE SENSING IMAGES,2017 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),2017,12,"Semantic segmentation for remote sensing images is a critical process in the workflow of object-based image analysis. Recently, convolutional neural networks(CNNs) are powerful visual models that yield hierarchies of features. In this paper, we propose a deep convolutional encoder-decoder model for remote sensing images segmentation. Specifically, we rely on the encoder network to extract the high-level semantic feature of ultra-high resolution images and the decoder network is employed to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise labeling. Also the fully connected conditional random field (CRF) is integrated into the model so that the network can be trained end-to-end. Experiments on the Vaihingen dataset demonstrate that our model can make promising performance.",Convolutional neural networks,conditional random field,semantic segmentation,remote sensing images,,"Sun, Xian",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_6,"Zhang, Xiangrong","Pan, Xian","Hou, Biao","Jiao, Licheng",Semantic structure tree with application to remote sensing image segmentation,IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XVI,2010,0,"This paper presents a new method based on Semantic Structure Tree (SST) for remote sensing image segmentation, in which, the semantic image analysis is used to construct the SST of the image. The leaves of the SST represent the semantics of the image and serve as human semantic understanding of the image. The root of the tree is the whole image. The SST uses grammar rules to construct a hierarchy structure of the image and gives a complete high-level semantics contents description of the image. Experimental results show that the tree can give efficient description of the semantic content of the remote sensing image, and can be well used in remote sensing image segmentation.",image semantics,image grammar,semantic structure tree,remote sensing image segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_7,"Xu, Zhe","Geng, Jie","Jiang, Wen",,MMT: Mixed-Mask Transformer for Remote Sensing Image Semantic Segmentation,,2023,9,"Remote sensing image semantic segmentation is a crucial step in the intelligent interpretation of remote sensing. Most of the current approaches are based on the attention mechanism to enhance long-range representations. However, these works ignore the key problem of foreground-background imbalance, and their performances encounter a bottleneck. In this article, we introduce mask classification into remote sensing image interpretation for the first time and propose a novel mixed-mask Transformer (MMT) for remote sensing image semantic segmentation. Specifically, we propose a mixed-mask attention mechanism, a simple but effective module, which assists the network to learn more explicit intraclass and interclass correlations by capturing long-range interdependent representations. In addition, a progressive multiscale learning strategy (MSL) is proposed to solve the problem of large-scale-varied targets in remote sensing images, which integrates semantic and visual representations of different scale targets by efficiently utilizing large-scale feature maps in Transformer. Experimental results show that the proposed MMT exceeds the existing alternative approaches and achieves state-of-the-art performance on three semantic segmentation datasets.",Attention mechanism,foreground-background imbalance,remote sensing,semantic segmentation,Transformer,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_8,"Chen, Yuxing","Bruzzone, Lorenzo",,,TOWARD OPEN-WORLD SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"In this work, we address the challenge of open-world semantic segmentation for remote sensing (RS) images, which involves segmenting arbitrary objects in images using open RS data. Previous efforts in open-world segmentation mostly focus on Internet-scale paired image-text data with rich vocabulary of concepts. However, these works cannot be directly transferred to RS domain due to the lack of large-scale RS data-text pairs and the corresponding annotations. To overcome this limitation, we propose using text descriptions and annotations from OpenStreetMap as a source of supervision while using images from satellite images. We utilize a conditional Unet model to predict segmentation masks given a text description, and leverage the rich information contained in a pretrained CLIP model to align the images and the corresponding text embeddings using a contrastive loss. Our experimental results demonstrate the potential of open-world segmentation on open RS data.",Semantic Segmentation,Open-world,Open Remote Sensing Data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_9,"Liu, Yuheng","Mei, Shaohui","Zhang, Shun","Wang, Ye",SEMANTIC SEGMENTATION OF HIGH-RESOLUTION REMOTE SENSING IMAGES USING AN IMPROVED TRANSFORMER,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,5,"Semantic segmentation has been widely researched for high level analysis of High Spatial Resolution (HSR) remote sensing images, where Convolutional Neural Network (CNN) is the mainstream method. However, the transformer with attention mechanism has its unique capacity of extracting global information which is generally ignored by CNN models. In this paper, a Swin Transformer with UPer head (STUP) is proposed to tackle with semantic segmentation problem on a challenging remote sensing land-cover dataset called LoveDA, which owns complex background samples and inconsistent classes distributions. The proposed STUP combines the Swin Transformer with Uper Head in the form of an encoder-decoder structure, to extract features of HSR images for segmentation. Furthermore, Focal Loss is adopted to handle the unbalanced distribution problem in the training step. Experimental results demonstrate that the proposed STUP clearly outperforms several state-of-the-art models.",Remote Sensing,High Spatial Resolution,Transformer,Semantic Segmentation,,"He, Mingyi","Du, Qian",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_10,"Zhang, Changxing","Bai, Xiangyu","Wang, Dapeng","Zhou, KeXin",Context Aggregation Network for Remote Sensing Image Semantic Segmentation,,SEP 2024,0,"In recent years, remote sensing technology has been widely applied in various industries, and semantic segmentation of remote sensing images has attracted much attention. Due to the complexity and special characteristics of remote sensing images, multi-scale object detection and accurate object localization are important challenges in remote sensing image semantic segmentation. Therefore, this paper proposes a context aggregation network (CANet). The design of CANet is influenced by advanced technologies such as attention mechanisms and feature fusion and enhancement. This network first introduces nested dilated residual module (NDRM), which can fully utilize the features extracted by the backbone network. Then, improved integrated successive dilation module (IISD) is proposed to effectively aggregate a series of contextual information scales. Next, Swim Transformer module is embedded to provide global contextual information. Finally, multi-resolution fusion module (MRFM) is proposed, allowing the comprehensive fusion of feature layers from different stages of the encoder, preserving more semantic and detailed information. The experimental results show that CANet outperforms other advanced models on the Potsdam and Vaihingen datasets.",Remote sensing,semantic segmentation,context,swin transformer,,,,,,INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,
Row_11,"Liu, Shuo","Ding, Wenrui","Liu, Chunhui","Liu, Yu",ERN: Edge Loss Reinforced Semantic Segmentation Network for Remote Sensing Images,,SEP 2018,70,"The semantic segmentation of remote sensing images faces two major challenges: high inter-class similarity and interference from ubiquitous shadows. In order to address these issues, we develop a novel edge loss reinforced semantic segmentation network (ERN) that leverages the spatial boundary context to reduce the semantic ambiguity. The main contributions of this paper are as follows: (1) we propose a novel end-to-end semantic segmentation network for remote sensing, which involves multiple weighted edge supervisions to retain spatial boundary information; (2) the main representations of the network are shared between the edge loss reinforced structures and semantic segmentation, which means that the ERN simultaneously achieves semantic segmentation and edge detection without significantly increasing the model complexity; and (3) we explore and discuss different ERN schemes to guide the design of future networks. Extensive experimental results on two remote sensing datasets demonstrate the effectiveness of our approach both in quantitative and qualitative evaluation. Specifically, the semantic segmentation performance in shadow-affected regions is significantly improved.",CNN,deep learning,edge loss reinforced network,remote sensing,semantic segmentation,"Wang, Yufeng","Li, Hongguang",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_12,"Feng, Mingzhe","Sun, Xin","Dong, Junyu","Zhao, Haoran",Gaussian Dynamic Convolution for Semantic Segmentation in Remote Sensing Images,,NOV 2022,4,"Different scales of the objects pose a great challenge for the segmentation of remote sensing images of special scenes. This paper focuses on the problem of large-scale variations of the target objects via a dynamical receptive field of the deep network. We construct a Gaussian dynamic convolution network by introducing a dynamic convolution layer to enhance remote sensing image understanding. Moreover, we propose a new Gaussian pyramid pooling (GPP) for multi-scale object segmentation. The proposed network can expand the size of the receptive field and improve its efficiency in aggregating contextual information. Experiments verify that our method outperforms the popular semantic segmentation methods on large remote sensing image datasets, including iSAID and LoveDA. Moreover, we conduct experiments to demonstrate that the Gaussian dynamic convolution works more effectively on remote sensing images than other convolutional layers.",remote sensing,semantic segmentation,deep learning,convolutional neural networks,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_13,"Liu, Keming","Liu, Fang","Liu, Jia","Xiao, Liang",UNSUPERVISED DOMAIN ADAPTION FOR REMOTE SENSING SEMANTIC SEGMENTATION WITH SELF-ATTENTION MECHANISM,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"The domain shift between the source and target domains limits the performance of traditional convolutional neural networks (CNNs) for feature extraction in remote sensing tasks. We propose an image translation network that uses generative adversarial networks (GANs) to transfer spectral distributions from training to test data, enhancing cross-domain semantic segmentation. Our approach fine-tunes the DeepLab-V3 framework on synthetic training data generated by the proposed network. Experimental results show improved performance in cross-domain semantic segmentation tasks for remote sensing images.",Domain adaption,remote sensing,semantic segmentation,image translation,generative adversarial networks,"Tang, Xu",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_14,"Zhang, Xiaoqin","Xiao, Zhiheng","Li, Dongyang","Fan, Mingyu",Semantic Segmentation of Remote Sensing Images Using Multiscale Decoding Network,,SEP 2019,27,"In this letter, we propose a practical convolutional neural network architecture for semantic pixelwise segmentation of remote sensing images, named Multiscale Decoding Network. The proposed method is built on the success of fully convolutional networks (FCNs) and the transfer of pretrained networks. The decoding network of our architecture utilizes the combination of three paths, namely, unpooling path, transposed convolution path, and dilated convolution path, in the form of an inception module. The whole network is trained in the end-to-end manner and the parameters of the three paths are learned automatically. Since the proposed method transfers the feature of pretrained networks and has three simplified decoding paths with fewer parameters, it requires less training data and training time. Compared with the classical networks FCN, SegNet, and U-net, our network shows better performance on remote sensing images segmentation.",Dilated convolution,remote sensing,semantic segmentation,,,"Zhao, Li",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,
Row_15,"Dimitrovski, Ivica","Spasev, Vlatko","Loshkovska, Suzana","Kitanovski, Ivan",U-Net Ensemble for Enhanced Semantic Segmentation in Remote Sensing Imagery,,JUN 2024,3,"Semantic segmentation of remote sensing imagery stands as a fundamental task within the domains of both remote sensing and computer vision. Its objective is to generate a comprehensive pixel-wise segmentation map of an image, assigning a specific label to each pixel. This facilitates in-depth analysis and comprehension of the Earth's surface. In this paper, we propose an approach for enhancing semantic segmentation performance by employing an ensemble of U-Net models with three different backbone networks: Multi-Axis Vision Transformer, ConvFormer, and EfficientNet. The final segmentation maps are generated through a geometric mean ensemble method, leveraging the diverse representations learned by each backbone network. The effectiveness of the base U-Net models and the proposed ensemble is evaluated on multiple datasets commonly used for semantic segmentation tasks in remote sensing imagery, including LandCover.ai, LoveDA, INRIA, UAVid, and ISPRS Potsdam datasets. Our experimental results demonstrate that the proposed approach achieves state-of-the-art performance, showcasing its effectiveness and robustness in accurately capturing the semantic information embedded within remote sensing images.",remote sensing imagery,U-Net,ensemble learning,semantic segmentation,land cover,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_16,"He, Chu","Li, Shenglin","Xiong, Dehui","Fang, Peizhang",Remote Sensing Image Semantic Segmentation Based on Edge Information Guidance,,MAY 2020,53,"Semantic segmentation is an important field for automatic processing of remote sensing image data. Existing algorithms based on Convolution Neural Network (CNN) have made rapid progress, especially the Fully Convolution Network (FCN). However, problems still exist when directly inputting remote sensing images to FCN because the segmentation result of FCN is not fine enough, and it lacks guidance for prior knowledge. To obtain more accurate segmentation results, this paper introduces edge information as prior knowledge into FCN to revise the segmentation results. Specifically, the Edge-FCN network is proposed in this paper, which uses the edge information detected by Holistically Nested Edge Detection (HED) network to correct the FCN segmentation results. The experiment results on ESAR dataset and GID dataset demonstrate the validity of Edge-FCN.",remote sensing image,semantic segmentation,edge information,Edge-FCN,,"Liao, Mingsheng",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_17,Shi Fangxing,Zhou Line,Zhu Daming,Fu Zhitao,DSNet-Based Remote Sensing Image Semantic Segmentation Method,,MAR 2023,0,"In view of the problems that the traditional neural network model tends to ignore difficult samples due to the unbalanced classification of remote sensing image semantic segmentation data, and the reasoning results are hollow and the segmentation accuracy decreases, a drill-shaped neural network semantic segmentation method is proposed. First, a new bridge module is defined to fuse the shallow and deep feature information, thus more building details can be captured by the network; second, in the deep learning segmentation model training, the multi loss function is used to improve the extraction of difficult sample information; finally, to balance the differences of category training, the feature information is extracted from remote sensing images at multiple levels, and the segmentation accuracy is improved. The experimental results show that the average intersection to union ratio of the proposed method reaches 0. 849, the building missing rate and wrong recognition rate are less, and the segmentation accuracy is improved compared with the existing methods.",remote sensing,deep learning,semantic segmentation,category imbalance,DSNet,,,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,,,,,,,,,,,,
Row_18,"Sun, Deyan","Chen, Wei","Liu, Hai","Chen, Dufeng",Attention Dual Adversarial Remote Sensing Image Semantic Segmentation,,2024,0,"Existing semi-supervised remote sensing image semantic segmentation methods neglect to improve the stability of the adversarial network, so that the adversarial network cannot be effectively used to assist segmentation network training, which limits the further improvement of semantic segmentation accuracy. To this end, this paper first introduces a dual confrontation network, and plays a three-way game with the generator to make the network converge as soon as possible, and combines the vertical and cross attention network to propose an attention dual confrontational semantic segmentation model for remote sensing images. The model can not only use dual confrontation training to improve the stability of the network, but also use the global context relationship of pixels to predict by introducing an attention mechanism, thereby improving the accuracy of remote sensing image semantic segmentation. The experimental results on the public remote sensing data set show that the MIOU of this method on US2D dataset reached 69.65%, which are higher than the existing fully-supervised and semi-supervised methods. The effectiveness of the method proposed in this paper is verified.",Remote Sensing Image,Dual Adversarial Networks,Cross Attention,Semantic Segmentation,Semi-supervised Learning,"Wang, Zehua","Wu, Yuliang","Xu, Tingting","Zhu, Pengcheng","ADVANCED INTELLIGENT COMPUTING TECHNOLOGY AND APPLICATIONS, PT I, ICIC 2024","Wang, Jiaqi",,,,,,,,,,,,,,,,,,,,,,
Row_19,"Dong, Zhe","Liu, Tianzhu","Gu, Yanfeng",,Spatial and Semantic Consistency Contrastive Learning for Self-Supervised Semantic Segmentation of Remote Sensing Images,,2023,11,"A critical requirement for the success of supervised deep learning lies in having numerous annotated images, which is often challenging to fulfill in remote sensing semantic segmentation tasks. Self-supervised contrastive learning (CL) offers a strategy for learning general feature representations by pretraining neural networks on vast amounts of unlabeled data and subsequently fine-tuning them on downstream tasks with limited annotations. However, the vast majority of CL methods are designed based on instance discriminative pretext tasks, focusing solely on learning the global representation of the entire image while disregarding the essential spatial and semantic correlations crucial for semantic segmentation tasks. To address the above issues, in this article, we propose a spatial and semantic consistency CL (SSCCL) framework for the semantic segmentation task of remote sensing images. Specifically, a consistency branch in SSCCL is designed to learn feature representations with spatial and semantic consistency by maximizing the similarity of the overlapping regions of the two augmented views. In addition, an instance branch is introduced to learn global representations by enforcing the similarity of two augmented views from one image. Through the integration of the consistency branch and instance branch, the proposed SSCCL framework can learn robust and informative feature representations for semantic segmentation in remote sensing scenarios. The proposed method was evaluated on three publicly available remote sensing semantic segmentation datasets, and the experiment results show that our method achieves superior segmentation performance with limited annotations compared to state-of-the-art CL methods as well as the ImageNet pretraining method.",Contrastive learning (CL),remote sensing images,self-supervised,semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_20,"Zhang, Junxi","Chen, Zhenzhong","Liu, Shan",,Remote Sensing Image Coding for Machines on Semantic Segmentation via Contrastive Learning,,2024,0,"Due to the huge data volume of high-resolution remote sensing imagery (RSI) and limited transmission bandwidth, RSIs are typically compressed for efficient transmission and storage. However, most of the existing compression algorithms are developed based on optimizing for the human perceptual that are not suitable for remote sensing image applications where RSIs are usually used for machine interpretation tasks, such as semantic segmentation for ground-object recognition. In this article, we propose an image coding for machines (ICMs) paradigm based on contrastive learning in a fully supervised manner to boost semantic segmentation of compressed RSIs. Specifically, we build an end-to-end compression framework to make full use of the global semantic information by clustering intracategory projected embeddings and spacing intercategory embeddings apart, to compensate for the loss of feature discriminability during the compression process and reconstruct the decision boundaries between different categories. Compared to the state-of-the-art image compression methods, our proposed method significantly improves the performance of semantic segmentation on the remote sensing labeling benchmark datasets.",Image coding,Remote sensing,Semantic segmentation,Feature extraction,Contrastive learning,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Object segmentation,Transform coding,Standards,Codecs,Bit rate,image coding for machines (ICMs),remote sensing interpretation,semantic segmentation,,,,,,,,,,,,,,
Row_21,"Ao, Wei","Zheng, Shunyi","Meng, Yan",,SEMPNet: enhancing few-shot remote sensing image semantic segmentation through the integration of the segment anything model,,DEC 31 2024,0,"Few-shot semantic segmentation has attracted increasing attention due to its potential for low dependence on annotated samples. While extensively explored in the computer vision community, these techniques are primarily designed for natural images, resulting in limited generalization to remote sensing images. In contrast to the mostly individual and distinct objects presented in natural images, remote sensing images often feature clustered and regular patterns of objects. To bridge this gap, we propose a novel approach for few-shot remote sensing image semantic segmentation, which takes into account the specific characteristics of remote sensing imagery. Our approach introduces a mask classification pipeline, which initially extracts all independent objects within an image and subsequently assigns specific categories to each object guided by semantic information derived from few support images. To accomplish this, a robust mask extractor is imperative. Fortunately, the impressive segment anything model (SAM) possesses the potential to fulfill this role. Leveraging its remarkable zero-shot segmentation capabilities, we present the SAM-enhanced mask parsing network (SEMPNet), a novel few-shot remote sensing image semantic segmentation model. The method generates a set of masks for each image using SAM, transforming the segmentation task into a mask classification problem. To precisely classify each mask, we calculate pixel-wise correlations between each mask and the support features through cross-image position attention. Finally, a mask parsing module is utilized to decode the correlation maps and generate the segmentation results. The experiments on two remote sensing datasets testify the superiority of our method. Our code will be available at https://github.com/TinyAway/SEMPNet.",SAM,few-shot semantic segmentation,remote sensing,mask classification,,,,,,GISCIENCE & REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_22,"Kang, Xudong","Hong, Yintao","Duan, Puhong","Li, Shutao",Fusion of hierarchical class graphs for remote sensing semantic segmentation,,SEP 2024,3,"Semantic segmentation of remote sensing images aims to assign a specific label or class to each pixel in an image, which plays an extremely important role in scene understanding. Currently, many advanced deep learning -based semantic segmentation methods have been developed. However, these methods are always based on disjoint labels to identify ground objects while ignoring the correlation (e.g., semantic, shapes, materials, etc.) among different ground objects, which limits the segmentation performance of remote sensing images. To solve this issue, we propose a hierarchical class graph for semantic segmentation of high resolution remote sensing images, which can learn structured relation among different ground objects. Specifically, first, we construct hierarchical class graphs based on different attributes and layers. Then, a three -layer hierarchical segmentation framework is developed to learn the correlation among different ground objects. Finally, a decision fusion method is designed to fuse the benefits of different hierarchical attributes and layers. More importantly, the influence of different hierarchical class graphs on the segmentation performance is detailedly analyzed. Extensive experiments on the iSAID and Vaihingen datasets reveal that all studied segmentation methods with hierarchical class graph can obtain better segmentation performance compared to ones without hierarchical class graph. The limitation of the proposed method is that the training time of the segmentation model tends to increase a bit because of considering the correlation among different ground objects.",Remote sensing images,Semantic segmentation,Hierarchical class representation,,,,,,,INFORMATION FUSION,,,,,,,,,,,,,,,,,,,,,,,
Row_23,"Zhang, Yue","Yang, Ruiqi","Dai, Qinling","Zhao, Yili",Boosting Semantic Segmentation of Remote Sensing Images by Introducing Edge Extraction Network and Spectral Indices,,NOV 2023,4,"Deep convolutional neural networks have greatly enhanced the semantic segmentation of remote sensing images. However, most networks are primarily designed to process imagery with red, green, and blue bands. Although it is feasible to directly utilize established networks and pre-trained models for remotely sensed images, they suffer from imprecise land object contour localization and unsatisfactory segmentation results. These networks still need to explore the domain knowledge embedded in images. Therefore, we boost the segmentation performance of remote sensing images by augmenting the network input with multiple nonlinear spectral indices, such as vegetation and water indices, and introducing a novel holistic attention edge detection network (HAE-RNet). Experiments were conducted on the GID and Vaihingen datasets. The results showed that the NIR-NDWI/DSM-GNDVI-R-G-B (6C-2) band combination produced the best segmentation results for both datasets. The edge extraction block benefits better contour localization. The proposed network achieved a state-of-the-art performance in both the quantitative evaluation and visual inspection.",multispectral remote sensing image,deep learning,semantic segmentation,domain knowledge,edge detection,"Xu, Weiheng","Wang, Jun","Wang, Leiguang",,REMOTE SENSING,,spectral index,,,,,,,,,,,,,,,,,,,,,
Row_24,"Lu, Wanxuan","Jin, Jidong","Sun, Xian","Fu, Kun",SEMI-SUPERVISED SEMANTIC GENERATIVE NETWORKS FOR REMOTE SENSING IMAGE SEGMENTATION,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"Semi-supervised remote sensing semantic segmentation is an efficient way to increase the use of unlabeled data and cut labelling costs. The unlabeled-to-labeled data ratio is employed in more recent methods, which is very different from what is really used in practise. In this paper, we propose a semi-supervised semantic generative network for remote sensing images, introducing a self-supervised learning method to enhance the feature representation of the model when the data ratio is high. Specifically, we design a new branch for unlabeled data, which includes modules for both semantic reconstruction and appearance reconstruction. It can effectively alleviate the category confusion in complicated remote sensing image when there are few labeled data. Comprehensive experiments on the ISPRS POTSDAM dataset demonstrate that the proposed method achieves promising results.",Deep learning,Segmentation,Semi-Supervised,Remote Sensing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_25,"Zeng, Zhi","ZhU, Jiasong","Zhang, Weiye","Hua, Yuansheng",Noise-Aware Regional Contrast Semantic Segmentation of Remote Sensing Images Using Crowdsourcing Labels,,2024,0,"Remote sensing semantic segmentation enables automated identification and monitoring of land cover categories in remote sensing images, providing precise geographic information support for urban planning, resource management, environmental protection, and broader geographical and environmental research. With the advancement of deep learning, methods based on convolutional neural networks (CNNs) have significantly improved pixel-level automatic classification accuracy in remote sensing image semantic segmentation. However, this progress relies on extensive, high-quality fine annotations, which are costly and time-consuming to acquire. To address these challenges, we propose utilizing easily accessible and cost-effective crowdsourced data from OpenStreetMap (OSM) to generate learning labels for network training. However, due to the open nature of crowdsourced data, the generated labels may contain noise interference, leading to decreased network performance during training. Meanwhile, supervised contrastive learning has shown strong potential in handling noisy labels, with previous research proposing selective contrastive learning strategies to address noisy labels in classification tasks. However, such research has been limited to classification, as segmentation poses memory explosion issues. To further tackle this problem, we introduce a semantic segmentation algorithm based on crowdsourced annotation and selective contrastive learning. The core idea involves selecting confident regions in the regional dimension, allowing pixel samples within these regions to participate in subsequent learning, thereby better capturing meaningful pixel features for classification while reducing sample quantity. By leveraging contrastive learning of paired features and selecting confident pairs from noise pairs for supervised contrastive learning, we enhance intra-class compactness, thereby improving network robustness and reducing noise interference. Experimental validation on the proposed crowdsourced semantic segmentation datasets demonstrates favorable outcomes.",Remote sensing images,OSM,Noisy labels,Semantic segmentation,,,,,,"COMPUTER VISION - ECCV 2018, PT VII",,,,,,,,,,,,,,,,,,,,,,,
Row_26,"Pan, Shaoming","Tao, Yulong","Nie, Congchong","Chong, Yanwen",PEGNet: Progressive Edge Guidance Network for Semantic Segmentation of Remote Sensing Images,,APR 2021,37,"Owing to the rapid development of deep neural networks, prominent advances have been recently achieved in the semantic segmentation of remote sensing images. As the vital components of computer vision, semantic segmentation, and edge detection have strong correlation whether in the extracted features or task objective. Prior studies treated edge detection as a postprocessing operation to semantic segmentation, or they implicitly combined the two tasks. We consider that pixels around the edges are easy to be misdivided because of the prevalence of intraclass inconsistencies and interclass indistinctions, which reflect the discriminative ability of models to distinguish different classes. In this letter, we propose a multipath atrous module to first enrich the deep semantic information. Then, we combine the enhanced deep semantic information and dilated edge information generated by canny and morphological operations to obtain edge-region maps via edge-region detection module, which identifies pixels around the edges. Then, we relearn these error-prone pixels using a guidance module for the segmentation branch in a progressive guided manner. Combined with edge and segmentation branches, our progressive edge guidance network achieves an overall accuracy of 91.0% on the ISPRS Vaihingen test set, which is the new state-of-the-art result.",Image edge detection,Semantics,Image segmentation,Feature extraction,Remote sensing,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Iron,Fuses,Discriminative ability,edge detection,progressive guided,remote sensing images,semantic segmentation,,,,,,,,,,,,,,,
Row_27,"Chen, Zhong","Zhao, Jun","Deng, He",,Global Multi-Attention UResNeXt for Semantic Segmentation of High-Resolution Remote Sensing Images,,APR 2023,3,"Semantic segmentation has played an essential role in remote sensing image interpretation for decades. Although there has been tremendous success in such segmentation with the development of deep learning in the field, several limitations still exist in the current encoder-decoder models. First, the potential interdependencies of the context contained in each layer of the encoder-decoder architecture are not well utilized. Second, multi-scale features are insufficiently used, because the upper-layer and lower-layer features are not directly connected in the decoder part. In order to solve those limitations, a global attention gate (GAG) module is proposed to fully utilize the interdependencies of the context and multi-scale features, and then a global multi-attention UResNeXt (GMAUResNeXt) module is presented for the semantic segmentation of remote sensing images. GMAUResNeXt uses GAG in each layer of the decoder part to generate the global attention gate (for utilizing the context features) and connects each global attention gate with the uppermost layer in the decoder part by using the Hadamard product (for utilizing the multi-scale features). Both qualitative and quantitative experimental results demonstrate that use of GAG in each layer lets the model focus on a certain pattern, which can help improve the effectiveness of semantic segmentation of remote sensing images. Compared with state-of-the-art methods, GMAUResNeXt not only outperforms MDCNN by 0.68% on the Potsdam dataset with respect to the overall accuracy but is also the MANet by 3.19% on the GaoFen image dataset. GMAUResNeXt achieves better performance and more accurate segmentation results than the state-of-the-art models.",remote sensing,attention module,semantic segmentation,,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_28,"Mi, Li","Chen, Zhenzhong",,,Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation,,JAN 2020,109,"Semantic segmentation plays an important role in remote sensing image understanding. Great progress has been made in this area with the development of Deep Convolutional Neural Networks (DCNNs). However, due to the complexity of ground objects' spectrum, DCNNs with simple classifier have difficulties in distinguishing ground object categories even though they can represent image features effectively. Additionally, DCNN-based semantic segmentation methods learn to accumulate contextual information over large receptive fields that causes blur on object boundaries. In this work, a novel approach named Superpixel-enhanced Deep Neural Forest (SDNF) is proposed to target the aforementioned problems. To improve the classification ability, we introduce Deep Neural Forest (DNF), where the representation learning of deep neural network is conducted by a completely differentiable decision forest. Therefore, better classification accuracy is achieved by combining DCNNs with decision forests in an end-to-end manner. In addition, considering the homogeneity within superpixels and heterogeneity between superpixels, a Superpixel-enhanced Region Module (SRM) is proposed to further alleviate the noises and strengthen edges of ground objects. Experimental results on the ISPRS 2D semantic labeling benchmark demonstrate that our model significantly outperforms state-of-the-art methods thus validate the efficiency of our proposed SDNF.",Neural forest,Superpixel,Remote sensing imagery,Semantic segmentation,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_29,"Bai, Tao","Cao, Yiming","Xu, Yonghao","Wen, Bihan",Stealthy Adversarial Examples for Semantic Segmentation in Remote Sensing,,2024,1,"Deep learning methods have been proven effective in remote sensing image analysis and interpretation, where semantic segmentation plays a vital role. These deep segmentation methods are susceptible to adversarial attacks, while most of the existing attack methods tend to manipulate the image globally, leading to noticeable perturbations and chaotic segmentation. In this work, we propose a novel stealthy attack for semantic segmentation (SASS), which can largely increase the effectiveness and stealthiness from the existing attack methods on remote sensing images. SASS manipulates specific victim classes or objects of interest while preserving the original segmentation results for other classes or objects. In practice, as different inference mechanisms, overlapped inference, can be applied in segmentation, the efficacy of SASS may be degraded. To this end, we further introduce the masked SASS (MSASS), which generates augmented adversarial perturbations that only affect victim areas. We evaluate the effectiveness of SASS and MSASS using four state-of-the-art semantic segmentation models on the Vaihingen and Zurich Summer datasets. Extensive experiments demonstrate that our SASS and MSASS methods achieve superior attack performances on victim areas while maintaining high accuracies of other areas (drop less than 2%). The detection success rates of adversarial examples for segmentation, as characterized by Xiao et al., significantly drop from 97.78% for the untargeted projected gradient descent (PGD) attack to 28.71% for our MSASS method on the Zurich Summer dataset. Our work contributes to the field of adversarial attacks in semantic segmentation for remote sensing images by improving stealthiness, flexibility, and robustness. We anticipate that our findings will inspire the development of defense methods to enhance the security and reliability of semantic segmentation models against our stealthy attack.",Remote sensing,Semantic segmentation,Task analysis,Perturbation methods,Sensors,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Buildings,Semantics,Adversarial attack,deep learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,
Row_30,"Sun, Li","Zou, Huanxin","Wei, Juan","Li, Meilin",SEMANTIC SEGMENTATION OF HIGH-RESOLUTION REMOTE SENSING IMAGES BASED ON SPARSE SELF-ATTENTION,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,2,"Semantic segmentation of high-resolution optical remote sensing images is an important but challenging task. To solve the problem that many semantic segmentation networks fail to efficiently utilize global and local context information to improve the segmentation performance, this paper proposes a semantic segmentation network based on sparse self-attention (SDANet) to model the global context dependencies. Specifically, the feature maps are first divided into four regions in spatial and channel dimensions, respectively, and the divided feature maps are rearranged to form new regions. Second, the position and channel self-attention operations are performed on the rearranged regions. Third, the feature maps are restored to the original combination and the position together with channel self-attention operations are performed again to obtain the output feature maps. Finally, semantic segmentation is completed based on the output feature maps. Extensive experiments conducted on the ISPRS Vaihingen dataset demonstrate that the proposed method is superior to self-attention-based DANet, CCNet, and other general semantic segmentation networks, such as FCN, Deeplabv3+, HRNet, etc.",Semantic segmentation,self-attention,remote sensing,context modeling,,"Cao, Xu","He, Shitian","Liu, Shuo",,,,,,,,,,,,,,,,,,,,,,,,,
Row_31,"Li, Rui","Zheng, Shunyi","Zhang, Ce","Duan, Chenxi",Multiattention Network for Semantic Segmentation of Fine-Resolution Remote Sensing Images,,2022,261,"Semantic segmentation of remote sensing images plays an important role in a wide range of applications, including land resource management, biosphere monitoring, and urban planning. Although the accuracy of semantic segmentation in remote sensing images has been increased significantly by deep convolutional neural networks, several limitations exist in standard models. First, for encoder-decoder architectures such as U-Net, the utilization of multiscale features causes the underuse of information, where low-level features and high-level features are concatenated directly without any refinement. Second, long-range dependencies of feature maps are insufficiently explored, resulting in suboptimal feature representations associated with each semantic class. Third, even though the dot-product attention mechanism has been introduced and utilized in semantic segmentation to model long-range dependencies, the large time and space demands of attention impede the actual usage of attention in application scenarios with large-scale input. This article proposed a multiattention network (MANet) to address these issues by extracting contextual dependencies through multiple efficient attention modules. A novel attention mechanism of kernel attention with linear complexity is proposed to alleviate the large computational demand in attention. Based on kernel attention and channel attention, we integrate local feature maps extracted by ResNet-50 with their corresponding global dependencies and reweight interdependent channel maps adaptively. Numerical experiments on two large-scale fine-resolution remote sensing datasets demonstrate the superior performance of the proposed MANet. Code is available at https://github.com/lironui/Multi-Attention-Network.",Semantics,Image segmentation,Feature extraction,Remote sensing,Task analysis,"Su, Jianlin","Wang, Libo","Atkinson, Peter M.",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Kernel,Complexity theory,Attention mechanism,fine-resolution remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,
Row_32,Guo Zhichao,Xu Junming,Liu Aidong,,Remote sensing image semantic segmentation method based on improved Deeplabv3+,INTERNATIONAL CONFERENCE ON IMAGE PROCESSING AND INTELLIGENT CONTROL (IPIC 2021),2021,2,"In recent years, with the continuous development of remote sensing technology and computer vision technology, the semantic segmentation of remote sensing images is of great significance in terms of earth observation, urban planning, military simulation, etc. This paper proposes a remote sensing image semantic segmentation method based on improved Deeplabv3+. Firstly, the backbone network is improved. Xception is selected to replace the traditional ResNe101 as the backbone network for the improved Deeplabv3+, and the network structure is deepened and depth separable. Optimization methods such as product replacement improve the segmentation efficiency; then, in order to improve the feature extraction effect of small targets in remote sensing images, the expansion rate of the cavity convolution in the ASSP module is optimized and adjusted. The experimental results show that the improved Deeplabv3+ algorithm has achieved good segmentation results on the data set, miou reached 91.23%, pixel accuracy reached 93.31%, and F1-score reached 89.2%, which is an increase of 2.4%, 1.9% and 2.7% compared with the original Deeplabv3+. At the same time, compared with mainstream U-net and SegNet algorithms, this algorithm also has strong advantages in semantic segmentation of remote sensing images.",Semantic segmentation,Deeplabv3+,remote sensing image,deep learning,neural network,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_33,"Zi, Wenjie","Xiong, Wei","Chen, Hao","Li, Jun",SGA-Net: Self-Constructing Graph Attention Neural Network for Semantic Segmentation of Remote Sensing Images,,NOV 2021,20,"Semantic segmentation of remote sensing images is always a critical and challenging task. Graph neural networks, which can capture global contextual representations, can exploit long-range pixel dependency, thereby improving semantic segmentation performance. In this paper, a novel self-constructing graph attention neural network is proposed for such a purpose. Firstly, ResNet50 was employed as backbone of a feature extraction network to acquire feature maps of remote sensing images. Secondly, pixel-wise dependency graphs were constructed from the feature maps of images, and a graph attention network is designed to extract the correlations of pixels of the remote sensing images. Thirdly, the channel linear attention mechanism obtained the channel dependency of images, further improving the prediction of semantic segmentation. Lastly, we conducted comprehensive experiments and found that the proposed model consistently outperformed state-of-the-art methods on two widely used remote sensing image datasets.",self-constructing graph,semantic segmentation,remote sensing,,,"Jing, Ning",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_34,"Zhao, Yang","Guo, Peng","Sun, Zihao","Chen, Xiuwan",ResiDualGAN: Resize-Residual DualGAN for Cross-Domain Remote Sensing Images Semantic Segmentation,,MAR 2023,18,"The performance of a semantic segmentation model for remote sensing (RS) images pre-trained on an annotated dataset greatly decreases when testing on another unannotated dataset because of the domain gap. Adversarial generative methods, e.g., DualGAN, are utilized for unpaired image-to-image translation to minimize the pixel-level domain gap, which is one of the common approaches for unsupervised domain adaptation (UDA). However, the existing image translation methods face two problems when performing RS image translation: (1) ignoring the scale discrepancy between two RS datasets, which greatly affects the accuracy performance of scale-invariant objects; (2) ignoring the characteristic of real-to-real translation of RS images, which brings an unstable factor for the training of the models. In this paper, ResiDualGAN is proposed for RS image translation, where an in-network resizer module is used for addressing the scale discrepancy of RS datasets and a residual connection is used for strengthening the stability of real-to-real images translation and improving the performance in cross-domain semantic segmentation tasks. Combined with an output space adaptation method, the proposed method greatly improves the accuracy performance on common benchmarks, which demonstrates the superiority and reliability of ResiDualGAN. At the end of the paper, a thorough discussion is conducted to provide a reasonable explanation for the improvement of ResiDualGAN. Our source code is also available.",ResiDualGAN,UDA,remote sensing,semantic segmentation,,"Gao, Han",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_35,"Zhao, Weiheng","Cao, Jiannong","Dong, Xueyan",,Multilateral Semantic With Dual Relation Network for Remote Sensing Images Segmentation,,2024,2,"Semantic segmentation of remote sensing images is an extensively employed and demanding task. Although deep convolutional neural networks have significantly increased the accuracy of semantic segmentation, the problems of losing detailed features in segmentation and ignoring rich contextual information of images still exist. To solve these challenges, we propose a multilateral semantic with dual relation network (MSDRNet) for remote sensing images segmentation. The proposed MSDRNet consists of two parallel modules, the detail semantic module and the global semantic module, for extracting image detail and global features, respectively. Subsequently, improved spatial relation block and channel relation block are introduced in two separate parallel modules to further enhance the contextual connection of the images. Finally, a feature refinement module is added to balance the multilateral features between the features extracted from the two branches. We display the robustness and effectiveness of the proposed MSDRNet on the publicly available ISPRS Potsdam and Vaihingen datasets. We further experimented with the Gaofen image dataset, which contains information on larger scale features, to demonstrate the validity of our model. The results of extensive experiments conducted on the aforementioned three datasets show that the proposed approach outperforms several state-of-the-art semantic segmentation methods.",Attention mechanisms,deep learning,remote sensing image,semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_36,"He, Qibin","Sun, Xian","Diao, Wenhui","Yan, Zhiyuan",Transformer-induced graph reasoning for multimodal semantic segmentation in remote sensing,,NOV 2022,45,"As a large amount of earth observation data is available on a global scale, it becomes possible to apply multimodal semantic segmentation technology to remote sensing scene analysis. However, the diversity of objects in large-scale scenes and the cross-modal gap between different images are still challenging in practical applications. To address these problems, we propose a Transformer-Induced Hierarchical Graph Network (GraFNet) for multimodal semantic segmentation in remote sensing scenes, which promotes the exploration of potential intra- and inter-modal relations by introducing a new modeling paradigm. Different from existing methods, GraFNet parses multimodal remote sensing images into semantic topological graphs, and exploits the structural information of land cover categories to learn joint representations. Specifically, an attentive heterogeneous information aggregation mechanism is presented to parse diverse objects in remote sensing scenes into semantic entities, and capture modality-specific object-object interaction patterns in a topology-aware environment. In addition, modality hierarchical dependency modeling is introduced to encode the interactive representation of cross-modal objects, and distinguish the modality-specific contribution to improve cross-modal compatibility. Extensive experiments on several multimodal remote sensing datasets demonstrate that the proposed GraFNet outperforms the state-of-the-art approaches, achieving F-1/mIoU accuracy 91.1%/82.4% on the ISPRS Vaihingen dataset, 93.4%/88.4% on ISPRS Potsdam dataset, and 91.8%/84.0% on the MSAW dataset.",Graph reasoning,Hierarchical representation,Multimodal remote sensing,Semantic segmentation,Transformer,"Yin, Dongshuo","Fu, Kun",,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_37,"Sun, Xudong","Xia, Min","Dai, Tianfang",,Controllable Fused Semantic Segmentation with Adaptive Edge Loss for Remote Sensing Parsing,,JAN 2022,12,"High-resolution remote sensing images have been put into the application in remote sensing parsing. General remote sensing parsing methods based on semantic segmentation still have limitations, which include frequent neglect of tiny objects, high complexity in image understanding and sample imbalance. Therefore, a controllable fusion module (CFM) is proposed to alleviate the problem of implicit understanding of complicated categories. Moreover, an adaptive edge loss function (AEL) was proposed to alleviate the problem of the recognition of tiny objects and sample imbalance. Our proposed method combining CFM and AEL optimizes edge features and body features in a coupled mode. The verification on Potsdam and Vaihingen datasets shows that our method can significantly improve the parsing effect of satellite images in terms of mIoU and MPA.",remote sensing parsing,satellite imagery,semantic segmentation,,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_38,"Wang, Libo","Dong, Sijun","Chen, Ying","Meng, Xiaoliang",MetaSegNet: Metadata-Collaborative Vision-Language Representation Learning for Semantic Segmentation of Remote Sensing Images,,2024,0,"Semantic segmentation of remote sensing images plays a vital role in a wide range of Earth Observation applications, such as land-use land-cover (LULC) mapping, environment monitoring, and sustainable development. Driven by rapid developments in artificial intelligence, deep learning (DL) has emerged as the mainstream for semantic segmentation and has achieved many breakthroughs in the field of remote sensing. However, most DL-based methods focus on unimodal visual data while ignoring rich multimodal information involved in the real world. Nonvisual data, such as text, can gather extra knowledge from the real world, which can strengthen the interpretability, reliability, and generalization of visual models. Inspired by this, we propose a novel metadata-collaborative segmentation network (MetaSegNet) that applies vision-language representation learning for the semantic segmentation of remote sensing images. Unlike the common model structure that only uses unimodal visual data, we extract the key characteristic (e.g., the climate zone) from freely available remote sensing image metadata and transfer it into geographic text prompts via the generic ChatGPT. Then, we construct an image encoder, a text encoder, and a crossmodal attention fusion subnetwork to extract the image and text feature and apply image-text interaction. Benefiting from such a design, the proposed MetaSegNet not only demonstrates superior generalization in zero-shot testing but also achieves competitive accuracy with the state-of-the-art semantic segmentation methods on the large-scale OpenEarthMap dataset [70.4% mean intersection over union (mIoU)] and the Potsdam dataset (93.3% mean {F}1 score) as well as the LoveDA dataset (52.0% mIoU).",Metadata-collaborative learning,multimodal remote sensing,semantic segmentation,semantic segmentation,vision-language representation learning,"Fang, Shenghui","Fei, Songlin",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,vision-language representation learning,,,,,,,,,,,,,,,,,,,,,
Row_39,"Wu, Honglin","Huang, Peng","Zhang, Min","Tang, Wenlong",CTFNet: CNN-Transformer Fusion Network for Remote-Sensing Image Semantic Segmentation,,2024,4,"Remote-sensing image semantic segmentation is usually based on convolutional neural networks (CNNs). CNNs demonstrate powerful local feature extraction capabilities through stacked convolution and pooling. However, the locality of the convolution operation limits the ability of CNNs to directly extract global information. Relying on the multihead self-attention (MHSA) mechanism, transformer shows great advantages in modeling global information. In this letter, we propose a CNN-transformer fusion network (CTFNet) for remote-sensing image semantic segmentation. CTFNet applies a U-shaped encoder-decoder structure to achieve the extraction and adaptive fusion of local features and global context information. Specifically, a lightweight W/P transformer block is proposed as the decoder to obtain global context information with low complexity and connected to the encoder through the skip connection. Finally, the channel and spatial attention fusion module (AFM) is exploited to adaptively fuse deep semantic features and shallow detail features. On the Vaihingen and Potsdam datasets of the International Society for Photogrammetry and Remote Sensing (ISPRS), the effectiveness of each module is demonstrated by ablation experiments. Compared with several classical networks, our proposed CTFNet can obtain superior performance.",Adaptive fusion,global context information,remote sensing,semantic segmentation,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,
Row_40,"Zhu, Shengyu",,,,SEMANTIC SEGMENTATION FOR REMOTE SENSING IMAGES BASED ON SWIN-TRANSFORMER AND MULTISCALE FEATURE REFINEMENT,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"Thanks to the development of deep learning, semantic segmentation of remote sensing images has made great advances. However, because of the widespread complex background and variable imaging conditions, the feature consistency of the same classes and the variability of different classes are more difficult to capture, leading to the existing methods still have the space for improvement. Thus, we combine the local convolution operation and swin transformer to design a novel semantic segmentation method (STFR). The proposed STFR can improve the multiscale feature representation and capture the global context information, which consists of three steps. Firstly, the multiscale features of the image is extracted by swin transformer; then, a parallel structure is constructed based on depth-wise separable convolution to refine the deep semantic information; finally, the semantic segmentation result can be obtained. Experimental results on a public remote sensing semantic segmentation dataset indicate the the proposed STFR can obtain a great performance.",Semantic segmentation,remote sensing images,swin transformer,parallel structure,feature refinement,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_41,"Wang, Hengyou","Li, Xiao","Huo, Lian-Zhi",,Key Feature Repairing Based on Self-Supervised for Remote Sensing Semantic Segmentation,,2024,0,"As one of the fundamental issues in remote sensing, semantic segmentation has always received widespread attention. However, different from natural images, remote sensing images contain more complex category information, which poses many challenges to researchers, e.g., the lack of large-scale labeled semantic segmentation datasets on remote sensing and the accurate distinguishment of the edge areas between different classes. Recently, self-supervised methods have tried to avoid the issue of great dependency on labeled datasets. However, existing self-supervised methods were typically based on randomly masking and repairing images to learn features from unlabeled images. Random masks cannot drive the model focus on the salient information of the image, thus the learned features are not representative. In this letter, to improve the accuracy and generalization ability of the model in remote sensing semantic segmentation, we propose a key feature repairing network based on self-supervised learning (SSL), called KFRNet. KFRNet calculates the similarity between each image patch and its surrounding patches and sorts them to find the patches with more prominent feature information for masking and repairing, effective obtaining image context information. Besides, to improve the model's ability to distinguish different classes of objects, we designed an image comparison branch to obtain the category features of the image by comparing positive and negative samples. The experimental results on the POTSDAM and LoveDA datasets show that the proposed method can effectively improve segmentation accuracy. The overall accuracy (OA), mean intersection over union (MIOU), and Fscore indices reached 89.73%, 83.96%, 91.15% (POTSDAM) and 70.81%, 53.40%, 68.86% (LoveDA), even surpassing some supervised learning methods.",Remote sensing,self-supervised learning (SSL),semantic segmentation,,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,
Row_42,"Hu, Xudong","Zhang, Penglin","Zhang, Qi","Yuan, Feng",GLSANet: Global-Local Self-Attention Network for Remote Sensing Image Semantic Segmentation,,2023,19,"Learning long-range contextual dependence is important for remote sensing (RS) image segmentation in complex patterns. Meanwhile, exploring local context information is conducive to the discrimination of fine details. Only underlining either global semantic correlations or local context details is insufficient to achieve accurate segmentation. In this letter, we propose an architecture with the global-local self-attention (GLSA) mechanism, called GLSANet, which can simultaneously consider both global and local contexts for segmentations. Particularly, the GLSA mechanism consists of the global atrous self-attention (GASA) and local window self-attention (LWSA) mechanisms. GASA can learn long-range semantic relations in a gapped manner, while LWSA can locally capture contextual details. As a bridge between the two self-attention (SA) branches, a context fusion module (CFM) is further designed to adaptively integrate global and local contexts. The experiments with public datasets show that the proposed GLSANet significantly refines semantic segmentation and outperforms other competing methods.",Semantics,Convolution,Correlation,Transformers,Semantic segmentation,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Remote sensing,Computer architecture,Deep learning,remote sensing (RS),self-attention (SA),semantic segmentation,,,,,,,,,,,,,,,,
Row_43,"Ouyang, Song","Li, Yansheng",,,Combining Deep Semantic Segmentation Network and Graph Convolutional Neural Network for Semantic Segmentation of Remote Sensing Imagery,,JAN 2021,54,"Although the deep semantic segmentation network (DSSN) has been widely used in remote sensing (RS) image semantic segmentation, it still does not fully mind the spatial relationship cues between objects when extracting deep visual features through convolutional filters and pooling layers. In fact, the spatial distribution between objects from different classes has a strong correlation characteristic. For example, buildings tend to be close to roads. In view of the strong appearance extraction ability of DSSN and the powerful topological relationship modeling capability of the graph convolutional neural network (GCN), a DSSN-GCN framework, which combines the advantages of DSSN and GCN, is proposed in this paper for RS image semantic segmentation. To lift the appearance extraction ability, this paper proposes a new DSSN called the attention residual U-shaped network (AttResUNet), which leverages residual blocks to encode feature maps and the attention module to refine the features. As far as GCN, the graph is built, where graph nodes are denoted by the superpixels and the graph weight is calculated by considering the spectral information and spatial information of the nodes. The AttResUNet is trained to extract the high-level features to initialize the graph nodes. Then the GCN combines features and spatial relationships between nodes to conduct classification. It is worth noting that the usage of spatial relationship knowledge boosts the performance and robustness of the classification module. In addition, benefiting from modeling GCN on the superpixel level, the boundaries of objects are restored to a certain extent and there are less pixel-level noises in the final classification result. Extensive experiments on two publicly open datasets show that DSSN-GCN model outperforms the competitive baseline (i.e., the DSSN model) and the DSSN-GCN when adopting AttResUNet achieves the best performance, which demonstrates the advance of our method.",deep semantic segmentation network (DSSN),graph convolutional neural network (GCN),remote sensing (RS),semantic segmentation,spatial relationship,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_44,"Zheng, Chengyu","Nie, Jie","Wang, Zhaoxin","Song, Ning",High-Order Semantic Decoupling Network for Remote Sensing Image Semantic Segmentation,,2023,15,"Low-order features based on convolution kernel are easy to be distorted when encountering dramatic view angle transformation and atmospheric scattering in remote sensing (RS) images. To address this concern, this article first proposes to operate semantic segmentation of RS images based on the high-order information, which can represent the relative relationship of low-order features and is robust and stable when suffering feature distortion. Besides, semantic decouples have recently been well researched and have achieved significant improvement in image understanding. Thus, in this article, a high-order semantic decoupling network (HSDN) is proposed to disentangle features by semantics based on high-order features. Specifically, HSDN first represents each pixel by calculating the pixel-level affinity as a high-order feature and then clusters these pixels into different semantics. Afterward, an attention-like mask generation module is designed for both intra-semantic and inter-semantic groups, leading to three kinds of masks, including the semantic decoupling mask (SDM), which utilizes each high-order cluster centroid as a mask to compact features intracluster and expand different interclusters, so as to improve semantic disentangle performance to a better extent; semantic enhancement mask (SEM), which records pixel-level relative correlation within a class to sufficiently exploit high-order features and could enhance feature robustness; and boundary supplementary mask (BSM), which aims to process borderline pixels to reduce cluster errors. Finally, by applying masks on pixels both within classes and on borderlines, semantic decoupled features are generated and concatenated to realize segmentation. The quantitative and qualitative experiments are conducted on two large-scale fine-resolution RS image datasets to demonstrate the significant performance of adopting high-order representation. Besides, we also implement numerous experiments to validate the effectiveness of the proposed semantic decouple framework in dealing with complicated and distortion-prone RS image segmentation tasks.",Semantics,Remote sensing,Semantic segmentation,Feature extraction,Scattering,"Wang, Jingyu","Wei, Zhiqiang",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Convolution,Task analysis,High-order representation (HR),remote sensing (RS),semantic decoupling,semantic segmentation,,,,,,,,,,,,,,,,
Row_45,"Kotaridis, Ioannis","Lazaridou, Maria",,,Remote sensing image segmentation advances: A meta-analysis,,MAR 2021,150,"The advances in remote sensing sensors during the last two decades have led to the production of very high spatial resolution multispectral images. In order to adapt to this rapid development and handle these data, object-based analysis has emerged. A critical part of such an analysis is image segmentation. The selection of optimal segmentation parameters' values generates a qualitative segmentation output and has a direct impact on feature extraction and subsequent overall classification accuracy. Even though several image segmentation methods have been developed and suggested in the literature, each of them has advantages and disadvantages. This article presents the conceptual characteristics of image segmentation methods with a special focus on semantic segmentation. In addition, a meta-analysis was conducted through a comprehensive review of recent image segmentation case studies. It includes statistics and quantitative data regarding the applied segmentation algorithm, the software utilized and the data source among others. Since there is no miraculous segmentation algorithm, the statistical results depict only the recent trend. Finally, a few interesting subjects are addressed, including identification of current problems, image segmentation on non-traditional data and hot topics for future research.",Image segmentation,Remote sensing,Semantic segmentation,Meta-analysis,Review,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_46,"Zhang, Di","Yue, Peicheng","Yan, Yuhang","Niu, Qianqian",Multi-Source Remote Sensing Images Semantic Segmentation Based on Differential Feature Attention Fusion,,DEC 2024,0,"Multi-source remote sensing image semantic segmentation can provide more detailed feature attribute information, making it an important research field for remote sensing intelligent interpretation. However, due to the complexity of remote sensing scenes and the feature redundancy caused by multi-source fusion, multi-source remote sensing semantic segmentation still faces some challenges. In this paper, we propose a multi-source remote sensing semantic segmentation method based on differential feature attention fusion (DFAFNet) to alleviate the problems of difficult multi-source discriminant feature extraction and the poor quality of decoder feature reconstruction. Specifically, we achieve effective fusion of multi-source remote sensing features through a differential feature fusion module and unsupervised adversarial loss. Additionally, we improve decoded feature reconstruction without introducing additional parameters by employing an attention-guided upsampling strategy. Experimental results show that our method achieved 2.8% and 2.0% mean intersection over union (mIoU) score improvements compared with the competitive baseline algorithm on the available US3D and ISPRS Potsdam datasets, respectively.",multi-source fusion,remote sensing semantic segmentation,differential feature,attention-guided upsampling,,"Zhao, Jiaqi","Ma, Huifang",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_47,"Jiang, Yi","Lu, Wanxuan","Guo, Zhi",,MULTI-STAGE SEMI-SUPERVISED TRANSFORMER FOR REMOTE SENSING SEMANTIC SEGMENTATION WITH VARIOUS DATA AUGMENTATION,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,0,"Existing research in semantic segmentation heavily relies on numerous manually annotated data, while the vast amount of unlabeled data still needs to be fully utilized. To address this challenge, this paper introduces a novel multi-stage semi-supervised method for remote sensing semantic segmentation, building upon a modified classical self-training scheme that leverages pseudo-labels. By dividing the data augmentation process into two stages, we employ various data augmentation strategies and balance the size of labels and pseudo-labels validated through rigorous experimentation, which can alleviate the student model from overfitting pseudo-labels. Furthermore, we also explore the efficacy of the Vision Transformer model in semi-supervised semantic segmentation, leading to further performance enhancements. The experimental results show that our semi-supervised remote sensing semantic segmentation method exhibits a more intuitive structure, easier deployment, and superior performance.",Semi-supervised learning (SSL),selftraining,vision transformer,semantic segmentation,remote sensing (RS),,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_48,"Sun, Deyan","Liu, Hai","Chen, Wei","Zhu, Pengcheng",Multi-scale Self-attention Based Semi-supervised Remote Sensing Image Semantic Segmentation,,2024,0,"Remote sensing image semantic segmentation has been an important research direction in the interpretation, due to the huge scale difference between target objects in the remote sensing images and the loss of spatial details in the semantic segmentation, the existing semi-supervised remote sensing image semantic segmentation methods often provide compromised performance. This paper proposes a multi-scale self-attention based semi-supervised remote sensing image semantic segmentation model, which consists of a multi-scale self-attention based generator and a confidence map based discriminator network. The multi-scale mutual attention module is introduced to obtain the pixel relations between different scale images and balance the weights of different target objects in order to improve the segmentation performance of small-scale objects. Experimental results on public remote sensing data sets show that the MIOU of our proposal on CCF2015 and US2D increases to 80.74% and 66.94% respectively, superior to the state-of-the-art semi-supervised methods.",Remote Sensing Image,Semantic Segmentation,Multi-scale Attention,Semi-supervised,,"Chen, Dufeng","Liu, Jueting","Wang, Jiaqi","Wu, Yuliang","ADVANCED INTELLIGENT COMPUTING TECHNOLOGY AND APPLICATIONS, PT VI, ICIC 2024",,,,,,,,,,,,,,,,,,,,,,,
Row_49,"Xiang, Shao","Xie, Quangqi","Wang, Mi",,Semantic Segmentation for Remote Sensing Images Based on Adaptive Feature Selection Network,,2022,23,"Semantic segmentation plays a vital role in the segmentation of remote sensing field for its wide range of applications. The major current method for segmentation of remotely sensed imagery is using multiple scales strategy to improve the performance of segmentation networks. However, the ground object with uncertain scale in high-resolution aerial imagery is difficult to be segmented with conventional models. To address this problem, an adaptive feature selection module is designed, in which attention module learns weight contributions of each feature blocks in different scales. We employ the pyramid scene parsing network (PSPNet), DeepLabV3, and U-Net with the proposed module to conduct experiments on two benchmarks (the Vaihingen set and the WHU Building data set). The experimental results and comprehensive analysis validate the efficiency and practicability of the proposed method in semantic segmentation of remote sensing images.",Image segmentation,Semantics,Remote sensing,Feature extraction,Training,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Adaptation models,Buildings,Adaptive feature selection (AFS),remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,
Row_50,"Mu, Juwei","Zhou, Shangbo","Sun, Xingjie",,PPMamba: Enhancing Semantic Segmentation in Remote Sensing Imagery by SS2D,,2025,0,"Remote sensing semantic segmentation is a critical technology in the field of remote sensing image processing, with broad applications in environmental monitoring, urban planning, disaster assessment, and resource exploration. Despite the transformative impact of convolutional neural networks (CNNs) on this domain, CNN-based methods often encounter limitations due to their localized receptive fields, which struggle to capture the global context necessary for accurate segmentation in complex remote sensing imagery. In this letter, a novel approach is presented for remote sensing semantic segmentation using a mamba-based model named PPmamba. The PPmamba model integrates Resblock and PPmamba within an encoder-decoder framework to effectively capture both local and global contextual information from high-resolution remote sensing images. Leveraging the strengths of the Mamba architecture, our model employs selective scanning to efficiently process long sequences, overcoming the limitations of traditional CNNs and transformers in handling large-scale images with complex scenes. Extensive experiments on two benchmark datasets (Potsdam and Vaihingen) demonstrate the superiority of our PPmamba model against state-of-the-art models, achieving significant improvements in segmentation results. The codes will be available at https://github.com/Jerrymo59/PPMambaSeg.",Feature extraction,Remote sensing,Training,Semantic segmentation,Decoding,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Convolutional neural networks,Sensors,Semantics,Deep learning,Computer architecture,Remote sensing image,semantic segmentation,visual space state model,,,,,,,,,,,,,,
Row_51,"Nie, Jie","Wang, Zhaoxin","Liang, Xinyue","Yang, Chenxue",Semantic Category Balance-Aware Involved Anti-Interference Network for Remote Sensing Semantic Segmentation,,2023,5,"In recent years, semantic segmentation technology plays an important role in land resource management tasks. However, many classic semantic segmentation methods often fail to obtain satisfactory results for remote sensing images with a large amount of interference information. To improve this situation, we propose semantic category balance-aware involved anti-interference network (SCBANet). SCBANet has an encoder-decoder structure similar to DeeplabV3+. On this basis, we propose clustering-guided semantic decoupling module (CGSDM), consistency-based anti-interference feature extraction module (CAFEM), relevance-based anti-interference feature extraction module (RAFEM), and optional decoder module based on semantic category balance (ODMSCB) to improve the accuracy of semantic segmentation. CGSDM aims to obtain the information of different semantic categories through K -means clustering algorithm. CAFEM performs an average operation on the feature vectors in each semantic category to obtain semantic consistency information. RAFEM deeply excavates the information contained in each semantic category through the modeling method with self-attention mechanism as the core, making the relationship between pixels within each semantic category to be better understood by the model. ODMSCB classifies the feature map according to the balance of different semantic categories, so that different decoders can be applied to feature maps with different semantic category balance. These four parts complement each other, greatly improving the model's anti-interference ability while also enhancing the ability to handle category imbalance issue. We compared our method with several of the most advanced deep learning methods on the Vaihingen and Potsdam datasets. The final results demonstrate the superiority of our method.",Semantics,Feature extraction,Remote sensing,Semantic segmentation,Decoding,"Zheng, Chengyu","Wei, Zhiqiang",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Interference,Convolution,Remote sensing images,semantic category,semantic segmentation,strong anti-interference,,,,,,,,,,,,,,,,
Row_52,"Ma, Xianping","Zhang, Xiaokang","Pun, Man-On","Liu, Ming",A Multilevel Multimodal Fusion Transformer for Remote Sensing Semantic Segmentation,,2024,34,"Accurate semantic segmentation of remote sensing data plays a crucial role in the success of geoscience research and applications. Recently, multimodal fusion-based segmentation models have attracted much attention due to their outstanding performance as compared to conventional single-modal techniques. However, most of these models perform their fusion operation using convolutional neural networks (CNNs) or the vision transformer (Vit), resulting in insufficient local-global contextual modeling and representative capabilities. In this work, a multilevel multimodal fusion scheme called FTransUNet is proposed to provide a robust and effective multimodal fusion backbone for semantic segmentation by integrating both CNN and Vit into one unified fusion framework. First, the shallow-level features are first extracted and fused through convolutional layers and shallow-level feature fusion (SFF) modules. After that, deep-level features characterizing semantic information and spatial relationships are extracted and fused by a well-designed fusion Vit (FVit). It applies adaptively mutually boosted attention (Ada-MBA) layers and self-attention (SA) layers alternately in a three-stage scheme to learn cross-modality representations of high interclass separability and low intraclass variations. Specifically, the proposed Ada-MBA computes SA and cross-attention (CA) in parallel to enhance intra- and cross-modality contextual information simultaneously while steering attention distribution toward semantic-aware regions. As a result, FTransUNet can fuse shallow-level and deep-level features in a multilevel manner, taking full advantage of CNN and transformer to accurately characterize local details and global semantics, respectively. Extensive experiments confirm the superior performance of the proposed FTransUNet compared with other multimodal fusion approaches on two fine-resolution remote sensing datasets, namely ISPRS Vaihingen and Potsdam. The source code in this work is available at https://github.com/sstary/SSRS.",Multilevel multimodal fusion,remote sensing,semantic segmentation,vision transformer (Vit),,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_53,"Xie, Jiajun","Pan, Bin","Xu, Xia","Shi, Zhenwei",MiSSNet: Memory-Inspired Semantic Segmentation Augmentation Network for Class-Incremental Learning in Remote Sensing Images,,2024,6,"With remote sensing images constantly being collected rapidly, the class-incremental semantic segmentation (CISS) task has attracted increasing attention. However, the semantic distribution shift problem of the background class in remote sensing images, which is a case of catastrophic forgetting, continues to limit available CISS algorithms. To address this challenge, we present a new memory-inspired semantic segmentation augmentation network (MiSSNet) for class-incremental learning in remote sensing images. The MiSSNet mainly includes two modules: the local semantic distillation (LSD) module and the class-specific regularization (CSR) module. LSD is a distillation structure that employs the local semantic features in retained memory to maintain correlation between pixels throughout the training process of incremental learning. It constructs a series of pixel-level correlation matrices and implicitly adjusts the semantic distribution shift problem of the background class. CSR is a classwise regularization term that utilizes the class-specific portion of the preserved memory to help the model keep repeating the learning of the old categories. It alleviates the background class shift problem by generating countless pixel-level instances of old classes. LSD and CSR work together to tackle the semantic distribution shift problem of background class from semantic information and class information aspects, respectively. In particular, MiSSNet only needs an additional single inference process for memory extraction and storage, and the whole algorithm does not add any new training parameters. Experimental results on three semantic segmentation datasets indicate the advantage of the proposed method.",Incremental learning,remote sensing images,semantic segmentation,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_54,"Jiang, Baode","An, Xiaoya","Xu, Shaofen","Chen, Zhanlong",Intelligent Image Semantic Segmentation: A Review Through Deep Learning Techniques for Remote Sensing Image Analysis,,SEP 2023,19,"Image semantic segmentation is an important part of fundamental in image interpretation and computer vision. With the development of convolutional neural network technology, deep learning-based image semantic segmentation methods have received more and more attention and research. At present, many excellent semantic segmentation methods have been proposed and applied in the field of remote sensing. In this paper, we summarized the semantic segmentation methods used for remote sensing image, including the traditional remote sensing image semantic segmentation methods and the methods based on deep learning, we emphasize on summarizing the remote sensing image semantic segmentation algorithms based on deep learning and classify them into different categories, and then we introduce the datasets that commonly used and data preparation methods including pre-processing and augmentation techniques. Finally, the challenges and future directions of research in this domain are analyzed and prospected. It is hoped that this study can widen the frontiers of knowledge and provide useful literature for researchers interested in advancing this field of research.",Deep learning,Image semantic segmentation,Remote sensing image,Computer vision,,,,,,JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_55,"Ding, Hao","Xia, Bo","Liu, Weilin","Zhang, Zekai",A Novel Mamba Architecture with a Semantic Transformer for Efficient Real-Time Remote Sensing Semantic Segmentation,,JUL 2024,4,"Real-time remote sensing segmentation technology is crucial for unmanned aerial vehicles (UAVs) in battlefield surveillance, land characterization observation, earthquake disaster assessment, etc., and can significantly enhance the application value of UAVs in military and civilian fields. To realize this potential, it is essential to develop real-time semantic segmentation methods that can be applied to resource-limited platforms, such as edge devices. The majority of mainstream real-time semantic segmentation methods rely on convolutional neural networks (CNNs) and transformers. However, CNNs cannot effectively capture long-range dependencies, while transformers have high computational complexity. This paper proposes a novel remote sensing Mamba architecture for real-time segmentation tasks in remote sensing, named RTMamba. Specifically, the backbone utilizes a Visual State-Space (VSS) block to extract deep features and maintains linear computational complexity, thereby capturing long-range contextual information. Additionally, a novel Inverted Triangle Pyramid Pooling (ITP) module is incorporated into the decoder. The ITP module can effectively filter redundant feature information and enhance the perception of objects and their boundaries in remote sensing images. Extensive experiments were conducted on three challenging aerial remote sensing segmentation benchmarks, including Vaihingen, Potsdam, and LoveDA. The results show that RTMamba achieves competitive performance advantages in terms of segmentation accuracy and inference speed compared to state-of-the-art CNN and transformer methods. To further validate the deployment potential of the model on embedded devices with limited resources, such as UAVs, we conducted tests on the Jetson AGX Orin edge device. The experimental results demonstrate that RTMamba achieves impressive real-time segmentation performance.",remote sensing,real-time semantic segmentation,Mamba,unmanned aerial vehicle (UAV),,"Zhang, Jinglin","Wang, Xing","Xu, Sen",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_56,"Cheng, Jian","Deng, Changjian","Su, Yanzhou","An, Zeyu",Methods and datasets on semantic segmentation for Unmanned Aerial Vehicle remote sensing images: A review,,MAY 2024,10,"Unmanned Aerial Vehicle (UAV) has seen a dramatic rise in popularity for remote-sensing image acquisition and analysis in recent years. It has brought promising results in low-altitude monitoring tasks that require detailed visual inspections. Semantic segmentation is one of the hot topics in UAV remote sensing image analysis, as its capability to mine contextual semantic information from UAV images is crucial for achieving a fine-grained understanding of scenes. However, in the remote sensing field, recent reviews have not focused on combining ""UAV remote sensing""and ""semantic segmentation""to summarize the advanced works and future trends. In this study, we focus primarily on describing various recent semantic segmentation methods applied in UAV remote sensing images and summarizing their advantages and limitations. According to the distinction in modeling contextual semantic information, we have categorized and outlined the methods based on graph-based contextual models and deep-learning-based models. Publicly available UAV-based image datasets are also gathered to encourage systematic research on advanced semantic segmentation methods. We provide quantitative results of representative methods on two high-resolution UAV-based image datasets for fair comparisons and discussions in terms of semantic segmentation accuracy and model inference efficiency. Besides, this paper concludes some remaining challenges and future directions in semantic segmentation for UAV remote sensing images and points out that methods based on deep learning will become the future research trend.",Semantic segmentation,Unmanned aerial vehicle,Remote sensing images,Deep learning,,"Wang, Qi",,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_57,"Rui, Xue","Li, Ziqiang","Cao, Yang","Li, Ziyang",DILRS: Domain-Incremental Learning for Semantic Segmentation in Multi-Source Remote Sensing Data,,MAY 12 2023,5,"With the exponential growth in the speed and volume of remote sensing data, deep learning models are expected to adapt and continually learn over time. Unfortunately, the domain shift between multi-source remote sensing data from various sensors and regions poses a significant challenge. Segmentation models face difficulty in adapting to incremental domains due to catastrophic forgetting, which can be addressed via incremental learning methods. However, current incremental learning methods mainly focus on class-incremental learning, wherein classes belong to the same remote sensing domain, and neglect investigations into incremental domains in remote sensing. To solve this problem, we propose a domain-incremental learning method for semantic segmentation in multi-source remote sensing data. Specifically, our model aims to incrementally learn a new domain while preserving its performance on previous domains without accessing previous domain data. To achieve this, our model has a unique parameter learning structure that reparametrizes domain-agnostic and domain-specific parameters. We use different optimization strategies to adapt to domain shift in incremental domain learning. Additionally, we adopt multi-level knowledge distillation loss to mitigate the impact of label space shift among domains. The experiments demonstrate that our method achieves excellent performance in domain-incremental settings, outperforming existing methods with only a few parameters.",incremental learning,multi-source remote sensing,semantic segmentation,catastrophic forgetting,,"Song, Weiguo",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_58,"Wang, Zhibao","Chang, Huan","Bai, Lu","Chen, Liangfu",A Creative Weak Supervised Semantic Segmentation for Remote Sensing Images,,2024,0,"In weakly supervised semantic segmentation (WSSS) tasks on remote sensing images, it is a common practice to train a classification network from scratch using a large batch of images with a limited number of classes. Subsequently, class activation maps are extracted from the model based on predefined class indices, and these maps are then optimized to obtain pseudolabels. To make this strategy effective when introducing a new class, a substantial amount of data needs to be provided to the model. In this article, we present an innovative framework, RS-TextWS-Seg, designed to efficiently generate high-quality segmentation results for a wide range of remote sensing objects using concise descriptions. Our proposed framework comprises three sequential stages: initially, we undertake parameter fine-tuning of the contrastive language-image pretraining (CLIP) model to swiftly strengthen its capacity for zero-shot detection of a limited number of remote sensing features. Subsequently, we introduce a text-driven background suppression mechanism aimed at deriving class activation maps from the refined CLIP model based on textual cues, while concurrently mitigating background noises. Finally, we use the segment anything model (SAM) to refine the edges of the extracted class activation map. We widely researched the leading-edge methodologies in WSSS and conducted a range of comparative experiments and ablation studies to prove the efficacy of our proposed framework. The research findings underscore that RS-TextWS-Seg outperforms other state-of-the-art methods on renowned datasets such as DLRSD and Potsdam, as well as on bespoke datasets specifically curated for overground petroleum pipelines and oil well fields.",Remote sensing,Semantic segmentation,Cams,Training,Feature extraction,"Bi, Xiuli",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Sensors,Semantics,Petroleum,Location awareness,Decoding,Fine-tuning,remote sensing image,text prompts,weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,
Row_59,"Lin, Baokai","Yang, Guang","Zhang, Qian","Zhang, Guixu",Semantic Segmentation Network Using Local Relationship Upsampling for Remote Sensing Images,,2022,9,"Semantic segmentation is a fundamental task in remote sensing image processing. It provides pixel-level classification, which is important for many applications, such as building extraction and land use mapping. The development of convolutional neural network has considerably improved the performance of semantic segmentation. Most semantic segmentation networks are the encoder-decoder structure. Bilinear interpolation is an ordinary upsampling method in the decoder, but bilinear interpolation only considers its own features and inserts three times its own features. This over-simple and data-independent bilinear upsampling may lead to suboptimal results. In this work, we propose an upsampling method based on local relations to replace bilinear interpolation. Upsampling is performed by correlating the local relationship of feature maps of adjacent stages, which can better integrate local and global information. We also design a fusion module based on local similarity. Our proposed method with ResNet101 as the backbone of the segmentation network can improve the average F-1 score and overall accuracy of the Vaihingen data set by 2.69% and 1.31%, respectively. Our proposed method also has fewer parameters and less inference time.",Decoder,local relationship upsampling,remote sensing,semantic segmentation,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,
Row_60,"Ben Hamida, A.","Benoit, A.","Lambert, P.","Klein, L.",DEEP LEARNING FOR SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES WITH RICH SPECTRAL CONTENT,2017 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),2017,14,"With the rapid development of Remote Sensing acquisition techniques, there is a need to scale and improve processing tools to cope with the observed increase of both data volume and richness. Among popular techniques in remote sensing, Deep Learning gains increasing interest but depends on the quality of the training data. Therefore, this paper presents recent Deep Learning approaches for fine or coarse land cover semantic segmentation estimation. Various 2D architectures are tested and a new 3D model is introduced in order to jointly process the spatial and spectral dimensions of the data. Such a set of networks enables the comparison of the different spectral fusion schemes. Besides, we also assess the use of a ""noisy ground truth"" (i.e. outdated and low spatial resolution labels) for training and testing the networks.",Remote Sensing,Multispectral,Deep Learning,Semantic Segmentation,Noisy Training,"Ben Amar, C.","Audebert, N.","Lefevre, S.",,,,,,,,,,,,,,,,,,,,,,,,,
Row_61,"Fan, Lili","Zhou, Yu","Liu, Hongmei","Li, Yunjie",Combining Swin Transformer With UNet for Remote Sensing Image Semantic Segmentation,,2023,13,"Remote sensing semantic segmentation plays a significant role in various applications such as environmental monitoring, land use planning, and disaster response. Convolutional neural networks (CNNs) have been dominating remote sensing semantic segmentation. However, due to the limitations of convolution operations, CNNs cannot effectively model global context. The success of transformers in the natural language processing (NLP) domain provides a new solution for global context modeling. Inspired by the Swin transformer, we propose a novel remote sensing semantic segmentation model called CSTUNet. This model employs a dual-encoder structure consisting of a CNN-based main encoder and a Swin transformer-based auxiliary encoder. We first utilize a detail-structure preservation module (DPM) to mitigate the loss of detail and structure information caused by Swin transformer downsampling. Then we introduce a spatial feature enhancement module (SFE) to collect contextual information from different spatial dimensions. Finally, we construct a position-aware attention fusion module (PAFM) to fuse contextual and local information. Our proposed model obtained 70.75% mean intersection over union (MIoU) on the ISPRS-Vaihingen dataset and 77.27% MIoU on the ISPRS-Potsdam dataset.",Transformers,Remote sensing,Feature extraction,Semantic segmentation,Context modeling,"Cao, Dongpu",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Convolutional neural networks,Task analysis,Feature fusion,remote sensing image,semantic segmentation,Swin transformer,,,,,,,,,,,,,,,,
Row_62,"Broni-Bediako, Clifford","Xia, Junshi","Yokoya, Naoto",,Real-Time Semantic Segmentation A brief survey and comparative study in remote sensing,,OCT 2023,1,"Real-time semantic segmentation of remote sensing imagery is a challenging task that requires a tradeoff between effectiveness and efficiency. It has many applications, including tracking forest fires, detecting changes in land use and land cover, crop health monitoring, and so on. With the success of efficient deep learning methods [i.e., efficient deep neural networks (DNNs)] for real-time semantic segmentation in computer vision, researchers have adopted these efficient DNNs in remote sensing image analysis. This article begins with a summary of the fundamental compression methods for designing efficient DNNs and provides a brief but comprehensive survey, outlining the recent developments in real-time semantic segmentation of remote sensing imagery. We examine several seminal efficient deep learning methods, placing them in a taxonomy based on the network architecture design approach. Furthermore, we evaluate the quality and efficiency of some existing efficient DNNs on a publicly available remote sensing semantic segmentation benchmark dataset, OpenEarthMap. The experimental results of an extensive comparative study demonstrate that most of the existing efficient DNNs have good segmentation quality, but they suffer low inference speed (i.e., a high latency rate), which may limit their capability of deployment in real-time applications of remote sensing image segmentation. We provide some insights into the current trend and future research directions for real-time semantic segmentation of remote sensing imagery.",Real-time systems,Semantic segmentation,Remote sensing,Computational modeling,Semantics,,,,,IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE,,Convolution,Deep learning,,,,,,,,,,,,,,,,,,,,
Row_63,"Meng, Xiaoliang","Yang, Yuechi","Wang, Libo","Wang, Teng",Class-Guided Swin Transformer for Semantic Segmentation of Remote Sensing Imagery,,2022,45,"Semantic segmentation of remote sensing images plays a crucial role in a wide variety of practical applications, including land cover mapping, environmental protection, and economic assessment. In the last decade, convolutional neural network (CNN) is the mainstream deep learning-based method of semantic segmentation. Compared with conventional methods, CNN-based methods learn semantic features automatically, thereby achieving strong representation capability. However, the local receptive field of the convolution operation limits CNN-based methods from capturing long-range dependencies. In contrast, Vision Transformer (ViT) demonstrates its great potential in modeling long-range dependencies and obtains superior results in semantic segmentation. Inspired by this, in this letter, we propose a class-guided Swin Transformer (CG-Swin) for semantic segmentation of remote sensing images. Specifically, we adopt a Transformer-based encoder-decoder structure, which introduces the Swin Transformer backbone as the encoder and designs a class-guided Transformer block to construct the decoder. The experimental results on ISPRS Vaihingen and Potsdam datasets demonstrate the significant breakthrough of the proposed method over ten benchmarks, outperforming both advanced CNN-based and recent Transformer-based approaches.",Transformers,Semantics,Remote sensing,Decoding,Feature extraction,"Li, Rui","Zhang, Ce",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Convolution,Residual neural networks,Class-guided mechanism,fully transformer network,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,
Row_64,"Zhou, Yongxiu","Wang, Honghui","Yang, Ronghao","Yao, Guangle",A Novel Weakly Supervised Remote Sensing Landslide Semantic Segmentation Method: Combining CAM and cycleGAN Algorithms,,AUG 2022,23,"With the development of deep learning algorithms, more and more deep learning algorithms are being applied to remote sensing image classification, detection, and semantic segmentation. The landslide semantic segmentation of a remote sensing image based on deep learning mainly uses supervised learning, the accuracy of which depends on a large number of training data and high-quality data annotation. At this stage, high-quality data annotation often requires the investment of significant human effort. Therefore, the high cost of remote sensing landslide image data annotation greatly restricts the development of a landslide semantic segmentation algorithm. Aiming to resolve the problem of the high labeling cost of landslide semantic segmentation with a supervised learning method, we proposed a remote sensing landslide semantic segmentation with weakly supervised learning method combing class activation maps (CAMs) and cycle generative adversarial network (cycleGAN). In this method, we used the image level annotation data to replace pixel level annotation data as the training data. Firstly, the CAM method was used to determine the approximate position of the landslide area. Then, the cycleGAN method was used to generate the fake image without a landslide, and to make the difference with the real image to obtain the accurate segmentation of the landslide area. Finally, the pixel-level segmentation of the landslide area on remote sensing image was realized. We used mean intersection-over-union (mIOU) to evaluate the proposed method, and compared it with the method based on CAM, whose mIOU was 0.157, and we obtain better result with mIOU 0.237 on the same test dataset. Furthermore, we made a comparative experiment using the supervised learning method of a u-net network, and the mIOU result was 0.408. The experimental results show that it is feasible to realize landslide semantic segmentation in a remote sensing image by using weakly supervised learning. This method can greatly reduce the workload of data annotation.",landslide semantic segmentation,remote sensing,weakly supervised learning,CAM,cycleGAN,"Xu, Qiang","Zhang, Xiaojuan",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_65,"Shen, Ziyang","Ni, Huan","Guan, Haiyan","Niu, Xiaonan",Optimal transport-based domain adaptation for semantic segmentation of remote sensing images,,JAN 17 2024,1,"Thanks to its great power in feature representation, deep learning (DL) is widely used in semantic segmentation tasks. However, the requirements for high distribution consistency of different domains are too tight to be met by large-scale remote sensing tasks due to the domain shift in imaging modes and geographic environments. In this case, trained models in a source domain can hardly achieve sufficient accuracy in a target domain with domain shift. To address this issue, a novel unsupervised domain adaptation (UDA) method driven by optimal transport (OT) with two-stage training is proposed to alleviate domain shift in remote sensing images (RSIs). In the first stage, a colour distribution alignment (CDA) module and a feature joint alignment (FJA) module based on OT were designed to mitigate the discrepancy between different domains. CDA transports source-domain distribution according to the target-domain colour style, and FJA aligns source and target domains in both feature and output spaces by minimizing OT-based losses. In the second stage, self-training with pseudo-label denoising (STPD) was proposed, which alleviated the interference of noises in pseudo-labels based on a joint OT distance. For the experiments, the Potsdam, Vaihingen and UAVid datasets were employed. Based on the characteristics of these datasets, five UDA tasks were introduced. The results of these UDA experiments indicate the superiority of our method. Our code will be available at https://github.com/Hcshenziyang/OT-Domain-Adaptation-Semantic-Segmentation.",Semantic segmentation,optimal transport,domain adaptation,self-training,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_66,"Wang, Qingpeng","Chen, Wei","Huang, Zhou","Tang, Hongzhao",MultiSenseSeg: A Cost-Effective Unified Multimodal Semantic Segmentation Model for Remote Sensing,,2024,9,"Semantic segmentation is an essential technique in remote sensing. Until recently, most related research has focused primarily on advancing semantic segmentation models based on monomodal imagery, and less attention has been given to models that utilize multimodal remote sensing data. Moreover, most current multimodal approaches consider only limited bimodal situations and cannot simultaneously utilize three or more modalities. The increase in expensive computational costs associated with previous feature fusion paradigms hinders their application in broader cases. How to design a unified method to cover a wide variety of quantity-agnostic modalities for multimodal semantic segmentation remains an unsolved issue. To address the aforementioned challenges, this study explores a feasible way and proposes a cost-effective multimodal sensing semantic segmentation model (MultiSenseSeg). MultiSenseSeg employs multiple lightweight modality-specific experts (MSEs), an adaptive multimodal matching (AMM) module, and a single feature extraction pipeline to efficiently model intramodal and intermodal relationships. Benefiting from these designs, the proposed MultiSenseSeg can serve as a unified multimodal model capable of addressing both monomodal and bimodal cases and readily extrapolating to scenarios with more modalities, thereby achieving semantic segmentation of arbitrary quantities of multimodal data. To evaluate the performance of our method, we select several state-of-the-art (SOTA) semantic segmentation models from the past three years and conduct extensive experiments on two public multimodal datasets. The results show that MultiSenseSeg can not only achieve higher accuracy but also exhibit user-friendly modality extrapolation, allowing end-to-end training for consumer-grade users based on limited hardware resources. The model's code will be available at https://github.com/W-qp/MultiSenseSeg.",Semantic segmentation,Feature extraction,Remote sensing,Data models,Computational modeling,"Yang, Lan",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Costs,Semantics,Deep learning,low-cost increment,multimodal,remote sensing,semantic segmentation,,,,,,,,,,,,,,,
Row_67,"Liu, Siyu","Cheng, Jian","Liang, Leikun","Bai, Haiwei",Light-Weight Semantic Segmentation Network for UAV Remote Sensing Images,,2021,37,"Semantic segmentation for unmanned aerial vehicle (UAV) remote sensing images has become one of the research focuses in the field of remote sensing at present, which could accurately analyze the ground objects and their relationships. However, conventional semantic segmentation methods based on deep learning require large-scale models that are not suitable for resource-constrained UAV remote sensing tasks. Therefore, it is important to construct a light-weight semantic segmentation method for UAV remote sensing images. With this motivation, we propose a light-weight neural network model with fewer parameters to solve the problem of semantic segmentation of UAV remote sensing images. The network adopts an encoder-decoder architecture. In the encoder, we build a light-weight convolutional neural network model with fewer channels of each layer to reduce the number of model parameters. Then, feature maps of different scales from the encoder are concatenated together after resizing to carry out the multiscale fusion. Moreover, we employ two attention modules to capture the global semantic information from the context and the correlation among channels in UAV remote sensing images. In the decoder part, the model obtains predictions of each pixel through the softmax function. We conducted experiments on the ISPRS Vaihingen dataset, UAVid dataset, and UDD6 dataset to verify the effectiveness of the light-weight network. Our method obtains quality semantic segmentation results evaluated on UAV remote sensing datasets with only 9 M parameters the model owns, which is competitive among popular methods with the same level of parameters.",Remote sensing,Semantics,Image segmentation,Task analysis,Unmanned aerial vehicles,"Dang, Wanli",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Feature extraction,Convolution,Attention mechanism,light-weight network,remote sensing,semantic segmentation,unmanned aerial vehicle images,,,,,,,,,,,,,,,
Row_68,"Wang, Linhan","Lei, Shuo","He, Jianfeng","Wang, Shengkun",Self-Correlation and Cross-Correlation Learning for Few-Shot Remote Sensing Image Semantic Segmentation,"31ST ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS, ACM SIGSPATIAL GIS 2023",2023,1,"Remote sensing image semantic segmentation is an important problem for remote sensing image interpretation. Although remarkable progress has been achieved, existing deep neural network methods suffer from the reliance on massive training data. Few-shot remote sensing semantic segmentation aims at learning to segment target objects from a query image using only a few annotated support images of the target class. Most existing few-shot learning methods stem primarily from their sole focus on extracting information from support images, thereby failing to effectively address the large variance in appearance and scales of geographic objects. To tackle these challenges, we propose a Self-Correlation and Cross-Correlation Learning Network for the few-shot remote sensing image semantic segmentation. Our model enhances the generalization by considering both self-correlation and cross-correlation between support and query images to make segmentation predictions. To further explore the self-correlation with the query image, we propose to adopt a classical spectral method to produce a class-agnostic segmentation mask based on the basic visual information of the image. Extensive experiments on two remote sensing image datasets demonstrate the effectiveness and superiority of our model in few-shot remote sensing image semantic segmentation. The code is available at https://github.com/linhanwang/SCCNet.",remote sensing image semantic segmentation,few-shot learning,,,,"Zhang, Min","Lu, Chang-Tien",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_69,"Liu, Jia","Hua, Wenyi","Zhang, Wenhua","Liu, Fang",Stair Fusion Network With Context-Refined Attention for Remote Sensing Image Semantic Segmentation,,2024,6,"Semantic segmentation of remote sensing images is essential in various fields, such as Earth resource census, environmental pollution monitoring, and land use planning. The segmentation performance has been significantly improved recently with the development of deep learning. However, there are still some challenges in dealing with remote sensing images. One of the main issues is that features within the same category in remote sensing images could vary significantly, while features between different categories could be more similar, leading to confusion in segmentation. Moreover, the presence of large shadow areas narrows the feature differences between categories, making segmentation even more difficult. To address these challenges, one way is to leverage contextual and multiscale information for accurate segmentation. As a consequence, in this article, we propose a stair fusion network with context-refined attention (SFCRNet). A context-based attention embedding module is proposed to enhance the representation of the processed features by utilizing the context to maximize information retention in the channel and spatial dimensions. It can retain the information on the original channel and the association between it and other channels. Furthermore, we present an SFN, where a stair-shaped architecture and corresponding fusion module are designed to ensure that rich semantic information from high-level features is continuously transmitted to low-level layers. The experimental results on three datasets demonstrate the effectiveness of our proposed method.",Attention mechanism,remote sensing images,semantic segmentation,stair fusion,,"Xiao, Liang",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_70,"Wang, Zhen","Zhang, Shanwen","Zhang, Chuanlei","Wang, Buhong",Hidden Feature-Guided Semantic Segmentation Network for Remote Sensing Images,,2023,19,"For semantic segmentation of remote sensing images, convolutional neural networks (CNNs) have proven to be powerful tools. However, the existing CNN-based methods have the problems of feature information loss, serious interference by clutter information, and ignoring the correlation between different scale features. To solve these problems, this article proposes a novel hidden feature-guided semantic segmentation network (HFGNet) for remote sensing images, which achieves accurate semantic segmentation by hierarchically extracting and fusing valuable feature information. Specifically, the hidden feature extraction module (HFE-M) is introduced to suppress the salient feature representation to mine more valuable hidden features. Meanwhile, the multifeature interactive fusion module (MIF-M) establishes the correlation between different features to achieve hierarchical feature fusion. The multiscale feature calibration module (MSFC) is constructed to enhance the diversity and refinement representation of hierarchical fusion features. Besides, the local-channel attention mechanism (LCA-M) is designed to improve the feature perception capability of the object region and suppress background information interference. We conducted extensive experiments on the widely used ISPRS 2-D Semantic Labeling dataset and the 15-Class Gaofen Image dataset. Experimental results demonstrate that the proposed HFGNet has advantages over several state-of-the-art methods. The source code and models are available at https://github.com/darkseid-arch/RS-HFGNet.",Attention mechanism,convolution neural networks (CNNs),hidden feature extraction,remote sensing image,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_71,"Aburaed, N.","Al-Saad, M.","Alkhatib, M. Q.","Zitouni, M. S.",SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGERY USING AN ENHANCED ENCODER-DECODER ARCHITECTURE,"GEOSPATIAL WEEK 2023, VOL. 10-1",2023,0,"Semantic segmentation is one of most the important computer vision tasks for the analysis of aerial imagery in many remote sensing applications, such as resource surveys, disaster detection, and urban planning. This area of research still faces unsolved challenges, especially in cluttered environments and complex sceneries. This study presents a repurposed Robust UNet (RUNet) architecture for semantic segmentation, and embeds the architecture with attention mechanism in order to enhance feature extraction and construction of segmentation maps. The attention mechanism is achieved using Squeeze-and-Excitation (SE) block. The resulting network is referred to as SE-RUNet. SE is also tested with the classical UNet, termed SE-UNet, to verify the efficiency of introducing SE. The proposed approach is trained and tested using ""Semantic Segmentation of Aerial Imagery"" dataset. The results are evaluated using Accuracy, Precision, Recall, F-score and mean Intersection over Union (mIoU) metrics. Comparative evaluation and experimental results show that using SE to embed attention mechanism into UNet and RUNet significantly improves the overall performance.",Deep Learning,Semantic Segmentation,RUNET,UNET,Remote Sensing,"Almansoori, S.","Al-Ahmad, H.",,,,,Squeeze and Excitation,,,,,,,,,,,,,,,,,,,,,
Row_72,"Alam, Muhammad","Wang, Jian-Feng","Guangpei, Cong","Yunrong, L., V",Convolutional Neural Network for the Semantic Segmentation of Remote Sensing Images,,FEB 2021,51,"In recent years, the success of deep learning in natural scene image processing boosted its application in the analysis of remote sensing images. In this paper, we applied Convolutional Neural Networks (CNN) on the semantic segmentation of remote sensing images. We improve the Encoder- Decoder CNN structure SegNet with index pooling and U-net to make them suitable for multi-targets semantic segmentation of remote sensing images. The results show that these two models have their own advantages and disadvantages on the segmentation of different objects. In addition, we propose an integrated algorithm that integrates these two models. Experimental results show that the presented integrated algorithm can exploite the advantages of both the models for multi-target segmentation and achieve a better segmentation compared to these two models.",Convolutional Neural Networks (CNN),Deep learning,Remote sensing images,Semantic segmentation,,"Chen, Yuanfang",,,,MOBILE NETWORKS & APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,
Row_73,"Chen, Xin","Li, Dongfen","Liu, Mingzhe","Jia, Jiaru",CNN and Transformer Fusion for Remote Sensing Image Semantic Segmentation,,SEP 2023,11,"Semantic segmentation of remote sensing images has been widely used in environmental protection, geological disaster discovery, and natural resource assessment. With the rapid development of deep learning, convolutional neural networks (CNNs) have dominated semantic segmentation, relying on their powerful local information extraction capabilities. Due to the locality of convolution operation, it can be challenging to obtain global context information directly. However, Transformer has excellent potential in global information modeling. This paper proposes a new hybrid convolutional and Transformer semantic segmentation model called CTFuse, which uses a multi-scale convolutional attention module in the convolutional part. CTFuse is a serial structure composed of a CNN and a Transformer. It first uses convolution to extract small-size target information and then uses Transformer to embed large-size ground target information. Subsequently, we propose a spatial and channel attention module in convolution to enhance the representation ability for global information and local features. In addition, we also propose a spatial and channel attention module in Transformer to improve the ability to capture detailed information. Finally, compared to other models used in the experiments, our CTFuse achieves state-of-the-art results on the International Society of Photogrammetry and Remote Sensing (ISPRS) Vaihingen and ISPRS Potsdam datasets.",segmentation,remote sensing,CNN,transformer,attention,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_74,"Li, Wenyuan","Chen, Hao","Shi, Zhenwei",,Semantic Segmentation of Remote Sensing Images With Self-Supervised Multitask Representation Learning,,2021,39,"Existing deep learning-based remote sensing images semantic segmentation methods require large-scale labeled datasets. However, the annotation of segmentation datasets is often too time-consuming and expensive. To ease the burden of data annotation, self-supervised representation learning methods have emerged recently. However, the semantic segmentation methods need to learn both high-level and low-level features, but most of the existing self-supervised representation learning methods usually focus on one level, which affects the performance of semantic segmentation for remote sensing images. In order to solve this problem, we propose a self-supervised multitask representation learning method to capture effective visual representations of remote sensing images. We design three different pretext tasks and a triplet Siamese network to learn the high-level and low-level image features at the same time. The network can be trained without any labeled data, and the trained model can be fine-tuned with the annotated segmentation dataset. We conduct experiments on Potsdam, Vaihingen dataset, and cloud/snow detection dataset Levir_CS to verify the effectiveness of our methods. Experimental results show that our proposed method can effectively reduce the demand of labeled datasets and improve the performance of remote sensing semantic segmentation. Compared with the recent state-of-the-art self-supervised representation learning methods and the mostly used initialization methods (such as random initialization and ImageNet pretraining), our proposed method has achieved the best results in most experiments, especially in the case of few training data. With only 10% to 50% labeled data, our method can achieve the comparable performance compared with random initialization. Codes are available at https://github.com/flyakon/SSLRemoteSensing.",Task analysis,Remote sensing,Semantics,Image segmentation,Training,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Sensors,Snow,Cloud detection,remote sensing images,self-supervised representation learning,semantic segmentation,,,,,,,,,,,,,,,,
Row_75,"Tian, Tian","Chu, Zhengquan","Hu, Qian","Ma, Li",Class-Wise Fully Convolutional Network for Semantic Segmentation of Remote Sensing Images,,AUG 2021,20,"Semantic segmentation is a fundamental task in remote sensing image interpretation, which aims to assign a semantic label for every pixel in the given image. Accurate semantic segmentation is still challenging due to the complex distributions of various ground objects. With the development of deep learning, a series of segmentation networks represented by fully convolutional network (FCN) has made remarkable progress on this problem, but the segmentation accuracy is still far from expectations. This paper focuses on the importance of class-specific features of different land cover objects, and presents a novel end-to-end class-wise processing framework for segmentation. The proposed class-wise FCN (C-FCN) is shaped in the form of an encoder-decoder structure with skip-connections, in which the encoder is shared to produce general features for all categories and the decoder is class-wise to process class-specific features. To be detailed, class-wise transition (CT), class-wise up-sampling (CU), class-wise supervision (CS), and class-wise classification (CC) modules are designed to achieve the class-wise transfer, recover the resolution of class-wise feature maps, bridge the encoder and modified decoder, and implement class-wise classifications, respectively. Class-wise and group convolutions are adopted in the architecture with regard to the control of parameter numbers. The method is tested on the public ISPRS 2D semantic labeling benchmark datasets. Experimental results show that the proposed C-FCN significantly improves the segmentation performances compared with many state-of-the-art FCN-based networks, revealing its potentials on accurate segmentation of complex remote sensing images.",semantic segmentation,fully convolutional network (FCN),remote sensing,class- wise features,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_76,"Miao, Wang","Xu, Zhe","Geng, Jie","Jiang, Wen",ECAE: Edge-Aware Class Activation Enhancement for Semisupervised Remote Sensing Image Semantic Segmentation,,2023,6,"Remote sensing image semantic segmentation (RSISS) remains challenging due to the scarcity of labeled data. Semisupervised learning can leverage pseudolabels to enhance the model ' s ability to learn from unlabeled data. However, accurately generating pseudolabels for RSISS remains a significant challenge that severely affects the model ' s performance, especially for the edges of different classes. To overcome these issues, we propose a semisupervised semantic segmentation framework for remote sensing images (RSIs) based on edge-aware class activation enhancement (ECAE). First, the baseline network is constructed based on the average teacher model, which separates the training of labeled and unlabeled data using student and teacher networks. Second, considering local continuity and global discreteness of object distribution in RSIs, the class activation mapping enhancement (CAME) network is designed to predict local areas more remarkably. Finally, the edge-aware network (EAN) is proposed to improve the performance of edge segmentation in RSIs. The combination of the CAME with the EAN further heightens the generation of high-confidence pseudolabels. Experiments were performed on two publicly available remote sensing semantic segmentation datasets, Potsdam and ISPRS Vaihingen, which verify the superiorities of the proposed ECAE model.",Class activation mapping,remote sensing images (RSIs),semantic segmentation,semisupervised learning.,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_77,"Zhao, Qi","Liu, Jiahui","Li, Yuewen","Zhang, Hong",Semantic Segmentation With Attention Mechanism for Remote Sensing Images,,2022,72,"Semantic segmentation for high-resolution remote sensing images is one of the most significant tasks in the field of remote sensing applications. Remote sensing images contain substantial detailed information of ground objects, such as shape, location, and texture. Therefore, these objects make the images exhibit large intraclass variance and small interclass variance, which makes it very difficult to be recognized. In this study, an end-to-end attention-based semantic segmentation network (SSAtNet) is proposed. A pyramid attention pooling module is proposed to introduce the attention mechanism into the multiscale module for adaptive features refinement. To correct the detailed information, the pooling index correction module integrates pooling index maps from the encoder with high-level feature maps, which can help recover the fine-grained features. In the encoder phase, a more effective ResNet-101 backbone is designed to capture detailed features. What is more, a series of data augmentation methods are proposed to enhance the model's robustness. The proposed model is compared with several previous advanced networks and achieves the state of the art on the ISPRS Vaihingen dataset. The experiment results prove the effectiveness of the SSAtNet.",Image segmentation,Task analysis,Semantics,Remote sensing,Convolution,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Indexes,Feature extraction,Attention mechanism,convolutional neural networks (CNNs),multiscale module,remote sensing,semantic segmentation,,,,,,,,,,,,,,,
Row_78,"Su, Yanzhou","Cheng, Jian","Wang, Wen","Bai, Haiwei",Semantic Segmentation for High-Resolution Remote-Sensing Images via Dynamic Graph Context Reasoning,,2022,4,"Semantic segmentation for high-resolution remote-sensing (HRRS) images is one of the most challenging tasks in remote-sensing images understanding. Capturing long-range dependencies in feature representations is crucial for semantic segmentation. Recent graph-based global reasoning networks (GloRe) focus on modeling the global contextual relationship between latent nodes based on fully connected graph in interaction space. However, such a dense operation is susceptible to redundant features. Most importantly, it treats each node equally, ignoring the contextual relationship between nodes in graphs. In this work, we propose to explore more effective contextual representations in semantic segmentation by introducing dynamic graph contextual reasoning module over GloRe, dubbed DGCR. It incorporates local semantic information that represents the relationships between nodes to perform long-range contextual reasoning. More specifically, to provide effectively and flexible reasoning in graph-based reasoning approaches, we construct k-nearest neighbor (KNN) graphs rather than fully connected graphs using only the k closest nodes depends on pairwise semantic distance. Extensive experiments on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam datasets demonstrate the effectiveness and superiority of our proposed DGCR module over other state-of-the-art methods.",Context,graph reasoning,high-resolution remote-sensing (HRRS) images,semantic segmentation,,"Liu, Haijun",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,
Row_79,"Nie, Jie","Wang, Chenglong","Yu, Shusong","Shi, Jinjin",MIGN: Multiscale Image Generation Network for Remote Sensing Image Semantic Segmentation,,2023,7,"With the development of computer vision, the semantic segmentation of remote sensing images, which has become an important topic, has been utilized in various applications for image content analysis and understanding, such as urban planning, natural disaster monitoring, and land resource management. Many approaches have been proposed to address these problems. However, due to obvious differences in resolution, spatial structure, and semantics between remote sensing images and ordinary images, the semantic segmentation of remote sensing images is still challenging. In this paper, we propose a novel multiscale image generation network (MIGN) that can efficiently generate high-resolution segmentation results by considering both details and boundary information. In particular, a multi-attention mechanism method for semantic segmentation of remote sensing images is designed. The attention weight is calculated by capturing the interaction of cross dimensions in a two-branch structure, which can learn the underlying feature information and guarantee the performance of each pixel feature for final classification. We also propose an edge supervised module to ensure that the segmentation boundary has a more accurate performance. A multiscale image fusion algorithm based on the Bayes model is proposed to improve the accuracy of the segmentation module. The performance of our model is evaluated on the ISPRS Vaihingen and Potsdam datasets. The results show that our method is superior to the most advanced image segmentation methods in terms of MIoU and pixel accuracy.",Semantic segmentation,remote sensing,multiscale,multi-attention,edge supervised,"Lv, Xiaowei","Wei, Zhiqiang",,,IEEE TRANSACTIONS ON MULTIMEDIA,,image fusion,,,,,,,,,,,,,,,,,,,,,
Row_80,"Zhang, Jing","Lin, Shaofu","Ding, Lei","Bruzzone, Lorenzo",Multi-Scale Context Aggregation for Semantic Segmentation of Remote Sensing Images,,FEB 2020,124,"The semantic segmentation of remote sensing images (RSIs) is important in a variety of applications. Conventional encoder-decoder-based convolutional neural networks (CNNs) use cascade pooling operations to aggregate the semantic information, which results in a loss of localization accuracy and in the preservation of spatial details. To overcome these limitations, we introduce the use of the high-resolution network (HRNet) to produce high-resolution features without the decoding stage. Moreover, we enhance the low-to-high features extracted from different branches separately to strengthen the embedding of scale-related contextual information. The low-resolution features contain more semantic information and have a small spatial size; thus, they are utilized to model the long-term spatial correlations. The high-resolution branches are enhanced by introducing an adaptive spatial pooling (ASP) module to aggregate more local contexts. By combining these context aggregation designs across different levels, the resulting architecture is capable of exploiting spatial context at both global and local levels. The experimental results obtained on two RSI datasets show that our approach significantly improves the accuracy with respect to the commonly used CNNs and achieves state-of-the-art performance.",semantic segmentation,convolutional neural network,deep learning,image analysis,remote sensing,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_81,"Hua, Yuansheng","Marcos, Diego","Mou, Lichao","Zhu, Xiao Xiang",Semantic Segmentation of Remote Sensing Images With Sparse Annotations,,2022,59,"Training convolutional neural networks (CNNs) for very high-resolution images requires a large quantity of high-quality pixel-level annotations, which is extremely labor-intensive and time-consuming to produce. Moreover, professional photograph interpreters might have to be involved in guaranteeing the correctness of annotations. To alleviate such a burden, we propose a framework for semantic segmentation of aerial images based on incomplete annotations, where annotators are asked to label a few pixels with easy-to-draw scribbles. To exploit these sparse scribbled annotations, we propose the FEature and Spatial relaTional regulArization (FESTA) method to complement the supervised task with an unsupervised learning signal that accounts for neighborhood structures both in spatial and feature terms. For the evaluation of our framework, we perform experiments on two remote sensing image segmentation data sets involving aerial and satellite imagery, respectively. Experimental results demonstrate that the exploitation of sparse annotations can significantly reduce labeling costs, while the proposed method can help improve the performance of semantic segmentation when training on such annotations. The sparse labels and codes are publicly available for reproducibility purposes.1",Annotations,Image segmentation,Semantics,Remote sensing,Training,"Tuia, Devis",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Kernel,Image color analysis,Aerial image,convolutional neural networks (CNNs),semantic segmentation,semisupervised learning,sparse scribbled annotation,,,,,,,,,,,,,,,
Row_82,"Huang, Cong","Yang, Yao","Wang, Huajun","Ma, Yu",Semantic segmentation of remote sensing images based on deep learning methods,,2021,0,"Remote sensing image segmentation has always been an important research direction in the field of remote sensing image processing, and it is a key step in the further understanding and analysis of remote sensing images. Image semantic segmentation is the process of classifying each pixel to form several sub-regions with respective characteristics, and extracting the objects of interest among them. However, due to the complex boundary and scale difference of the remote sensing image, the traditional algorithm can not meet the actual needs well, resulting in low segmentation accuracy. In order to further improve the accuracy of remote sensing image segmentation, this paper combines deep convolutional neural network with remote sensing image, based on the U-Net, firstly compares the model's segmentation accuracy under different learning strategies, and introduces a new learning strategy to improve the learning effect of the model; secondly, in the loss function part of the model, a new compound loss function is proposed to speed up the convergence of the network and improve the segmentation accuracy. Based on full experimental research on the WHDLD remote sensing image dataset, the results show that the improved method has 1.5% accuracy improvement compare to the U-Net.",remote sensing images,semantic segmentation,composite loss function,learning strategies,,"Zhao, Jinquan","Wan, Jun",,,"MEDICAL IMAGE COMPUTING AND COMPUTER-ASSISTED INTERVENTION, PT III",,,,,,,,,,,,,,,,,,,,,,,
Row_83,"Guo, Yongjie","Wang, Feng","Xiang, Yuming","You, Hongjian",Semisupervised Semantic Segmentation With Certainty-Aware Consistency Training for Remote Sensing Imagery,,2023,1,"Semisupervised learning is a forcible method to lessen the cost of annotation for remote sensing semantic segmentation tasks. Recent related research works indicate that consistency training is one of the most effective strategies in semisupervised learning. The core of consistency training is maintaining model outputs consistent under various perturbations. However, the current consistency training-based semisupervised semantic segmentation frameworks lack the analysis of model uncertainty, which increases the generation of semantic ambiguity on remote sensing images. Therefore, we propose the certainty-aware consistency training (CACT) strategy to mitigate the influence of semantic ambiguity caused by model uncertainty. The CACT strategy consists of two novel parts: certainty-aware consistency correction (CACC) and class-balanced-adaptive threshold (CBAT) strategy. The CACC starts with generating a high-quality prediction target, then models the importance of the consistent output target and corrects the output predictions according to the certainty map, increasing the focus on reliable predictions. The CBAT strategy uses a dynamic class-balanced adaptive threshold to filter out unreliable predictions, further reducing the impact of semantic ambiguity. Finally, considerable experimental results on the DLRSD, WHDLD, and Potsdam demonstrate that our framework has an excellent performance on semisupervised remote sensing semantic segmentation scenarios.",Training,Remote sensing,Semantic segmentation,Semantics,Adaptation models,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Predictive models,Uncertainty,Consistency training,remote sensing image,semantic segmentation,semisupervised learning,,,,,,,,,,,,,,,,
Row_84,"Huang, Liwei","Jiang, Bitao","Lv, Shouye","Liu, Yanbo",Deep-Learning-Based Semantic Segmentation of Remote Sensing Images: A Survey,,2024,14,"Semantic segmentation of remote sensing images (SSRSIs), which aims to assign a category to each pixel in remote sensing images, plays a vital role in a broad range of applications, such as environmental monitoring, urban planning, and land resource utilization. Recently, with the successful application of deep learning in remote sensing, a substantial amount of work has been aimed at developing SSRSI methods using deep learning models. In this survey, we provide a comprehensive review of SSRSI. First, we review the current mainstream semantic segmentation models based on deep learning. Next, we analyze the main challenges faced by SSRSI and comprehensively summarize the current research status of deep-learning-based SSRSI, especially some new directions in SSRSI are outlined, including semisupervised and weakly-supervised SSRSI, unsupervised domain adaption in SSRSI, multimodal data-fusion-based SSRSI, and pretrained models for SSRSI. Then, we examine the most widely used datasets and metrics and review the quantitative results and experimental performance of some representative methods of SSRSI. Finally, we discuss promising future research directions in this area.",Deep learning,multimodal fusion,pretrained models,remote sensing images,semantic segmentation,"Fu, Ying",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,semi-supervised,unsupervised domain adaptation,weakly-supervised,,,,,,,,,,,,,,,,,,,
Row_85,"Wang, Hao","Tao, Chao","Qi, Ji","Xiao, Rong",Avoiding Negative Transfer for Semantic Segmentation of Remote Sensing Images,,2022,17,"Reducing the feature distribution shift caused by the factor of visual-environment changes, named visual-environment changes (VE-changes), is a hot issue in domain adaptation learning. However, in the semantic segmentation task of remote sensing imageries, besides VE-changes, the change of semantic-scene changes (SS-changes) is another factor raising the domain gap, which brings the label distribution shift. For example, although urban and rural share the same landcover label, there is still a gap in label distribution. If there is little relation that can be found in neither feature nor label space, forcibly adapting to a new domain could have a high risk of negative transfer. Hence, we propose a new Transitive Domain Adaptation method for Remote Sensing (TDARS) images. First, we introduce an intermediate domain to enlarge the relation between the given source and target domains. Second, we learn from primary and nonprimary confident classes to increase the likelihood of transferring valuable information. As a result, TDARS enables the given source and target domains to be connected through the selected intermediate domain and performs effective knowledge transfer among all domains. The proposed method is evaluated on three domain adaptation datasets of remote sensing images. Extensive experiments show that the approach can effectively handle the domain shift problem from remote sensing images compared to other state-of-the-art domain adaptation methods.",Remote sensing,Semantics,Image segmentation,Visualization,Knowledge transfer,"Li, Haifeng",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Urban areas,Sensors,Domain adaptation,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,
Row_86,"Liu, Bo","Hu, Jinwu","Bi, Xiuli","Li, Weisheng",PGNet: Positioning Guidance Network for Semantic Segmentation of Very-High-Resolution Remote Sensing Images,,SEP 2022,12,"Semantic segmentation of very-high-resolution (VHR) remote sensing images plays an important role in the intelligent interpretation of remote sensing since it predicts pixel-level labels to the images. Although many semantic segmentation methods of VHR remote sensing images have emerged recently and achieved good results, it is still a challenging task because the objects of VHR remote sensing images show large intra-class and small inter-class variations, and their size varies in a large range. Therefore, we proposed a novel semantic segmentation framework for VHR remote sensing images, called Positioning Guidance Network (PGNet), which consists of the feature extractor, a positioning guiding module (PGM), and a self-multiscale collection module (SMCM). First, the PGM can extract long-range dependence and global context information with the help of the transformer architecture and effectively transfer them to each pyramid-level feature, thus effectively improving the segmentation effectiveness between different semantic objects. Secondly, the SMCM we designed can effectively extract multi-scale information and generate high-resolution feature maps with high-level semantic information, thus helping to segment objects in small and varying sizes. Without bells and whistles, the mIoU scores of the proposed PGNet on the iSAID dataset and ISPRS Vaihingn dataset are 1.49% and 2.40% higher than FactSeg, respectively.",remote sensing images,semantic segmentation,positioning guiding module,self-multiscale collection module,transformer,"Gao, Xinbo",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_87,"Jiang, Jionghui","Feng, Xi'an","Ye, QiLei","Hu, Zhongyi",Semantic segmentation of remote sensing images combined with attention mechanism and feature enhancement U-Net,,OCT 2 2023,4,"Target segmentation of remote sensing images has always been a hotspot in image processing. This paper proposes a new semantic segmentation technology for remote sensing images, which uses Unet as the backbone and combines attention mechanism and feature enhancement module. The feature enhancement module can enlarge the information of the region of interest (ROI) to improve the contrast of the image; the attention mechanism includes spatial and channel attention modules, which can obtain more detailed information of the desired target while suppressing other useless information. This paper improves the loss function of the traditional Unet. On the basis of the sparse categorical cross-entropy function, the mean squared logarithmic error function is added, which can effectively improve the accuracy of semantic segmentation. The experimental results show that the algorithm has higher computational accuracy than Unet, DeepLabV3, SegNet, PSPNet, CBAM and DAnet while having the computational speed of FCN and Unet in model testing and validation.",Semantic Segmentation,Remote Sensing Images,Attention Mechanism,,,"Gu, Zhiyang","Huang, Hui",,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_88,Ni Xianyang,Cheng Yinbao,Wang Zhongyu,,Remote sensing semantic segmentation with convolution neural network using attention mechanism,PROCEEDINGS OF 2019 14TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONIC MEASUREMENT & INSTRUMENTS (ICEMI),2019,4,"Semantic image segmentation is an essential part of remote sensing image processing because accurate understanding of the ground information is the first step in obtaining useful knowledge of surface coverage. The popular semantic segmentation convolutional neural network model (DeepLab v3+) cannot electively use attention information, resulting in coarse segmentation boundaries. In this work, a new type of bottleneck using attention information which can extract semantic information and more abundant features from images is proposed. Compared with original network, the model using new bottleneck finely segments the target regions, solves the problem of segmentation boundary roughness better, leading to higher mloU and accuracy. Experimental results based on the dataset in the ISPRS benchmark on urban object classification show bringing attention model into semantic segmentation neural network improves performance.",Remote sensing,semantic segmentation,convolution neural network,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_89,"Sun, Yan","Zheng, Wenxi",,,HRNet- and PSPNet-based multiband semantic segmentation of remote sensing images,,APR 2023,24,"High-resolution remote sensing images have become mainstream remote sensing data, but there is an obvious ""salt and pepper phenomenon"" in the existing semantic segmentation methods of high-resolution remote sensing images. The purpose of this paper is to propose an improved deep convolutional neural network based on HRNet and PSPNet to segment and realize deep scene analysis and improve the pixel-level semantic segmentation representation of high-resolution remote sensing images. Based on hierarchical multiscale segmentation technology research, the main method is multiband segmentation; the vegetation, buildings, roads, waters and bare land rule sets in the experimental area are established, the classification is extracted, and the category is labeled at each pixel in the image. Using the image classification network structure, different levels of feature vectors can be used to meet the judgment requirements. The HRNet and PSPNet algorithms are used to analyze the scene and obtain the category labels of all pixels in an image. Experiments have shown that artificial intelligence uses the pyramid pooling module in the classification and recognition of CCF satellite images. In the context of integrating different regions, PSPNet affects the region segmentation accuracy. FCN, DeepLab and PSPNet are now the best methods and achieve 98% accuracy. However, the PSPNet object recognition algorithm has better advantages in specific areas. Experiments show that this method has high segmentation accuracy and good generalization ability and can be used in practical engineering.",Remote sensing image,Semantic segmentation,Ensemble learning,Convolutional neural network,,,,,,NEURAL COMPUTING & APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,
Row_90,"Huang, Wei","Shi, Yilei","Xiong, Zhitong","Zhu, Xiao Xiang",Decouple and weight semi-supervised semantic segmentation of remote sensing images,,JUN 2024,2,"Semantic understanding of high-resolution remote sensing (RS) images is of great value in Earth observation, however, it heavily depends on numerous pixel-wise manually-labeled data, which is laborious and thereby limits its practical application. Semi-supervised semantic segmentation (SSS) of RS images would be a promising solution, which uses both limited labeled data and dominant unlabeled data to train segmentation models, significantly mitigating the annotation burden. The current mainstream methods of remote sensing semi-supervised semantic segmentation (RS-SSS) utilize the hard or soft pseudo-labels of unlabeled data for model training and achieve excellent performance. Nevertheless, their performance is bottlenecked because of two inherent problems: irreversible wrong pseudo-labels and long-tailed distribution among classes. Aiming at them, we propose a decoupled weighting learning (DWL) framework for RS-SSS in this study, which consists of two novel modules, decoupled learning and ranking weighting, corresponding to the above two problems, respectively. During training, the decoupled learning module separates the predictions of the labeled and unlabeled data to decrease the negative impact of the self-training of the wrongly pseudo-labeled unlabeled data on the supervised training of the labeled data. Furthermore, the ranking weighting module tries to adaptively weight each pseudo-label of the unlabeled data according to its relative confidence ranking in its pseudo-class to alleviate model bias to majority classes as a result of the long-tailed distribution. To verify the effectiveness of the proposed DWL framework, extensive experiments are conducted on three widely- used RS semantic segmentation datasets in the semi-supervised setting. The experimental results demonstrate the superiority of our method to some state-of-the-art SSS methods. Our code will be available at https: //github.com/zhu-xlab/RS-DWL.",Remote sensing,Semi-supervised semantic segmentation,Decoupled learning,Weighting learning,,,,,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_91,"Wang, Junxiao","Feng, Zhixi","Jiang, Yao","Yang, Shuyuan",Orientation Attention Network for semantic segmentation of remote sensing images?,,MAY 12 2023,14,"With the increasing resolution of remote sensing images, semantic segmentation has become a challenging task for the extremely abundant textures and edges that existed in images. In this paper, an Orientation Attention Network (OANet) is proposed to learn both orientation features and global semantic features of ground objects for accurate segmentation. Firstly, an Asymmetrical Convolution (AC) is constructed to explore the directional anisotropy of objects. Then an Orientation Attention Module (OAM) is advanced to enhance the intrinsic geometric features of objects, by defining two branches with stacked asymmetrical convolutions along the coordinate axis and adaptively selecting features which are beneficial for segmentation. Finally, the OANet, which combines OAM with a Global Feature Module (GFM), is proposed for both structural and semantic sensitive representations of images. Extensive experiments on four well-known public datasets show the effectiveness of the OANet.",Semantic segmentation,Remote sensing images,Asymmetrical convolution,Orientation attention,,"Meng, Huixiao",,,,KNOWLEDGE-BASED SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,
Row_92,"Zhou, Han","Yang, Jianyu","Zhang, Tingting","Dai, Anjin",EAS-CNN: automatic design of convolutional neural network for remote sensing images semantic segmentation,,JUL 3 2023,1,"Accurate and effective semantic segmentation methods for remote sensing are important for applications such as precision agriculture, urban planning, and disaster monitoring. Convolutional neural networks (CNN) have achieved remarkable performance in the field of remote sensing semantic segmentation. However, the design of CNNs is both time-consuming and necessitates a substantial amount of domain expertise and experience. To address the aforementioned issues, we propose a neural architecture search method called EAS-CNN. The method constructs a search space based on a U-shaped structure and utilizes a fixed-length encoding solution based on gene expression suppression to preserve potential useful information during the evolution process. Furthermore, an improved genetic strategy is proposed to enhance search efficiency and save computational resources. In this paper, we evaluate the proposed EAS-CNN method against state-of-the-art semantic segmentation methods and verify its effectiveness. Experimental results show that EAS-CNN achieves high OA values of 91.2% and 91.6% on the Vaihingen and Postman datasets, respectively. Furthermore, we conduct a thorough analysis of the experimental results and summarize effective design patterns for model architecture to enhance remote sensing semantic segmentation tasks.",Remote sensing semantic segmentation,neural network architecture search,convolutional neural network,>,,"Wu, Chunxiao",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_93,"He, Shuyi","Li, Qingyong","Liu, Yang","Wang, Wen",Semantic Segmentation of Remote Sensing Images With Self-Supervised Semantic-Aware Inpainting,,2022,5,"Semantic segmentation of remote sensing imageries plays a crucial role in resource exploration, urban planning, weather forecasting, etc. For this task, deep learning-based methods have shown significant achievement, typically trained with large-scale labeled data. However, these methods often suffer the performance deterioration facing limited labeled data in real-world applications. To address this problem, a novel self-supervised semantic segmentation framework is proposed for remote sensing imageries with limited labeled data. Specifically, image inpainting is acted as pixel-level pretext task for learning dense feature representations suitable for semantic segmentation. Furthermore, rather than trivially leveraging the conventional random inpainting strategy, a novel adversarial training scheme is proposed to drive the pretext task to adaptively mask and restore salient local regions. The adversarial training scheme consists of instructor network and inpainting network, the instructor network increasingly predicts meaningful salient regions as erased regions, and meanwhile the inpainting network seeks for restoring the corrupted image as pretext task to learn its intrinsic representation. Moreover, the structural similarity (SSIM) is applied as a patch-level loss function for semantic segmentation considering that remote sensing images are highly structured. The experimental results on the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam dataset demonstrate that our method outperforms state-of-the-art self-supervised methods and the ImageNet pre-training methods. The source code is available at https://github.com/JasmineBJTU/self-supervised_RSSS",Task analysis,Remote sensing,Training,Image restoration,Semantics,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Self-supervised learning,Image reconstruction,Adversarial training,image inpainting,self-supervised learning,semantic segmentation,structural similarity (SSIM),,,,,,,,,,,,,,,
Row_94,"Zhao, Jiaqi","Zhang, Di","Shi, Boyu","Zhou, Yong",Multi-source collaborative enhanced for remote sensing images semantic segmentation,,JUL 7 2022,22,"Remote sensing images semantic segmentation is a difficult instance of image understanding. Due to the regional variability and uncertainty of real-world ground cover features, the semantic segmentation of remote sensing images becomes a challenging task. In this paper, we propose an end-to-end multisource remote sensing image semantic segmentation network (MCENet) aiming at the problems of intra-class inconsistency and inter-class indistinguishability in remote sensing images. Firstly, we design a collaborative enhanced fusion module to mine complementary characteristics of multi-source remote sensing images. Among them, the collaborative fusion module is used to solve the problem of intra-class difference, and the enhanced aggregation module is used to solve the problem of inter-class similarity. Secondly, a multi-scale decoder is proposed to improve the robustness of the model for small targets and large-scale changes by learning scale invariance features. Experimental results show that our method achieved 2.2% and 1.11% mean intersection over union (mIoU) score improvements compared with other methods on the US3D and ISPRS Potsdam data sets, respectively. In addition, the method proposed in this paper also has strong competitiveness in terms of parameter quantity and inference speed. (c) 2022 Elsevier B.V. All rights reserved.",Semantic segmentation,Multi-source remote sensing image,Collaborative enhanced fusion,Multi-scale feature decoder,,"Chen, Jingyang","Yao, Rui","Xue, Yong",,NEUROCOMPUTING,,,,,,,,,,,,,,,,,,,,,,,
Row_95,"Jia, Jintong","Song, Jiarui","Kong, Qingqiang","Yang, Huan",Multi-Attention-Based Semantic Segmentation Network for Land Cover Remote Sensing Images,,MAR 2023,5,"Semantic segmentation is a key technology for remote sensing image analysis widely used in land cover classification, natural disaster monitoring, and other fields. Unlike traditional image segmentation, there are various targets in remote sensing images, with a large feature difference between the targets. As a result, segmentation is more difficult, and the existing models retain low accuracy and inaccurate edge segmentation when used in remote sensing images. This paper proposes a multi-attention-based semantic segmentation network for remote sensing images in order to address these problems. Specifically, we choose UNet as the baseline model, using a coordinate attention-based residual network in the encoder to improve the extraction capability of the backbone network for fine-grained features. We use a content-aware reorganization module in the decoder to replace the traditional upsampling operator to improve the network information extraction capability, and, in addition, we propose a fused attention module for feature map fusion after upsampling, aiming to solve the multi-scale problem. We evaluate our proposed model on the WHDLD dataset and our self-labeled Lu County dataset. The model achieved an mIOU of 63.27% and 72.83%, and an mPA of 74.86% and 84.72%, respectively. Through comparison and confusion matrix analysis, our model outperformed commonly used benchmark models on both datasets.",remote sensing image,attention mechanism,image segmentation,deep learning,semantic segmentation,"Teng, Yunhe","Song, Xuan",,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,
Row_96,"He, Guangjun","Dong, Zhe","Feng, Pengming","Muhtar, Dilxat",Dual-Range Context Aggregation for Efficient Semantic Segmentation in Remote Sensing Images,,2023,9,"Although introducing self-attention mechanisms is beneficial to establish long-range dependencies and explore global context information in the task of remote sensing image semantic segmentation, it results in expensive computation and large memory cost. In this letter, we address this dilemma by proposing a lightweight dual-range context aggregation network (LDCANet) for efficient remote sensing image semantic segmentation. First, a dual-range context aggregation module (DCAM) is designed to aggregate the local features and the global semantic context acquired by convolutions and self-attention, respectively, where self-attention is implemented easily by applying two cascaded linear layers to reduce the computational complexity. Furthermore, a simple and lightweight decoder is employed to combine information from different levels, in which a multilayer perceptron (MLP)-based efficient linear block (ELB) is proposed to yield a strong and efficient representation. Experiments conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen dataset and the Gaofen Image dataset (GID) prove that our LDCANet achieves an excellent trade-off between segmentation accuracy and computational efficiency. In particular, our method achieves 74.12% mean intersection over union (mIoU) on the ISPRS Vaihingen dataset and 61.42% mIoU on the GID with only 4.98-M parameter size.",Convolutional neural networks,lightweight network,multilayer perceptron (MLP),remote sensing,semantic segmentation,"Zhang, Xueliang",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,
Row_97,"Broni-Bediako, Clifford","Xia, Junshi","Song, Jian","Chen, Hongruixuan",Generalized Few-Shot Semantic Segmentation in Remote Sensing: Challenge and Benchmark,,2024,0,"Learning with limited labeled data is a challenging problem in various applications, including remote sensing. Few-shot semantic segmentation is one approach that can encourage deep learning models to learn from few labeled examples for novel classes not seen during the training. The generalized few-shot segmentation setting has an additional challenge which encourages models not only to adapt to the novel classes but also to maintain strong performance on the training base classes. While previous datasets and benchmarks discussed the few-shot segmentation setting in remote sensing, we are the first to propose a generalized few-shot segmentation benchmark for remote sensing. The generalized setting is more realistic and challenging, which necessitates exploring it within the remote sensing context. We release the dataset augmenting OpenEarthMap (OEM) with additional classes labeled for the generalized few-shot evaluation setting. The dataset is released during the OEM land cover mapping generalized few-shot challenge in the learning with limited labeled data for image and video understanding (L3D-IVU) workshop in conjunction with computer vision and pattern recognition (CVPR) 2024. In this work, we summarize the dataset and challenge details in addition to providing the benchmark results on the two phases of the challenge for the validation and test sets.",Benchmark dataset,deep learning,few-shot semantic segmentation,few-shot semantic segmentation,land cover mapping,"Siam, Mennatullah","Yokoya, Naoto",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,land cover mapping,remote sensing,remote sensing,remote sensing,,,,,,,,,,,,,,,,,,
Row_98,"Wang, Jia-Xin","Chen, Si-Bao","Ding, Chris H. Q.","Tang, Jin",RanPaste: Paste Consistency and Pseudo Label for Semisupervised Remote Sensing Image Semantic Segmentation,,2022,44,"With the development of deep learning, remote sensing (RS) image segmentation has been applied with marked success. However, in the process of model training, the large number of labeled images required more expensive annotation. A key challenge is how to make full use of extensive unlabeled images available to improve the segmentation model. In this article, we propose a semisupervised remote sensing image semantic segmentation method defined as RanPaste, which combines labeled images with unlabeled images to improve segmentation performance. First, we obtain pseudo label by randomly pasting part of the ground truth label into the predicted segmentation map. Then, we combine the labeled and unlabeled images to generate rough predictions after strong augmentation. Finally, by using the semisupervised loss, we achieve better performance on remote sensing image segmentation. Our method combines consistency regularization and pseudo label and then utilizes thresholds to gradually improve the model performance. RanPaste enables the model to learn more underlying information in the unlabeled data. Experimental results on six datasets show that RanPaste can learn more latent information from unlabeled data to improve segmentation performance. Besides, our approach achieves better segmentation results on different network structures and datasets.",Image segmentation,Remote sensing,Semantics,Training,Roads,"Luo, Bin",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Task analysis,Sensors,Convolutional neural network,remote sensing (RS),semantic segmentation,semisupervised learning,,,,,,,,,,,,,,,,
Row_99,"Cheng, Shiwei","Li, Baozhu","Sun, Le","Chen, Yuwen",HRRNet: Hierarchical Refinement Residual Network for Semantic Segmentation of Remote Sensing Images,,MAR 2023,7,"Semantic segmentation of high-resolution remote sensing images plays an important role in many practical applications, including precision agriculture and natural disaster assessment. With the emergence of a large number of studies on convolutional neural networks, the performance of the semantic segmentation model of remote sensing images has been dramatically promoted. However, many deep convolutional network models do not fully refine the segmentation result maps, and, in addition, the contextual dependencies of the semantic feature map have not been adequately exploited. This article proposes a hierarchical refinement residual network (HRRNet) to address these issues. The HRRNet mainly consists of ResNet50 as the backbone, attention blocks, and decoders. The attention block consists of a channel attention module (CAM) and a pooling residual attention module (PRAM) and residual structures. Specifically, the feature map output by the four blocks of Resnet50 is passed through the attention block to fully explore the contextual dependencies of the position and channel of the semantic feature map, and, then, the feature maps of each branch are fused step by step to realize the refinement of the feature maps, thereby improving the segmentation performance of the proposed HRRNet. Experiments show that the proposed HRRNet improves segmentation result maps compared with various state-of-the-art networks on Vaihingen and Potsdam datasets.",deep convolution convolutional neural network,attention mechanism,semantic segmentation,remote sensing images,residual structure,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_100,"Wang, Libo","Zhang, Ce","Li, Rui","Duan, Chenxi",Scale-Aware Neural Network for Semantic Segmentation of Multi-Resolution Remote Sensing Images,,DEC 2021,14,"Assigning geospatial objects with specific categories at the pixel level is a fundamental task in remote sensing image analysis. Along with the rapid development of sensor technologies, remotely sensed images can be captured at multiple spatial resolutions (MSR) with information content manifested at different scales. Extracting information from these MSR images represents huge opportunities for enhanced feature representation and characterisation. However, MSR images suffer from two critical issues: (1) increased scale variation of geo-objects and (2) loss of detailed information at coarse spatial resolutions. To bridge these gaps, in this paper, we propose a novel scale-aware neural network (SaNet) for the semantic segmentation of MSR remotely sensed imagery. SaNet deploys a densely connected feature network (DCFFM) module to capture high-quality multi-scale context, such that the scale variation is handled properly and the quality of segmentation is increased for both large and small objects. A spatial feature recalibration (SFRM) module was further incorporated into the network to learn intact semantic content with enhanced spatial relationships, where the negative effects of information loss are removed. The combination of DCFFM and SFRM allows SaNet to learn scale-aware feature representation, which outperforms the existing multi-scale feature representation. Extensive experiments on three semantic segmentation datasets demonstrated the effectiveness of the proposed SaNet in cross-resolution segmentation.",deep convolutional neural network,multiple spatial resolutions,remote sensing,scale-aware feature representation,semantic segmentation,"Meng, Xiaoliang","Atkinson, Peter M.",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_101,"Wang, Shunli","Hu, Qingwu","Wang, Shaohua","Zhao, Pengcheng",Category attention guided network for semantic segmentation of Fine-Resolution remote sensing images,,MAR 2024,3,"The semantic segmentation task is an essential issue in various fields, including land cover classification and cultural heritage investigation. The CNN and Transformer have been widely utilized in semantic segmentation tasks due to notable advancements in deep learning technologies. However, these methodologies may not fully account for remote sensing images' distinctive attributes, including the large intra-class variation and the small inter-class variation. Driven by it, we propose a category attention guided network (CAGNet). Initially, a local feature extraction module is devised to cater to striped objects and features at different scales. Then, we propose a novel concept of category attention for remote sensing images as a feature representation of category differences between pixels. Meanwhile, we designed the Transformer-based and CNN-based category attention guided modules to integrate the proposed category attention into the global scoring functions and local category feature weights, respectively. The network is designed to give more attention to the category features by updating these weights during the training process. Finally, a feature fusion module is developed to integrate global, local, and category multi-scale features and contextual information. A series of extensive experiments along with ablation studies on the UAVid, Vaihingen, and Potsdam datasets indicate that our network outperforms existing methods, including those based on CNN and Transformer.",Category attention,Semantic segmentation,Remote sensing images,CNN,Transformer,"Li, Jiayuan","Ai, Mingyao",,,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,,,,,,,,,,,,,,,,,,,,,,,
Row_102,"Liang, Min","Wang, XiLi",,,DOMAIN ADAPTATION AND SUPER-RESOLUTION BASED BI-DIRECTIONAL SEMANTIC SEGMENTATION METHOD FOR REMOTE SENSING IMAGES,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,1,"Image semantic segmentation methods based on convolutional neural network rely on supervised learning with ground truth, thus cannot be well extended to datasets that all of the data are unlabeled. Domain adaptation can solve the problem of inconsistent feature distribution between target and source domains. However, when the spatial resolution of remote sensing images in the source and target domains are not the same, those domain adaptation methods are not effective. In this paper, we propose a bi-directional semantic segmentation method based on super-resolution and domain adaption (BSSM-SRDA). With the help of generative adversarial learning, the method accomplishes semantic segmentation task from a low-resolution labelled data source domain to a high-resolution unlabelled data target domain by reducing differences in resolution and feature distribution. In addition, we propose a self-supervised learning algorithm that helps the domain discriminator to focus on those target data that has not been aligned with the source domain. The experiments demonstrate the superiority of the proposed method over other state-of-the-art methods on two remote sensing image datasets.",Remote sensing image,semantic segmentation,domain adaptation,super resolution,self-supervised learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_103,"Feng, Jiangfan","Chen, Panyu","Gu, Zhujun","Zeng, Maimai",MDSNet: a multiscale decoupled supervision network for semantic segmentation of remote sensing images,,DEC 31 2023,3,"Recent deep-learning successes have led to a new wave of semantic segmentation in remote sensing (RS) applications. However, most approaches rarely distinguish the role of the body and edge of RS ground objects; thus, our understanding of these semantic parts has been frustrated by the lack of detailed geometry and appearance. Here we present a multiscale decoupled supervision network for RS semantic segmentation. Our proposed framework extends a densely supervised encoder-decoder network with a feature decoupling module that can decouple semantic features with different scales into distinct body and edge components. We further conduct multiscale supervision of the original and decoupled body and edge features to enhance inner consistency and spatial boundaries in remote sensing image (RSI) ground objects, enabling new segmentation designs and semantic components that can learn to perform multiscale geometry and appearance. Our results outperform the previous algorithm and are robust to different datasets. These results demonstrate that decoupled supervision is an effective solution to semantic segmentation tasks of RS images.",Semantic segmentation,remote sensing images,edge supervision,multiscale,y,"Zheng, Wei",,,,INTERNATIONAL JOURNAL OF DIGITAL EARTH,,,,,,,,,,,,,,,,,,,,,,,
Row_104,"Feng, Jiangfan","Yang, Xinyu","Gu, Zhujun","Zeng, Maimai",SMBCNet: A Transformer-Based Approach for Change Detection in Remote Sensing Images through Semantic Segmentation,,JUL 2023,6,"Remote sensing change detection (RSCD) is crucial for our understanding of the dynamic pattern of the Earth's surface and human influence. Recently, transformer-based methodologies have advanced from their powerful global modeling capabilities in RSCD tasks. Nevertheless, they remain under excessive parameterization, which continues to be severely constrained by time and computation resources. Here, we present a transformer-based RSCD model called the Segmentation Multi-Branch Change Detection Network (SMBCNet). Our proposed approach combines a hierarchically structured transformer encoder with a cross-scale enhancement module (CEM) to extract global information with lower complexity. To account for the diverse nature of changes, we introduce a plug-and-play multi-branch change fusion module (MCFM) that integrates temporal features. Within this module, we transform the change detection task into a semantic segmentation problem. Moreover, we identify the Temporal Feature Aggregation Module (TFAM) to facilitate integrating features from diverse spatial scales. These results demonstrate that semantic segmentation is an effective solution to change detection (CD) problems in remote sensing images.",change detection,remote sensing image,semantic segmentation,transformer,,"Zheng, Wei",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_105,"He, Xin","Zhou, Yong","Liu, Bing","Zhao, Jiaqi",Remote sensing image semantic segmentation via class-guided structural interaction and boundary perception,,OCT 15 2024,2,"Existing remote sensing semantic segmentation methods generally ignore the structural information of objects that is vital in the human visual recognition system. The absence of overall structural information often results in weak perceptions of subtle textures and fragmented predictions, especially for complex and variable ground object scenarios. Besides, they still suffer from the semantic ambiguity caused by the unclear object boundary features in remote sensing images. In this paper, we propose a novel remote sensing semantic segmentation framework, called CSBNet, which aims to enhance the capacity of class-guided structural interaction and boundary perception simultaneously. It consists of a class-guided structure interaction module (CSIM), a Transformer-based context aggregation module (TCAM) and a class-guided boundary supervision module (CBSM). The CSIM has the ability to progressively extract the class-specific structural features, i.e., refining the structural information of each class by iteratively exchanging information between initial coarse class tokens and contexts. Meanwhile, the TCAM is constructed to provide CSIM with more discriminative multi- scale contexts without losing spatial features. In particular, the CBSM plays an auxiliary role, which applies the boundary information obtained from the class tokens to supervise the segmentation of boundary regions. When tested on the ISPRS dataset, LoveDA dataset, UAVid dataset, our method significantly outperforms the state-of-the-art remote sensing semantic segmentation approaches.",Transformer,Remote sensing,Semantic segmentation,Structural information,Boundary learning,"Yao, Rui",,,,EXPERT SYSTEMS WITH APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,
Row_106,"Wen, Zhiqiang","Huang, Hongxu","Liu, Shuai",,Multi-scale attention fusion network for semantic segmentation of remote sensing images,,DEC 17 2023,1,"In the realm of high-resolution remote sensing image (HRSI) segmentation, convolutional neural networks have shown their effectiveness and superiority. However, there are still two problems in the segmentation model that generally adopts the encoder-decoder structure in the face of HRSI: 1) Fusing high-level feature maps and low-level feature maps directly in the decoder will make spatial detail features easy to mask; 2) Although self-attention has been used to capture the long-distance dependence of features, the consumption of computing power and memory makes it have many restrictions in practical applications. Aiming at these two problems, this paper proposes a new HRSI segmentation model (named MLWNet). First, the introduction of the maximum pooling module improves the quality of the feature map and obtains the receptive field of the whole map and rich global semantic information. Then, based on a new linear complexity self-attention mechanism, we design a multi-scale linear self-attention module to abstract the correlation between contexts. Finally, the weighted feature fusion helps the feature map restore spatial details and refine the segmentation results. On the two HRSI datasets of ISPRS Potsdam and ISPRS Vaihingen, MLWNet achieved mIOU segmentation accuracy of 78.19% and 71.61%, respectively, which not only outperforms other mainstream segmentation models but also has only 17.423 M parameters. The segmentation model in this study has high precision and small parameters, which can provide decision information for real-time use of remote sensing images.",High-resolution remote sensing images,self-attention,semantic segmentation,,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_107,"Li, Xin","Xu, Feng","Li, Linyang","Xu, Nan",AAFormer: Attention-Attended Transformer for Semantic Segmentation of Remote Sensing Images,,2024,9,"The rapid advancements in remote sensing technology have enabled the widespread availability of fine-resolution remote sensing images (RSIs), offering rich spatial details and semantics. Despite the applicability and scalability of transformers in semantic segmentation of RSIs by learning pairwise contextual affinity, they inevitably introduce irrelevant context, hindering accurate inference of patch semantics. To address this, we propose a novel multihead attention-attended module (AAM) that refines the multihead self-attention mechanism (AM). The AAM filters out irrelevant context while highlighting informative ones by considering the relevance between self-attention maps and the query vector. The AAM generates an attention gate to complement contextual affinity and emphasize the useful ones with a higher weight simultaneously. Leveraging multihead AAM as the core unit, we construct a lightweight attention-attended transformer block (ATB). Subsequently, we devise AAFormer, a pure transformer with a mask transformer decoder, for achieving semantic segmentation of RSIs. We extensively evaluate our approach on the ISPRS Potsdam and LoveDA datasets, demonstrating compelling performance compared to mainstream methods. Additionally, we conduct evaluations to analyze the effects of AAM.",Active appearance model,Transformers,Semantic segmentation,Remote sensing,Semantics,"Liu, Fan","Yuan, Chi","Chen, Ziqi","Lyu, Xin",IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Decoding,Merging,High-resolution remote sensing images (RSIs),local and global contexts,semantic segmentation,transformer,,,,,,,,,,,,,,,,
Row_108,"Li, Xin","Xu, Feng","Xia, Runliang","Lyu, Xin",Hybridizing Cross-Level Contextual and Attentive Representations for Remote Sensing Imagery Semantic Segmentation,,AUG 2021,18,"Semantic segmentation of remote sensing imagery is a fundamental task in intelligent interpretation. Since deep convolutional neural networks (DCNNs) performed considerable insight in learning implicit representations from data, numerous works in recent years have transferred the DCNN-based model to remote sensing data analysis. However, the wide-range observation areas, complex and diverse objects and illumination and imaging angle influence the pixels easily confused, leading to undesirable results. Therefore, a remote sensing imagery semantic segmentation neural network, named HCANet, is proposed to generate representative and discriminative representations for dense predictions. HCANet hybridizes cross-level contextual and attentive representations to emphasize the distinguishability of learned features. First of all, a cross-level contextual representation module (CCRM) is devised to exploit and harness the superpixel contextual information. Moreover, a hybrid representation enhancement module (HREM) is designed to fuse cross-level contextual and self-attentive representations flexibly. Furthermore, the decoder incorporates DUpsampling operation to boost the efficiency losslessly. The extensive experiments are implemented on the Vaihingen and Potsdam benchmarks. In addition, the results indicate that HCANet achieves excellent performance on overall accuracy and mean intersection over union. In addition, the ablation study further verifies the superiority of CCRM.",semantic segmentation,remote sensing imagery,cross-level contextual information,representation enhancement,,"Gao, Hongmin","Tong, Yao",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_109,"Zhao, Danpei","Yuan, Bo","Gao, Yue","Qi, Xinhu",UGCNet: An Unsupervised Semantic Segmentation Network Embedded With Geometry Consistency for Remote-Sensing Images,,2022,7,"In remote-sensing image (RSI) semantic segmentation, the dependence on large-scale and pixel-level annotated data has been a critical factor restricting its development. In this letter, we propose an unsupervised semantic segmentation network embedded with geometry consistency (UGCNet) for RSIs, which imports the adversarial-generative learning strategy into a semantic segmentation network. The proposed UGCNet can be trained on a source-domain dataset and achieve accurate segmentation results on a different target-domain dataset. Furthermore, for refining the remote-sensing target geometric representation such as densely distributed buildings, we propose a geometry-consistency (GC) constraint that can be embedded in both image-domain adaptation process and semantic segmentation network. Therefore, our model could achieve cross-domain semantic segmentation with target geometric property preservation. The experimental results on Massachusetts and Inria buildings datasets prove that the proposed unsupervised UGCNet could achieve a very comparable segmentation accuracy with the fully supervised model, which validates the effectiveness of the proposed method.",Image segmentation,Semantics,Training,Adaptation models,Remote sensing,"Shi, Zhenwei",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Geometry,Decoding,Generative-adversarial learning,geometry consistency (GC),remote-sensing images (RSIs),semantic segmentation,unsupervised,,,,,,,,,,,,,,,
Row_110,"Chen, Guanzhou","He, Chanjuan","Wang, Tong","Zhu, Kun",A Superpixel-Guided Unsupervised Fast Semantic Segmentation Method of Remote Sensing Images,,2022,8,"Semantic segmentation is one of the fundamental tasks of pixel-level remote sensing image analysis. Currently, most high-performance semantic segmentation methods are trained in a supervised learning manner. These methods require a large number of image labels as support, but manual annotations are difficult to obtain. To address the problem, we propose an efficient unsupervised remote sensing image segmentation method based on superpixel segmentation and fully convolutional networks (FCNs) in this letter. Our method can achieve pixel-level images segmentation of various scales rapidly without any manual labels or prior knowledge. We use the superpixel segmentation results as synthetic ground truth to guide the gradient descent direction during FCN training. In experiments, our method achieved high performance compared with current unsupervised image segmentation methods on three public datasets. Specifically, our method achieves an adjusted mutual information (AMI) score of 0.2955 on the Gaofen Image Dataset (GID), while processing each image of size 7200 x 6800 pixels in just 30 s.",Deep learning (DL),fully convolutional networks (FCNs),remote sensing,semantic segmentation,superpixel,"Liao, Puyun","Zhang, Xiaodong",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,unsupervised learning,,,,,,,,,,,,,,,,,,,,,
Row_111,"Guo, Shichen","Yang, Qi","Xiang, Shiming","Wang, Pengfei",Dynamic High-Resolution Network for Semantic Segmentation in Remote-Sensing Images,,APR 26 2023,2,"Semantic segmentation of remote-sensing (RS) images is one of the most fundamental tasks in the understanding of a remote-sensing scene. However, high-resolution RS images contain plentiful detailed information about ground objects, which scatter everywhere spatially and have variable sizes, styles, and visual appearances. Due to the high similarity between classes and diversity within classes, it is challenging to obtain satisfactory and accurate semantic segmentation results. This paper proposes a Dynamic High-Resolution Network (DyHRNet) to solve this problem. Our proposed network takes HRNet as a super-architecture, aiming to leverage the important connections and channels by further investigating the parallel streams at different resolution representations of the original HRNet. The learning task is conducted under the framework of a neural architecture search (NAS) and channel-wise attention module. Specifically, the Accelerated Proximal Gradient (APG) algorithm is introduced to iteratively solve the sparse regularization subproblem from the perspective of neural architecture search. In this way, valuable connections are selected for cross-resolution feature fusion. In addition, a channel-wise attention module is designed to weight the channel contributions for feature aggregation. Finally, DyHRNet fully realizes the dynamic advantages of data adaptability by combining the APG algorithm and channel-wise attention module simultaneously. Compared with nine classical or state-of-the-art models (FCN, UNet, PSPNet, DeepLabV3+, OCRNet, SETR, SegFormer, HRNet+FCN, and HRNet+OCR), DyHRNet has shown high performance on three public challenging RS image datasets (Vaihingen, Potsdam, and LoveDA). Furthermore, the visual segmentation results, the learned structures, the iteration process analysis, and the ablation study all demonstrate the effectiveness of our proposed model.",semantic segmentation,remote-sensing image,neural architecture search,sparse regularization,HRNet,"Wang, Xuezhi",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_112,"Zheng, Chen","Li, Jingying","Chen, Yuncheng","Wang, Leiguang",A Generalization Sample Learning Method of Deep Learning for Semantic Segmentation of Remote Sensing Images,,2023,1,"Deep learning methods have been widely studied in the semantic segmentation field of the remote sensing image. Training images play an important role in these methods; however, each training image usually contains not only the generalization information of each land category but also the specific interclass context between different categories. The specific interclass context prevents deep learning methods from focusing on generalization information learning during training and limits the performance on different data distributions. This article proposes a generalization sampling learning method of deep convolutional neural network (GSL-CNN) to emphasize generalization information learning for the semantic segmentation of remote sensing images. The proposed method develops a new CBR sampling strategy that contains three modules: category grouping (C), basic unit extraction (B), and random combination (R). Module C collects each land category map and strips away the specific interclass context from the raw annotated image. Module B extracts basic units with different granularities from each land category map, and each basic unit can keep the generalization information of this category. Module R aims to enhance the robustness against different data distributions by randomly picking basic units of different categories and randomly generating their interclass context. The new GSL-CNN method integrates the CBR sampling strategy with the convolutional neural network (CNN) model for semantic segmentation. Experiments on different remote sensing datasets and 15 state-of-the-art CNN models validated that the proposed method has the potential of improving the generalization ability of the CNN method from a sampling perspective.",Deep learning,generalization,remote sensing,sampling,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_113,"Sun, Wenjie","Zhang, Jie","Lei, Yujie","Hong, Danfeng",RSProtoSeg: High Spatial Resolution Remote Sensing Images Segmentation Based on Non-Learnable Prototypes,,2024,3,"Semantic segmentation of high spatial resolution (HSR) remote sensing images presents unique challenges due to the imbalanced foreground-background distribution and large intraclass variance. This study proposes a novel semantic segmentation algorithm based on non-learnable prototypes, named RSProtoSeg. This approach optimizes the spatial relationship between foreground-background prototypes and intraclass prototypes. Specifically, we propose a foreground-background distance optimization loss function to enhance sparsity between these phototypes, effectively mitigating foreground-background distribution imbalances. Moreover, we introduce an online discrete clustering module that represents each class with a set of prototypes and adds an adaptive regular term penalty to promote sparse structure and reduce the variance issue. Evaluation on three remote sensing datasets (iSAID, ISPRS Potsdam, and Vaihingen) demonstrates significant accuracy improvements, aligning our approach with state-of-the-art methods. Our non-learnable prototype-based approach offers a promising solution for semantic segmentation in HSR remote sensing images.",Neural network,prototype learning,remote sensing,semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_114,"Du, Wen-Liang","Gu, Yang","Zhao, Jiaqi","Zhu, Hancheng",A Mamba-Diffusion Framework for Multimodal Remote Sensing Image Semantic Segmentation,,2024,0,"Recent advances in deep learning have made significant progress in multimodal remote sensing semantic segmentation. However, current methods face challenges in maintaining geometric consistency, particularly when dealing with large objects, resulting in fragmented segmentation masks. We propose a Mamba-diffusion framework to preserve geometric consistency in segmentation masks. This framework preserves geometric consistency by introducing a generative diffusion-based semantic segmentation pipeline and developing a Mamba-based multimodal fusion model. The fusion model fuses the multimodal images in multiple scales and scanning mechanisms by a double cross-fusion (DCF) module. Then, the cross-modal information is further integrated by a dual-splitting structured state-space (DS-S4) model. Finally, the diffusion-based segmentation pipeline predicts semantic masks by progressively refining random Gaussian noise, guided by fused multimodal features. Our experimental results, verified on WHU-OPT-SAR and Hunan datasets, demonstrate that the proposed framework surpasses state-of-the-art (SOTA) methods by a considerable margin. Our codes are available at https://github.com/WenliangDu/MambaDiffusion.",Semantics,Semantic segmentation,Pipelines,Transformers,Remote sensing,"Yao, Rui","Zhou, Yong",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Visualization,Noise measurement,Training,Shape,Diffusion-based segmentation,Mamba-based fusion,multimodal semantic segmentation,remote sensing,,,,,,,,,,,,,
Row_115,"Wang, Zhechao","Cheng, Peirui","Duan, Shujing","Chen, Kaiqiang",DCP-Net: A Distributed Collaborative Perception Network for Remote Sensing Semantic Segmentation,,JUL 2024,1,"Collaborative perception enhances onboard perceptual capability by integrating features from other platforms, effectively mitigating the compromised accuracy caused by a restricted observational range and vulnerability to interference. However, current implementations of collaborative perception overlook the prevalent issues of both limited and low-reliability communication, as well as misaligned observations in remote sensing. To address this problem, this article presents an innovative distributed collaborative perception network (DCP-Net) specifically designed for remote sensing applications. Firstly, a self-mutual information match module is proposed to identify collaboration opportunities and select suitable partners. This module prioritizes critical collaborative features and reduces redundant transmission for better adaptation to weak communication in remote sensing. Secondly, a related feature fusion module is devised to tackle the misalignment between local and collaborative features due to the multiangle observations, improving the quality of fused features for the downstream task. We conduct extensive experiments and visualization analyses using three semantic segmentation datasets, namely Potsdam, iSAID, and DFC23. The results demonstrate that DCP-Net outperforms the existing collaborative perception methods comprehensively, improving mIoU by 2.61% to 16.89% at the highest collaboration efficiency and achieving state-of-the-art performance.",collaborative perception,distributed neural network,semantic segmentation,remote sensing,,"Wang, Zhirui","Li, Xinming","Sun, Xian",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_116,"Qian, Zhaoyong","Cao, Yuhua","Shi, Zengkai","Qiu, Luyi",A Semantic Segmentation Method for Remote Sensing Images based on Deeplab v3,2021 2ND INTERNATIONAL CONFERENCE ON BIG DATA & ARTIFICIAL INTELLIGENCE & SOFTWARE ENGINEERING (ICBASE 2021),2021,5,"As a basic technology for image analysis and scene understanding, image semantic segmentation is widely used in the field of remote sensing images. It can better help humans understand the world's scenes and analyze potential changes from the top view of the earth, which has high practical value and development prospects. In this paper, a semantic segmentation method for high-resolution remote sensing images based on DeepLab v3[9] is proposed. The network structure of this method uses the atrous spatial pyramid pooling (ASPP) to extract the multi-scale feature information of high-resolution remote sensing images. Besides, the experimental part of this paper is evaluated on the public dataset of remote sensing images named ISPRS Vaihingen. Experimental results show that the proposed method can achieve an average accuracy of 81.72% on the ISPRS Vaihingen dataset. Therefore, this method can be used as an automated tool for remote sensing semantic segmentation.",remote sensing images,semantic segmentation,multi-scale feature information,ASPP module,DeepLab v3,"Shi, Chenguang",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_117,"Wang, Jia-Xin","Chen, Si-Bao","Ding, Chris H. Q.","Tang, Jin",Semi-Supervised Semantic Segmentation of Remote Sensing Images With Iterative Contrastive Network,,2022,27,"With the development of deep learning, semantic segmentation of remote sensing images has made great progress. However, segmentation algorithms based on deep learning usually require a huge number of labeled images for model training. For remote sensing images, pixel-level annotation usually consumes expensive resources. To alleviate this problem, this letter proposes a semi-supervised segmentation method of remote sensing images based on an iterative contrastive network. This method combines few labeled images and more unlabeled images to significantly improve the model performance. First, contrastive networks continuously learn more potential information by using better pseudo labels. Then, the iterative training method keeps the differences between models to better improve the segmentation performance. The semi-supervised experiments on different remote sensing datasets prove that this method has a better performance than the related methods. Code is available at https://github.com/VCISwang/ICNet.",Training,Image segmentation,Predictive models,Remote sensing,Data models,"Luo, Bin",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Iterative methods,Semantics,Contrastive network,remote sensing,semantic segmentation,semi-supervised learning,,,,,,,,,,,,,,,,
Row_118,"Chen, Guanzhou","Tan, Xiaoliang","Guo, Beibei","Zhu, Kun",SDFCNv2: An Improved FCN Framework for Remote Sensing Images Semantic Segmentation,,DEC 2021,29,"Semantic segmentation is a fundamental task in remote sensing image analysis (RSIA). Fully convolutional networks (FCNs) have achieved state-of-the-art performance in the task of semantic segmentation of natural scene images. However, due to distinctive differences between natural scene images and remotely-sensed (RS) images, FCN-based semantic segmentation methods from the field of computer vision cannot achieve promising performances on RS images without modifications. In previous work, we proposed an RS image semantic segmentation framework SDFCNv1, combined with a majority voting postprocessing method. Nevertheless, it still has some drawbacks, such as small receptive field and large number of parameters. In this paper, we propose an improved semantic segmentation framework SDFCNv2 based on SDFCNv1, to conduct optimal semantic segmentation on RS images. We first construct a novel FCN model with hybrid basic convolutional (HBC) blocks and spatial-channel-fusion squeeze-and-excitation (SCFSE) modules, which occupies a larger receptive field and fewer network model parameters. We also put forward a data augmentation method based on spectral-specific stochastic-gamma-transform-based (SSSGT-based) during the model training process to improve generalizability of our model. Besides, we design a mask-weighted voting decision fusion postprocessing algorithm for image segmentation on overlarge RS images. We conducted several comparative experiments on two public datasets and a real surveying and mapping dataset. Extensive experimental results demonstrate that compared with the SDFCNv1 framework, our SDFCNv2 framework can increase the mIoU metric by up to 5.22% while only using about half of parameters.",fully convolutional networks (FCNs),convolutional neural networks (CNNs),deep learning,semantic segmentation,remote sensing,"Liao, Puyun","Wang, Tong","Wang, Qing","Zhang, Xiaodong",REMOTE SENSING,,SDFCN,,,,,,,,,,,,,,,,,,,,,
Row_119,"Chen, Yaxiong","Wang, Yujie","Xiong, Shengwu","Lu, Xiaoqiang",Integrating Detailed Features and Global Contexts for Semantic Segmentation in Ultrahigh-Resolution Remote Sensing Images,,2024,4,"Semantic segmentation of ultrahigh-resolution (UHR) remote sensing images is a fundamental task for many downstream applications. Achieving precise pixel-level classification is paramount for obtaining exceptional segmentation results. This challenge becomes even more complex due to the need to address intricate segmentation boundaries and accurately delineate small objects within the remote sensing imagery. To meet these demands effectively, it is critical to integrate two crucial components: global contextual information and spatial detail feature information. In response to this imperative, the multilevel context-aware segmentation network (MCSNet) emerges as a promising solution. MCSNet is engineered to not only model the overarching global context but also extract intricate spatial detail features, thereby optimizing segmentation outcomes. The strength of MCSNet lies in its two pivotal modules, the spatial detail feature extraction (SDFE) module and the refined multiscale feature fusion (RMFF) module. Moreover, to further harness the potential of MCSNet, a multitask learning approach is employed. This approach integrates boundary detection and semantic segmentation, ensuring that the network is well-rounded in its segmentation capabilities. The efficacy of MCSNet is rigorously demonstrated through comprehensive experiments conducted on two established international society for photogrammetry and remote sensing (ISPRS) 2-D semantic labeling datasets: Potsdam and Vaihingen. These experiments unequivocally establish MCSNet stands as a pioneering solution, that delivers state-of-the-art performance, as evidenced by its outstanding mean intersection over union (mIoU) and mean $F1$ -score (mF1) metrics. The code is available at: https://github.com/WUTCM-Lab/MCSNet.",Semantics,Cascade,multilevel fusion,multitask learning,remote sensing,"Zhu, Xiao Xiang","Mou, Lichao",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_120,"Moliner, Eloi","Romero, Luis Salgueiro","Vilaplana, Veronica",,WEAKLY SUPERVISED SEMANTIC SEGMENTATION FOR REMOTE SENSING HYPERSPECTRAL IMAGING,"2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING",2020,7,"This paper studies the problem of training a semantic segmentation neural network with weak annotations, in order to be applied in aerial vegetation images from Teide National Park. It proposes a Deep Seeded Region Growing system which consists on training a semantic segmentation network from a set of seeds generated by a Support Vector Machine. A region growing algorithm module is applied to the seeds to progressively increase the pixel-level supervision. The proposed method performs better than an SVM, which is one of the most popular segmentation tools in remote sensing image applications.",Weakly-supervised segmentation,remote sensing,hyperspectral image,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_121,"Liu, Yuheng","Zhang, Yifan","Wang, Ye","Mei, Shaohui",Rethinking Transformers for Semantic Segmentation of Remote Sensing Images,,2023,45,"Transformer has been widely applied in image processing tasks as a substitute for convolutional neural networks (CNNs) for feature extraction due to its superiority in global context modeling and flexibility in model generalization. However, the existing transformer-based methods for semantic segmentation of remote sensing (RS) images are still with several limitations, which can be summarized into two main aspects: 1) the transformer encoder is generally combined with CNN-based decoder, leading to inconsistency in feature representations; and 2) the strategies for global and local context information utilization are not sufficiently effective. Therefore, in this article, a global-local transformer segmentor (GLOTS) framework is proposed for the semantic segmentation of RS images to acquire consistent feature representations by adopting transformers for both encoding and decoding, in which a masked image modeling (MIM) pretrained transformer encoder is adopted to learn semantic-rich representations of input images and a multiscale global-local transformer decoder is designed to fully exploit the global and local features. Specifically, the transformer decoder uses a feature separation-aggregation module (FSAM) to utilize the feature adequately at different scales and adopts a global-local attention module (GLAM) containing global attention block (GAB) and local attention block (LAB) to capture the global and local context information, respectively. Furthermore, a learnable progressive upsampling strategy (LPUS) is proposed to restore the resolution progressively, which can flexibly recover the fine-grained details in the upsampling process. The experiment results on the three benchmark RS datasets demonstrate that the proposed GLOTS is capable of achieving better performance with some state-of-the-art methods, and the superiority of the proposed framework is also verified by ablation studies. The code will be available at https://github.com/lyhnsn/GLOTS.",Encoder-decoder structure,global-local transformer,remote sensing (RS),semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_122,"Du, Xinran","He, Shumeng","Yang, Houqun","Wang, Chunxiao",Multi-Field Context Fusion Network for Semantic Segmentation of High-Spatial-Resolution Remote Sensing Images,,NOV 2022,4,"High spatial resolution (HSR) remote sensing images have a wide range of application prospects in the fields of urban planning, agricultural planning and military training. Therefore, the research on the semantic segmentation of remote sensing images becomes extremely important. However, large data volume and the complex background of HSR remote sensing images put great pressure on the algorithm efficiency. Although the pressure on the GPU can be relieved by down-sampling the image or cropping it into small patches for separate processing, the loss of local details or global contextual information can lead to limited segmentation accuracy. In this study, we propose a multi-field context fusion network (MCFNet), which can preserve both global and local information efficiently. The method consists of three modules: a backbone network, a patch selection module (PSM), and a multi-field context fusion module (FM). Specifically, we propose a confidence-based local selection criterion in the PSM, which adaptively selects local locations in the image that are poorly segmented. Subsequently, the FM dynamically aggregates the semantic information of multiple visual fields centered on that local location to enhance the segmentation of these local locations. Since MCFNet only performs segmentation enhancement on local locations in an image, it can improve segmentation accuracy without consuming excessive GPU memory. We implement our method on two high spatial resolution remote sensing image datasets, DeepGlobe and Potsdam, and compare the proposed method with state-of-the-art methods. The results show that the MCFNet method achieves the best balance in terms of segmentation accuracy, memory efficiency, and inference speed.",semantic segmentation,high spatial resolution remote sensing images,memory efficiency,,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_123,"Zhang, Yongjun","Wang, Yameng","Wan, Yi","Zhou, Wenming",PointBoost: LiDAR-Enhanced Semantic Segmentation of Remote Sensing Imagery,,2023,3,"Semantic segmentation of imagery is typically reliant on texture information from raster images, which limits its accuracy due to the inherently 2-D nature of the plane. To address the nonnegligible domain gap between different metric spaces, multimodal methods have been introduced that incorporate Light Detection and Ranging (LiDAR) derived feature maps. This converts multimodal joint semantic segmentation between 3-D point clouds and 2-D optical imagery into a feature extraction process for the 2.5-D product, which is achieved by concatenating LiDAR-derived feather maps, such as digital surface models, with the optical images. However, the information sources for these methods are still limited to 2-D, and certain properties of point clouds are lost as a result. In this study, we propose PointBoost, an effective sequential segmentation framework that can work directly with cross-modal data of LiDAR point clouds and imagery, which is able to extract richer semantic features from cross-dimensional and cross-modal information. Ablation experiments demonstrate that PointBoost can take full advantage of the 3-D topological structure between points and attribute information of point clouds, which is often discarded by other methods. Experiments on three multimodal datasets, namely N3C-California, ISPRS Vaihingen, and GRSS DFC 2018, show that our method achieves superior performance with good generalization.",Light Detection and Ranging (LiDAR),remote sensing imagery,semantic segmentation,,,"Zhang, Bin",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_124,"Zhao, Shuang","Feng, Zezhen","Chen, Lei","Li, Guandian",DANet: A Semantic Segmentation Network for Remote Sensing of Roads Based on Dual-ASPP Structure,,AUG 2023,6,"Semantic segmentation of roads in remote-sensing images is a challenging task. This paper proposes a semantic segmentation model, DANet, for remote-sensing image road semantic segmentation. The model addresses the problems of missing, misclassification, and strong jaggedness of segmented target edges faced by other semantic segmentation networks when dealing with complex and diverse remote-sensing images. The proposed model uses two ASPP structures for multi-scale feature fusion and combines the DarkNet network structure for downsampling with the SegNet network structure for upsampling. This improves the model's ability to extract road feature information from remote-sensing images. Using the CHN-CUG Roads Dataset, we have confirmed that the proposed network structure, Re, has demonstrated a 1.15% improvement in accuracy compared to U-Net. Furthermore, the road IoU has shown a 1.09% enhancement compared to HRNet-V2. Additionally, there is a 1.13% increase in F1-score compared to U-Net.",semantic segmentation,remote-sensing images,roads,dual-ASPP modules,,,,,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,
Row_125,"Li, Yifan","Liu, Ziqian","Yang, Junli","Zhang, Haopeng",Wavelet Transform Feature Enhancement for Semantic Segmentation of Remote Sensing Images,,DEC 2023,6,"With developments in deep learning, semantic segmentation of remote sensing images has made great progress. Currently, mainstream methods are based on convolutional neural networks (CNNs) or vision transformers. However, these methods are not very effective in extracting features from remote sensing images, which are usually of high resolution with plenty of detail. Operations including downsampling will cause the loss of such features. To address this problem, we propose a novel module called Hierarchical Wavelet Feature Enhancement (WFE). The WFE module involves three sequential steps: (1) performing multi-scale decomposition of an input image based on the discrete wavelet transform; (2) enhancing the high-frequency sub-bands of the input image; and (3) feeding them back to the corresponding layers of the network. Our module can be easily integrated into various existing CNNs and transformers, and does not require additional pre-training. We conducted experiments on the ISPRS Potsdam and ISPRS Vaihingen datasets, with results showing that our method improves the benchmarks of CNNs and transformers while performing little additional computation.",discrete wavelet transform,remote sensing images,feature enhancement,semantic segmentation,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_126,"Xiao, Hongfeng","Yao, Wei","Chen, Haobin","Cheng, Li",SCDA: A Style and Content Domain Adaptive Semantic Segmentation Method for Remote Sensing Images,,OCT 2023,2,"Due to the differences in imaging methods and acquisition areas, remote sensing datasets can exhibit significant variations in both image style and content. In addition, the ground objects can be quite different in scale even within the same remote sensing image. These differences should be considered in remote sensing image segmentation tasks. Inspired by the recently developed domain generalization model WildNet, we propose a domain adaption framework named ""Style and Content Domain Adaptation"" (SCDA) for semantic segmentation tasks involving multiple remote sensing datasets with different data distributions. SCDA uses residual style feature transfer (RSFT) in the shallow layer of the baseline network model to enable source domain images to obtain style features from the target domain and reduce the loss of source domain content information. Considering the scale difference of different ground objects in remote sensing images, SCDA uses the projection of the source domain images, the style-transferred source domain images, and the target domain images to construct a multiscale content adaptation learning (MCAL) loss. This enables the model to capture multiscale target domain content information. Experiments show that the proposed method has obvious domain adaptability in remote sensing image segmentation. When performing cross-domain segmentation tasks from VaihingenIRRG to PotsdamIRRG, mIOU is 48.64%, and the F1 is 63.11%, marking improvements of 1.21% and 0.45%, respectively, compared with state-of-the-art methods. When performing cross-domain segmentation tasks from VaihingenIRRG to PotsdamRGB, the mIOU is 44.38%, an improvement of 0.77% over the most advanced methods. In summary, SCDA improves the semantic segmentation of remote sensing images through domain adaptation for both style and content. It fully utilizes multiple innovative modules and strategies to enhance the performance and the stability of the model.",remote sensing images,domain adaptation,cross-domain semantic segmentation,contrast learning,style transfer,"Li, Bo","Ren, Longfei",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_127,"Xu, Zhiyong","Zhang, Weicun","Zhang, Tianxiang","Li, Jiangyun",HRCNet: High-Resolution Context Extraction Network for Semantic Segmentation of Remote Sensing Images,,JAN 2021,106,"Semantic segmentation is a significant method in remote sensing image (RSIs) processing and has been widely used in various applications. Conventional convolutional neural network (CNN)-based semantic segmentation methods are likely to lose the spatial information in the feature extraction stage and usually pay little attention to global context information. Moreover, the imbalance of category scale and uncertain boundary information meanwhile exists in RSIs, which also brings a challenging problem to the semantic segmentation task. To overcome these problems, a high-resolution context extraction network (HRCNet) based on a high-resolution network (HRNet) is proposed in this paper. In this approach, the HRNet structure is adopted to keep the spatial information. Moreover, the light-weight dual attention (LDA) module is designed to obtain global context information in the feature extraction stage and the feature enhancement feature pyramid (FEFP) structure is promoted and employed to fuse the contextual information of different scales. In addition, to achieve the boundary information, we design the boundary aware (BA) module combined with the boundary aware loss (BAloss) function. The experimental results evaluated on Potsdam and Vaihingen datasets show that the proposed approach can significantly improve the boundary and segmentation performance up to 92.0% and 92.3% on overall accuracy scores, respectively. As a consequence, it is envisaged that the proposed HRCNet model will be an advantage in remote sensing images segmentation.",semantic segmentation,remote sensing,deep learning,high resolution,global context information,,,,,REMOTE SENSING,,boundary,,,,,,,,,,,,,,,,,,,,,
Row_128,"Chen, Hui","Qin, Yuanshou","Liu, Xinyuan","Wang, Haitao",An improved DeepLabv3+lightweight network for remote-sensing image semantic segmentation,,APR 2024,4,"To improve the accuracy of remote-sensing image semantic segmentation in complex scenario, an improved DeepLabv3+ lightweight neural network is proposed. Specifically, the lightweight network MobileNetv2 is used as the backbone network. In atrous spatial pyramid pooling (ASPP), to alleviate the gridding effect, the Dilated Convolution in original DeepLabv3+ network is replaced with the Hybrid Dilated Convolution (HDC) module. In addition, the traditional spatial mean pooling is replaced by the strip pooling module (SPN) to improve the local segmentation effect. In the decoder, to obtain the rich low-level target edge information, the ResNet50 residual network is added after the low-level feature fusion. To enhance the shallow semantic information, the efficient and lightweight Normalization-based Attention Module (NAM) is added to capture the feature information of small target objects. The results show that, under the INRIA Aerial Image Dataset and same parameter setting, the Mean Pixel Accuracy (MPA) and Mean Intersection over Union (MIoU) are generally best than DeepLabv3+ , U-Net, and PSP-Net, which are respectively improved by 1.22%, - 0.22%, and 2.22% and 2.17%, 1.35%, and 3.42%. Our proposed method has also a good performance on the small object segmentation and multi-object segmentation. What's more, it significantly converges faster with fewer model parameters and stronger computing power while ensuring the segmentation effect. It is proved to be robust and can provide a methodological reference for high-precision remote-sensing image semantic segmentation.",Remote-sensing image,Semantic segmentation,DeepLabv3+,Deep learning,Lightweight network,"Zhao, Jinling",,,,COMPLEX & INTELLIGENT SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,
Row_129,"Zhang, Guangzhen","Jiang, Wangyang",,,Remote Sensing Image Semantic Segmentation Method Based on a Deep Convolutional Neural Network and Multiscale Feature Fusion,,2023,1,"There are many problems with remote sensing images, such as large data scales, complex illumination conditions, occlusion, and dense targets. The existing semantic segmentation methods for remote sensing images are not accurate enough for small and irregular target segmentation results, and the edge extraction results are poor. The authors propose a remote sensing image segmentation method based on a DCNN and multiscale feature fusion. Firstly, an end-to-end remote sensing image segmentation model using complete residual connection and multiscale feature fusion was designed based on a deep convolutional encoder-decoder network. Secondly, weighted high-level features were obtained using an attention mechanism, which better preserved the edges, texture, and other information of remote sensing images. The experimental results on ISPRS Potsdam and Urban Drone datasets show that compared with the comparison methods, this method has better segmentation effect on small and irregular objects and achieves the best segmentation performance while ensuring the computation speed.",Attention Mechanism,Deep Learning,Multiscale Feature Fusion,Remote Sensing Images,Semantic Segmentation,,,,,INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,
Row_130,"Zhao, Yuanhao","Sun, Genyun","Ling, Ziyan","Zhang, Aizhu",Point-Based Weakly Supervised Deep Learning for Semantic Segmentation of Remote Sensing Images,,2024,0,"Weakly supervised semantic segmentation methods can effectively alleviate the problem of high cost and difficult access to annotation in traditional methods. Among these approaches, point annotated semantic label not only offers a more affordable option but also provides accurate location and category information, playing an indispensable role in current research. However, point annotation labeling encounters challenges such as missing global and texture information, and limiting segmentation accuracy and efficiency while being susceptible to noise interference. For the above problems, a weakly supervised remote sensing image classification framework based on point annotated semantic label is proposed, which consists of three components: data augmentation, Pixel-Net, and iterative superpixel-based sample expansion (ISSE). First, the data augmentation method is used to generate a sufficient number of training samples. Subsequently, the weakly supervised network Pixel-Net is trained using point annotated semantic labels. PixelNet incorporates traditional image processing techniques such as edge detection and blurring into deep learning, enabling effective learning of edge and spectral semantic details while reducing the impact of noise on classification results. Finally, ISSE leverages contextual information from superpixels and pseudo-labels to enrich the valuable information in weakly supervised labels, thereby improving the model's classification performance. In the experiments, existing semantic segmentation methods and Pixel-Net are evaluated on the Vaihingen and Zurich Summer datasets, and the effectiveness of ISSE is verified. The results show that Pixel-Net achieves the best segmentation accuracy on both datasets, while ISSE can effectively utilize the existing point annotation labels to mitigate the effect of noise and thus improve the accuracy of weakly supervised semantic segmentation.",Point annotated semantic label,remote sensing,semantic segmentation,weakly supervised learning,,"Jia, Xiuping",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_131,"Li, Boyang","Zhang, Yu","Zhang, Youmei","Li, Bin",Dual-Path Feature Fusion Network for Semantic Segmentation of Remote Sensing Images,,2024,0,"Both global contextual information and local texture information are of vital importance for the semantic segmentation of remote sensing images due to the high spatial resolution of remote sensing images and large variations in intraclass object size. In this letter, we propose a novel dual-path feature fusion semantic segmentation network for remote sensing images. A pure convolutional module called dual-path feature extraction (DPFE) module is applied to model global contextual and local texture features simultaneously with low complexity. Inspired by ConvNeXt with comparable global contextual modeling capacity with Transformer, the global path of DPFE draws some successful strategies of ConvNeXt to generate powerful global feature. Meanwhile, an attention feature fusion (AFF) module is proposed, which achieves the global and local feature comprehensive fusion by exploring the correlation of channels through attention mechanism. The proposed network is evaluated on Vaihingen and Potsdam benchmarks and the quantitative results show the proposed network can achieve overall accuracy (OA) of 91.3% and 89.7%, respectively, which are better than several representative semantic segmentation approaches.",Feature extraction,Remote sensing,Transformers,Semantic segmentation,Correlation,"Li, Zhenhao",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Convolutional neural networks,Sensors,Attention mechanism,ConvNeXt,global contextual information,local texture feature,remote sensing image,semantic segmentation,,,,,,,,,,,,,,
Row_132,"Long, Wei","Zhang, Yongjun","Cui, Zhongwei","Xu, Yujie",Threshold Attention Network for Semantic Segmentation of Remote Sensing Images,,2023,15,"Semantic segmentation of remote sensing images is essential for various applications, including vegetation monitoring, disaster management, and urban planning. Previous studies have demonstrated that the self-attention mechanism (SA) is an effective approach for designing segmentation networks that can capture long-range pixel dependencies. SA enables the network to model the global dependencies between the input features, resulting in improved segmentation outcomes. However, the high density of attentional feature maps used in this mechanism causes exponential increases in computational complexity. In addition, it introduces redundant information that negatively impacts the feature representation. Inspired by traditional threshold segmentation algorithms, we propose a novel threshold attention mechanism (TAM). This mechanism significantly reduces computational effort while also better modeling the correlation between different regions of the feature map. Based on TAM, we present a threshold attention network (TANet) for semantic segmentation. The TANet consists of an attentional feature enhancement module (AFEM) for global feature enhancement of shallow features and a threshold attention pyramid pooling (TAPP) module for acquiring feature information at different scales for deep features. We have conducted extensive experiments on the international society for photogrammetry and remote sensing (ISPRS) Vaihingen and Potsdam datasets. The results demonstrate the validity and superiority of our proposed TANet compared with most state-of-the-art models.",Feature extraction,Semantics,Semantic segmentation,Computational modeling,Remote sensing,"Zhang, Xuexue",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Correlation,Computational complexity,Remote sensing images,self-attention mechanism (SA),semantic segmentation,threshold attention mechanism (TAM),threshold attention network (TANet),,,,,,,,,,,,,,,
Row_133,"Yang, Zhujun","Yan, Zhiyuan","Diao, Wenhui","Zhang, Qiang",Label Propagation and Contrastive Regularization for Semisupervised Semantic Segmentation of Remote Sensing Images,,2023,8,"Remarkable progress based on deep neural networks has been achieved in the semantic segmentation of remote sensing images (RSIs). However, pixel-level labeling is expensive for RSIs. Semisupervised semantic segmentation becomes an alternative approach to reduce the cost of annotation, and it is crucial to utilize efficiently a large number of unlabeled data. Nevertheless, inevitably, there is an unbalanced class distribution between labeled and unlabeled data in a remote sensing scene. Existing semisupervised methods train unlabeled images in isolation from labeled images and only learn reliable pixel pseudo-labels, leading to underutilization of unlabeled images. This article proposes a novel semisupervised semantic segmentation approach based on label propagation and contrastive regularization for RSIs. Specifically, the unlabeled images are augmented by randomly copy-pasting the class regions from labeled images. A prototype feature constraint module is used to enforce the constraint on the pixel features of unlabeled images relying on the prototype features from labeled images, achieving feature alignment on the entire dataset. Furthermore, we present the region contrastive learning (RCL) module that guides the model to learn feature consistency under different perturbations and compact feature representations over class regions on unlabeled images. Extensive experimental results on multiple remote sensing datasets demonstrate that our proposed approach achieves superior performance compared with state-of-the-art semisupervised semantic segmentation methods.",Semantic segmentation,Training,Perturbation methods,Remote sensing,Prototypes,"Kang, Yuzhuo","Li, Junxi","Li, Xinming","Sun, Xian",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Reliability,Sensors,Contrastive regularization,label propagation,remote sensing images (RSIs),semantic segmentation,semisupervised learning (SSL),,,,,,,,,,,,,,,
Row_134,"Zhou, Xuanyu","Zhou, Lifan","Gong, Shengrong","Zhang, Haizhen",Hybrid CNN and Transformer Network for Semantic Segmentation of UAV Remote Sensing Images,,MAR 2024,3,"Semantic segmentation of unmanned aerial vehicle (UAV) remote sensing images is a recent research hotspot, offering technical support for diverse types of UAV remote sensing missions. However, unlike general scene images, UAV remote sensing images present inherent challenges. These challenges include the complexity of backgrounds, substantial variations in target scales, and dense arrangements of small targets, which severely hinder the accuracy of semantic segmentation. To address these issues, we propose a convolutional neural network (CNN) and transformer hybrid network for semantic segmentation of UAV remote sensing images. The proposed network follows an encoder-decoder architecture that merges a transformer-based encoder with a CNN-based decoder. First, we incorporate the Swin transformer as the encoder to address the limitations of CNN in global modeling, mitigating the interference caused by complex background information. Second, to effectively handle the significant changes in target scales, we design the multiscale feature integration module (MFIM) that enhances the multiscale feature representation capability of the network. Finally, the semantic feature fusion module (SFFM) is designed to filter the redundant noise during the feature fusion process, which improves the recognition of small targets and edges. Experimental results demonstrate that the proposed method outperforms other popular methods on the UAVid and Aeroscapes datasets.",Remote sensing,semantic segmentation,Swin transformer,unmanned aerial vehicle (UAV),unmanned aerial vehicle (UAV),"Zhong, Shan","Xia, Yu","Huang, Yizhou",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,semantic segmentation,Swin transformer,unmanned aerial vehicle (UAV),,,,,,,,,,,,,,,,,,,
Row_135,"Liu, Zhiqiang","Li, Jiaojiao","Song, Rui","Wu, Chaoxiong",Edge Guided Context Aggregation Network for Semantic Segmentation of Remote Sensing Imagery,,MAR 2022,8,"Semantic segmentation of remote sensing imagery (RSI) has obtained great success with the development of deep convolutional neural networks (DCNNs). However, most of the existing algorithms focus on designing end-to-end DCNNs, but neglecting to consider the difficulty of segmentation in imbalance categories, especially for minority categories in RSI, which limits the performance of RSI semantic segmentation. In this paper, a novel edge guided context aggregation network (EGCAN) is proposed for the semantic segmentation of RSI. The Unet is employed as backbone. Meanwhile, an edge guided context aggregation branch and minority categories extraction branch are designed for a comprehensive enhancement of semantic modeling. Specifically, the edge guided context aggregation branch is proposed to promote entire semantic comprehension of RSI and further emphasize the representation of edge information, which consists of three modules: edge extraction module (EEM), dual expectation maximization attention module (DEMA), and edge guided module (EGM). EEM is created primarily for accurate edge tracking. According to that, DEMA aggregates global contextual features with different scales and the edge features along spatial and channel dimensions. Subsequently, EGM cascades the aggregated features into the decoder process to capture long-range dependencies and further emphasize the error-prone pixels in the edge region to acquire better semantic labels. Besides this, the exploited minority categories extraction branch is presented to acquire rich multi-scale contextual information through an elaborate hybrid spatial pyramid pooling module (HSPP) to distinguish categories taking a small percentage and background. On the Tianzhi Cup dataset, the proposed algorithm EGCAN achieved an overall accuracy of 84.1% and an average cross-merge ratio of 68.1%, with an accuracy improvement of 0.4% and 1.3% respectively compared to the classical Deeplabv3+ model. Extensive experimental results on the dataset released in ISPRS Vaihingen and Potsdam benchmarks also demonstrate the effectiveness of the proposed EGCAN over other state-of-the-art approaches.",remote sensing imagery,semantic segmentation,deep learning,context aggregation,,"Liu, Wei","Li, Zan","Li, Yunsong",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_136,"Li, Weitao","Gao, Hui","Su, Yi","Momanyi, Biffon Manyura",Unsupervised Domain Adaptation for Remote Sensing Semantic Segmentation with Transformer,,OCT 2022,11,"With the development of deep learning, the performance of image semantic segmentation in remote sensing has been constantly improved. However, the performance usually degrades while testing on different datasets because of the domain gap. To achieve feasible performance, extensive pixel-wise annotations are acquired in a new environment, which is time-consuming and labor-intensive. Therefore, unsupervised domain adaptation (UDA) has been proposed to alleviate the effort of labeling. However, most previous approaches are based on outdated network architectures that hinder the improvement of performance in UDA. Since the effects of recent architectures for UDA have been barely studied, we reveal the potential of Transformer in UDA for remote sensing with a self-training framework. Additionally, two training strategies have been proposed to enhance the performance of UDA: (1) Gradual Class Weights (GCW) to stabilize the model on the source domain by addressing the class-imbalance problem; (2) Local Dynamic Quality (LDQ) to improve the quality of the pseudo-labels via distinguishing the discrete and clustered pseudo-labels on the target domain. Overall, our proposed method improves the state-of-the-art performance by 8.23% mIoU on Potsdam -> Vaihingen and 9.2% mIoU on Vaihingen -> Potsdam and facilitates learning even for difficult classes such as clutter/background.",unsupervised domain adaptation,semantic segmentation,remote sensing image,transformer,self-training,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_137,"Hu, Hangtao","Cai, Shuo","Wang, Wei","Zhang, Peng",A Semantic Segmentation Approach Based on DeepLab Network in High-Resolution Remote Sensing Images,,2019,2,"Recently, more and more applications for high-resolution remote sensing image intelligent processing are required. Therefore, the semantic segmentation based on deep learning has successfully attracted people's attention. In this paper, the improved Deeplabv3 network is used in the application of image semantic segmentation. The problem of segmenting objects of multiple scales of high-resolution remote sensing image is handled, and the Chinese GaoFen NO. 2(GF-2) remote sensing image is taken as the main research object. Firstly, the original image is pre-processed. Next, use data augmentation and expansion for the pre-processed training image to avoid over-fitting. Finally, it is studied the adaptability and accuracy of the model of high-resolution remote sensing images, while is found the appropriate parameters to improve the precise of the result models compared. And explore the effectiveness of the model in the case of a fewer samples. This model is demonstrated that could be achieved the well classification result.",Remote sensing image classification,Deep learning,Semantic segmentation,,,"Li, Zhiyong",,,,"IMAGE AND GRAPHICS, ICIG 2019, PT III",,,,,,,,,,,,,,,,,,,,,,,
Row_138,"Wang, Yu","Li, Yansheng","Chen, Wei","Li, Yunzhou",DNAS: Decoupling Neural Architecture Search for High-Resolution Remote Sensing Image Semantic Segmentation,,AUG 2022,8,"Deep learning methods, especially deep convolutional neural networks (DCNNs), have been widely used in high-resolution remote sensing image (HRSI) semantic segmentation. In literature, most successful DCNNs are artificially designed through a large number of experiments, which often consume lots of time and depend on rich domain knowledge. Recently, neural architecture search (NAS), as a direction for automatically designing network architectures, has achieved great success in different kinds of computer vision tasks. For HRSI semantic segmentation, NAS faces two major challenges: (1) The task's high complexity degree, which is caused by the pixel-by-pixel prediction demand in semantic segmentation, leads to a rapid expansion of the search space; (2) HRSI semantic segmentation often needs to exploit long-range dependency (i.e., a large spatial context), which means the NAS technique requires a lot of display memory in the optimization process and can be tough to converge. With the aforementioned considerations in mind, we propose a new decoupling NAS (DNAS) framework to automatically design the network architecture for HRSI semantic segmentation. In DNAS, a hierarchical search space with three levels is recommended: path-level, connection-level, and cell-level. To adapt to this hierarchical search space, we devised a new decoupling search optimization strategy to decrease the memory occupation. More specifically, the search optimization strategy consists of three stages: (1) a light super-net (i.e., the specific search space) in the path-level space is trained to get the optimal path coding; (2) we endowed the optimal path with various cross-layer connections and it is trained to obtain the connection coding; (3) the super-net, which is initialized by path coding and connection coding, is populated with kinds of concrete cell operators and the optimal cell operators are finally determined. It is worth noting that the well-designed search space can cover various network candidates and the optimization process can be done efficiently. Extensive experiments on the publicly open GID and FU datasets showed that our DNAS outperformed the state-of-the-art methods, including artificial networks and NAS methods.",decoupling neural architecture search (DNAS),high-resolution remote sensing image,semantic segmentation,deep learning,,"Dang, Bo",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_139,"Zhao, Xin","Guo, Jiayi","Zhang, Yueting","Wu, Yirong",Memory-Augmented Transformer for Remote Sensing Image Semantic Segmentation,,NOV 2021,9,"The semantic segmentation of remote sensing images requires distinguishing local regions of different classes and exploiting a uniform global representation of the same-class instances. Such requirements make it necessary for the segmentation methods to extract discriminative local features between different classes and to explore representative features for all instances of a given class. While common deep convolutional neural networks (DCNNs) can effectively focus on local features, they are limited by their receptive field to obtain consistent global information. In this paper, we propose a memory-augmented transformer (MAT) to effectively model both the local and global information. The feature extraction pipeline of the MAT is split into a memory-based global relationship guidance module and a local feature extraction module. The local feature extraction module mainly consists of a transformer, which is used to extract features from the input images. The global relationship guidance module maintains a memory bank for the consistent encoding of the global information. Global guidance is performed by memory interaction. Bidirectional information flow between the global and local branches is conducted by a memory-query module, as well as a memory-update module, respectively. Experiment results on the ISPRS Potsdam and ISPRS Vaihingen datasets demonstrated that our method can perform competitively with state-of-the-art methods.",semantic segmentation,remote sensing imagery,memory-augmented transformer,memory mechanism,self-attention,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_140,"Nie, Jie","Zheng, Chengyu","Wang, Chenglong","Zuo, Zijie",Scale-Relation Joint Decoupling Network for Remote Sensing Image Semantic Segmentation,,2022,7,"As we all know, remote sensing (RS) images contain multiscale and numerous RS objects, along with massive and complex spatial topological relationships, such as the adjacency, proximity relationships of same-scale objects, and inclusion relationships of cross-scale objects. However, the existing semantic segmentation methods have never explored the cross-scale relationships, which are especially important when it comes to the situation that the RS objects cannot be accurately identified, they could be supplemented by the surrounding contents. To address the above concern, we propose a scale-relation joint decoupling network (SRJDN) for the semantic segmentation of RS images by simultaneously considering decoupling scales and decoupling relationships to excavate more complete relationships of multiscale RS objects. The SRJDN is performed by following three steps, namely, scale decoupling (SD), relation decoupling (RD), and fine-granularity guided fusion (FGF). The SD module uses dilated convolution with different rates to decouple RS objects into different scale feature groups, from small to large scales. Afterward, the RD considers all the spatial topological relationships and decouples these relationships according to the scale, which is divided into two parts, including same-scale relation extraction (SSRE) and cross-scale relation extraction (CSRE). The SSRE establishes the graph structures at each scale independently to mine the relationships of same-scale RS objects and the CSRE constructs the graph in a unified pattern between cross-scales to explore cross-scale target relationships. Third, the FGF module regards small-scale features as fine-granularity representation and applies its attention map to guide the learning of other scale features, which could mine more reliable and comprehensive saliency information and improve the feature consistency. Numerical experiments conducted on two large-scale fine-resolution RS image datasets empirically demonstrate the robustness of the proposed joint decoupling strategy and the effectiveness of FGF in RS image semantic segmentation tasks.",Decoupling,remote sensing (RS),scale-relation joint,semantic segmentation,,"Lv, Xiaowei","Yu, Shusong","Wei, Zhiqiang",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_141,"Wu, Zhihuan","Gao, Yongming","Li, Lei","Xue, Junshi",Semantic segmentation of high-resolution remote sensing images using fully convolutional network with adaptive threshold,,APR 3 2019,50,"Semantic segmentation is an important method to implement fine-grained semantically understand for high-resolution remote sensing images by dividing images into pixel groupings which can then be labelled and classified. In the field of computer vision (CV), the methods based on fully convolutional network (FCN) are the hotspot and have achieved state-of-the-art results. Compared with popular datasets in CV such as PASCAL and COCO, class imbalance is a problem for multiclass semantic segmentation in remote sensing datasets. In this paper, an FCN-based model is proposed to implement pixel-wise classifications for remote sensing image in an end-to-end way, and an adaptive threshold algorithm is proposed to adjust the threshold of Jaccard index in each class. Experiments on DSTL dataset show that the proposed method produces accurate classifications in an end-to-end way. Results show that the adaptive threshold algorithm can increase the score of average Jaccard index from 0.614 to 0.636 and achieve better segmentation results.",Semantic segmentation,remote sensing images,fully convolutional network,class imbalance,adaptive threshold,"Li, Yuntao",,,,CONNECTION SCIENCE,,,,,,,,,,,,,,,,,,,,,,,
Row_142,"Gao, Yupeng","Zhang, Shengwei","Zuo, Dongshi","Yan, Weihong",TMNet: A Two-Branch Multi-Scale Semantic Segmentation Network for Remote Sensing Images,,JUL 2023,3,"Pixel-level information of remote sensing images is of great value in many fields. CNN has a strong ability to extract image backbone features, but due to the localization of convolution operation, it is challenging to directly obtain global feature information and contextual semantic interaction, which makes it difficult for a pure CNN model to obtain higher precision results in semantic segmentation of remote sensing images. Inspired by the Swin Transformer with global feature coding capability, we design a two-branch multi-scale semantic segmentation network (TMNet) for remote sensing images. The network adopts the structure of a double encoder and a decoder. The Swin Transformer is used to increase the ability to extract global feature information. A multi-scale feature fusion module (MFM) is designed to merge shallow spatial features from images of different scales into deep features. In addition, the feature enhancement module (FEM) and channel enhancement module (CEM) are proposed and added to the dual encoder to enhance the feature extraction. Experiments were conducted on the WHDLD and Potsdam datasets to verify the excellent performance of TMNet.",remote sensing images,global modeling,semantic segmentation,Swin transformer,,"Pan, Xin",,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,
Row_143,"Chen, Hongkun","Luo, Huilan",,,Multilevel Feature Interaction Network for Remote Sensing Images Semantic Segmentation,,2024,0,"High-spatial resolution (HSR) remote sensing images present significant challenges due to their highly complex backgrounds, a large number of densely distributed small targets, and the potential for confusion with land targets. These characteristics render existing methods ineffective in accurately segmenting small targets and prone to boundary blurring. In response to these challenges, we introduce a novel multilevel feature interaction network (MFIN). The MFIN model was designed as a dual-branch U-shaped interactive decoding structure that effectively achieves semantic segmentation and edge detection. Notably, this study is the first to address ways to enhance the performance for HSR remote sensing image analysis by iteratively refining features at multilevels for different tasks. We designed the feature interaction module (FIM), which refines semantic features through multiscale attention and interacts with edge features of the same scale for optimization, then serving as input for iterative optimization in the next scale's FIM. In addition, a lightweight global feature module is designed to adaptively extract global contextual information from different scales features, thereby enhancing the semantic accuracy of the features. Furthermore, to mitigate the semantic dilution issues caused by upsampling, a semantic-guided fusion module is introduced to enhance the propagation of rich semantic information among features. The proposed methods achieve state-of-the-art segmentation performance across four publicly available remote sensing datasets: Potsdam, Vaihingen, LoveDA, and UAVid. Notably, our MFIN has only 15.4 MB parameters and 34.2 GB GFLOPs, achieving an optimal balance between accuracy and efficiency.",Image edge detection,Semantics,Feature extraction,Remote sensing,Semantic segmentation,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Decoding,Adaptation models,Convolution,Accuracy,Context modeling,Feature interaction,multilevel features,remote sensing images analysis,semantic segmentation,,,,,,,,,,,,,
Row_144,"Bai, Qinyan","Luo, Xiaobo","Wang, Yaxu","Wei, Tengfei",DHRNet: A Dual-Branch Hybrid Reinforcement Network for Semantic Segmentation of Remote Sensing Images,,2024,2,"In the field of remote sensing image processing, semantic segmentation has always been a hot research topic. Currently, deep convolutional neural networks (DCNNs) are the mainstream methods for the semantic segmentation of remote sensing image (RSI). There are two commonly used semantic segmentation methods based on DCNNs: multiscale feature extraction based on deep-level features, and global modeling. The former can better extract object features of different scales in complex scenes. However, this method lacks sufficient spatial information, resulting in poor edge segmentation ability. The latter can effectively solve the problem of limited receptive field in DCNNs obtaining more comprehensive feature extraction results. Unfortunately, this method is prone to misclassification, resulting in incorrect predictions of local pixels. To address these issues, we propose the dual-branch hybrid reinforcement network (DHRNet) for more precise semantic segmentation of RSI. This model is a dual-branch parallel structure with a multiscale feature extraction branch and a global context and detail enhancement branch. This structure decomposes the complex semantic segmentation task, allowing each branch to extract features with different emphases while retaining sufficient spatial information. The results of both branches are fused to obtain a more comprehensive segmentation result. After conducting extensive experiments on three publicly available RSI datasets, ISPRS Potsdam, ISPRS Vaihingen, and LoveDA, DHRNet demonstrates excellent results with the mean intersection over union of 86.97%, 83.53%, and 54.48% on the three datasets, respectively.",Global context modeling,multiscale feature extraction,remote sensing,semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_145,"Tong, Qixiang","Zhu, Zhipeng","Zhang, Min","Cao, Kerui",CrossFormer Embedding DeepLabv3+for Remote Sensing Images Semantic Segmentation,,2024,1,"High-resolution remote sensing image segmentation is a challenging task. In urban remote sensing, the presence of occlusions and shadows often results in blurred or invisible object boundaries, thereby increasing the difficulty of segmentation. In this paper, an improved network with a cross-region self-attention mechanism for multi-scale features based on DeepLabv3+ is designed to address the difficulties of small object segmentation and blurred target edge segmentation. First, we use CrossFormer as the backbone feature extraction network to achieve the interaction between large- and small-scale features, and establish self-attention associations between features at both large and small scales to capture global contextual feature information. Next, an improved atrous spatial pyramid pooling module is introduced to establish multi-scale feature maps with large- and small-scale feature associations, and attention vectors are added in the channel direction to enable adaptive adjustment of multi-scale channel features. The proposed network model is validated using the Potsdam and Vaihingen datasets. The experimental results show that, compared with existing techniques, the network model designed in this paper can extract and fuse multiscale information, more clearly extract edge information and small-scale information, and segment boundaries more smoothly. Experimental results on public datasets demonstrate the superiority of our method compared with several state-of-the-art networks.",Semantic segmentation,remote sensing,multiscale,self -attention,,"Xing, Haihua",,,,CMC-COMPUTERS MATERIALS & CONTINUA,,,,,,,,,,,,,,,,,,,,,,,
Row_146,"Zheng, Chengyu","Jiang, Yanru","Lv, Xiaowei","Nie, Jie",SSDT: Scale-Separation Semantic Decoupled Transformer for Semantic Segmentation of Remote Sensing Images,,2024,3,"As we all know, semantic segmentation of remote sensing (RS) images is to classify the images pixel by pixel to realize the semantic decoupling of the images. Most traditional semantic decoupling methods only decouple and do not perform scale-separation operations, which leads to serious problems. In the semantic decoupling process, if the feature extractor is too large, it will ignore the small-scale targets; if the feature extractor is too small, it will lead to the separation of large-scale target objects and reduce the segmentation accuracy. To address this concern, we propose a scale-separated semantic decoupled transformer (SSDT), which first performs scale-separation in the semantic decoupling process and uses the obtained scale information-rich semantic features to guide the Transformer to extract features. The network consists of five modules, scale-separated patch extraction (SPE), semantic decoupled transformer (SDT), scale-separated feature extraction (SFE), semantic decoupling (SD), and multiview feature fusion decoder (MFFD). In particular, SPE turns the original image into a linear embedding sequence of three scales; SD divides pixels into different semantic clusters by K-means, and further obtains scale information-rich semantic features; SDT improves the intraclass compactness and interclass looseness by calculating the similarity between semantic features and image features, the core of which is decoupled attention. Finally, MFFD is proposed to fuse salient features from different perspectives to further enhance the feature representation. Our experiments on two large-scale fine-resolution RS image datasets (Vaihingen and Potsdam) demonstrate the effectiveness of the proposed SSDT strategy in RS image semantic segmentation tasks.",Semantics,Feature extraction,Transformers,Semantic segmentation,Remote sensing,"Liang, Xinyue","Wei, Zhiqiang",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Computational modeling,Vegetation mapping,Geophysical image processing,geoscience and remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,
Row_147,"Chen, Jie","Zhu, Jingru","Sun, Geng","Li, Jianhui",SMAF-Net: Sharing Multiscale Adversarial Feature for High-Resolution Remote Sensing Imagery Semantic Segmentation,,NOV 2021,11,"Semantic segmentation of high-resolution remote sensing imagery (HRSI) is a major task in remote sensing analysis. Although deep convolutional neural network (DCNN)-based semantic segmentation models have powerful capacity in pixel-wise classification, they still face challenge in obtaining intersemantic continuity and extraboundary accuracy because of the geo-object's characteristic feature of diverse scales and various distributions in HRSI. Inspired by the transfer learning, in this study, we propose an efficient semantic segmentation framework named SMAF-Net, which shares multiscale adversarial features into a U-shaped semantic segmentation model. Specifically, it uses multiscale adversarial feature representation obtained from a well-trained generative adversarial network to grasp the pixel correlation and further improve the boundary accuracy of multiscale geo-objects. Comparison experiments on the Potsdam and Vaihingen data sets demonstrate that the proposed framework can achieve considerable improvement in the semantic segmentation of HRSI.",Semantics,Feature extraction,Image segmentation,Remote sensing,Gallium nitride,"Deng, Min",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Generators,Data mining,Generative adversarial network (GAN),high-resolution remote sensing imagery (HRSI),multiscale feature,semantic segmentation,,,,,,,,,,,,,,,,
Row_148,"Xiao, Ruijie","Zhong, Chuan","Zeng, Wankang","Cheng, Ming",Novel Convolutions for Semantic Segmentation of Remote Sensing Images,,2023,11,"The networks are required to be capable of learning low-level features well when applied to remote sensing image (RSI) semantic segmentation tasks. To capture accurate and abundant low-level semantic information, the early feature extractor layer is crucial to the whole network because all the subsequent features are inferred from that base. To address the low-level feature extraction issue and overcome the shortcomings of traditional convolution such as too many parameters or limited receptive field, some novel convolution units have been proposed in the literature. In this article, we propose two elaborately designed and portable yet effective convolution units, i.e., directional convolution (DC) and large field convolution (LFC), combined as the extractor of low-level semantic features. DC is designed to extract directional features from specific directions, and LFC can achieve a large receptive field with few parameters. Experimental results on two public datasets provide evidence that our convolution units can help deep learning networks improve performance stably and comprehensively compared to the baseline networks.",Feature extraction,Semantic segmentation,Kernel,Strips,Semantics,"Wang, Cheng",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Data mining,Task analysis,Low-level feature extractor,novel convolution,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,
Row_149,"Wang, Libo","Li, Rui","Duan, Chenxi","Zhang, Ce",A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images,,2022,155,"The fully convolutional network (FCN) with an encoder-decoder architecture has been the standard paradigm for semantic segmentation. The encoder-decoder architecture utilizes an encoder to capture multilevel feature maps, which are incorporated into the final prediction by a decoder. As the context is crucial for precise segmentation, tremendous effort has been made to extract such information in an intelligent fashion, including employing dilated/atrous convolutions or inserting attention modules. However, these endeavors are all based on the FCN architecture with ResNet or other backbones, which cannot fully exploit the context from the theoretical concept. By contrast, we introduce the Swin Transformer as the backbone to extract the context information and design a novel decoder of densely connected feature aggregation module (DCFAM) to restore the resolution and produce the segmentation map. The experimental results on two remotely sensed semantic segmentation datasets demonstrate the effectiveness of the proposed scheme.",Transformers,Semantics,Image segmentation,Feature extraction,Remote sensing,"Meng, Xiaoliang","Fang, Shenghui",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Decoding,Standards,Fine-resolution remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,
Row_150,"Yang, Kunping","Tong, Xin-Yi","Xia, Gui-Song","Shen, Weiming",Hidden Path Selection Network for Semantic Segmentation of Remote Sensing Images,,2022,11,"Targeting at depicting land covers with pixelwise semantic categories, semantic segmentation in remote sensing images needs to portray diverse distributions over vast geographical locations, which is difficult to be achieved by the homogeneous pixelwise forward paths in the architectures of existing deep models. Although specific algorithms have been designed to select pixelwise adaptive forward paths for natural image analysis, it still lacks theoretical supports on how to obtain optimal selections. In this article, we provide mathematical analyses in terms of the parameter optimization, which guides us to design a method called hidden path selection network (HPS-Net). With the help of hidden variables deriving from an extra mini-branch, HPS-Net is able to tackle the inherent problem about inaccessible global optimums by adjusting the direct relationships between feature maps and pixelwise path selections in existing algorithms, which we call hidden path selection. For the better training and evaluation, we further refine and expand the 5-class Gaofen image dataset (GID-5) to a new one with 15 land-cover categories, i.e., GID-15. The experimental results on both GID-5 and GID-15 demonstrate that the proposed modules can stably improve the performance of different deep structures, which validates the proposed mathematical analyses.",Semantics,Remote sensing,Image segmentation,Optimization,Mathematical analysis,"Zhang, Liangpei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Manifolds,Benchmark testing,Benchmark dataset,hidden path selection,remote sensing image,semantic segmentation,,,,,,,,,,,,,,,,
Row_151,"Yim, Ji Hyeon","Kim, Min-A","Kim, Ku-hyeok","Lee, Jeayeol",Comparative Analysis of Methods Based on Semantic Segmentation for Cloud Detection in Remote Sensing Imagery,12TH INTERNATIONAL CONFERENCE ON ICT CONVERGENCE (ICTC 2021): BEYOND THE PANDEMIC ERA WITH ICT CONVERGENCE INNOVATION,2021,0,"Cloud coverage estimation is a fundamental step in remote sensing imagery because it can be useful information to user using imagery. Recently, cloud detection using semantic segmentation is being actively studied to automatically detect cloud regions. However, there are many limitations to obtaining accurate cloud regions; therefore, the related methods need to be analyzed. Accordingly, this study aims to identify the best network architecture that is applicable for future works, and evaluates its performance using datasets created for this research, First, we classified multiple network architectures and public datasets for the semantic segmentation of clouds in remote sensing imagery. Next, we selected the best architecture by assessing the accuracy of each architecture and evaluate it using our datasets. We believe this work will be beneficial for future research on cloud detection in the field of remote sensing imagery.",Cloud detection,Semantic segmentation,Remote sensing,Satellite,,"Lee, Myeong Shin","Chung, Dae-won","Jeon, Kyeongmi","Koo, Jamyoung",,,,,,,,,,,,,,,,,,,,,,,,
Row_152,"Ma, Ailong","Wang, Junjue","Zhong, Yanfei","Zheng, Zhuo",FactSeg: Foreground Activation-Driven Small Object Semantic Segmentation in Large-Scale Remote Sensing Imagery,,2022,90,"The small object semantic segmentation task is aimed at automatically extracting key objects from high-resolution remote sensing (HRS) imagery. Compared with the large-scale coverage areas for remote sensing imagery, the key objects, such as cars and ships, in HRS imagery often contain only a few pixels. In this article, to tackle this problem, the foreground activation (FA)-driven small object semantic segmentation (FactSeg) framework is proposed from perspectives of structure and optimization. In the structure design, FA object representation is proposed to enhance the awareness of the weak features in small objects. The FA object representation framework is made up of a dual-branch decoder and collaborative probability (CP) loss. In the dual-branch decoder, the FA branch is designed to activate the small object features (activation) and suppress the large-scale background, and the semantic refinement (SR) branch is designed to further distinguish small objects (refinement). The CP loss is proposed to effectively combine the activation and refinement outputs of the decoder under the CP hypothesis. During the collaboration, the weak features of the small objects are enhanced with the activation output, and the refined output can be viewed as the refinement of the binary outputs. In the optimization stage, small object mining (SOM)-based network optimization is applied to automatically select effective samples and refine the direction of the optimization while addressing the imbalanced sample problem between the small objects and the large-scale background. The experimental results obtained with two benchmark HRS imagery segmentation datasets demonstrate that the proposed framework outperforms the state-of-the-art semantic segmentation methods and achieves a good tradeoff between accuracy and efficiency. Code will be available at: http://rsidea.whu.edu.cn/FactSeg.htm",Semantics,Image segmentation,Task analysis,Remote sensing,Optimization,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Feature extraction,Decoding,Deep learning,high-resolution remote sensing (HRS) imagery,semantic segmentation,small objects,,,,,,,,,,,,,,,,
Row_153,"Huo, Yan","Gang, Shuang","Dong, Liang","Guan, Chao",An Efficient Semantic Segmentation Method for Remote-Sensing Imagery Using Improved Coordinate Attention,,MAY 2024,1,"Semantic segmentation stands as a prominent domain within remote sensing that is currently garnering significant attention. This paper introduces a pioneering semantic segmentation model based on TransUNet architecture with improved coordinate attention for remote-sensing imagery. It is composed of an encoding stage and a decoding stage. Notably, an enhanced and improved coordinate attention module is employed by integrating two pooling methods to generate weights. Subsequently, the feature map undergoes reweighting to accentuate foreground information and suppress background information. To address the issue of time complexity, this paper introduces an improvement to the transformer model by sparsifying the attention matrix. This reduces the computing expense of calculating attention, making the model more efficient. Additionally, the paper uses a combined loss function that is designed to enhance the training performance of the model. The experimental results conducted on three public datasets manifest the efficiency of the proposed method. The results indicate that it excels in delivering outstanding performance for semantic segmentation tasks pertaining to remote-sensing images.",remote-sensing image,sparse matrix,vision transformer,coordinate attention,semantic segmentation,,,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,
Row_154,"Pereira, Matheus B.","dos Santos, Jefersson A.",,,AN END-TO-END FRAMEWORK FOR LOW-RESOLUTION REMOTE SENSING SEMANTIC SEGMENTATION,2020 IEEE LATIN AMERICAN GRSS & ISPRS REMOTE SENSING CONFERENCE (LAGIRS),2020,12,"High-resolution images for remote sensing applications are often not affordable or accessible, especially when in need of a wide temporal span of recordings. Given the easy access to low-resolution (LR) images from satellites, many remote sensing works rely on this type of data. The problem is that LR images are not appropriate for semantic segmentation, due to the need for high-quality data for accurate pixel prediction for this task, In this paper, we propose an end-to-end framework that unites a super-resolution and a semantic segmentation module in order to produce accurate thematic maps from LR inputs. It allows the semantic segmentation network to conduct the reconstruction process, modifying the input image with helpful textures, We evaluate the framework with three remote sensing datasets. The results show that the framework is capable of achieving a semantic segmentation performance close to native high-resolution data, while also surpassing the performance of a network trained with LR inputs.",Super-resolution,semantic segmentation,remotesensing,end-to-end framework,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_155,"Zhou, Yin","Li, Tianyi","Li, Xianju","Feng, Ruyi",MCNet: A Multi-scale and Cascade Network for Semantic Segmentation of Remote Sensing Images,,2024,0,"High resolution remote sensing images that can show more detailed ground information play an important role in land classification. However, existing segmentation methods have the problems of insufficient use of multi-scale feature and semantic information. In this study, a multi-scale and cascade semantic segmentation network (MCNet) was proposed and tested on the Potsdam and Vaihingen datasets. (1) Multi-scale feature extraction module: using dilated convolution and a parallel structure to fully extract multi-scale feature information. (2) Cross-layer feature selection module: adaptively selecting features in different levels to avoid the loss of key features. (3) Multi-scale object guidance module: weighting the features at different scales to express the multi-scale ground objects. (4) Cascade structure in the decoder part: increasing the information flow and enhancing the decoding capability of the network. Results show that the proposed MCNet outperformed the baseline networks, achieving an average overall accuracy of 86.91% and 87.82% on the two datasets, respectively. In conclusion, the multi-scale and cascade semantic segmentation network can improve the accuracy of land cover classification by using remote sensing images.",remote sensing,semantic segmentation,multi-scale feature,,,,,,,"WEB AND BIG DATA, PT II, APWEB-WAIM 2023",,,,,,,,,,,,,,,,,,,,,,,
Row_156,"He, Xin","Zhou, Yong","Zhao, Jiaqi","Zhang, Di",Swin Transformer Embedding UNet for Remote Sensing Image Semantic Segmentation,,2022,349,"Global context information is essential for the semantic segmentation of remote sensing (RS) images. However, most existing methods rely on a convolutional neural network (CNN), which is challenging to directly obtain the global context due to the locality of the convolution operation. Inspired by the Swin transformer with powerful global modeling capabilities, we propose a novel semantic segmentation framework for RS images called ST-U-shaped network (UNet), which embeds the Swin transformer into the classical CNN-based UNet. ST-UNet constitutes a novel dual encoder structure of the Swin transformer and CNN in parallel. First, we propose a spatial interaction module (SIM), which encodes spatial information in the Swin transformer block by establishing pixel-level correlation to enhance the feature representation ability of occluded objects. Second, we construct a feature compression module (FCM) to reduce the loss of detailed information and condense more small-scale features in patch token downsampling of the Swin transformer, which improves the segmentation accuracy of small-scale ground objects. Finally, as a bridge between dual encoders, a relational aggregation module (RAM) is designed to integrate global dependencies from the Swin transformer into the features from CNN hierarchically. Our ST-UNet brings significant improvement on the ISPRS-Vaihingen and Potsdam datasets, respectively. The code will be available at https://github.com/XinnHe/ST-UNet.",Transformers,Semantics,Image segmentation,Feature extraction,Convolutional neural networks,"Yao, Rui","Xue, Yong",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Remote sensing,Task analysis,Global information embedding,remote sensing (RS),semantic segmentation,Swin transformer,,,,,,,,,,,,,,,,
Row_157,"Liu, Yikun","Kang, Xudong","Huang, Yuwen","Wang, Kuikui",Unsupervised Domain Adaptation Semantic Segmentation for Remote-Sensing Images via Covariance Attention,,2022,5,"Semantic segmentation for remote sensing is a crucial but challenging task. Many supervised semantic segmentation methods rely heavily on a large-scale pixelwise annotated dataset, but it is time-consuming and laborious to provide manual annotation. However, due to the common domain shift of remote-sensing images, a direct transfer might not perform well. Therefore, many unsupervised domain adaptation (UDA) methods have been proposed to solve the data distribution discrepancy in remote-sensing datasets, but these methods cannot completely utilize the features extracted in the training process. In addition, the correlations between feature map channels are crucial for the pixelwise classification task. In this letter, a covariance-based channel attention module is proposed to capture correlations by covariance metric and weighting the feature map channels. To further improve the domain adaptation performance, we propose a three-stage UDA semantic segmentation method for remote-sensing images, and we fine-tune the model that has been trained on the source domain on the target domain via self-training and knowledge distillation (KD). To test the effectiveness of the proposed method, experiments are conducted on the ISPRS 2-D Semantic Labeling dataset and an urban drone dataset (UDD). Our method shows a better performance advantage compared with other state-of-the-art methods.",Feature extraction,Semantics,Remote sensing,Image segmentation,Adaptation models,"Yang, Gongping",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Task analysis,Training,Covariance attention,domain adaptation,knowledge distillation (KD),self-training,semantic segmentation,,,,,,,,,,,,,,,
Row_158,"Cheng, Xiang","Lei, Hong",,,Semantic Segmentation of Remote Sensing Imagery Based on Multiscale Deformable CNN and DenseCRF,,MAR 2023,5,"The semantic segmentation of remote sensing images is a significant research direction in digital image processing. The complex background environment, irregular size and shape of objects, and similar appearance of different categories of remote sensing images have brought great challenges to remote sensing image segmentation tasks. Traditional convolutional-neural-network-based models often ignore spatial information in the feature extraction stage and pay less attention to global context information. However, spatial context information is important in complex remote sensing images, which means that the segmentation effect of traditional models needs to be improved. In addition, neural networks with a superior segmentation performance often suffer from the problem of high computational resource consumption. To address the above issues, this paper proposes a combination model of a modified multiscale deformable convolutional neural network (mmsDCNN) and dense conditional random field (DenseCRF). Firstly, we designed a lightweight multiscale deformable convolutional network (mmsDCNN) with a large receptive field to generate a preliminary prediction probability map at each pixel. The output of the mmsDCNN model is a coarse segmentation result map, which has the same size as the input image. In addition, the preliminary segmentation result map contains rich multiscale features. Then, the multi-level DenseCRF model based on the superpixel level and the pixel level is proposed, which can make full use of the context information of the image at different levels and further optimize the rough segmentation result of mmsDCNN. To be specific, we converted the pixel-level preliminary probability map into a superpixel-level predicted probability map according to the simple linear iterative clustering (SILC) algorithm and defined the potential function of the DenseCRF model based on this. Furthermore, we added the pixel-level potential function constraint term to the superpixel-based Gaussian potential function to obtain a combined Gaussian potential function, which enabled our model to consider the features of various scales and prevent poor superpixel segmentation results from affecting the final result. To restore the contour of the object more clearly, we utilized the Sketch token edge detection algorithm to extract the edge contour features of the image and fused them into the potential function of the DenseCRF model. Finally, extensive experiments on the Potsdam and Vaihingen datasets demonstrated that the proposed model exhibited significant advantages compared to the current state-of-the-art models.",semantic segmentation of remote sensing imagery,deep learning,convolutional neural network (CNN),conditional random field (CRF),,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_159,"Zeng, Xiaopeng","Wang, Tengfei","Dong, Zhe","Zhang, Xiangrong",Superpixel Consistency Saliency Map Generation for Weakly Supervised Semantic Segmentation of Remote Sensing Images,,2023,9,"The weakly supervised semantic segmentation (WSSS) method aims to assign semantic labels to each image pixel from weak (image-level) instead of strong (pixel-level) labels, which can greatly reduce human labor costs. However, there are some problems in WSSS of remote sensing images, such as how to locate labels accurately and how to get precise segmentation edges. To address these issues, we propose a novel framework directly transferring the scene classification model to perform semantic segmentation. We first train a multilabel scene classification network as the encoder to obtain the pretrained model, and then, the feature learned by the model is transferred to the decoder. Different from other methods, we propose a saliency map generator (SMG) instead of the class activation map (CAM) for more accurate location information by making pixels belonging to the same class lie close together while different classes are separated in feature space. Meanwhile, we take the superpixel patch as processing unit to provide precise boundary inhibition for the saliency map. To assign semantic labels for each patch, combined with extracted salient region, we propose a module responsible for exploiting the consistency of spatial and semantic similarity between different patches. Finally, we incorporate the above two modules to supervise the training process of the decoder without generating pseudolabels as most methods do, thus simplifying the training process. Experimental results show that our method outperforms other weakly supervised approaches on dense labeling remote sensing dataset (DLRSD) and Wuhan dense labeling dataset (WHDLD) with at least a 3% improvement on mean intersection over union (mIoU).",Semantics,Remote sensing,Semantic segmentation,Feature extraction,Generators,"Gu, Yanfeng",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Decoding,Convolutional neural network (CNN),deep learning,semantic segmentation,weakly supervised learning (WSL),,,,,,,,,,,,,,,,
Row_160,"Cui, Wei","He, Xin","Yao, Meng","Wang, Ziwei",Knowledge and Spatial Pyramid Distance-Based Gated Graph Attention Network for Remote Sensing Semantic Segmentation,,APR 2021,15,"The pixel-based semantic segmentation methods take pixels as recognitions units, and are restricted by the limited range of receptive fields, so they cannot carry richer and higher-level semantics. These reduce the accuracy of remote sensing (RS) semantic segmentation to a certain extent. Comparing with the pixel-based methods, the graph neural networks (GNNs) usually use objects as input nodes, so they not only have relatively small computational complexity, but also can carry richer semantic information. However, the traditional GNNs are more rely on the context information of the individual samples and lack geographic prior knowledge that reflects the overall situation of the research area. Therefore, these methods may be disturbed by the confusion of ""different objects with the same spectrum"" or ""violating the first law of geography"" in some areas. To address the above problems, we propose a remote sensing semantic segmentation model called knowledge and spatial pyramid distance-based gated graph attention network (KSPGAT), which is based on prior knowledge, spatial pyramid distance and a graph attention network (GAT) with gating mechanism. The model first uses superpixels (geographical objects) to form the nodes of a graph neural network and then uses a novel spatial pyramid distance recognition algorithm to recognize the spatial relationships. Finally, based on the integration of feature similarity and the spatial relationships of geographic objects, a multi-source attention mechanism and gating mechanism are designed to control the process of node aggregation, as a result, the high-level semantics, spatial relationships and prior knowledge can be introduced into a remote sensing semantic segmentation network. The experimental results show that our model improves the overall accuracy by 4.43% compared with the U-Net Network, and 3.80% compared with the baseline GAT network.",remote sensing,semantic segmentation,knowledge,spatial relationship,spatial pyramid distance,"Hao, Yuanjie","Li, Jie","Wu, Weijie","Zhao, Huilin",REMOTE SENSING,"Xia, Cong",GAT,,,,,,,,,"Li, Jin","Cui, Wenqi",,,,,,,,,,,
Row_161,"Li, Haifeng","Li, Yi","Zhang, Guo","Liu, Ruoyun",Global and Local Contrastive Self-Supervised Learning for Semantic Segmentation of HR Remote Sensing Images,,2022,117,"Recently, supervised deep learning has achieved a great success in remote sensing image (RSI) semantic segmentation. However, supervised learning for semantic segmentation requires a large number of labeled samples, which is difficult to obtain in the field of remote sensing. A new learning paradigm, self-supervised learning (SSL), can be used to solve such problems by pretraining a general model with a large number of unlabeled images and then fine-tuning it on a downstream task with very few labeled samples. Contrastive learning is a typical method of SSL that can learn general invariant features. However, most existing contrastive learning methods are designed for classification tasks to obtain an image-level representation, which may be suboptimal for semantic segmentation tasks requiring pixel-level discrimination. Therefore, we propose a global style and local matching contrastive learning network (GLCNet) for RSI semantic segmentation. Specifically, first, the global style contrastive learning module is used to better learn an image-level representation, as we consider that style features can better represent the overall image features. Next, the local features matching the contrastive learning module is designed to learn the representations of local regions, which is beneficial for semantic segmentation. We evaluate four RSI semantic segmentation datasets, and the experimental results show that our method mostly outperforms the state-of-the-art self-supervised methods and the ImageNet pretraining method. Specifically, with 1% annotation from the original dataset, our approach improves Kappa by 6% on the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam dataset relative to the existing baseline. Moreover, our method outperforms supervised learning methods when there are some differences between the datasets of upstream tasks and downstream tasks. Our study promotes the development of SSL in the field of RSI semantic segmentation. Since SSL could directly learn the essential characteristics of data from unlabeled data, which is easy to obtain in the remote sensing field, this may be of great significance for tasks such as global mapping. The source code is available at https://github.com/GeoX-Lab/G-RSIM.",Semantics,Remote sensing,Task analysis,Image segmentation,Supervised learning,"Huang, Haozhe","Zhu, Qing","Tao, Chao",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Force,Feature extraction,Contrastive learning,remote sensing image (RSI) semantic segmentation,self-supervised learning (SSL),,,,,,,,,,,,,,,,,
Row_162,"Zhang, Zixuan","Huang, Liang","Tang, Bo-Hui","Le, Weipeng",MATNet: multiattention Transformer network for cropland semantic segmentation in remote sensing images,,DEC 31 2024,1,"Remote sensing image semantic segmentation methods have become the main approach for extracting cropland information. However, in the mountainous regions of southwestern China, croplands exhibit narrow and fragmented shapes, as well as complex planting patterns, making it difficult for traditional semantic segmentation methods to accurately delineate fine-grained cropland boundaries. To address these challenges, a multiattention Transformer network named MATNet is proposed in this paper, for fine-grained extraction of cropland at the parcel level in complex scenes. MATNet built upon the fusion of CNN encoder and Transformer decoder. In the encoder, spatial and channel reconstruction units are introduced, reducing information redundancy in the convolutional layers. The Transformer decoder incorporates multiple attention mechanisms, this design feature enhances the attention window's perception of local content and improves the model's ability to extract features from fine-grained cropland parcels through optimized computationnal al location. Taking the experimental results of the Dali cropland dataset as an illustration, MATNet achieved the highest values across five evaluation metrics, including mIoU. Specifically, the Recall, F1, and mIoU scores were 94.68%, 94.69%, and 89.92%, respectively. Compared with six other advanced models, MATNet consistently performed best in terms of extracting fine-grained cropland parcels.",Remote sensing,semantic segmentation,cropland extraction,Transformer,multiattention,"Wang, Meiqi","Cheng, Jiapei","Wu, Qiang",,INTERNATIONAL JOURNAL OF DIGITAL EARTH,,,,,,,,,,,,,,,,,,,,,,,
Row_163,"Gong, Zhi","Duan, Lijuan","Xiao, Fengjin","Wang, Yuxi",MSAug: Multi-Strategy Augmentation for rare classes in semantic segmentation of remote sensing images,,SEP 2024,2,"Recently, remote sensing images have been widely used in many scenarios, gradually becoming the focus of social attention. Nevertheless, the limited annotation of scarce classes severely reduces segmentation performance. This phenomenon is more prominent in remote sensing image segmentation. Given this, we focus on image fusion and model feedback, proposing a multi-strategy method called MSAug to address the remote sensing imbalance problem. Firstly, we crop rare class images multiple times based on prior knowledge at the image patch level to provide more balanced samples. Secondly, we design an adaptive image enhancement module at the model feedback level to accurately classify rare classes at each stage and dynamically paste and mask different classes to further improve the model's recognition capabilities. The MSAug method is highly flexible and can be plug-and-play. Experimental results on remote sensing image segmentation datasets show that adding MSAug to any remote sensing image semantic segmentation network can bring varying degrees of performance improvement.",Data augmentation,Remote sensing images,Semantic segmentation,Rare classes,,,,,,DISPLAYS,,,,,,,,,,,,,,,,,,,,,,,
Row_164,Song Xirui,Ge Hongwei,,,Remote Sensing Image Semantic Segmentation Algorithm Based on TransMANet,,MAY 2024,0,"Herein, we propose a Transformer multiattention network (TransMANet), a network structure based on Transformer and attention mechanisms, to address the issues of low segmentation accuracy, inadequate global feature extraction, and insufficient association between the multiattention network (MANet) algorithm and image semantic information. This network structure features a dual-branch decoder that combines local and global contexts and enhances the semantic information of shallow networks. First, we introduce a local attention embedding mechanism that enhances the embedding of context information and semantic information of high-level features into low-level features. Then, we design a dual-branch decoder that combines Transformer and convolutional neural networks, which extracts global context information and detailed information with different scales, thereby modeling global and local information. Finally, we improve the original loss function and use a joint loss function that combines cross-entropy loss and Dice loss to address the class imbalance problem often encountered in remote sensing datasets and thus improve segmentation accuracy. Our experimental results demonstrate the superiority of TransMANet over MANet and other advanced methods in terms of intersection over union on UAVid, LoveDA, Potsdam, and Vaihingen datasets. This indicates the strong generalization capability of TransMANet and its effectiveness in achieving accurate segmentation results.",image processing,semantic segmentation,attention mechanism,Transformer,high-resolution remote sensing image,,,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,,,,,,,,,,,,
Row_165,"Yao, Hongtai","Zhao, Le","Tian, Meng","Jin, Yong",Semantic Segmentation for Remote Sensing Image Using the Multigranularity Object-Based Markov Random Field With Blinking Coefficient,,2023,3,"Semantic segmentation is one of the most important tasks in remote sensing. In the semantic segmentation of remote sensing images, some regions are repeatedly transformed between multiclasses, which affects the convergence speed and segmentation accuracy. This is because the increased spatial resolution makes the spectral distribution of geographic targets differ from the overall category distribution. The Markov random field (MRF) model is widely used for semantic segmentation of remote sensing images because of its outstanding spatial description ability. Some scholars have made improvements on MRF models to extract more information or enhance semantic inference. However, these improvements fail to capture the correlation between the multigranularity layers and the historical information. In this article, we propose a new MRF-based model that adopts multigranularity layers to realize the multigranularity correlation representation of targets and the spatial-temporal inference of segmentation labels. First, the algorithm constructs a multigrained layer structure based on remote sensing images to enhance feature extraction for targets of different sizes in images. Second, for the multilayer feature field, a cross-layer Gauss-Markov model is constructed based on intra-inter-layer feature correlation constraints. Then, for the multigranularity layer label field, a self-renewing pairwise spatial-temporal potential function with blinking coefficients is constructed based on the newly defined cross-layer augmented neighborhood system, which can accelerate the convergence of segmentation by using the history information and spatial neighborhood information. The proposed method is tested on texture images, SPOT-5, and Gaofen-2 images. Experiments show that the proposed method has a better performance compared to other state-of-the-art MRF-based methods.",Markov random field (MRF),multigranularity,remote sensing image,semantic segmentation,,"Hu, Zhentao","Peng, Qinglan","Qiu, Qian",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_166,"Ma, Xianping","Zhang, Xiaokang","Pun, Man-On",,RS3Mamba: Visual State Space Model for Remote Sensing Image Semantic Segmentation,,2024,17,"Semantic segmentation of remote sensing images is a fundamental task in geoscience research. However, convolutional neural networks (CNNs) and transformers have some significant shortcomings. The former are limited by insufficient long-range modeling capabilities, while the latter are hampered by computational complexity. Recently, a novel visual state space (VSS) model represented by Mamba has emerged, capable of modeling long-range relationships with linear computability. In this research, we propose a novel dual-branch network named remote sensing image semantic segmentation Mamba (RS(3)Mamba) designed specifically for remote sensing tasks. RS(3)Mamba uses VSS blocks to construct an auxiliary branch, providing additional global information to a convolution-based main branch. Moreover, considering the distinct characteristics of the two branches, we introduce a collaborative completion module (CCM) to refine and fuse features from the dual-encoder using a novel adaptive mechanism. Through experiments on two widely used datasets, the proposed RS(3)Mamba was found to outperform the state-of-the-art methods in terms of mIoU with 0.66% on ISPRS Vaihingen and 1.70% on LoveDA Urban, demonstrating its effectiveness and potential. The source code is available at https://github.com/sstary/SSRS.",Remote sensing,Feature extraction,Computational modeling,Semantics,Decoding,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Visualization,Transformers,semantic segmentation,visual state space (VSS) model,,,,,,,,,,,,,,,,,,
Row_167,"Fu, Yujia","Zhang, Xiangrong","Wang, Mingyang",,DSHNet: A Semantic Segmentation Model of Remote Sensing Images Based on Dual Stream Hybrid Network,,2024,5,"Semantic segmentation is an important issue in intelligent interpretation of remote sensing, playing an important role in applications such as Earth observation and land data update. However, remote sensing images often contain complex ground objects and the boundaries between them are blurred, which poses a huge challenge to the semantic segmentation task of remote sensing images. This article proposes a dual stream hybrid network (DSHNet) model, which focuses on parallel extraction of semantic and boundary features in remote sensing images, and improves the performance of semantic segmentation by fully integrating dual stream information. In the semantic stream, the ViT model pretrained on remote sensing images is used as the backbone network for feature extraction. In the boundary stream, the boundary detection operator Sobel is used to capture the boundaries of different ground objects in the image, and a boundary enhancement mechanism is taken to optimize and enhance the feature representation of ground object boundaries. In addition, DSHNet designs a feature fusion module to cross-aggregate features from both semantic and boundary streams. Compared with the state-to-art semantic segmentation methods, DSHNet model has achieved the best performance on two datasets of Yellow River Estuary Wetland and Gaofen image dataset.",Semantics,Feature extraction,Streaming media,Remote sensing,Semantic segmentation,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Transformers,Data mining,Boundary detection,cross-fusion,dual-stream remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,
Row_168,"Li, Linhui","Zhang, Wenjun","Zhang, Xiaoyan","Emam, Mahmoud",Semi-Supervised Remote Sensing Image Semantic Segmentation Method Based on Deep Learning,,JAN 2023,20,"In this paper, we study the semi-supervised semantic segmentation problem via limited labeled samples and a large number of unlabeled samples. We propose a self-learning semi-supervised approach for the semantic segmentation of high-resolution remote sensing images. Our approach uses two networks (UNet and DeepLabV3) to predict the labels of the same unlabeled sample, and the pseudo labels samples with high prediction consistency are added to the training samples to improve the accuracy of semantic segmentation under the condition of limited labeled samples. Our method expands training data samples by using unlabeled data samples with pseudo labels. In order to verify the effectiveness of the proposed method, some experiments were conducted on the improved ISPRS Vaihingen 2D Semantic Labeling dataset using the method that we proposed. We focus on the extraction of forest and vegetation information and focus on the impact of a large number of unlabeled samples on the precision of semantic segmentation, we combine water, surface, buildings, cars, and background into one category and named others, and we call the changed dataset the improved ISPRS Vaihingen dataset. The experimental results show that the proposed method can effectively improve the semantic segmentation accuracy of high-scoring remote sensing images with limited samples than common deep semi-supervised learning.",convolutional neural network,high-resolution remote sensing images,semi-supervised,semantic segmentation,,"Jing, Weipeng",,,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,
Row_169,"Ni, Yue","Liu, Jiahang","Chi, Weijian","Wang, Xiaozhen",CGGLNet: Semantic Segmentation Network for Remote Sensing Images Based on Category-Guided Global-Local Feature Interaction,,2024,5,"As spatial resolution increases, the information conveyed by remote sensing images becomes more and more complex. Large-scale variation and highly discrete distribution of objects greatly increase the challenge of the semantic segmentation task for remote sensing images. Mainstream approaches usually use implicit attention mechanisms or transformer modules to achieve global context for good results. However, these approaches fail to explicitly extract intraobject consistency and interobject saliency features leading to unclear boundaries and incomplete structures. In this article, we propose a category-guided global-local feature interaction network (CGGLNet), which utilizes category information to guide the modeling of global contextual information. To better acquire global information, we proposed a category-guided supervised transformer module (CGSTM). This module guides the modeling of global contextual information by estimating the potential class information of pixels so that features of the same class are more aggregated and those of different classes are more easily distinguished. To enhance the representation of local detailed features of multiscale objects, we designed the adaptive local feature extraction module (ALFEM). By parallel connection of the CGSTM and the ALFEM, our network can extract rich global and local context information contained in the image. Meanwhile, the designed feature refinement segmentation head (FRSH) helps to reduce the semantic difference between deep and shallow features and realizes the full integration of different levels of information. Extensive ablation and comparison experiments on two public remote sensing datasets (ISPRS Vaihingen dataset and ISPRS Potsdam dataset) indicate that our proposed CGGLNet achieves superior performance compared to the state-of-the-art methods.",Category-guided,global contextual information,remote sensing,semantic segmentation,transformer,"Li, Deren",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_170,"Xu, Fan","Shang, Zhigao","Wu, Qihui","Zhang, Xiaofei",MUFNet: Toward Semantic Segmentation of Multi-spectral Remote Sensing Images,AICCC 2021: 2021 4TH ARTIFICIAL INTELLIGENCE AND CLOUD COMPUTING CONFERENCE,2021,2,"In this paper, a new convolutional neural network called multi-U fusion networks (MUFNet) is proposed for accurate semantic segmentation of multi-spectral remote sensing. Essentially, MUFNet is inspired by UNet, MFNet and CAM and fully combines their advantages. First, MUFNet introduces the skip connections into a multi-encoder-to-mono-decoder architecture, thereby facilitating the fusion of multi-scale and multi-channel spectral information. Second, the shortcut module in the decoder is revised by concatenating multiple spectral features from different encoders and then feeding the concatenated data into a CAM unit. Thus, the multispectral context semantics are fused and also the redundant feature maps are attention-compressed. Extensive simulations were conducted by testing UNet, UNet-4ch, MFNet and MUFNet on the 8400 RGB-NIR multi-spectral images with five categories from the GID image dataset. The visual results clearly showed that the proposed MUFNet can achieve more smoothing and complete segmentation performance than the other networks. Moreover, the measure values of mIoU, FWIoU and PA indicate that the proposed MUFNet can outperform the other networks in average semantic segmentation accuracy.",MUFNet,Semantic segmentation,Multi-spectral remote sensing images,Revised shortcut,,"Lin, Zebin","Shao, Shuning",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_171,"Lv, Ning","Zhang, Zenghui","Li, Cong","Deng, Jiaxuan",A hybrid-attention semantic segmentation network for remote sensing interpretation in land-use surveillance,,FEB 2023,35,"Remote sensing interpretation for surveillance of land use often needs to mark out construction disturbance on satellite imagery, such as illegal buildings or spoil area. These disturbance region annotated by a set of surveillance rules contain the corresponding image characteristics which are regarded as semantic information in computer vision. Different from the natural Landscapes interpretation, the semantic information of construciton disturbance region shows more complex to extract with lack of available training dataset and interference of the various sizes of targets. This paper proposes a hybrid attention semantic segmentation network (HAssNet) which can extract the target and its surroundings through a large receptive field for multi-scale targets. Based on the full convolutional networks (FCN), spatial attention mechanism is firstly introduced to acquire the position of segmentation target with the global correlations, so that the small targets in large scale scene are guaranteed not to be omitted in semantic features extraction. Secondly, channel attention mechanism is designed to assign higher weights to task-related channels for semantic consistency. Experimental results on an open remote sensing dataset show that HAssNet achieves average 6.7% improvement in mIoU than the state-of-the-art segmentation networks. In a land use surveillance project, HAssNet shows considerable performance compared with manual interpretation.",Remote sensing,Image interpretation,Semantic segmentation,Attention mechanism,,"Su, Tao","Chen, Chen","Zhou, Yang",,INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS,,,,,,,,,,,,,,,,,,,,,,,
Row_172,"Liu, Honghao","Yang, Ruixia","Xu, Yue","Chen, Zhengchao",DiffRSS: A Diffusion-Guided Multi-Scale Features Remote Sensing Image Segmentation Method,,2025,0,"Semantic segmentation in remote sensing is a fundamental task with crucial applications across various domains. Traditional approaches primarily utilize bottom-up discriminative methods, where network architectures learn image features to generate segmentation masks. However, the complexity of remote sensing images, characterized by diverse ground object types and intricate scenes, often results in information redundancy and confusion during feature extraction, impacting segmentation accuracy. To address these challenges, we introduce a novel segmentation framework, DiffRSS, based on the denoising model paradigm. This top-down generative approach learns the data distribution of sample labels and uses image features as guiding priors for generating segmentation masks. We conceptualize the semantic segmentation of remote sensing images as a conditional generation task and design a Multiscale Cyclic Denoising Module (MSCDM), which effectively leverages multiscale features of remote sensing images, leading to superior segmentation outcomes. Inspired by diffusion models, our denoising structure, MSCDM, can be reused multiple times during inference, enhancing the quality of segmentation masks. This method allows for more precise capture and utilization of image features, resulting in finer and more accurate segmentation masks. Extensive testing on three public remote sensing datasets the ISPRS Vaihingen, ISPRS Potsdam, and GID Fine Land Cover Classification datasets demonstrates that our method achieves competitive results.",Remote sensing,Semantic segmentation,diffusion model,Semantic segmentation,diffusion model,"Zheng, Yuyang",,,,IEEE ACCESS,,,,,,,,,,,,,,,,,,,,,,,
Row_173,"Zhao, Danpei","Wang, Chenxu","Gao, Yue","Shi, Zhenwei",Semantic Segmentation of Remote Sensing Image Based on Regional Self-Attention Mechanism,,2022,22,"In remote sensing images (RSIs), accurate semantic segmentation faces more challenges because of small targets, unbalanced categories, and complex scenes. Restricted by local receptive field of convolution layers, the traditional semantic segmentation models cannot use global information of RSIs. According to the characteristics of RSIs, we propose an RSANet based on regional self-attention mechanism. Our model is no longer limited by the locality of convolution, but transfers the information flow in the whole image. It can mine out the relationship between pixels in the surrounding areas, which is more logical for understanding images content. Moreover, compared with the traditional self-attention mechanism, RSANet can effectively reduce the noise of feature maps and the interference of redundant features. Our model can get better semantic segmentation results than other current models on the DroneDeploy data set and the Chreos semantic segmentation data set. The experiments show that our RSANet achieves 2% higher mean intersection over union (mIoU) than the baseline model, especially in terms of fineness, edge integrity, and classification accuracy.",Convolution,Semantics,Feature extraction,Correlation,Image segmentation,"Xie, Fengying",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Roads,Remote sensing,Convolutional neural network,region descriptors,remote sensing image (RSI),self-attention mechanism,semantic segmentation,,,,,,,,,,,,,,,
Row_174,"Liu, Yutong","Gao, Kun","Wang, Hong","Wang, Junwei",TRANSFORMER AND CNN HYBRID NETWORK FOR SUPER-RESOLUTION SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGERY,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2023,1,"Super-resolution semantic segmentation (SRSS) based on Convolutional neural network (CNN) cannot establish long-range dependencies due to limited receptive field, which limits the SRSS to obtain accurate high-resolution (HR) segmentation results from the low-resolution (LR) input images. In this paper, we design a Transformer and CNN hybrid SRSS network that consists of two branches: Transformer and CNN hybrid SRSS branch and super-resolution guided branch. In the Transformer and CNN hybrid SRSS branch, Transformer extracts global context information from the feature map of the CNN, while skip connection is used to retain the local context information extracted from the CNN and combines both features to further improve the segmentation performance. In addition, the super-resolution guided branch is designed to supplement rich structure information and guide the semantic segmentation (SS). We test the proposed method on the ISPRS Vaihingen benchmark data set, and our network is superior to other state-of-the-art methods.",Remote Sensing,Semantic Segmentation,Super-Resolution Semantic Segmentation,Transformer,,"Zhang, Xiaodian","Wang, Pengyu","Li, Shuzhong",,,,,,,,,,,,,,,,,,,,,,,,,
Row_175,"Yuan, Xiaohui","Shi, Jianfang","Gu, Lichuan",,A review of deep learning methods for semantic segmentation of remote sensing imagery,,MAY 1 2021,377,"Semantic segmentation of remote sensing imagery has been employed in many applications and is a key research topic for decades. With the success of deep learning methods in the field of computer vision, researchers have made a great effort to transfer their superior performance to the field of remote sensing image analysis. This paper starts with a summary of the fundamental deep neural network architectures and reviews the most recent developments of deep learning methods for semantic segmentation of remote sensing imagery including non conventional data such as hyperspectral images and point clouds. In our review of the literature, we identified three major challenges faced by researchers and summarize the innovative development to address them. As tremendous efforts have been devoted to advancing pixel-level accuracy, the emerged deep learning methods demonstrated much-improved performance on several public data sets. As to handling the non-conventional, unstructured point cloud and rich spectral imagery, the performance of the state-of-the-art methods is, on average, inferior to that of the satellite imagery. Such a performance gap also exists in learning from small data sets. In particular, the limited non-conventional remote sensing data sets with labels is an obstacle to developing and evaluating new deep learning methods.",Semantic image segmentation,Deep neural networks,Remote sensing imagery,,,,,,,EXPERT SYSTEMS WITH APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,
Row_176,"Wang, Jiaqi","Liu, Bing","Zhou, Yong","Zhao, Jiaqi",Semisupervised Multiscale Generative Adversarial Network for Semantic Segmentation of Remote Sensing Image,,2022,4,"Semantic segmentation of remote sensing images based on deep neural networks has gained wide attention recently. Although many methods have achieved amazing performance, they need large amounts of labeled images to distinguish the differences in angle, color, size, and other aspects for small targets in remote sensing data sets. However, with a few labeled images, it is difficult to extract the key features of small targets. We propose a semisupervised multiscale generative adversarial network (GAN), which not only utilizes the multipath input and atrous spatial pyramid pooling (ASPP) module but leverages unlabeled images and semisupervised learning strategy to improve the performance of small target segmentation in semantic segmentation when labeled data amount is small. Experimental results show that our model outperforms state-of-the-art methods with insufficient labeled data.",Image segmentation,Semantics,Remote sensing,Feature extraction,Generative adversarial networks,"Xia, Shixiong","Yang, Yuancan","Zhang, Man","Ming, Liu Ming",IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Gallium nitride,Training,Generative adversarial network (GAN),multiscale,remote sensing,semantic segmentation,semisupervised,,,,,,,,,,,,,,,
Row_177,"Zhang, Hua","Jiang, Zhengang","Xu, Jun","Pan, Xin",Advancing high-resolution remote sensing: a compact and powerful approach to semantic segmentation,,SEP 16 2024,0,"Deep learning (DL)-based approaches are notable for their ability to establish feature associations without relying on physical constraints, unlike traditional strategies that are complex and dependent on expert experience. However, three main challenges hinder the versatility of semantic segmentation models. First, the targets in these images are dense and exist at varying spatial scales, which imposes higher demands on the model for accurate segmentation across scales. Second, the segmentation of small targets in the images is often overlooked, leading to a compromise between fine segmentation and model efficiency. Lastly, the data-intensive nature of remote sensing images and the resource-intensive operations of large-scale networks impose significant communication and computation burdens on edge devices, which may not have sufficient resources to handle them effectively. To address these challenges, this paper proposes a lightweight semantic segmentation method for remote sensing images to achieve high-precision segmentation for multi-scale targets while maintaining low computational complexity. The main components include: (1) embedding the inverted residual block structure to minimize the number of model parameters and computational costs; (2) introducing the parallel irregular space pyramid pooling module to efficiently aggregate multi-scale contextual information for fine-grained recognition of small targets; and (3) embedding transfer learning into the encoder-decoder structure to speed up the convergence rate and improve multi-scale feature fusion capability, thereby reducing semantic information loss. The proposed lightweight method has been extensively tested on real-world high-resolution remote sensing datasets. It achieved PA, MPA, MIoU, and FWIoU scores of 87.90%, 75.76%, 66.29%, and 78.81% on the Vaihingen dataset; 87.03%, 85.31%, 74.85%, and 77.54% on the Potsdam dataset; and 95.37%, 83.33%, 75.70%, and 91.31% on the Aeroscapes dataset. Compared to other popular semantic segmentation models, the proposed method achieved the highest values in all four evaluation indicators, demonstrating its effectiveness and superiority.",Remote sensing,image analysis,neural networks,semantic,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_178,"Wang, Yan","Cao, Li","Deng, He",,MFMamba: A Mamba-Based Multi-Modal Fusion Network for Semantic Segmentation of Remote Sensing Images,,NOV 2024,0,"Semantic segmentation of remote sensing images is a fundamental task in computer vision, holding substantial relevance in applications such as land cover surveys, environmental protection, and urban building planning. In recent years, multi-modal fusion-based models have garnered considerable attention, exhibiting superior segmentation performance when compared with traditional single-modal techniques. Nonetheless, the majority of these multi-modal models, which rely on Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs) for feature fusion, face limitations in terms of remote modeling capabilities or computational complexity. This paper presents a novel Mamba-based multi-modal fusion network called MFMamba for semantic segmentation of remote sensing images. Specifically, the network employs a dual-branch encoding structure, consisting of a CNN-based main encoder for extracting local features from high-resolution remote sensing images (HRRSIs) and of a Mamba-based auxiliary encoder for capturing global features on its corresponding digital surface model (DSM). To capitalize on the distinct attributes of the multi-modal remote sensing data from both branches, a feature fusion block (FFB) is designed to synergistically enhance and integrate the features extracted from the dual-branch structure at each stage. Extensive experiments on the Vaihingen and the Potsdam datasets have verified the effectiveness and superiority of MFMamba in semantic segmentation of remote sensing images. Compared with state-of-the-art methods, MFMamba achieves higher overall accuracy (OA) and a higher mean F1 score (mF1) and mean intersection over union (mIoU), while maintaining low computational complexity.",semantic segmentation,multi-modal remote sensing data,feature fusion,,,,,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,
Row_179,"Boulila, Wadii",,,,A top-down approach for semantic segmentation of big remote sensing images,,SEP 2019,28,"The increasing amount of remote sensing data has opened the door to new challenging research topics. Nowadays, significant efforts are devoted to pixel and object based classification in case of massive data. This paper addresses the problem of semantic segmentation of big remote sensing images. To do this, we proposed a top-down approach based on two main steps. The first step aims to compute features at the object-level. These features constitute the input of a multi-layer feed-forward network to generate a structure for classifying remote sensing objects. The goal of the second step is to use this structure to label every pixel in new images. Several experiments are conducted based on real datasets and results show good classification accuracy of the proposed approach. In addition, the comparison with existing classification techniques proves the effectiveness of the proposed approach especially for big remote sensing data.",Semantic segmentation,Remote sensing images,Neural networks,Big data,,,,,,EARTH SCIENCE INFORMATICS,,,,,,,,,,,,,,,,,,,,,,,
Row_180,"Li, Aijin","Jiao, Licheng","Zhu, Hao","Li, Lingling",Multitask Semantic Boundary Awareness Network for Remote Sensing Image Segmentation,,2022,77,"In remote sensing images, boundary information plays a crucial role in land-cover segmentation. However, it is a challenging problem that sufficiently extracts complete and sharp boundaries from complex very-high-resolution (VHR) remote sensing images. To tackle this problem, we propose a semantic boundary awareness network (SBANet). The SBANet captures refined boundary information of land covers in feature extraction and then supervises its learning with a designed boundary loss. The key of SBANet includes boundary attention module (BA-module) and adaptive weights of multitask learning (AWML). The BA-module is proposed to capture land-cover boundary information from hierarchical features aggregation in a bottom-up manner. It emphasizes useful boundary information and relieves noise information in low-level features with the guidance of high-level features. To directly learn the boundary information, AWML adds a boundary loss to the original semantic loss by an adaptive fusion manner. This multitask learning enables the semantic information and the boundary information to work collaboratively and promote each other. Note that the BA-module and AWML are plug-and-play. Experimental results demonstrate the effectiveness of the proposed SBANet on the available ISPRS 2-D semantic labeling Potsdam and Vaihingen data sets. The SBANet also achieves the state-of-the-art performance in terms of overall accuracy (OA) and mean score (m-).",Semantics,Feature extraction,Image segmentation,Remote sensing,Task analysis,"Liu, Fang",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Convolution,Spatial resolution,Boundary attention,multilevel aggregation,multitask learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,
Row_181,"Muhtar, Dilxat","Zhang, Xueliang","Xiao, Pengfeng",,Index Your Position: A Novel Self-Supervised Learning Method for Remote Sensing Images Semantic Segmentation,,2022,27,"Learning effective visual representations without human supervision is a critical problem for the task of semantic segmentation of remote sensing images (RSIs), where pixel-level annotations are difficult to obtain. Self-supervised learning (SSL), which learns useful representations by creating artificial supervised learning problems, has recently emerged as an effective method to learn from unlabelled data. Current SSL methods are generally trained on ImageNet through image-level prediction tasks. We argue that this is suboptimal for application in semantic segmentation of RSIs since it does not take into account spatial position information between objects, which is critical for the segmentation of RSIs characterized by multiobject. In this study, we propose a novel self-supervised dense representation learning method, IndexNet, for the semantic segmentation of RSIs. On the one hand, considering the multiobject characteristics of RSIs, IndexNet learns pixel-level representations by tracking object positions, while maintaining sensitivity to object position changes to ensure that no mismatches are caused. On the other hand, by combining image-level contrast and pixel-level contrast, IndexNet can learn spatiotemporal invariant features. Experimental results show that our method works better than ImageNet pretraining and outperforms state-of-the-art (SOTA) SSL methods. Code and pretrained models will be available at https://github.com/pUmpKin-Co/offical-IndexNet.",Image segmentation,Semantics,Task analysis,Indexes,Remote sensing,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Computer architecture,Crops,Remote sensing images (RSIs),self-supervised learning (SSL),semantic segmentation,,,,,,,,,,,,,,,,,
Row_182,"Li, Zhuoxuan","Yang, Junli","Wang, Bin","Li, Yaqi",MASKFORMER WITH IMPROVED ENCODER-DECODER MODULE FOR SEMANTIC SEGMENTATION OF FINE-RESOLUTION REMOTE SENSING IMAGES,"2022 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",2022,6,"In 2021, the Transformer based models have demonstrated extraordinary achievement in the field of computer vision. Among which, Maskformer, a Transformer based model adopting the mask classification method, is an outstanding model in both semantic segmentation and instance segmentation. Considering the specific characteristics of semantic segmentation of remote sensing images (RSIs), we design CADA-MaskFormer(a Mask classification-based model with Cross-shaped window self-Attention and Densely connected feature Aggregation) based on Maskformer by improving its encoder and pixel decoder. Concretely, the mask classification that generates one or even more masks for specific category to perform the elaborate segmentation is especially suitable for handling the characteristic of large within-class and small between-class variance of RSIs. Furthermore, we apply the Cross-Shaped Window self-attention mechanism to model the long-range context information contained in RSIs at maximum extent without the increasing of computational complexity. In addition, the Densely Connected Feature Aggregation Module (DCFAM) is used as the pixel decoder to incorporate multi-level feature maps from the encoder to get a finer semantic segmentation map. Extensive experiments conducted on two remotely sensed semantic segmentation datasets Potsdam and Vaihingen achieves 91.88% and 91.01% in OA index respectively, outperforming most of competitive models designed for RSIs. The code is available from https://github.com/lqwrl542293/JL-Yang_CV/tree/master/CADA_Maskformer",Semantic segmentation,remote sensing images,Transformer,,,"Pan, Ting",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_183,"Nie, Jie","Huang, Lei","Zheng, Chengyu","Lv, Xiaowei",Cross-scale Graph Interaction Network for Semantic Segmentation of Remote Sensing Images,,NOV 2023,8,"Semantic segmentation of remote sensing (RS) images plays a vital role in a variety of fields, including urban planning, natural disaster monitoring, and land resource management. Due to the complexity and low resolution of RS images, many approaches have been proposed to handle the related task. However, these previously developed approaches dedicate to contextual interaction but ignore the cross-scale semantic correlation and multi-scale boundary information. Therefore, we propose a Cross-scale Graph Interaction Network (CGIN) to address semantic segmentation problems of RS images, which consists of a semantic branch and a boundary branch. In the semantic branch, we first apply atrous convolution to extract multi-scale semantic features of RS images. Particularly, based on the multi-scale semantic features, a Cross-scale Graph Interaction (CGI) module is introduced, which establishes cross-scale graph structures and performs adaptive graph reasoning to capture the cross-scale semantic correlation of RS objects. In the boundary branch, we propose a Multiscale Boundary Feature Extraction (MBFE) module that utilizes atrous convolutions with different dilation rates to extract multi-scale boundary features. Finally, to address the problem of sparse boundary pixels in the fusion process of the two branches, we propose a Multi-scale Similarity-guided Aggregation (MSA) module by calculating the similarity of semantic features and boundary features at the corresponding scale, which can emphasize the boundary information in semantic features. Our proposed CGIN outperforms state-of-the-art approaches in numerical experiments conducted on two benchmark remote sensing datasets.",Remote sensing,semantic segmentation,cross-scale,graph convolutional network,boundary,"Wang, Rui",,,,ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,
Row_184,"Yang, Xue","Fan, Xiang","Peng, Mingjun","Guan, Qingfeng",Semantic segmentation for remote sensing images based on an AD-HRNet model,,DEC 31 2022,10,"Semantic segmentation for remote sensing images faces challenges of unbalanced category weight, rich context causing difficulties of recognition, blurred boundaries of multi-scale objects, and so on. To address these problems, we propose a new model by combining HRNet with attention mechanisms and dilated convolution, denoted as: AD-HRNet for the semantic segmentation of remote sensing images. In the framework of AD-HRNet, we obtained the weight value of each category based on an improved weighted cross-entropy function by introducing the median frequency balance method to solve the issue of class weight imbalance. The Shuffle-CBAM module with channel attention and spatial attention in AD-HRNet framework was applied to extract more global context information of images through slightly increasing the amount of computation. To address the problem of blurred boundaries caused by multi-scale object segmentation and edge segmentation, we developed an MDC-DUC module in AD-HRNet framework to capture the context information of multi-scale objects and the edge information of many irregular objects. Taking Postdam, Vaihingen, and SAMA-VTOL datasets as materials, we verified the performance of AD-HRNet by comparing with eight typical semantic segmentation models. Experimental results shown that AD-HRNet increases the mIoUs to 75.59% and 71.58% based on the Postdam and Vaihingen datasets, respectively.",Semantic segmentation,convolutional neural networks,dilated convolution,attention mechanism,remote sensing,"Tang, Luliang",,,,INTERNATIONAL JOURNAL OF DIGITAL EARTH,,,,,,,,,,,,,,,,,,,,,,,
Row_185,"Duan, Yiping","Tao, Xiaoming","Han, Chaoyi","Lu, Jianhua",HIERARCHICAL MULTINOMIAL LATENT MODEL WITH G0 DISTRIBUTION FOR REMOTE SENSING IMAGE SEMANTIC SEGMENTATION,2017 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP 2017),2017,5,"Considering the scattering statistics and multi scale characteristics of the remote sensing images, this paper presents a hierarchical multinomial latent model with G(0) distribution (HML-G(0)) for remote sensing image semantic segmentation. In the proposed approach, hierarchical multinomial latent model is proposed to capture the multi scale information of the remote sensing images. Moreover, the flexibility of G(0) distribution is plugged into the hierarchical multinomial latent model for the segmentation of various types of land covers. Then, the developed Bayesian inference on the quadtree is incorporated in our approach, and the semantic segmentation map is achieved by bottom-up and top-down probability computation. Experimental results demonstrate that our proposed hierarchical scheme produces the semantic segmentation maps, and the exhibiting performance improvements in terms of labeling consistency and the detail preservation.",Remote sensing images,semantic segmentation,hierarchical multinomial latent model,G(0) distribution,Bayesian inference,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_186,Liu Yu-Xi,Zhang Bo,Wang Bin,,Semi-supervised semantic segmentation based on Generative Adversarial Networks for remote sensing images,,AUG 2020,5,"Semantic segmentation of very high resolution (VHR) remote sensing images is one of the hot topics in the field of remote sensing image processing. Traditional supervised segmentation methods demand a huge mass of labeled data while the labeling process is very consuming. To solve this problem, a semi-supervised semantic segmentation method for VHR remote sensing images based on Generative Adversarial Networks (GANs) is proposed, and only a few labeled samples are needed to obtain pretty good segmentation results. A fully convolutional auxiliary adversarial network is added to the segmentation network, conducing to keeping the consistency of labels in the segmentation results of VHR remote sensing images. Furthermore, a novel adversarial loss with attention mechanism is proposed in the paper in order to solve the problem of easy sample over-whelming during the updating process of the segmentation network constrained by the discriminator when the segmentation results can confuse the discriminator. The experimental results on ISPRS Vaihingen 2D Semantic Labeling Challenge Dataset show that the proposed method can greatly improve the segmentation accuracy of remote sensing images compared with other state-of-the-art methods.",very high resolution remote sensing images,semantic segmentation,deep learning,generative adversarial networks,loss function,,,,,JOURNAL OF INFRARED AND MILLIMETER WAVES,,,,,,,,,,,,,,,,,,,,,,,
Row_187,"Liu, Songlin","Gao, Kai","Qin, Jinchun","Gong, Hui",SE2Net: semantic segmentation of remote sensing images based on self-attention and edge enhancement modules,,MAY 19 2021,4,"The semantic segmentation of optical satellite remote sensing images is more challenging than that of natural images, owing to the considerable differences in the texture, shape, topology, and scale of ground features in different areas and the coexistence of dense and sparse arrangements. To alleviate these difficulties and ensure a high accuracy, a semantic segmentation framework named the self-attention and edge enhancement network (SE(2)Net) is constructed considering two aspects. First, because the self-attention mechanism can capture more useful semantic information by modeling large neighborhood correlations, we embed a self-attention module known as spatial expectation maximization attention (SEMA) in the considered network. Second, the Laplace operator is adopted to explore the significant edge information to design an edge enhancement module (EEM). Finally, both SEMA and EEM are embedded in the proposed SE(2)Net, thereby forming an end-to-end network. To validate the performance of the proposed approach, we construct a semantic segmentation dataset (SSD) using Tian-Hui 1 satellite images and conduct extensive experiments on both the SSD and the gaofen image dataset (GID). The results demonstrate the superiority of the proposed method over other state-of-the-art approaches and the effectiveness of the constructed SSD. (C) 2021 Society of Photo-Optical Instrumentation Engineers (SPIE)",semantic segmentation,optical satellite remote sensing images,edge enhancement,self-attention mechanism,,"Wang, Haiyan","Zhang, Li","Gong, Danchao",,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_188,"Hu, Zaiyi","Gao, Junyu","Yuan, Yuan","Li, Xuelong",Contrastive Tokens and Label Activation for Remote Sensing Weakly Supervised Semantic Segmentation,,2024,0,"In recent years, there has been remarkable progress in weakly supervised semantic segmentation (WSSS), with vision transformer (ViT) architectures emerging as a natural fit for such tasks due to their inherent ability to leverage global attention for comprehensive object information perception. However, directly applying ViT to WSSS tasks can introduce challenges. The characteristics of ViT can lead to an oversmoothing problem, particularly in dense scenes of remote sensing images, significantly compromising the effectiveness of class activation maps (CAMs) and posing challenges for segmentation. Moreover, existing methods often adopt multistage strategies, adding complexity and reducing training efficiency. To overcome these challenges, a comprehensive framework Contrastive Token and Foreground Activation (CTFA) based on the ViT architecture for WSSS of remote sensing images is presented. Our proposed method includes a contrastive token learning module (CTLM), incorporating both patch-wise and class-wise token learning to enhance model performance. In patch-wise learning, we leverage the semantic diversity preserved in intermediate layers of ViT and derive a relation matrix from these layers and employ it to supervise the final output tokens, thereby improving the quality of CAM. In class-wise learning, we ensure the consistency of representation between global and local tokens, revealing more entire object regions. Additionally, by activating foreground features in the generated pseudo label using a dual-branch decoder, we further promote the improvement of CAM generation. Our approach demonstrates outstanding results across three well-established datasets, providing a more efficient and streamlined solution for WSSS. Code will be available at: https://github.com/ZaiyiHu/CTFA.",Remote sensing,Semantic segmentation,Training,Task analysis,Semantics,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Convolutional neural networks,Transformers,Deep learning,remote sensing images,vision transformer (ViT),weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,,
Row_189,"Zhong, Bo","Du, Jiang","Liu, Minghao","Yang, Aixia",Region-Enhancing Network for Semantic Segmentation of Remote-Sensing Imagery,,NOV 2021,1,"Semantic segmentation for high-resolution remote-sensing imagery (HRRSI) has become increasingly popular in machine vision in recent years. Most of the state-of-the-art methods for semantic segmentation of HRRSI usually emphasize the strong learning ability of deep convolutional neural network to model the contextual relationship in the image, which takes too much consideration on every pixel in images and subsequently causes the problem of overlearning. Annotation errors and easily confused features can also lead to the confusion problem while using the pixel-based methods. Therefore, we propose a new semantic segmentation network-the region-enhancing network (RE-Net)-to emphasize the regional information instead of pixels to solve the above problems. RE-Net introduces the regional information into the base network, to enhance the regional integrity of images and thus reduce misclassification. Specifically, the regional context learning procedure (RCLP) can learn the context relationship from the perspective of regions. The region correcting procedure (RCP) uses the pixel aggregation feature to recalibrate the pixel features in each region. In addition, another simple intra-network multi-scale attention module is introduced to select features at different scales by the size of the region. A large number of comparative experiments on four different public datasets demonstrate that the proposed RE-Net performs better than most of the state-of-the-art ones.",semantic segmentation,remote sensing imagery (HRRSI),deep convolutional neural network,regional integrity of images,,"Wu, Junjun",,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,
Row_190,"Zhou, Zheng","Zheng, Change","Liu, Xiaodong","Tian, Ye",A Dynamic Effective Class Balanced Approach for Remote Sensing Imagery Semantic Segmentation of Imbalanced Data,,APR 2023,12,"The wide application and rapid development of satellite remote sensing technology have put higher requirements on remote sensing image segmentation methods. Because of its characteristics of large image size, large data volume, and complex segmentation background, not only are the traditional image segmentation methods difficult to apply effectively, but the image segmentation methods based on deep learning are faced with the problem of extremely unbalanced data between categories. In order to solve this problem, first of all, according to the existing effective sample theory, the effective sample calculation method in the context of semantic segmentation is firstly proposed in the highly unbalanced dataset. Then, a dynamic weighting method based on the effective sample concept is proposed, which can be applied to the semantic segmentation of remote sensing images. Finally, the applicability of this method to different loss functions and different network structures is verified on the self-built Landsat8-OLI remote sensing image-based tri-classified forest fire burning area dataset and the LoveDA dataset, which is for land-cover semantic segmentation. It has been concluded that this weighting algorithm can enhance the minimal-class segmentation accuracy while ensuring that the overall segmentation performance in multi-class segmentation tasks is verified in two different semantic segmentation tasks, including the land use and land cover (LULC) and the forest fire burning area segmentation In addition, this proposed method significantly improves the recall of forest fire burning area segmentation by as much as about 30%, which is of great reference value for forest fire research based on remote sensing images.",remote sensing,segmentation,imbalanced dataset,dynamic weighting,effective sample,"Chen, Xiaoyi","Chen, Xuexue","Dong, Zixun",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_191,"Nong, Zhixian","Su, Xin","Liu, Yi","Zhan, Zongqian",Boundary-Aware Dual-Stream Network for VHR Remote Sensing Images Semantic Segmentation,,2021,11,"Semantic segmentation for very-high-resolution remote sensing images has been a research hotspot in the field of remote sensing image analysis. However, most existing methods still suffer from a challenge that object boundaries cannot be finely recovered. To tackle the problem, we develop a dual-stream network based on the U-Net architecture, Instead of the traditional skip connections, a boundary attention module is proposed to introduce the boundary information from the EDN module to the SSN module. Experiments on ISPRS Potsdam and Vaihingen datasets show the effectiveness of the proposed network, especially in man-made objects with distinct boundaries.",Semantics,Image edge detection,Image segmentation,Remote sensing,Feature extraction,"Yuan, Qiangqiang",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Convolution,Task analysis,Attention module,edge detection subnetwork,semantic segmentation,very high spatial resolution,,,,,,,,,,,,,,,,
Row_192,"Wei, Haoran","Xu, Xiangyang","Ou, Ni","Zhang, Xinru",DEANet: Dual Encoder with Attention Network for Semantic Segmentation of Remote Sensing Imagery,,OCT 2021,21,"Remote sensing has now been widely used in various fields, and the research on the automatic land-cover segmentation methods of remote sensing imagery is significant to the development of remote sensing technology. Deep learning methods, which are developing rapidly in the field of semantic segmentation, have been widely applied to remote sensing imagery segmentation. In this work, a novel deep learning network-Dual Encoder with Attention Network (DEANet) is proposed. In this network, a dual-branch encoder structure, whose first branch is used to generate a rough guidance feature map as area attention to help re-encode feature maps in the next branch, is proposed to improve the encoding ability of the network, and an improved pyramid partial decoder (PPD) based on the parallel partial decoder is put forward to make fuller use of the features form the encoder along with the receptive filed block (RFB). In addition, an edge attention module using the transfer learning method is introduced to explicitly advance the segmentation performance in edge areas. Except for structure, a loss function composed with the weighted Cross Entropy (CE) loss and weighted Union subtract Intersection (UsI) loss is designed for training, where UsI loss represents a new region-based aware loss which replaces the IoU loss to adapt to multi-classification tasks. Furthermore, a detailed training strategy for the network is introduced as well. Extensive experiments on three public datasets verify the effectiveness of each proposed module in our framework and demonstrate that our method achieves more excellent performance over some state-of-the-art methods.",remote sensing,land cover classification,deep learning,semantic segmentation,encoder-decoder,"Dai, Yaping",,,,REMOTE SENSING,,attention mechanism,,,,,,,,,,,,,,,,,,,,,
Row_193,"Zhang, Zehua","Liu, Bailin","Li, Yani",,FURSformer: Semantic Segmentation Network for Remote Sensing Images with Fused Heterogeneous Features,,JUL 2023,3,"Semantic segmentation of remote sensing images poses a formidable challenge within this domain. Our investigation commences with a pilot study aimed at scrutinizing the advantages and disadvantages of employing a Transformer architecture and a CNN architecture in remote sensing imagery (RSI). Our objective is to substantiate the indispensability of both local and global information for RSI analysis. In this research article, we harness the potential of the Transformer model to establish global contextual understanding while incorporating an additional convolution module for localized perception. Nonetheless, a direct fusion of these heterogeneous information sources often yields subpar outcomes. To address this limitation, we propose an innovative hierarchical fusion feature information module that this model can fuse Transformer and CNN features using an ensemble-to-set approach, thereby enhancing information compatibility. Our proposed model, named FURSformer, amalgamates the strengths of the Transformer architecture and CNN. The experimental results clearly demonstrate the effectiveness of this approach. Notably, our model achieved an outstanding accuracy of 90.78% mAccuracy on the DLRSD dataset.",semantic segmentation,remote sensing images,Transformer,CNN,,,,,,ELECTRONICS,,,,,,,,,,,,,,,,,,,,,,,
Row_194,"Liu, Rui","Mi, Li","Chen, Zhenzhong",,AFNet: Adaptive Fusion Network for Remote Sensing Image Semantic Segmentation,,SEP 2021,77,"Semantic segmentation of remote sensing images plays an important role in many applications. However, a remote sensing image typically comprises a complex and heterogenous urban landscape with objects in various sizes and materials, which causes challenges to the task. In this work, a novel adaptive fusion network (AFNet) is proposed to improve the performance of very high resolution (VHR) remote sensing image segmentation. To coherently label size-varied ground objects from different categories, we design multilevel architecture with the scale-feature attention module (SFAM). By SFAM, at the location of small objects, low-level features from the shallow layers of convolutional neural network (CNN) are enhanced, whilst for large objects, high-level features from deep layers are enhanced. Thus, the features of size-varied objects could be preserved during fusing features from different levels, which helps to label size-varied objects. As for labeling the category with high intra-class difference and varied scales, the multiscale structure with a scale-layer attention module (SLAM) is utilized to learn representative features, where an adjacent score map refinement module (ACSR) is employed as the classifier. By SLAM, when fusing multiscale features, based on the interested objects scale, feature map from appropriate scale is given greater weights. With such a scale-aware strategy, the learned features can be more representative, which is helpful to distinguish objects for semantic segmentation. Besides, the performance is further improved by introducing several nonlinear layers to the ACSR. Extensive experiments conducted on two well-known public high-resolution remote sensing image data sets show the effectiveness of our proposed model. Code and predictions are available at https://github.com/athauna/AFNet/",Image segmentation,Simultaneous localization and mapping,Adaptive systems,Semantics,Optical imaging,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Labeling,Convolutional neural networks,Attention mechanism,convolutional neural network (CNN),semantic segmentation,,,,,,,,,,,,,,,,,
Row_195,Su Zhipeng,Li Jingwen,Jiang Jianwu,Lu Yanling,Semantic Segmentation Method for Remote Sensing Images Based on Improved DeepLabV3+,,MAR 2023,1,"A remote sensing image segmentation network called AFSM-Net, which combines a feature map segmentation module and an attention mechanism module, is proposed to address the issues of low recognition and low segmentation accuracy of small targets in remote sensing image segmentation using conventional convolutional neural networks. First, the feature map segmentation module is introduced in the coding stage to enlarge each segmented feature map and extract features by sharing parameters; then, the extracted features are fused with the original output image of the network; and finally, the attention mechanism module is introduced into the network model to make it pay more attention to the effective feature information in the image and ignore the irrelevant background information, to improve the feature extraction ability of the model for small target objects. The experimental results show that the average intersection ratio of the proposed method is 86. 42%, which is 3. 94 percentage points higher than that of the DeepLabV3+ model. The proposed method fully considers the attention of small and medium targets in image segmentation, and improves the segmentation accuracy of remote sensing images.",remote sensing,remote sensing image,DeepLabV3+,feature image cut,attention mechanism,Zhu Ming,,,,LASER & OPTOELECTRONICS PROGRESS,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_196,"Meng, Xichen","Zhu, Liqun","Han, Yilong","Zhang, Hanchao",We Need to Communicate: Communicating Attention Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,JUL 2023,2,"Traditional models that employ CNNs as encoders do not sufficiently combine high-level features and low-level features. However, high-level features are rich in semantic information but lack spatial detail, while low-level features are the opposite. Therefore, the integrated utilization of multi-level features and the bridging of the gap between them is crucial to promote the accuracy of semantic segmentation. To address this issue, we presented communicating mutual attention (CMA) and communicating self-attention (CSA) modules to enhance the interaction and fusion of different levels of feature maps. On the one hand, CMA aggregates the global context information of high-level features into low-level features and embeds the spatial detail localization characteristics of low-level features in high-level features. On the other hand, the CSA module is deployed to integrate the spatially detailed representation of low-level features into the attention map of high-level features. We have experimented with the communicating attention network (CANet), a U-net-like network composed of multiple CMA and CSA modules, on the ISPRS Vaihingen and Potsdam datasets with mean F1-scores of 89.61% and 92.60%, respectively. The results demonstrate that CANet embodies superior performance in the semantic segmentation task of remote sensing of images.",attention mechanism,remote sensing,semantic segmentation,,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_197,"Zheng, Guoxun","Jiang, Zhengang","Zhang, Hua","Yao, Xuekun",Deep semantic segmentation of unmanned aerial vehicle remote sensing images based on fully convolutional neural network,,MAR 28 2023,1,"In the era of artificial intelligence and big data, semantic segmentation of images plays a vital role in various fields, such as people's livelihoods and the military. The accuracy of semantic segmentation results directly affects the subsequent data analysis and intelligent applications. Presently, semantic segmentation of unmanned aerial vehicle (UAV) remote-sensing images is a research hotspot. Compared with manual segmentation and object-based segmentation methods, semantic segmentation methods based on deep learning are efficient and highly accurate segmentation methods. The author has seriously studied the implementation principle and process of the classical deep semantic segmentation model-the fully convolutional neural network (FCN), including convolution and pooling in the encoding stage, deconvolution and upsampling, etc., in the decoding stage. The author has applied the three structures (i.e., FCN-32s, FCN-16s, and FCN-8s) to the UAV remote sensing image dataset AeroScapes. And the results show that the accuracy of vegetation recognition is stable at about 94%. The accuracy of road recognition can reach up to more than 88%. The mean pixel accuracy rate of the whole test dataset is above 91%. Applying full convolution neural network to semantic segmentation of UAV remote sensing images can improve the efficiency and accuracy of semantic segmentation significantly.",deep learning,unmanned aerial vehicle,remote sensing,semantic segmentation,FCN,,,,,FRONTIERS IN EARTH SCIENCE,,,,,,,,,,,,,,,,,,,,,,,
Row_198,He Shumeng,Xu Gaodi,Yang Houqun,,A semantic segmentation method for remote sensing images based on multiple contextual feature extraction,,JAN 25 2023,2,"Semantic segmentation of remote sensing images plays a significant role in many applications such as urban planning and ecological protection, but its semantic segmentation suffers from large intra-category variation and large differences in the scale of objects, so it is prone to misclassification. To cope with this issue, an embedded channel's categorical attention module (ECCA) is proposed to extract contextual information from the perspective of categories, and a channel attention module is embedded in it to achieve multiple contextual information extraction. Combined with the remote sensing atrous spatial pyramid pooling module (RSASPP), which is composed of atrous convolution with different expansion rates, feature fusion of objects at different scales is achieved. The refinement module (RM) is added for boundary refinement to achieve finer segmentation. Experiments are conducted on the WHDLD dataset to prove the effectiveness of the method.",attention mechanism,contextual information,remote sensing images,semantic segmentation,,,,,,CONCURRENCY AND COMPUTATION-PRACTICE & EXPERIENCE,,,,,,,,,,,,,,,,,,,,,,,
Row_199,"Hwang, Gyutae","Jeong, Jiwoo","Lee, Sang Jun",,SFA-Net: Semantic Feature Adjustment Network for Remote Sensing Image Segmentation,,SEP 2024,2,"Advances in deep learning and computer vision techniques have made impacts in the field of remote sensing, enabling efficient data analysis for applications such as land cover classification and change detection. Convolutional neural networks (CNNs) and transformer architectures have been utilized in visual perception algorithms due to their effectiveness in analyzing local features and global context. In this paper, we propose a hybrid transformer architecture that consists of a CNN-based encoder and transformer-based decoder. We propose a feature adjustment module that refines the multiscale feature maps extracted from an EfficientNet backbone network. The adjusted feature maps are integrated into the transformer-based decoder to perform the semantic segmentation of the remote sensing images. This paper refers to the proposed encoder-decoder architecture as a semantic feature adjustment network (SFA-Net). To demonstrate the effectiveness of the SFA-Net, experiments were thoroughly conducted with four public benchmark datasets, including the UAVid, ISPRS Potsdam, ISPRS Vaihingen, and LoveDA datasets. The proposed model achieved state-of-the-art accuracy on the UAVid, ISPRS Vaihingen, and LoveDA datasets for the segmentation of the remote sensing images. On the ISPRS Potsdam dataset, our method achieved comparable accuracy to the latest model while reducing the number of trainable parameters from 113.8 M to 10.7 M.",remote sensing image,segmentation,transformer,hybrid architecture,feature adjustment module,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_200,"Zhang, Xiaolu","Wang, Zhaoshun","Zhang, Jianheng","Wei, Anlei",MSANet: an improved semantic segmentation method using multi-scale attention for remote sensing images,,DEC 2 2022,5,"With the development of deep learning technology in the field of computer vision, the land cover classification of remote sensing images has gradually developed. In this paper, a deep learning algorithm for land cover classification of remote sensing images is proposed. Two dual-attention mechanism modules with multi-scale spatial attention and channel attention are designed and integrated into the DeepLabv3+ network, called multi-scale convolutional attention network (MCAN) and multi-scale atrous convolutional attention network (MACAN). Besides, Inception-Squeeze-and-Excitation network (ISNet) is added. So, the network can focus on the parts of remote sensing images that need attention at different scales, and it can improve the accuracy of remote sensing semantic segmentation. The experimental results show that the optimized network can achieve a Mean Intersection over Union (mIoU) of 74.03% on the DeepGlobe datasets, which is 14.55%, 13.47%, and 5.73% higher than that of UNet, SegNet, and DeepLabv3+. It also achieved good results on Vaihingen datasets. This result indicates that the optimized network can effectively improve the accuracy of remote sensing land cover classification.",Remote sensing semantic segmentation,land cover classification,attention mechanism,deep learning,,,,,,REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,
Row_201,"Li, Jiarui","Cheng, Shuli",,,AFENet: An Attention-Focused Feature Enhancement Network for the Efficient Semantic Segmentation of Remote Sensing Images,,DEC 2024,0,"The semantic segmentation of high-resolution remote sensing images (HRRSIs) faces persistent challenges in handling complex architectural structures and shadow occlusions, limiting the effectiveness of existing deep learning approaches. To address these limitations, we propose an attention-focused feature enhancement network (AFENet) with a novel encoder-decoder architecture. The encoder architecture combines ResNet50 with a parallel multistage feature enhancement group (PMFEG), enabling robust feature extraction through optimized channel reduction, scale expansion, and channel reassignment operations. Building upon this foundation, we develop a global multi-scale attention mechanism (GMAM) in the decoder that effectively synthesizes spatial information across multiple scales by learning comprehensive global-local relationships. The architecture is further enhanced by an efficient feature-weighted fusion module (FWFM) that systematically integrates remote spatial features with local semantic information to improve segmentation accuracy. Experimental results across diverse scenarios demonstrate that AFENet achieves superior performance in building structure detection, exhibiting enhanced segmentation connectivity and completeness compared to state-of-the-art methods.",remote sensing,semantic segmentation,multi-scale feature,attention mechanism,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_202,"Zhu, Jiahang","Zhou, Yuan","Xu, Nuo","Huo, Chunlei",Collaborative Learning Network for Change Detection and Semantic Segmentation of Remote Sensing Images,,2023,1,"Change detection of high-resolution remote sensing images is more attractive, since it can not only identify areas of changes but also identify types of changes. In this context, simultaneous change detection and semantic segmentation are natural and necessary. However, traditional methods put less emphasis on the cooperation of the above two tasks. In this letter, a novel method is proposed to realize the collaborative learning of change detection and semantic segmentation. By elaborately exploring the relevance and consistency between change detection and semantic segmentation, the proposed method synchronously enhanced feature separability of two tasks, and it outperformed a single change detection network or semantic segmentation network. Specifically, the proposed approach extracts multilevel bitemporal features by a backbone network, followed by two layer-by-layer decoders for learning change features and semantic features. On one hand, the interactive fusion module (IFM) fuses the changing features and semantic features together to increase the collaboration between the two tasks. On the other hand, the contrastive loss (CL) enhances the constraints between the two tasks. The advantages of the proposed method are demonstrated with respect to change region detection and change-type identification.",Change detection,collaborative learning,semantic segmentation,,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,
Row_203,"Chen, Yuhan","Liu, Pengyuan","Zhao, Jiechen","Huang, Kaijian",Shallow-Guided Transformer for Semantic Segmentation of Hyperspectral Remote Sensing Imagery,,JUL 2023,10,"Convolutional neural networks (CNNs) have achieved great progress in the classification of surface objects with hyperspectral data, but due to the limitations of convolutional operations, CNNs cannot effectively interact with contextual information. Transformer succeeds in solving this problem, and thus has been widely used to classify hyperspectral surface objects in recent years. However, the huge computational load of Transformer poses a challenge in hyperspectral semantic segmentation tasks. In addition, the use of single Transformer discards the local correlation, making it ineffective for remote sensing tasks with small datasets. Therefore, we propose a new Transformer layered architecture that combines Transformer with CNN, adopts a feature dimensionality reduction module and a Transformer-style CNN module to extract shallow features and construct texture constraints, and employs the original Transformer Encoder to extract deep features. Furthermore, we also designed a simple Decoder to process shallow spatial detail information and deep semantic features separately. Experimental results based on three publicly available hyperspectral datasets show that our proposed method has significant advantages compared with other traditional CNN, Transformer-type models.",vision transformer,convolutional neural networks (CNNs),feature representations,hyperspectral images (HSIs),semantic segmentation,"Yan, Qingyun",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_204,"Zheng, Chen","Zhang, Yun","Wang, Leiguang",,Multigranularity Multiclass-Layer Markov Random Field Model for Semantic Segmentation of Remote Sensing Images,,DEC 2021,16,"Semantic segmentation is one of the most important tasks in remote sensing. However, as spatial resolution increases, distinguishing the homogeneity of each land class and the heterogeneity between different land classes are challenging. The Markov random field model (MRF) is a widely used method for semantic segmentation due to its effective spatial context description. To improve segmentation accuracy, some MRF-based methods extract more image information by constructing the probability graph with pixel or object granularity units, and some other methods interpret the image from different semantic perspectives by building multilayer semantic classes. However, these MRF-based methods fail to capture the relationship between different granularity features extracted from the image and hierarchical semantic classes that need to be interpreted. In this article, a new MRF-based method is proposed to incorporate the multigranularity information and the multilayer semantic classes together for semantic segmentation of remote sensing images. The proposed method develops a framework that builds a hybrid probability graph on both pixel and object granularities and defines a multiclass-layer label field with hierarchical semantic over the hybrid probability graph. A generative alternating granularity inference is suggested to provide the result by iteratively passing and updating information between different granularities and hierarchical semantics. The proposed method is tested on texture images, different remote sensing images obtained by the SPOT5, Gaofen-2, GeoEye, and aerial sensors, and Pavia University hyperspectral image. Experiments demonstrate that the proposed method shows a better segmentation performance than other state-of-the-art methods.",Semantics,Image segmentation,Remote sensing,Feature extraction,Biological system modeling,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Spatial resolution,Nonhomogeneous media,Granularity,Markov random field (MRF),remote sensing,segmentation,semantic,,,,,,,,,,,,,,,
Row_205,"Liu, Jiachao","Xiong, Xinyue","Li, Jiaojiao","Wu, Chaoxiong",DILATED RESIDUAL NETWORK BASED ON DUAL EXPECTATION MAXIMIZATION ATTENTION FOR SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,3,"Compared with common RGB images, remote sensing images (RSIs) have larger size and lower spatial resolution. RSIs are usually cropped into sub-images for training convolutional neural networks (CNNs), which loses amounts of context information, thus limiting the extraction of feature interdependencies and reducing the accuracy of semantic segmentation. In this paper, a novel dilated residual network based on dual expectation maximization attention (DE-MANet) is proposed for semantic segmentation of RSIs. In specific, we append a dual expectation maximization attention (DEMA) module on top of the dilated CNN. The spatial expectation maximization attention (SEMA) can model spatial feature interdependencies to acquire rich long-range contextual information. The channel expectation maximization attention (CEMA) enhances discriminant ability of channel-wise feature representations through extracting the channel dependencies. We evaluate the model on the dataset released in the Tianzhi Cup Artificial Intelligence Challenge and achieve 85.60% pixel accuracy and 69.00% mean intersection over union (mIoU).",Remote Sensing Images,Semantic Segmentation,DEMANet,Dual Expectation Maximization Attention,,"Song, Rui",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_206,"Wang, Huaijun","Qiao, Luqi","Li, He","Li, Xiujuan",Remote sensing image semantic segmentation method based on small target and edge feature enhancement,,OCT 1 2023,5,"Semantic segmentation of high-resolution remote sensing images based on deep learning has become a hot research topic and has been widely applied. At present, based on the structure of the convolutional neural network, when extracting target features through multiple layer convolutional layers, it is easy to cause the loss of small target features and fuzzy boundary of ground object classification. Therefore, we propose a remote sensing image semantic segmentation method P-Net to detect small target and enhance edge feature. The proposed network was based on an Encoder-Decoder structure. The decoder included the following components: a progressive small target feature enhancement network (IFEN), a boundary thinning module (BRM), and a feature aggregation module (FIAM). Firstly, the dense side output features of the encoder network were utilized to learn and acquired small target feature information and target edge features. Secondly, the pyramid segmentation attention module was introduced to effectively extract fine-grained and multi-scale spatial information. This module enhanced the feature expression of small targets and obtained high-level semantic feature information. The boundary refinement module was designed to refine the low-level spatial feature information extracted by the encoder. Finally, in order to improve the accuracy of remote sensing image object segmentation boundaries, skip connections were used to fuse high-level semantic information and low-level spatial information acrossed layers. These skip connections had the same spatial resolution but different semantic information. In this paper, six evaluation indices including mean intersection over union, frequency weighted intersection over union, pixel accuracy, F1, recall, and precision were used to verify on two public datasets of high-resolution remote sensing images, Gaofen image dataset (GID) and wuhan dense labeling dataset (WHDLD). In the GID dataset, each index reached 78.90%, 78.87%, 87.76%, 87.74%, 87.51%, and 88.04%, respectively; in the WHDLD dataset, each index reached 63.21%, 75.20%, 84.67%, 75.79%, 76.56%, and 75.45%, respectively. The results show that the performance of proposed method is better than that of DeepLabv3+, U-NET, PSPNet, and DUC_HDC methods. More precisely, the recognition performance of small target features is better, and the boundary obtained between object categories is clearer.",remote sensing image,semantic segmentation,pyramidal syncopated attention module,feature fusion,small target,"Li, Junhuai","Cao, Ting","Zhang, Chunyi",,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_207,"Huang, Wubiao","Ding, Mingtao","Deng, Fei",,Domain-Incremental Learning for Remote Sensing Semantic Segmentation With Multifeature Constraints in Graph Space,,2024,0,"The use of deep learning techniques for semantic segmentation in remote sensing has been increasingly prevalent. Effectively modeling remote contextual information and integrating high-level abstract features with low-level spatial features are critical challenges for semantic segmentation tasks. This article addresses these challenges by constructing a graph space reasoning (GSR) module and a dual-channel cross-attention upsampling (DCAU) module. Meanwhile, a new domain-incremental learning (DIL) framework is designed to alleviate catastrophic forgetting when the deep learning model is used in cross-domain. This framework makes a balance between retaining prior knowledge and acquiring new information through the use of frozen feature layers and multifeature joint loss optimization. Based on this, a new DIL of remote sensing semantic segmentation with multifeature constraints in graph space (GSMF-RS-DIL) framework is proposed. Extensive experiments, including ablation experiments on the ISPRS and LoveDA datasets, demonstrate that the proposed method achieves superior performance and optimal computational efficiency in both single-domain and cross-domain tasks. The code is publicly available at https://github.com/Huang WBill/GSMF-RS-DIL.",Cross attention,domain-incremental learning (DIL),graph space reasoning (GSR),remote sensing image,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Cross attention,domain-incremental learning (DIL),graph space reasoning (GSR),remote sensing image,semantic segmentation,,,,,,,,,,,,,,,,,
Row_208,"Zheng, Yalan","Yang, Mengyuan","Wang, Min","Qian, Xiaojun",Semi-Supervised Adversarial Semantic Segmentation Network Using Transformer and Multiscale Convolution for High-Resolution Remote Sensing Imagery,,APR 2022,16,"Semantic segmentation is a crucial approach for remote sensing interpretation. High-precision semantic segmentation results are obtained at the cost of manually collecting massive pixelwise annotations. Remote sensing imagery contains complex and variable ground objects and obtaining abundant manual annotations is expensive and arduous. The semi-supervised learning (SSL) strategy can enhance the generalization capability of a model with a small number of labeled samples. In this study, a novel semi-supervised adversarial semantic segmentation network is developed for remote sensing information extraction. A multiscale input convolution module (MICM) is designed to extract sufficient local features, while a Transformer module (TM) is applied for long-range dependency modeling. These modules are integrated to construct a segmentation network with a double-branch encoder. Additionally, a double-branch discriminator network with different convolution kernel sizes is proposed. The segmentation network and discriminator network are jointly trained under the semi-supervised adversarial learning (SSAL) framework to improve its segmentation accuracy in cases with small amounts of labeled data. Taking building extraction as a case study, experiments on three datasets with different resolutions are conducted to validate the proposed network. Semi-supervised semantic segmentation models, in which DeepLabv2, the pyramid scene parsing network (PSPNet), UNet and TransUNet are taken as backbone networks, are utilized for performance comparisons. The results suggest that the approach effectively improves the accuracy of semantic segmentation. The F1 and mean intersection over union (mIoU) accuracy measures are improved by 0.82-11.83% and 0.74-7.5%, respectively, over those of other methods.",semantic segmentation,semi-supervised learning,transformer,adversarial learning,remote sensing,"Yang, Rui","Zhang, Xin","Dong, Wen",,REMOTE SENSING,,building extraction,,,,,,,,,,,,,,,,,,,,,
Row_209,"Qin, Yiqing","Chi, Mingmin",,,RSImageNet: A Universal Deep Semantic Segmentation Lifecycle for Remote Sensing Images,,2020,5,"In real applications, there is a lack of labeled data to train a proper deep neural network (DNN) model for map generation of remote sensing images. The aim of newly acquired data in spaceborne or airborne platforms is often to consistently observe the Earth for new tasks in the applications such as disaster monitoring, climate change, disease control. To fulfill the tasks, the corresponding classification maps should be obtained traditionally based on the assumption that a classification model should be learnt by the labeled data for the same task from the same scene or at least from the historical labeled remote sensing image pixels provided by domain experts in the same areas by the same sensor, which is denoted as labeled target data. In the paper, a universal deep semantic segmentation lifecycle is proposed against the assumption aforementioned, i.e., there is no need to have the labeled data for the same/similar task from the same locations and sensors to define a proper DNN model. In particular, a general labeled dataset is generated through a feature binding strategy in terms of real-world existed remote sensing images, which is named RSImageNet. In addition, a special training strategy is proposed by using the RSImageNet dataset to train a universal deep semantic segmentation model with a balanced constraint for the loss function. Without the labeled target data from the area observed, we gain an average overall accuracy of 77.32% in the range of 67.28-94.63% on 6 real world datasets by taking advantage of the proposed universal deep semantic segmentation lifecycle and the generated RSImageNet dataset.",Semantics,Image segmentation,Remote sensing,Training,Task analysis,,,,,IEEE ACCESS,,Data models,Machine learning,Semantic segmentation,RSImageNet,remote sensing images,fully convolutional network,big data,,,,,,,,,,,,,,,
Row_210,"Zhang, Qian","Yang, Guang","Zhang, Guixu",,Collaborative Network for Super-Resolution and Semantic Segmentation of Remote Sensing Images,,2022,30,"In the past few years, multitask learning (MTL) has been widely used in a single model to solve the problems of multiple businesses. MTL enables each task to achieve high performance and greatly reduces computational resource overhead. In this work, we designed a collaborative network that simultaneously solves the super-resolution semantic segmentation and super-resolution image reconstruction. This algorithm can obtain high-resolution semantic segmentation and super-resolution reconstruction results by taking relatively low-resolution images as input when high-resolution data are inconvenient or computing resources are limited. The framework consists of three parts: the semantic segmentation branch (SSB), the super-resolution branch (SRB), and the structural affinity block (SAB). Specifically, the SSB, SRB, and SAB are responsible for completing super-resolution semantic segmentation, image super-resolution reconstruction, and associated features, respectively. Our proposed method is simple and efficient, and it can replace the different branches with most of the state-of-the-art models. The International Society for Photogrammetry and Remote Sensing (ISPRS) segmentation benchmarks were used to evaluate our models. In particular, super-resolution semantic segmentation on the Potsdam dataset reduced Intersection over Union (IoU) by only 1.8% when the resolution of the input image was reduced by a factor of two. The experimental results showed that our framework can obtain more accurate semantic segmentation and super-resolution reconstruction results than the single model.",Image segmentation,Semantics,Superresolution,Task analysis,Remote sensing,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Image reconstruction,Image resolution,Multitask learning (MTL),remote sensing,semantic segmentation,super resolution,,,,,,,,,,,,,,,,
Row_211,"Zhang, Libao","Ma, Jie","Lv, Xiruan","Chen, Donghui",Hierarchical Weakly Supervised Learning for Residential Area Semantic Segmentation in Remote Sensing Images,,JAN 2020,35,"Residential-area segmentation is one of the most fundamental tasks in the field of remote sensing. Recently, fully supervised convolutional neural network (CNN)-based methods have shown superiority in the field of semantic segmentation. However, a serious problem for those CNN-based methods is that pixel-level annotations are expensive and laborious. In this study, a novel hierarchical weakly supervised learning (HWSL) method is proposed to realize pixel-level semantic segmentation in remote sensing images. First, a weakly supervised hierarchical saliency analysis is proposed to capture a sequence of class-specific hierarchical saliency maps by computing the gradient maps with respect to the middle layers of the CNN. Then, superpixels and low-rank matrix recovery are introduced to highlight the common salient areas and fuse class-specific saliency maps with adaptive weights. Finally, a subtraction operation between class-specific saliency maps is conducted to generate hierarchical residual saliency maps and fulfill residential-area segmentation. Comprehensive evaluations with two remote sensing data sets and comparison with seven methods validate the superiority of the proposed HWSL model.",Image segmentation,Semantics,Remote sensing,Task analysis,Image color analysis,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Fuses,Deep learning,remote sensing,saliency analysis,semantic segmentation,weakly supervised,,,,,,,,,,,,,,,
Row_212,"Guo, Shichen","Yang, Qi","Xiang, Shiming","Wang, Shuwen",Mask2Former with Improved Query for Semantic Segmentation in Remote-Sensing Images,,MAR 2024,3,"Semantic segmentation of remote sensing (RS) images is vital in various practical applications, including urban construction planning, natural disaster monitoring, and land resources investigation. However, RS images are captured by airplanes or satellites at high altitudes and long distances, resulting in ground objects of the same category being scattered in various corners of the image. Moreover, objects of different sizes appear simultaneously in RS images. For example, some objects occupy a large area in urban scenes, while others only have small regions. Technically, the above two universal situations pose significant challenges to the segmentation with a high quality for RS images. Based on these observations, this paper proposes a Mask2Former with an improved query (IQ2Former) for this task. The fundamental motivation behind the IQ2Former is to enhance the capability of the query of Mask2Former by exploiting the characteristics of RS images well. First, we propose the Query Scenario Module (QSM), which aims to learn and group the queries from feature maps, allowing the selection of distinct scenarios such as the urban and rural areas, building clusters, and parking lots. Second, we design the query position module (QPM), which is developed to assign the image position information to each query without increasing the number of parameters, thereby enhancing the model's sensitivity to small targets in complex scenarios. Finally, we propose the query attention module (QAM), which is constructed to leverage the characteristics of query attention to extract valuable features from the preceding queries. Being positioned between the duplicated transformer decoder layers, QAM ensures the comprehensive utilization of the supervisory information and the exploitation of those fine-grained details. Architecturally, the QSM, QPM, and QAM as well as an end-to-end model are assembled to achieve high-quality semantic segmentation. In comparison to the classical or state-of-the-art models (FCN, PSPNet, DeepLabV3+, OCRNet, UPerNet, MaskFormer, Mask2Former), IQ2Former has demonstrated exceptional performance across three publicly challenging remote-sensing image datasets, 83.59 mIoU on the Vaihingen dataset, 87.89 mIoU on Potsdam dataset, and 56.31 mIoU on LoveDA dataset. Additionally, overall accuracy, ablation experiment, and visualization segmentation results all indicate IQ2Former validity.",semantic segmentation,remote-sensing image,transformer,Mask2Former,query,"Wang, Xuezhi",,,,MATHEMATICS,,,,,,,,,,,,,,,,,,,,,,,
Row_213,"Jin, Jianhui","Zhou, Wujie","Yang, Rongwang","Ye, Lv",Edge Detection Guide Network for Semantic Segmentation of Remote-Sensing Images,,2023,37,"The acquisition of high-resolution satellite and airborne remote sensing images has been significantly simplified due to the rapid development of sensor technology. Several practical applications of high-resolution remote sensing images (HRRSIs) are based on semantic segmentation. However, single-modal HRRSIs are difficult to classify accurately in the complex situation of some scene objects; therefore, the semantic segmentation of multi-source information fusion is gaining popularity. The inherent difference between multimodal features and the semantic gap between multi-level features typically affect the performance of existing multi-mode fusion methods. We propose a multimodal fusion network based on edge detection to address these issues. This method aids multimodal information fusion by utilizing spatial information contained in the boundary. An edge detection guide module is included in the feature extraction stage to realize the boundary information through the fusion of details and semantics between high-level and low-level features. The boundary information is extended into the well-designed multimodal adaptive fusion block (MAFB) to obtain the multimodal fusion features. Furthermore, a residual adaptive fusion block (RAFB) and a spatial position module (SPM) in the feature decoding stage have been designed to fuse multi-level features from the standpoint of local and global dependence. We compared our method to several state-of-the-art (SOTA) methods using the International Society for Photogrammetry and Remote Sensing's (ISPRS) Vaihingen and Potsdam datasets. The final results demonstrate that our method achieves excellent performance.",Feature extraction,Semantics,Semantic segmentation,Remote sensing,Image edge detection,"Yu, Lu",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Convolution,Optical sensors,Edge detection,multi-level,multimodal,semantic segmentation,,,,,,,,,,,,,,,,
Row_214,"Yang, Zimeng","Wu, Qiulan","Zhang, Feng","Zhang, Xueshen",A New Semantic Segmentation Method for Remote Sensing Images Integrating Coordinate Attention and SPD-Conv,,MAY 8 2023,6,"Semantic segmentation is an important task for the interpretation of remote sensing images. Remote sensing images are large in size, contain substantial spatial semantic information, and generally exhibit strong symmetry, resulting in images exhibiting large intraclass variance and small interclass variance, thus leading to class imbalance and poor small-object segmentation. In this paper, we propose a new remote sensing image semantic segmentation network, called CAS-Net, which includes coordinate attention (CA) and SPD-Conv. In the model, we replace stepwise convolution with SPD-Conv convolution in the feature extraction network and add a pooling layer into the network to avoid the loss of detailed information, effectively improving the segmentation of small objects. The CA is introduced into the atrous spatial pyramid pooling (ASPP) module, thus improving the recognizability of classified objects and target localization accuracy in remote sensing images. Finally, the Dice coefficient was introduced into the cross-entropy loss function to maximize the gradient optimization of the model and solve the classification imbalance problem in the image. The proposed model is compared with several state-of-the-art models on the ISPRS Vaihingen dataset. The experimental results demonstrate that the proposed model significantly optimizes the segmentation effect of small objects in remote sensing images, effectively solves the problem of class imbalance in the dataset, and improves segmentation accuracy.",remote sensing,semantic segmentation,coordinate attention mechanism,SPD-Conv,small objects,"Chen, Xuefei","Gao, Yue",,,SYMMETRY-BASEL,,,,,,,,,,,,,,,,,,,,,,,
Row_215,"Zheng, Xianwei","Wu, Xiujie","Huan, Linxi","He, Wei",A Gather-to-Guide Network for Remote Sensing Semantic Segmentation of RGB and Auxiliary Image,,2022,23,"Convolutional neural network (CNN)-based feature fusion of RGB and auxiliary remote sensing data is known to enable improved semantic segmentation. However, such fusion is challengeable because of the substantial variance in data characteristics and quality (e.g., data uncertainties and misalignment) between two modality data. In this article, we propose a unified gather-to-guide network (G2GNet) for remote sensing semantic segmentation of RGB and auxiliary data. The key aspect of the proposed architecture is a novel gather-to-guide module (G2GM) that consists of a feature gatherer and a feature guider. The feature gatherer generates a set of cross-modal descriptors by absorbing the complementary merits of RGB and auxiliary modality data. The feature guider calibrates the RGB feature response by using the channel-wise guide weights extracted from the cross-modal descriptors. In this way, the G2GM can perform RGB feature calibration with different modality data in a gather-to-guide fashion, thus preserving the informative features while suppressing redundant and noisy information. Extensive experiments conducted on two benchmark datasets show that the proposed G2GNet is robust to data uncertainties while also improving the semantic segmentation performance of RGB and auxiliary remote sensing data.",Semantics,Image segmentation,Remote sensing,Feature extraction,Convolutional neural networks,"Zhang, Hongyan",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Calibration,Task analysis,Deep learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,
Row_216,"Lv, Jinna","Shen, Qi","Lv, Mingzheng","Li, Yiran",Deep learning-based semantic segmentation of remote sensing images: a review,,JUL 14 2023,14,"Semantic segmentation is a fundamental but challenging problem of pixel-level remote sensing (RS) data analysis. Semantic segmentation tasks based on aerial and satellite images play an important role in a wide range of applications. Recently, with the successful applications of deep learning (DL) in the computer vision (CV) field, more and more researchers have introduced and improved DL methods to the task of RS data semantic segmentation and achieved excellent results. Although there are a large number of DL methods, there remains a deficiency in the evaluation and advancement of semantic segmentation techniques for RS data. To solve the problem, this paper surveys more than 100 papers in this field in the past 5 years and elaborates in detail on the aspects of technical framework classification discussion, datasets, experimental evaluation, research challenges, and future research directions. Different from several previously published surveys, this paper first focuses on comprehensively summarizing the advantages and disadvantages of techniques and models based on the important and difficult points. This research will help beginners quickly establish research ideas and processes in this field, allowing them to focus on algorithm innovation without paying too much attention to datasets, evaluation indicators, and research frameworks.",remote sensing,deep learning,convolutional neural network,semantic segmentation,satellite image,"Shi, Lei","Zhang, Peiying",,,FRONTIERS IN ECOLOGY AND EVOLUTION,,,,,,,,,,,,,,,,,,,,,,,
Row_217,"Ren, Dong","Li, Falin","Sun, Hang","Liu, Li",Local-enhanced multi-scale aggregation swin transformer for semantic segmentation of high-resolution remote sensing images,,JAN 2 2024,1,"Semantic segmentation of remote sensing images is crucial for various practical applications. In the field of deep learning, convolutional neural network (CNN) has been the primary approach for semantic segmentation over the past decade. Recently, Transformer-based models have achieved superior segmentation performance due to their exceptional global modelling capabilities. However, the Transformer-based models tend to focus more on extracting global contextual information, leading to suboptimal performance in segmenting local edges and difficulties in preserving fine-grained details during the patch token downsampling process. Inspired by the local receptive field of CNN, this article proposes a Local-Enhanced Multi-Scale Aggregation Swin Transformer (LMA-Swin) for semantic segmentation of high-resolution remote sensing images. Specifically, we adopt Swin Transformer as main encoder, introduce convolutional blocks as auxiliary encoder, and design a feature modulation module (FMM) to integrate the local contextual modelling ability of CNN into the Transformer backbone. Additionally, we propose a novel cross-aggregation decoder (CAD) to effectively aggregate shallow edge information and deep semantic information, thereby enhancing the discriminative ability for multi-scale objects. On the ISPRS Vaihingen and Potsdam datasets, experimental results illustrate noteworthy improvement in segmentation performance accomplished through the proposed approach. Code: https://github.com/patricklee16/LMA-Swin.",Local enhancement,multi-scale aggregation,feature modulation,semantic segmentation,remote sensing,"Ren, Shun","Yu, Mei",,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_218,"Zhang, Zheng","Liu, Fanchen","Liu, Changan","Tian, Qing",ACTNet: A Dual-Attention Adapter with a CNN-Transformer Network for the Semantic Segmentation of Remote Sensing Imagery,,APR 29 2023,8,"In recent years, the application of semantic segmentation methods based on the remote sensing of images has become increasingly prevalent across a diverse range of domains, including but not limited to forest detection, water body detection, urban rail transportation planning, and building extraction. With the incorporation of the Transformer model into computer vision, the efficacy and accuracy of these algorithms have been significantly enhanced. Nevertheless, the Transformer model's high computational complexity and dependence on a pre-training weight of large datasets leads to a slow convergence during the training for remote sensing segmentation tasks. Motivated by the success of the adapter module in the field of natural language processing, this paper presents a novel adapter module (ResAttn) for improving the model training speed for remote sensing segmentation. The ResAttn adopts a dual-attention structure in order to capture the interdependencies between sets of features, thereby improving its global modeling capabilities, and introduces a Swin Transformer-like down-sampling method to reduce information loss and retain the original architecture while reducing the resolution. In addition, the existing Transformer model is limited in its ability to capture local high-frequency information, which can lead to an inadequate extraction of edge and texture features. To address these issues, this paper proposes a Local Feature Extractor (LFE) module, which is based on a convolutional neural network (CNN), and incorporates multi-scale feature extraction and residual structure to effectively overcome this limitation. Further, a mask-based segmentation method is employed and a residual-enhanced deformable attention block (Deformer Block) is incorporated to improve the small target segmentation accuracy. Finally, a sufficient number of experiments were performed on the ISPRS Potsdam datasets. The experimental results demonstrate the superior performance of the model described in this paper.",remote sensing,semantic segmentation,transformer,adapter,,"Qu, Hongquan",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_219,"Wu, Jiayi","Qin, Chuan","Ren, Yanli","Feng, Guorui",EPFNet: Edge-Prototype Fusion Network Toward Few-Shot Semantic Segmentation for Aerial Remote-Sensing Images,,2023,3,"Few-shot semantic segmentation is a technique that is receiving increasing attention. The aim of this approach is to enable models to segment objects with a few support images (usually 1, 5, 10, etc.). At present, few-shot semantic segmentation has made great progress in the field of natural scene images (NSIs), but these methods cannot be applied directly to the field of remote-sensing images (RSIs). To overcome this challenge, we propose a novel semantic segmentation network structure that integrates prototype information with global edge information to achieve more accurate prototype-matching results. In addition, we design a comprehensive weighted loss function to monitor the training process to help overcome the challenges. Results of the performance comparison with state-of-the-art few-shot semantic segmentation methods demonstrate the superiority of the proposed method.",Edge-prototype fusion,few-shot semantic segmentation,remote-sensing image (RSI),,,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,
Row_220,"Ulku, Irem","Akagunduz, Erdem",,,Semantic Segmentation of Crop Areas in Remote Sensing Imagery using Spectral Indices and Multiple Channels,"FIFTEENTH INTERNATIONAL CONFERENCE ON MACHINE VISION, ICMV 2022",2023,0,"This study focuses on pixel-wise semantic segmentation of crop production regions by using satellite remote sensing multispectral imagery. One of the principal aims of the study is to find out whether the raw multiple channel inputs are more effective in the training process of the semantic segmentation models or if the formularized counterparts as the spectral indices are more effective. For this purpose, the vegetation indices NDVI, ARVI and SAVI and the water indices NDWI, NDMI, and WRI are employed as inputs. Additionally, using 8, 10 and 16 channels, multiple channel inputs are utilized. Moreover, all spectral indices are taken as separate channels to form a multiple channel input. We conduct deep learning experiments using two semantic segmentation architectures, namely U-Net and DeepLabV3+. Our results show that, in general, feeding raw multiple channel inputs to semantic segmentation models performs much better than feeding the spectral indices. Hence, regarding crop production region segmentation, deep learning models are capable of encoding multispectral information. When the spectral indices are compared among themselves, ARVI, which reduces the atmospheric scattering effects, achieves better accuracy for both architectures. The results also reveal that spatial resolution of multispectral data has a significant effect on the semantic segmentation performance, and therefore the RGB band, which has the lowest ground sample distance (0.31 m) outperforms multispectral bands and shortwave infrared bands.",Semantic Segmentation,Remote Sensing,Spectral Indices,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_221,"Li, Jinsong","Zhang, Shujun","Sun, Yukang","Han, Qi",Frequency-Driven Edge Guidance Network for Semantic Segmentation of Remote Sensing Images,,2024,3,"Semantic segmentation plays a significant role in parsing remote sensing images. However, mainstream segmentation models lack a thorough understanding of the complex structures and scale differences, and struggle to effectively locate and emphasize diverse edges. Aiming at these limitations, we propose a frequency-driven edge guidance network, named FDEG-Net, for semantic segmentation of remote sensing images. First, we design a joint sparse context aggregation module that integrates both dense local context and sparse long-range context to improve the analysis of intricate and multiscale objects. Second, an edge guidance module is developed for strong interclass edge acquisition. It applies a 2-D discrete wavelet transform, coefficient superposition method, and adaptive edge feature enhancement algorithm to reduce low-frequency information and highlight salient boundaries in spatial features. This module has two significant advantages. 1) The edge positions are defined in pixel intensity with high interpretability. 2) The modular design without additional edge labels is plug-and-play. The effectiveness and robustness of this module are validated through edge visualization results. The proposed FDEG-Net is evaluated on the Potsdam, Vaihingen, and GID datasets, demonstrating its excellent performance in accurately capturing the rich semantics of geographic space features.",Feature extraction,Image edge detection,Semantics,Wavelet transforms,Decoding,"Sun, Yuanyuan","Wang, Yimin",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Semantic segmentation,Remote sensing,Context extraction,edge guidance,remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,
Row_222,"He, Xin","Zhou, Yong","Zhao, Jiaqi","Zhang, Man",Semantic Segmentation of Remote-Sensing Images Based on Multiscale Feature Fusion and Attention Refinement,,2022,11,"In recent years, the automatic extraction of remote-sensing image information has attracted full attention. However, the particularity of remote-sensing images and the scarcity of data sets with label information have brought new challenges to existing methods. Therefore, we develop a lightweight semantic segmentation network based onmultiscale feature fusion (MFF) and attention refinement (MFFANet). Our network relies on three crucial modules for improved performance. The multiscale attention refinement module strengthens the representation ability of feature maps extracted by the deep residual network. The MFF module aggregates the information carried by the high-level and low-level features while restoring the image resolution. Furthermore, the boundary enhancement module captures boundary details to solve the semantic ambiguity problem. We achieve 83.5% mean intersection over union (MIoU) on the Urban Semantic 3-D (US3D) data set and 69.3% MIoU on the Vaihingen data set with only 8.2M parameters.",Semantics,Image segmentation,Remote sensing,Feature extraction,Image resolution,"Yao, Rui","Liu, Bing","Li, Haichao",,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Decoding,Training,Attention,feature fusion,multiscale feature,remote-sensing images,semantic segmentation,,,,,,,,,,,,,,,
Row_223,"Brilhador, Anderson","Lazzaretti, Andre Eugenio","Lopes, Heitor Silverio",,A Prototypical Metric Learning Approach for Open-Set Semantic Segmentation on Remote Sensing Images,,2024,1,"Semantic segmentation has received wide attention as a feasible solution to effectively interpret the information in remote sensing images. Solutions are typically built with a static closed-set perception, where all labels are known a priori. However, in real-world applications, such as remote sensing images, one has to handle objects from unknown classes. Open-set semantic segmentation (OSSS) is an approach that incorporates open-set perception into semantic segmentation, allowing the recognition of unknown classes of objects. Different studies have explored the use of OSSS in remote sensing images. However, their performance is limited due to the poor and overlapped representation of the features extracted from images. This results in an embedding space with low discrimination among the classes. This article introduces a novel loss function called prototypical triplet loss, which uses prototype representation and metric learning to improve open-set recognition. In addition, two open-set classifiers, one based on principal components and the other on prototypical distance, were also proposed once they took advantage of the features obtained by the prototypical triplet loss. Experiments were done with two public remote sensing image datasets: Vaihingen and Potsdam. The results demonstrate that the proposed methods improve OSSS compared to other state-of-the-art approaches. These results reinforce the importance of this type of approach, enabling applications in real systems that require open-set recognition. All codes are freely available at https://github.com/Brilhador/tgrs2023 to foster further research in this area.",Semantic segmentation,Measurement,Remote sensing,Buildings,Vegetation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Automobiles,Training,Metric learning,open-set,prototype representation,remote sensing,semantic segmentation,,,,,,,,,,,,,,,
Row_224,"Liu, Runrui","Tao, Fei","Liu, Xintao","Na, Jiaming",RAANet: A Residual ASPP with Attention Framework for Semantic Segmentation of High-Resolution Remote Sensing Images,,JUL 2022,68,"Classification of land use and land cover from remote sensing images has been widely used in natural resources and urban information management. The variability and complex background of land use in high-resolution imagery poses greater challenges for remote sensing semantic segmentation. To obtain multi-scale semantic information and improve the classification accuracy of land-use types in remote sensing images, the deep learning models have been wildly focused on. Inspired by the idea of the atrous-spatial pyramid pooling (ASPP) framework, an improved deep learning model named RAANet (Residual ASPP with Attention Net) is constructed in this paper, which constructed a new residual ASPP by embedding the attention module and residual structure into the ASPP. There are 5 dilated attention convolution units and a residual unit in its encoder. The former is used to obtain important semantic information at more scales, and residual units are used to reduce the complexity of the network to prevent the disappearance of gradients. In practical applications, according to the characteristics of the data set, the attention unit can select different attention modules such as the convolutional block attention model (CBAM). The experimental results obtained from the land-cover domain adaptive semantic segmentation (LoveDA) and ISPRS Vaihingen datasets showed that this model can enhance the classification accuracy of semantic segmentation compared to the current deep learning models.",semantic segmentation,remote sensing,convolutional block attention module,dual attention module,residual structure,"Leng, Hongjun","Wu, Junjie","Zhou, Tong",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_225,"Li, Xin","Xu, Feng","Liu, Fan","Lyu, Xin",A Synergistical Attention Model for Semantic Segmentation of Remote Sensing Images,,2023,58,"In remotely sensed images, high intraclass variance and interclass similarity are ubiquitous due to complex scenes and objects with multivariate features, making semantic segmentation a challenging task. Deep convolutional neural networks can solve this problem by modeling the context of features and improving their discriminability. However, current learning paradigms model the feature affinity in spatial dimension and channel dimension separately and then fuse them in a sequential or parallel manner, leading to suboptimal performance. In this study, we first analyze this problem practically and summarize it as attention bias that reduces the capability of network in distinguishing weak and discretely distributed objects from wide-range objects with internal connectivity, when modeled only in spatial or channel domain. To jointly model both spatial and channel affinity, we design a synergistic attention module (SAM), which allows for channelwise affinity extraction while preserving spatial details. In addition, we propose a synergistic attention perception neural network (SAPNet) for the semantic segmentation of remote sensing images. The hierarchical-embedded synergistic attention perception module aggregates SAM-refined features and decoded features. As a result, SAPNet enriches inference clues with desired spatial and channel details. Experiments on three benchmark datasets show that SAPNet is competitive in accuracy and adaptability compared with state-of-the-art methods. The experiments also validate the hypothesis of attention bias and the efficiency of SAM.",Remote sensing,Convolution,Semantic segmentation,Feature extraction,Task analysis,"Tong, Yao","Xu, Zhennan","Zhou, Jun",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantics,Correlation,Attention bias,contextual affinity,remote sensing images,semantic segmentation,synergistic attention,,,,,,,,,,,,,,,
Row_226,"Liu, Qianqian","Wang, Xili",,,Bidirectional Feature Fusion and Enhanced Alignment Based Multimodal Semantic Segmentation for Remote Sensing Images,,JUL 2024,1,"Image-text multimodal deep semantic segmentation leverages the fusion and alignment of image and text information and provides more prior knowledge for segmentation tasks. It is worth exploring image-text multimodal semantic segmentation for remote sensing images. In this paper, we propose a bidirectional feature fusion and enhanced alignment-based multimodal semantic segmentation model (BEMSeg) for remote sensing images. Specifically, BEMSeg first extracts image and text features by image and text encoders, respectively, and then the features are provided for fusion and alignment to obtain complementary multimodal feature representation. Secondly, a bidirectional feature fusion module is proposed, which employs self-attention and cross-attention to adaptively fuse image and text features of different modalities, thus reducing the differences between multimodal features. For multimodal feature alignment, the similarity between the image pixel features and text features is computed to obtain a pixel-text score map. Thirdly, we propose a category-based pixel-level contrastive learning on the score map to reduce the differences among the same category's pixels and increase the differences among the different categories' pixels, thereby enhancing the alignment effect. Additionally, a positive and negative sample selection strategy based on different images is explored during contrastive learning. Averaging pixel values across different training images for each category to set positive and negative samples compares global pixel information while also limiting sample quantity and reducing computational costs. Finally, the fused image features and aligned pixel-text score map are concatenated and fed into the decoder to predict the segmentation results. Experimental results on the ISPRS Potsdam, Vaihingen, and LoveDA datasets demonstrate that BEMSeg is superior to comparison methods on the Potsdam and Vaihingen datasets, with improvements in mIoU ranging from 0.57% to 5.59% and 0.48% to 6.15%, and compared with Transformer-based methods, BEMSeg also performs competitively on LoveDA dataset with improvements in mIoU ranging from 0.37% to 7.14%.",remote sensing image,multimodal feature fusion,multimodal feature alignment,semantic segmentation,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_227,"Shen, Weihao","Ma, Ailong","Wang, Junjue","Zheng, Zhuo",Adaptive Self-Supporting Prototype Learning for Remote Sensing Few-Shot Semantic Segmentation,,2024,1,"The semantic segmentation of remote sensing images with few shots has important theoretical and application value. Most of the existing few-shot semantic segmentation frameworks are based on prototype learning methods, in which a single support prototype is designed to guide the query set for prediction. However, the visual differences between the support set and the query set make it difficult for a single support prototype, generated from the support set, to comprehensively encapsulate the semantic information of all the query images. This article introduces an adaptive self-supporting prototype learning network designed for few-shot segmentation (FSS), in order to tackle the challenges mentioned earlier. We propose adaptive hyperprototype representation (HPR), which consists of hyperprototype clustering (HPC) and guided prototype matching (GPM), to generate and assign multiple representative prototypes to compensate for the limitations of a single prototype in representing the semantic information of the query images. Specifically, HPC is a parameter-free and adaptive approach, which can extract more representative prototypes by aggregating similar feature vectors utilizing superpixel feature clustering. Meanwhile, GPM can select matched prototypes to provide more accurate guidance, allowing for uniformly aligned representation of multiple prototypes and complex image semantic information. We also introduce self-supporting matching (SSM) prototype learning, which can accurately guide the query set segmentation by acquiring query set prototypes. SSM generates initial pseudo labels for the query set based on the support set prototypes, and further guides the query set using the pseudo labels, along with the query prototypes generated by its own features, thus effectively avoiding visual differences between the support set and query set. The proposed adaptive self-supporting prototype learning network substantially improves the prototype quality and achieves a superior performance on object-level remote sensing datasets.",Prototypes,Remote sensing,Semantic segmentation,Feature extraction,Semantics,"Zhong, Yanfei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Task analysis,Measurement,Few-shot learning,few-shot segmentation (FSS),prototype learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,
Row_228,"Rong, Xuee","Sun, Xian","Diao, Wenhui","Wang, Peijin",Historical Information-Guided Class-Incremental Semantic Segmentation in Remote Sensing Images,,2022,25,"Despite the extraordinary success of the deep architectures on semantic segmentation for remote sensing (RS) images, they have difficulties in learning new classes from a sequential data stream because of catastrophic forgetting. Continual learning for semantic segmentation (CSS) is an emerging trend for its capability to cope with the above problems effectively. However, old classes from previous steps are collapsed into the background, which further aggravates the challenge of CSS in the RS scene. In this article, we revisit the knowledge distillation (KD) strategy and the characteristics of class-incremental semantic segmentation (CISS) and then present a generalized and effective framework to learn new classes while preserving knowledge of the learned classes. In particular, we propose two novel historical information-guided modules: the feature global perception module and the label reconstruction (LR) module. The former enables the current model to pay more attention to the region related to the old categories identified by the historical information when learning new classes. Meanwhile, the latter retrieves pixels belonging to the learned classes from the background to handle the background shift problem and maintain the high performance of old classes. We have conducted comprehensive experiments on two RS semantic segmentation datasets of Instance Segmentation in Aerial Images Dataset (iSAID) and Gao Fen (GF) challenge semantic segmentation dataset (GCSS). The experimental results outperform the current state-of-the-art methods in most incremental settings, which demonstrates the effectiveness of the proposed framework.",Task analysis,Semantics,Image segmentation,Image reconstruction,Transformers,"Yuan, Zhiqiang","Wang, Hongqi",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Data models,Catastrophic forgetting (CF),continual learning,remote sensing (RS),semantic segmentation,,,,,,,,,,,,,,,,
Row_229,"Yao, Min","Zhang, Yaozu","Liu, Guofeng","Pang, Dongdong",SSNet: A Novel Transformer and CNN Hybrid Network for Remote Sensing Semantic Segmentation,,2024,11,"There are still various challenges in remote sensing semantic segmentation due to objects diversity and complexity. Transformer-based models have significant advantages in capturing global feature dependencies for segmentation. However, it unfortunately ignores local feature details. On the other hand, convolutional neural network (CNN), with a different interaction mechanism from transformer-based models, captures more small-scale local features instead of global features. In this article, a new semantic segmentation net framework named SSNet is proposed, which incorporates an encoder-decoder structure, optimizing the advantages of both local and global features. In addition, we build feature fuse module and feature inject module to largely fuse these two-style features. The former module captures the dependencies between different positions and channels to extract multiscale features, which promotes the segmentation precision on similar objects. The latter module condenses the global information in transformer and injects it into CNN to obtain a broad global field of view, in which the depthwise strip convolution improves the segmentation accuracy on tiny objects. A CNN-based decoder progressively recovers the feature map size, and a block called atrous spatial pyramid pooling is adopted in decoder to obtain a multiscale context. The skip connection is established between the decoder and the encoder, which retains important feature information of the shallow layer network and is conducive to achieving flow of multiscale features. To evaluate our model, we compare it with current state-of-the-art models on WHDLD and Potsdam datasets. The experimental results indicate that our proposed model achieves more precise semantic segmentation.",Fusion features,multiscale features,remote sensing (RS),semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_230,"Xu, Qingsong","Yuan, Xin","Ouyang, Chaojun",,Class-Aware Domain Adaptation for Semantic Segmentation of Remote Sensing Images,,2022,40,"Unsupervised domain adaptation (UDA) for the semantic segmentation of remote sensing images is challenging since the same class of objects may have different spectra while the different class of objects may have the same spectrum. To address this issue, we propose a class-aware generative adversarial network (CaGAN) for UDA semantic segmentation of multisource remote sensing images, which explicitly models the discrepancies of intraclass and the interclass between the source domain images with labels and the target domain images without labels. Specifically, first, to enhance the global domain alignment (GDA), we propose a transferable attention alignment (TAA) procedure to add more fine-grained features into the adversarial learning framework. Then, we propose a novel class-aware domain alignment (CDA) approach in semantic segmentation. CDA mainly includes two parts: the first one is adaptive category selection, which is to alleviate the class imbalance and select the reliable per-category centers in the source and target domains; the second one is adaptive category alignment, which is to model the intraclass compactness and interclass separability from source-only, target-only, and joint source and target images. Finally, the CDA plays as a penalty of GDA to train GaGAN in an alternating and iterative manner. Experiments on domain adaptation of space to space, spectrum to spectrum, both space-to-space and spectrum-to-spectrum data sets demonstrate that CaGAN outperforms the current state-of-the-art methods, which may serve as a starting point and baseline for the comprehensive applications of semantic segmentation in cross-space and cross-spectrum remote sensing images.",Semantics,Image segmentation,Remote sensing,Adaptation models,Generative adversarial networks,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Gallium nitride,Training,Class-aware domain alignment (CDA),class-aware generative adversarial network (CaGAN),cross-scene and cross-spectrum remote sensing images,global domain alignment (GDA),unsupervised domain adaptation (UDA) semantic segmentation,,,,,,,,,,,,,,,
Row_231,"Rong, Xuee","Wang, Peijin","Diao, Wenhui","Yang, Yiran",MiCro: Modeling Cross-Image Semantic Relationship Dependencies for Class-Incremental Semantic Segmentation in Remote Sensing Images,,2023,7,"Continual learning is an effective way to overcome catastrophic forgetting (CF) in incremental learning for semantic segmentation. The existing continual semantic segmentation (CSS) methods of remote sensing (RS) ignore the semantic relationships among pixels across different images, which will lead to disappointing segmentation results, such as edge pixel misclassification and small object omission. In this article, we propose a framework for modeling cross-image semantic relationship dependencies (MiCro), which aims to learn an interclass separable and intraclass cohesive feature space from the pixel relationships across various images to ensure that learned categories can prevent CF in the incremental process. Specifically, we exploit the relationships among pixels of images in minibatch to construct three losses: 1) cross-image feature relationship distillation (CFRD) loss, which builds a well-structured feature space; 2) cross-image intraclass feature cohesion (CIFC) loss, which is devised to make intraclass features more cohesive; and 3) cross-image class-area weighted cross-entropy (CCWCE) loss, which is mainly employed to inversely weight the proportion of category area in minibatch. The effectiveness of the proposed approach is demonstrated by extensive experiments on three RS semantic segmentation datasets from ISPRS Vaihingen, ISPRS Potsdam, and iSAID. MiCro is superior to the current most advanced methods in most incremental settings, especially improving mIoU by 11.59% on ISPRS Vaihingen, 13.17% on ISPRS Potsdam, and 15.01% on iSAID in the most difficult incremental settings, which promotes the CSS to a state-of-the-art (SOTA) level. The code will be available at https://github.com/RongXueE/MiCro.",Catastrophic forgetting (CF),class-incremental learning (CIL),remote sensing (RS),semantic segmentation,,"Yin, Wenxin","Zeng, Xuan","Wang, Hongqi","Sun, Xian",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_232,"Shang, Ronghua","Zhang, Jiyu","Jiao, Licheng","Li, Yangyang",Multi-scale Adaptive Feature Fusion Network for Semantic Segmentation in Remote Sensing Images,,MAR 2020,73,"Semantic segmentation of high-resolution remote sensing images is highly challenging due to the presence of a complicated background, irregular target shapes, and similarities in the appearance of multiple target categories. Most of the existing segmentation methods that rely only on simple fusion of the extracted multi-scale features often fail to provide satisfactory results when there is a large difference in the target sizes. Handling this problem through multi-scale context extraction and efficient fusion of multi-scale features, in this paper we present an end-to-end multi-scale adaptive feature fusion network (MANet) for semantic segmentation in remote sensing images. It is a coding and decoding structure that includes a multi-scale context extraction module (MCM) and an adaptive fusion module (AFM). The MCM employs two layers of atrous convolutions with different dilatation rates and global average pooling to extract context information at multiple scales in parallel. MANet embeds the channel attention mechanism to fuse semantic features. The high- and low-level semantic information are concatenated to generate global features via global average pooling. These global features are used as channel weights to acquire adaptive weight information of each channel by the fully connected layer. To accomplish an efficient fusion, these tuned weights are applied to the fused features. Performance of the proposed method has been evaluated by comparing it with six other state-of-the-art networks: fully convolutional networks (FCN), U-net, UZ1, Light-weight RefineNet, DeepLabv3+, and APPD. Experiments performed using the publicly available Potsdam and Vaihingen datasets show that the proposed MANet significantly outperforms the other existing networks, with overall accuracy reaching 89.4% and 88.2%, respectively and with average of F1 reaching 90.4% and 86.7% respectively.",multi-scale context,adaptive fusion,remote sensing image,semantic segmentation,CNN,"Marturi, Naresh","Stolkin, Rustam",,,REMOTE SENSING,,deep learning,,,,,,,,,,,,,,,,,,,,,
Row_233,"Li, Bo","Lv, Pengyuan","Zhong, Yanfei","Zhang, Liangpei",HIGH RESOLUTION REMOTE SENSING IMAGE SEMANTIC SEGMENTATION BASED ON ULTRA-LIGHTWEIGHT FULLY CONVOLUTION NEURAL NETWORK,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,2,"In recent years, fully convolutional neural networks (FCNs) have been widely used in the field of remote sensing image semantic segmentation. However, these networks have huge amount of parameters and cost much computational efficiency. In this paper, an ultra-lightweight network (ULN) is proposed to overcome this problem. The proposed ULN model uses the encoder-decoder architecture to acquire the pixelwise result. In ULN, the efficient spatial pyramid network (ESPNet) is used to extract deep semantic features with fewer parameters. Considering the dilated convolutions will lose some semantic information in the encoding process, the feature enhancement block (FEB) is proposed. The recurrent criss-cross attention module is added at the end of skip connection to acquire the global contextual information. The proposed ULN is tested on the ISPRS Vaihingen dataset, the results show that our network achieves competitive results with fewer parameters(1.5M).",Remote sensing image,high spatial resolution semantic segmentation,ultra-lightweight network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_234,"An, Ke","Wang, Yupei","Chen, Liang",,Encouraging the Mutual Interact Between Dataset-Level and Image-Level Context for Semantic Segmentation of Remote Sensing Image,,2024,0,"Recently, semantic segmentation of remote sensing images has witnessed rapid advancement with the adoption of deep neural networks. Contextual cues, referring to the long-range correlation between pixels, are crucial for achieving accurate segmentation results, particularly for objects with less discriminative characteristics in these images. Currently, most studies are centered on incorporating contextual cues by aggregating context information at the dataset level or image level. However, current research often treats contextual cue modeling at the dataset-level and image level as independent procedures, neglecting the intrinsic correlation between these two feature levels. Consequently, the obtained contextual cues are suboptimal. This issue is particularly critical in the semantic segmentation of remote sensing images. To address this, we propose to encourage mutual interaction between dataset-level and image-level contextual cues. Firstly, we propose an interactive dataset-image context aggregation scheme to obtain complementary and consistent multilevel contextual cues. Additionally, we introduce a parallel feature interaction network (PFI-Net) that progressively extracts and fuses features across multiple layers, enabling effective integration of multilevel contexts. Furthermore, we introduce an enhanced shifted window-based cross-attention mechanism to augment model efficiency. Extensive experimental results on the widely used Vaihingen dataset, GaoFen-2 dataset, and instance segmentation in aerial images dataset (iSAID) effectively demonstrate the superiority of our proposed method over the other state-of-the-art methods.",Contextual cue,remote sensing image,semantic segmentation,,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_235,"Xia, Liegang","Zhang, Junxia","Zhang, Xiongbo","Yang, Haiping",Precise Extraction of Buildings from High-Resolution Remote-Sensing Images Based on Semantic Edges and Segmentation,,AUG 2021,14,"Building extraction is a basic task in the field of remote sensing, and it has also been a popular research topic in the past decade. However, the shape of the semantic polygon generated by semantic segmentation is irregular and does not match the actual building boundary. The boundary of buildings generated by semantic edge detection has difficulty ensuring continuity and integrity. Due to the aforementioned problems, we cannot directly apply the results in many drawing tasks and engineering applications. In this paper, we propose a novel convolutional neural network (CNN) model based on multitask learning, Dense D-LinkNet (DDLNet), which adopts full-scale skip connections and edge guidance module to ensure the effective combination of low-level information and high-level information. DDLNet has good adaptability to both semantic segmentation tasks and edge detection tasks. Moreover, we propose a universal postprocessing method that integrates semantic edges and semantic polygons. It can solve the aforementioned problems and more accurately locate buildings, especially building boundaries. The experimental results show that DDLNet achieves great improvements compared with other edge detection and semantic segmentation networks. Our postprocessing method is effective and universal.",building extraction,high-resolution remote-sensing image,semantic edge detection,semantic segmentation,,"Xu, Meixia",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_236,"Fang, Leyuan","Zhou, Peng","Liu, Xinxin","Ghamisi, Pedram",Context Enhancing Representation for Semantic Segmentation in Remote Sensing Images,,MAR 2024,11,"As the foundation of image interpretation, semantic segmentation is an active topic in the field of remote sensing. Facing the complex combination of multiscale objects existing in remote sensing images (RSIs), the exploration and modeling of contextual information have become the key to accurately identifying the objects at different scales. Although several methods have been proposed in the past decade, insufficient context modeling of global or local information, which easily results in the fragmentation of large-scale objects, the ignorance of small-scale objects, and blurred boundaries. To address the above issues, we propose a contextual representation enhancement network (CRENet) to strengthen the global context (GC) and local context (LC) modeling in high-level features. The core components of the CRENet are the local feature alignment enhancement module (LFAEM) and the superpixel affinity loss (SAL). The LFAEM aligns and enhances the LC in low-level features by constructing contextual contrast through multilayer cascaded deformable convolution and is then supplemented with high-level features to refine the segmentation map. The SAL assists the network to accurately capture the GC by supervising semantic information and relationship learned from superpixels. The proposed method is plug-and-play and can be embedded in any FCN-based network. Experiments on two popular RSI datasets demonstrate the effectiveness of our proposed network with competitive performance in qualitative and quantitative aspects.",Semantics,Context modeling,Image segmentation,Convolution,Decoding,"Chen, Siwei",,,,IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS,,Remote sensing,Predictive models,Context modeling,deep learning,feature alignment and enhancement,remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,
Row_237,"Wang, Zhen","Guo, Jianxin","Huang, Wenzhun","Zhang, Shanwen",High-resolution remote sensing image semantic segmentation based on a deep feature aggregation network,,SEP 2021,13,"Semantic segmentation of high-resolution remote sensing images has a wide range of applications, such as territorial planning, geographic monitoring and smart cities. The proper operation of semantic segmentation for remote sensing images remains challenging due to the complex and diverse transitions between different ground areas. Although several convolution neural networks (CNNs) have been developed for remote sensing semantic segmentation, the performance of CNNs is far from the expected target. This study presents a deep feature aggregation network (DFANet) for remote sensing image semantic segmentation. It is composed of a basic feature representation layer, an intermediate feature aggregation layer, a deep feature aggregation layer and a feature aggregation module (FAM). Specially, the basic feature representation layer is used to obtain feature maps with different resolutions: the intermediate feature aggregation layer and deep feature aggregation layer can fuse various resolution features and multi-scale features; the FAM is used to splice the features and form more abundant spatial feature maps; and the conditional random field module is used to optimize semantic segmentation results. We have performed extensive experiments on the ISPRS two-dimensional Vaihingen and Potsdam remote sensing image datasets and compared the proposed method with several variations of semantic segmentation networks. The experimental results show that DFANet outperforms the other state-of-the-art approaches.",high resolution remote sensing image,semantic segmentation,convolution neural networks (CNNs),deep feature aggregation network (DFANet),conditional random field (CRF),,,,,MEASUREMENT SCIENCE AND TECHNOLOGY,,,,,,,,,,,,,,,,,,,,,,,
Row_238,"Cui, Hao","Zhang, Guo","Qi, Ji","Li, Haifeng","MDANet: Unsupervised, Mixed-Domain Adaptation for Semantic Segmentation of Remote Sensing Images",,2022,3,"The imaging process of optical remote sensing images (RSIs) is easily affected by external conditions. Therefore, RSIs under different imaging conditions often show color differences, resulting in feature distribution differences between the source and target domains, hindering the migration of semantic segmentation models between domains. Currently, most domain adaptation (DA) methods are for single-source and single-target domains. Here, we proposed a novel and concise method, coined mixed-DA network (MDANet), for the adaptation of patch images of multisource and multitarget domains and for reducing the distribution differences of different patch images by projecting them onto the virtual center of a mixed domain. MDANet is a lightweight and self-supervised network that can be grafted with any semantic segmentation model. Our method significantly improved the segmentation accuracy of semantic segmentation models and showed higher stability and competitiveness than the existing methods.",Image reconstruction,Remote sensing,Semantics,Adaptation models,Decoding,"Tao, Chao","Li, Xue","Hou, Shasha","Li, Deren",IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Training,Image edge detection,Domain adaptation (DA),mixed domain,remote sensing image (RSI),self-supervision,semantic segmentation,,,,,,,,,,,,,,,
Row_239,"Pan, Shaoming","Tao, Yulong","Chen, Xiaoshu","Chong, Yanwen",Progressive Guidance Edge Perception Network for Semantic Segmentation of Remote-Sensing Images,,2022,8,"Remarkable improvements have been seen in the semantic segmentation of remote-sensing images. As an effective structure to aggregate shallow information and deep information, encoder-decoder structure has been widely used in many state-of-the-art models, but it possesses two drawbacks that have not been fully addressed. On the one hand, encoder-decoder structure fuses the features obtained from shallow and deep layers directly; despite harvesting some detailed information, it also brings in noisy features owing to the poor discriminant ability of the shallow layers. On the other hand, existing encoder-decoder structure merely fuses the high-level information generated by the last layer of encoder once, which neglects its guidance ability to the feature aggregation process in the decoder. In this letter, we first propose an edge perception module (EPM) to eliminate the noisy features in the shallow information, as well as enhance features' structural information. And then, we generate the most suitable guidance information adaptively for different stages in the decoder through high-level information module (HIM). Finally, we apply the guidance information to achieve feature aggregation in the feature aggregation module (FAM). Combined with EPM, HIM, and FAM, our proposed model achieves 89.5% overall accuracy (OA) on the challenging ISPRS Vaihingen test set, which is the new state-of-the-art in the semantic segmentation of remote-sensing images.",Semantics,Convolution,Noise measurement,Remote sensing,Image segmentation,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Decoding,Encoder-decoder structure,high-level information,noisy features,remote-sensing images,semantic segmentation,,,,,,,,,,,,,,,
Row_240,"Igonin, Dmitry M.","Tiumentsev, Yury V.",,,Semantic Segmentation of Images Obtained by Remote Sensing of the Earth,"ADVANCES IN NEURAL COMPUTATION, MACHINE LEARNING, AND COGNITIVE RESEARCH III",2020,1,"In the last decade, computer vision algorithms, including those related to the problem of understanding images, have developed a lot. One of the tasks within the framework of this problem is semantic segmentation of images, which provides the classification of objects available in the image at the pixel level. This kind of segmentation is essential as a source of information for robotic UAV behavior control systems. One of the types of pictures that are used in this case is the images obtained by remote sensing of the earth's surface. A significant number of various neuroarchitecture based on convolutional neural networks were proposed for solving problems of semantic segmentation of images. However, for some reasons, not all of them are suitable for working with pictures of the earth's surface obtained using remote sensing. Neuroarchitectures that are potentially suitable for solving the problem of semantic segmentation of images of the earth's surface are identified, a comparative analysis of their effectiveness as applied to this task is carried out.",Earth remote sensing,Aerial and satellite imaging,2D image,Semantic segmentation,Convolutional neural networks,,,,,,,Comparative analysis,,,,,,,,,,,,,,,,,,,,,
Row_241,"Zhang, Xiaoyan","Li, Linhui","Di, Donglin","Wang, Jian",SERNet: Squeeze and Excitation Residual Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,OCT 2022,22,"The semantic segmentation of high-resolution remote sensing images (HRRSIs) is a basic task for remote sensing image processing and has a wide range of applications. However, the abundant texture information and wide imaging range of HRRSIs lead to the complex distribution of ground objects and unclear boundaries, which bring huge challenges to the segmentation of HRRSIs. To solve this problem, in this paper we propose an improved squeeze and excitation residual network (SERNet), which integrates several squeeze and excitation residual modules (SERMs) and a refine attention module (RAM). The SERM can recalibrate feature responses adaptively by modeling the long-range dependencies in the channel and spatial dimensions, which enables effective information to be transmitted between the shallow and deep layers. The RAM pays attention to global features that are beneficial to segmentation results. Furthermore, the ISPRS datasets were processed to focus on the segmentation of vegetation categories and introduce Digital Surface Model (DSM) images to learn and integrate features to improve the segmentation accuracy of surface vegetation, which has certain prospects in the field of forestry applications. We conduct a set of comparative experiments on ISPRS Vaihingen and Potsdam datasets. The results verify the superior performance of the proposed SERNet.",remote sensing,forestry technology,smart forestry,residual module,semantic segmentation,"Chen, Guangsheng","Jing, Weipeng","Emam, Mahmoud",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_242,"Chong, Qianpeng","Xu, Jindong","Ding, Yang","Dai, Zhe",A multiscale bidirectional fuzzy-driven learning network for remote sensing image segmentation,,NOV 2 2023,4,"Semantic segmentation is a fundamental but meaningful task in the remote sensing image understanding community. Great progress has been made in optical sensor photography technology, which poses an opportunity and a challenge for remote sensing image segmentation task. But, in fact, a longstanding and intractable problem is that many hard pixels in special position, i.e. the prevalent intra-class noise and a poor boundary delineation, is difficult to classify due to their inherent uncertainty. In this paper, we comprehensively consider the characteristics of deep learning and introduce traditional pattern recognition methods to drive structure learning, which can leverage the corresponding fuzzy logic model to alleviate the aforementioned problem in remote sensing images. Specifically, this paper designs a multiscale bidirectional fuzzy-driven learning network (MBFNet), which takes advantage of both deep learning and fuzzy logic to effectively alleviate the inherent uncertainty of these hard pixels. The structure of convolutional neural networks driven by fuzzy systems also provides a new modelling paradigm for solving the uncertain problem in remote sensing images. Meanwhile, multiscale techniques and bidirectional fusion are introduced to enhance feature aggregation and avoid the potential adverse effects of fuzzy systems, respectively. Experimental results on two datasets demonstrate qualitatively and quantitatively that the proposed MBFNet is competitive.",Remote sensing,semantic segmentation,fuzzy learning,bidirectional fusion,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_243,"Bai, Haiwei","Cheng, Jian","Huang, Xia","Liu, Siyu",HCANet: A Hierarchical Context Aggregation Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,2022,15,"Many practical applications of high-resolution remote sensing images (HRRSIs) are based on semantic segmentation. However, due to the complex ground object information contained in remote sensing images, it is difficult to make precise semantic segmentation of HRRSIs. In this letter, we proposed a hierarchical context aggregation network (HCANet) for the semantic segmentation of HRRSIs. The HCANet has an encoder-decoder structure which is similar to UNet. In the HCANet, we designed two Compact Atrous Spatial Pyramid Pooling (CASPP and CASPP+) modules. The CASPP modules replace the copy and crop operation in UNet to extract the multiscale context information of the multisemantic features of ResNet. The CASPP+ module is embedded in the middle layer of HCANet's decoder to provide a strong aggregation path of contextual information. In the decoder of HCANet, the multiscale context information obtained by CASPP modules is hierarchically merged layer by layer for the semantic segmentation of HRRSIs. We compared our method with several of the most advanced methods on the ISPRS Vaihingen and Potsdam data sets. The final results demonstrate that our method can achieve outstanding performance.",Semantics,Feature extraction,Image segmentation,Remote sensing,Convolution,"Deng, Changjian",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Data mining,Decoding,Hierarchical context aggregation,high-resolution remote sensing images (HRRSIs),multiscale context information,semantic segmentation,,,,,,,,,,,,,,,,
Row_244,"Wang, Feiting","Zhang, Yuan","Hu, Qiongqiong","Zhu, Yu",Remote sensing image semantic segmentation network based on multi-scale feature enhancement fusion,,JAN 1 2024,0,"Semantic segmentation is a crucial method for recognizing and classifying objects in high-resolution remote sensing images (HRRSIs). However, due to the problems of varying target scale and difficulty in determining the edges of small-scale targets in remote sensing images, traditional semantic segmentation models perform poorly. To address this issue, we propose a multi-scale feature enhancement network (MFENet) to improve the segmentation accuracy of small-scale objects in HRRSIs. MFENet considers the differences between objects of different scales and selects more suitable receptive fields to enhance the extraction of multi-scale semantic features. We propose a composite atrous multi-scale feature fusion (CAMFF) module to enhance the extraction of spatial detail and semantic information of features at different scales. In addition, we propose an improved composite atrous spatial pyramid pooling (C-ASPP) module to enhance the network feature extraction capability across multiple scales. We also propose a network structure that combines the C-ASPP module with the efficient channel attention (ECA) module in parallel, which performs better to extract contextual information. Our experimental evaluations on the Potsdam and Vaihingen datasets demonstrate the effectiveness of our Network, It F1 score reaching 93.33% and 94.66% respectively.",Semantic segmentation,remote sensing images,multi-scale objects,spatial detail information,,,,,,GEOCARTO INTERNATIONAL,,,,,,,,,,,,,,,,,,,,,,,
Row_245,"Li, Jinglun","Xiu, Jiapeng","Yang, Zhengqiu","Liu, Chen",Dual Path Attention Net for Remote Sensing Semantic Image Segmentation,,OCT 2020,20,"Semantic segmentation plays an important role in being able to understand the content of remote sensing images. In recent years, deep learning methods based on Fully Convolutional Networks (FCNs) have proved to be effective for the sematic segmentation of remote sensing images. However, the rich information and complex content makes the training of networks for segmentation challenging, and the datasets are necessarily constrained. In this paper, we propose a Convolutional Neural Network (CNN) model called Dual Path Attention Network (DPA-Net) that has a simple modular structure and can be added to any segmentation model to enhance its ability to learn features. Two types of attention module are appended to the segmentation model, one focusing on spatial information the other focusing upon the channel. Then, the outputs of these two attention modules are fused to further improve the network's ability to extract features, thus contributing to more precise segmentation results. Finally, data pre-processing and augmentation strategies are used to compensate for the small number of datasets and uneven distribution. The proposed network was tested on the Gaofen Image Dataset (GID). The results show that the network outperformed U-Net, PSP-Net, and DeepLab V3+ in terms of the mean IoU by 0.84%, 2.54%, and 1.32%, respectively.",remote sensing image,semantic segmentation,fully convolutional network,convolutional neural network,self-attention mechanism,,,,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,,,,,,,,,,,,,,,,,,,,,,
Row_246,"Chang, Wanjun","Zhang, Dongfang",,,A Novel Semantic Segmentation Approach Using Improved SegNet and DSC in Remote Sensing Images,,2023,0,"An improved SegNet semantic segmentation model is proposed to address the issue of traditional classification algorithms and shallow learning algorithms not being suitable for extracting information from high-resolution remote sensing images. During the research process, space remote sensing images obtained from the GF-1 satellite were used as the data source. In order to improve the operational efficiency of the encoding network, the pooling layer in the encoding network is removed and the ordinary convolutional layer is replaced with a depth-wise separable convolution. By decoding the last layer of the network to obtain the reshaped output results, and then calculating the probability of each classification using a Softmax classifier, the classification of pixels can be achieved. The output result of the classifier is the final result of the remote sensing image semantic segmentation model. The results showed that the proposed algorithm had the highest Kappa coefficient of 0.9531, indicating good classification performance.",Deep Learning,Depth-Wise Separable Convolution,Remote Sensing Image,SegNet,Semantic Segmentation,,,,,INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,
Row_247,"Gao, Liang","Liu, Hui","Yang, Minhang","Chen, Long",STransFuse: Fusing Swin Transformer and Convolutional Neural Network for Remote Sensing Image Semantic Segmentation,,2021,133,"The applied research in remote sensing images has been pushed by convolutional neural network (CNN). Because of the fixed size of the perceptual field, CNN is unable to model global semantic relevance. Modeling global semantic information is possible with the self-attentive Transformer-based model. However, the method of patch computation used by Transformer for self-attentive computation ignores the spatial information inside each patch. To address these issues, we offer the STransFuse model as a new semantic segmentation method for remote sensing images. It is a model that combines the benefits of Transformer with CNN to improve the segmentation quality of various remote sensing images. We employ a staged model to extract coarse-grained and fine-grained feature representations at various semantic scales, unlike earlier techniques based on Transformer model fusion. In order to take full advantage of the features acquired at different stages, we designed an adaptive fusion module. This module adaptively fuses the semantic information between features at different scales employing a self-attentive mechanism. The overall accuracy (OA) of our proposed model on the Vaihingen dataset is 1.36% higher than the baseline, and 1.27% improvement in OA over baseline on the Potsdam dataset. When compared to other advanced models, the STransFuse model performs admirably.",Remote sensing,Transformers,Semantics,Image segmentation,Computational modeling,"Wan, Yaling","Xiao, Zhengqing","Qian, Yurong",,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Feature extraction,Context modeling,Remote sensing,self-attention,semantic segmentation,Transformer,,,,,,,,,,,,,,,,
Row_248,"Liu, Hongrong","Liu, Minghua","Song, Shuhua","Guo, Guolong",ECGNet: edge and class guided semantic segmentation network for remote sensing urban scene images,,JUL 1 2024,0,"Semantic segmentation of remote sensing images in urban scenes suffers from blurred multi-scale target boundaries, insufficient use of global context, and classification errors caused by high inter-class variance and low intra-class variance. Therefore, we propose a semantic segmentation network with edge and class guidance (ECGNet). First, ECGNet introduces multi-scale edge prior knowledge to address the problem of blurred target boundaries. Second, ECGNet applies synergistic class augmented attention to introduce class prior knowledge while retaining rich spatial dimensional localization information to alleviate the problem of classification errors caused by low intra-class variance and high inter-class variance. Finally, the multi-scale large receptive field attention in ECGNet simulates a large convolutional kernel to capture multi-scale global context information. Experiments conducted on the ISPRS Vaihingen and ISPRS Potsdam datasets show that the proposed method is competitive.",remote sensing images of urban scenes,computer vision,semantic segmentation,priori knowledge,large convolutional kernel,"Yuan, Zhengyi","Chen, Kai","Yang, Shuai","Yu, Jiangfeng",JOURNAL OF APPLIED REMOTE SENSING,"Zhang, Hongwei",,,,,,,,,,,,,,,,,,,,,,
Row_249,"Jiang, Jionghui","Feng, Xi'an","Huang, Hui",,Semantic segmentation of remote sensing images based on dual-channel attention mechanism,,JUL 2024,3,"Due to the inadequate utilization of data correlation and complementarity in the feature extraction process of multimodal remote sensing images, the paper proposes a deep learning semantic segmentation algorithm based on the Dual Channel Attention Mechanism (DCAM). This algorithm uses U-Net as the backbone, combining the RGB remote sensing image as one input channel with the Convolutional Block Attention Module to extract colour space features. Simultaneously, it utilizes near-infrared (NIR) as another input channel with the Self-Attention Module (SAM) to extract shape space features. Finally, by concatenating the multi-scale attention features of the RGB remote sensing image channel and the NIR remote sensing image channel, it achieves the correlation and complementarity of contextual features between the two modal remote sensing images. Experimental results on the GID-15 dataset demonstrate that the DCAM algorithm significantly improves the segmentation accuracy, edge segmentation quality, and object segmentation integrity for various types of targets compared to current mainstream segmentation methods.The proposed algorithm takes RGB remote sensing images as input to a U-Net downsampling network combined with the Convolutional Block Attention Module to extract colour space features. Simultaneously, it takes near-infrared remote sensing images as input to another U-Net downsampling network combined with the Self-Attention Module to extract shape space features. image",convolutional neural nets,image segmentation,remote sensing,,,,,,,IET IMAGE PROCESSING,,,,,,,,,,,,,,,,,,,,,,,
Row_250,"Ding, Rong-Xing","Xu, Yi-Han","Liu, Jie","Zhou, Wen",LSENet: Local and Spatial Enhancement to Improve the Semantic Segmentation of Remote Sensing Images,,2024,0,"The semantic segmentation of remote sensing images is extensively used in crop cover and type analysis and environmental monitoring. In the semantic segmentation of remote sensing images, owning to the specificity of remote sensing images, not only the local context is required, but also the global context information makes an important role in it. Inspired by the powerful global modeling capability of Swin Transformer, we propose the Local and Spatial Enhancement Net (LSENet) network, which follows the encoder-decoder architecture of the UNet network. In the encoding phase, we propose spatial enhancement module (SEM), which helps Swin Transformer further enhance feature extraction by encoding spatial information. In the decoding stage, we propose local enhancement module (LEM), which is embedded in the Swin Transformer to improve the Swin Transformer to assist the network to obtain more local semantic information so as to classify pixels more accurately, especially in the edge region, the adding of LEM enables to obtain smoother edges. The experimental results on the Vaihingen and Potsdam datasets demonstrate the effectiveness of our proposed method. Specifically, the mIoU metric is 78.58% on the Potsdam dataset and 72.59% on the Vaihingen dataset.",Remote sensing,Transformers,Feature extraction,Convolution,Semantics,"Chen, Chen",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Forestry,Training,Deep learning,remote sensing,semantic segmentation,Swin Transformer,,,,,,,,,,,,,,,,
Row_251,"Xu, Rongtao","Wang, Changwei","Zhang, Jiguang","Xu, Shibiao",RSSFormer: Foreground Saliency Enhancement for Remote Sensing Land-Cover Segmentation,,2023,62,"High spatial resolution (HSR) remote sensing images contain complex foreground-background relationships, which makes the remote sensing land cover segmentation a special semantic segmentation task. The main challenges come from the large-scale variation, complex background samples and imbalanced foreground-background distribution. These issues make recent context modeling methods sub-optimal due to the lack of foreground saliency modeling. To handle these problems, we propose a Remote Sensing Segmentation framework (RSSFormer), including Adaptive TransFormer Fusion Module, Detail-aware Attention Layer and Foreground Saliency Guided Loss. Specifically, from the perspective of relation-based foreground saliency modeling, our Adaptive Transformer Fusion Module can adaptively suppress background noise and enhance object saliency when fusing multi-scale features. Then our Detail-aware Attention Layer extracts the detail and foreground-related information via the interplay of spatial attention and channel attention, which further enhances the foreground saliency. From the perspective of optimization-based foreground saliency modeling, our Foreground Saliency Guided Loss can guide the network to focus on hard samples with low foreground saliency responses to achieve balanced optimization. Experimental results on LoveDA datasets, Vaihingen datasets, Potsdam datasets and iSAID datasets validate that our method outperforms existing general semantic segmentation methods and remote sensing segmentation methods, and achieves a good compromise between computational overhead and accuracy.",Remote sensing,Transformers,Semantic segmentation,Task analysis,Buildings,"Meng, Weiliang","Zhang, Xiaopeng",,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,Background noise,Convolution,Remote sensing segmentation,foreground saliency enhancement,transformer,,,,,,,,,,,,,,,,,
Row_252,"Li, Yansheng","Shi, Te","Zhang, Yongjun","Ma, Jiayi",SPGAN-DA: Semantic-Preserved Generative Adversarial Network for Domain Adaptive Remote Sensing Image Semantic Segmentation,,2023,28,"Unsupervised domain adaptation for remote sensing semantic segmentation seeks to adapt a model trained on the labeled source domain to the unlabeled target domain. One of the most promising ways is to translate images from the source domain to the target domain to align the spectral information or imaging mode by the generative adversarial network (GAN). However, source-to-target translation often brings bias in the translated images causing limited performance, as semantic information is not well considered in the translation procedure. To overcome this limitation, we present an innovative semantic-preserved generative adversarial network (SPGAN), designed to mitigate the image translation bias and then leverage the translated images as well as unlabeled target images by class distribution alignment (CDA) module to train a domain adaptive semantic segmentation model. The above two stages are coupled together to form a unified framework called SPGAN-DA. Specifically, we first conduct semantic invariant translation from source to target domain, which is achieved by introducing representation-invariant and semantic-preserved constraints to the GAN model. To further narrow the landscape layout gap between the translated and target images, CDA semantic segmentation is proposed. CDA semantic segmentation consists of two aspects. At the model input level, object discrepancy is eliminated by introducing the ClassMix operation. At the model output level, boundary enhancement is proposed to refine the performance of object boundaries. Extensive experiments on three typical remote sensing cross-domain semantic segmentation benchmarks demonstrate the effectiveness and generality of our proposed method, which competes favorably against existing state-of-the-art methods.",Index Terms-Class distribution alignment (CDA),domain adaptive semantic segmentation,generative adversarial network (GAN),semantic-preserved generative adversarial network (SPGAN),unbiased image translation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_253,"Wang, Yupei","Shi, Hao","Dong, Shan","Zhuang, Yin",Dual-Path Sparse Hierarchical Network for Semantic Segmentation of Remote Sensing Images,,2022,4,"Semantic segmentation of remote sensing images aims to label every pixel with the correct semantic category. The core challenge of the current deep convolutional network (ConvNet)-based methods lies in the difficulty of effectively aggregating high-level categorical semantics and low-level local details along the hierarchy of backbone. Most current approaches consider only fusing adjacent feature layers gradually with short-range feature connections, which lack the diversity of feature interactions, such as long-range cross-scale connections. To this end, we propose a novel dual-path sparse hierarchical network that is characterized by rich cross-scale feature interactions. Multiscale features are first sparsely grouped with a predefined interval, which is then aggregated via both long-range and short-range cross-scale connections in a hierarchical manner. Moreover, in order to further enrich the diversity of feature interactions, we also introduce another fusion path in parallel but with different sparsity for feature grouping, forming a dual-path network. In this way, our model is able to effectively aggregate multilevel features by incorporating both long-range and short-range feature interactions in both parallel and hierarchical manner. Meanwhile, the semantic and resolution gap between multilevel features can also be bridged.",Semantics,Image segmentation,Remote sensing,Spatial resolution,Location awareness,"Chen, Liang",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Aggregates,Deep learning,remote sensing image understanding,semantic segmentation,,,,,,,,,,,,,,,,,
Row_254,"Chong, Qianpeng","Xu, Jindong","Jia, Fei","Liu, Zhaowei",A multiscale fuzzy dual-domain attention network for urban remote sensing image segmentation,,JUL 18 2022,10,"Semantic segmentation of high-resolution remote sensing images plays an important role in the remote sensing community. However, many indistinguishable objects are prevalent within urban remote sensing images, and some objects belonging to the same class are different and many objects that do not belong to the same class are similar. These tricky objects make the images exhibit low-interclass variance and high-intraclass variance, which significantly limits segmentation performance. Therefore, a fresh insight was presented to alleviate this issue by incorporating the fuzzy pattern recognition method and deep-learning method. Specifically, we proposed a multiscale fuzzy dual-domain attention network (MFDAN). In MFDAN, a two-dimensional Gaussian fuzzy learning module is proposed to eliminate those factors that influence the intraclass and interclass variance. In addition, a dual-domain attention module is proposed to derive more informative semantic representations in the channel and spatial domains, respectively. These two modules will be integrated in a multiscale perspective. Extensive experiments on the benchmark datasets illustrate qualitatively and quantitatively that the proposed MFDAN is competitive.",Attention mechanism,fuzzy learning,remote-sensing imagery,semantic segmentation,,"Yan, Weiqing","Wang, Xuan","Song, Yongchao",,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_255,"de Carvalho, Osmar Luiz Ferreira","de Carvalho Junior, Osmar Abilio","Silva, Cristiano Rosa e","de Albuquerque, Anesmar Olino",Panoptic Segmentation Meets Remote Sensing,,FEB 2022,24,"Panoptic segmentation combines instance and semantic predictions, allowing the detection of countable objects and different backgrounds simultaneously. Effectively approaching panoptic segmentation in remotely sensed data is very promising since it provides a complete classification, especially in areas with many elements as the urban setting. However, some difficulties have prevented the growth of this task: (a) it is very laborious to label large images with many classes, (b) there is no software for generating DL samples in the panoptic segmentation format, (c) remote sensing images are often very large requiring methods for selecting and generating samples, and (d) most available software is not friendly to remote sensing data formats (e.g., TIFF). Thus, this study aims to increase the operability of panoptic segmentation in remote sensing by providing: (1) a pipeline for generating panoptic segmentation datasets, (2) software to create deep learning samples in the Common Objects in Context (COCO) annotation format automatically, (3) a novel dataset, (4) leverage the Detectron2 software for compatibility with remote sensing data, and (5) evaluate this task on the urban setting. The proposed pipeline considers three inputs (original image, semantic image, and panoptic image), and our software uses these inputs alongside point shapefiles to automatically generate samples in the COCO annotation format. We generated 3400 samples with 512 x 512 pixel dimensions and evaluated the dataset using Panoptic-FPN. Besides, the metric analysis considered semantic, instance, and panoptic metrics, obtaining 93.865 mean intersection over union (mIoU), 47.691 Average (AP) Precision, and 64.979 Panoptic Quality (PQ). Our study presents the first effective pipeline for generating panoptic segmentation data for remote sensing targets.",deep learning,aerial image,dataset,semantic segmentation,instance segmentation,"Santana, Nickolas Castro","Borges, Dibio Leandro","Gomes, Roberto Arnaldo Trancoso","Guimaraes, Renato Fontes",REMOTE SENSING,,panoptic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_256,"Zhang, Yijie","Cheng, Jian","Su, Yanzhou","Wu, Yuheng",ORBNet: Original Reinforcement Bilateral Network for High-Resolution Remote Sensing Image Semantic Segmentation,,2024,0,"Semantic segmentation of high-resolution remote sensing images (HRRSIs) is a basic research in the field of remote sensing image processing. Many current CNN-based methods complete detailed segmentation by building an encoder-decoder network. However, the representative selection features of ground objects are often ignored and the semantic gap between high-level features and low-level features, resulting in redundant information and erroneous annotation results. In this article, we propose an original reinforcement bilateral network (ORBNet) to improve the performance of HRRSIs semantic segmentation. The ORBNet consists of two branches-the detail branch and the semantic branch, which are responsible for extracting low-level features and high-level features, respectively. The feature alignment and fusion (FAF) modules are used to align features at different levels between two branches and produce shallow features and deep features. Furthermore, we use the detail loss in the detail branch to supervise the generation of low-level features. The class-specific discriminative loss is used to help the semantic branch distinguish features of different ground objects. The spatial-channel attention (SCA) modules are used in the feature fusion stage to select representative features. We conducted extensive experiments on two open-source ISPRS remote sensing datasets, and the experimental results verified the superior performance of our ORBNet.",Semantic segmentation,Feature extraction,Semantics,Remote sensing,Transformers,"Ma, Qijun",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Task analysis,Decoding,Deep learning,feature fusion,semantic segmentation,spatial-channel attention,,,,,,,,,,,,,,,,
Row_257,"Yu, Bo","Yang, Lu","Chen, Fang",,Semantic Segmentation for High Spatial Resolution Remote Sensing Images Based on Convolution Neural Network and Pyramid Pooling Module,,SEP 2018,141,"Semantic segmentation provides a practical way to segment remotely sensed images into multiple ground objects simultaneously, which can be potentially applied to multiple remote sensed related aspects. Current classification algorithms in remotely sensed images are mostly limited by different imaging conditions, the multiple ground objects are difficult to be separated from each other due to high intraclass spectral variances and interclass spectral similarities. In this study, we propose an end-to-end framework to semantically segment high-resolution aerial images without postprocessing to refine the segmentation results. The framework provides a pixel-wise segmentation result, comprising convolutional neural network structure and pyramid pooling module, which aims to extract feature maps at multiple scales. The proposed model is applied to the ISPRS Vaihingen benchmark dataset from the ISPRS 2D Semantic Labeling Challenge. Its segmentation results are compared with previous state-of-the-art method UZ _1, UPB and three other methods that segment images into objects of all the classes (including clutter/background) based on true orthophoto tiles, and achieve the highest overall accuracy of 87.8% over the published performances, to the best of our knowledge. The results validate the efficiency of the proposed model in segmenting multiple ground objects from remotely sensed images simultaneously.",Remotely sensed images,semantic segmentation,,,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_258,"Li, Jiahao","Sun, Bin","Li, Shutao","Kang, Xudong",Semisupervised Semantic Segmentation of Remote Sensing Images With Consistency Self-Training,,2022,30,"Semisupervised semantic segmentation is an effective way to reduce the expensive manual annotation cost and take advantage of the unlabeled data for remote sensing (RS) image interpretation. Recent related research has mainly adopted two strategies: self-training and consistency regularization. Self-training tries to acquire accurate pseudo-labels to explicitly expand the train set. However, the existing methods cannot accurately identify false pseudo-labels, suffering from their negative impact on model optimization. The consistency regularization constrains the model by producing consistent predictions robust to the perturbations introduced in the sample or feature domain but requires a sufficient number of training data. Therefore, we propose a strategy for the semisupervised semantic segmentation of the RS images. The proposed model in the generative adversarial network (GAN) framework is optimized by consistency self-training, learning the distributions of both labeled and unlabeled data. The discriminator is optimized by accurate pixel-level training labels instead of the image-level ones, thereby assessing the confidence for the prediction of each pixel, which is then used to reweight the loss of the unlabeled data in self-training. The generator is optimized with the consistency constraint with respect to all random perturbations on the unlabeled data, which increases the sample diversity and prompts the model to learn the underlying distribution of the unlabeled data. Experimental results on the the large-scale and densely annotated Instance Segmentation in Aerial Images Dataset (iSAID) datasets and the International Society for Photogrammetry and Remote Sensing (ISPRS) datasets show that our framework outperforms several state-of-the-art semisupervised semantic segmentation methods.",Semantics,Predictive models,Image segmentation,Generative adversarial networks,Perturbation methods,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Semisupervised learning,Consistency self-training,generative adversarial network (GAN),remote sensing (RS) image,semantic segmentation,semisupervised learning,,,,,,,,,,,,,,,
Row_259,"Jung, Hoin","Choi, Han-Soo","Kang, Myungjoo",,Boundary Enhancement Semantic Segmentation for Building Extraction From Remote Sensed Image,,2022,80,"Image processing via convolutional neural network (CNN) has been developed rapidly for remote sensing technology. Moreover, techniques for accurately extracting building footprints from remote sensed images have attracted considerable interest owing to their wide variety of common applications, including monitoring natural disasters and urban development. Extraction of building footprints can be performed easily by semantic segmentation using U-Net-like CNN architectures. However, obtaining precise boundaries of segmentation masks remains challenging due to various impediments surrounding target objects. In this study, we propose a method to elaborate edges of buildings detected in remote sensed images to enhance the boundaries of segmentation masks. The proposed method adopts holistically nested edge detection (HED), which extracts edge features at an encoder of a given architecture. In the proposed boundary enhancement (BE) module, an extracted edge and segmentation mask are combined, sharing mutual information. To enable the proposed method efficiently to adapt to a wide variety of conditions, we design a distinctive approach adopting a HED unit and BE module, which is applicable to various semantic segmentation networks containing encoder-decoder structures. Experiments were conducted on five different datasets (DeepGlobe, Urban3D, WHU [high-resolution (HR), low-resolution (LR)], and Massachusetts). The results demonstrate that our proposed approaches improved on the performance of prior methods for extracting building footprints. Comparative experiments were conducted on various backbone architectures including U-Net, ResUNet++, TernausNet, and U-shape spatial pyramid pooling (USPP) to ensure the effectiveness of the proposed method. Based on various evaluation metrics and qualitative analysis, our results show that the proposed method achieved improved performance compared with prior methods for all datasets and backbone networks.",Semantics,Remote sensing,Buildings,Feature extraction,Image edge detection,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Image segmentation,Convolutional neural networks,Boundary enhancement (BE),building footprint extraction,convolutional neural network (CNN),remote sensing,satellite imagery,semantic segmentation,,,,,,,,,,,,,,
Row_260,"Yin, Peng","Zhang, Dongmei","Han, Wei","Li, Jiang",High-Resolution Remote Sensing Image Semantic Segmentation via Multiscale Context and Linear Self-Attention,,2022,7,"Remote sensing image semantic segmentation, which aims to realize pixel-level classification according to the content of remote sensing images, has broad applications in various fields. Thanks to the superiority of deep learning (DL), the semantic segmentation model based on the convolutional neural network (CNN) dramatically promotes the development of remote sensing image semantic segmentation. Due to the high resolution, comprehensive coverage, extensive data, and sizeable spectral difference of high-resolution remote sensing images (HRRSI), the existing GPU is not suitable for directly semantic segmentation of the whole image. Cutting the image into small patches will lead to the loss of context information, resulting in the decline of accuracy. To address this issue, we propose the multiscale context self-attention network (MSCSANet). It combines the benefits of the self-attention mechanism with CNN to improve the segmentation quality of various remote sensing images. The MSCSANet extracts multiscale features from multiscale context images to solve the problem of feature loss caused by image segmentation. In addition, in order to make use of the feature of large-scale context, the multiscale context patches are used to guide the local image patch to focus on different fine-grained objects to enhance the feature of the local image patch. Moreover, considering the limited computing resources, we designed a linear self-attention module to reduce the computational complexity. Compared with other DL models, our proposed model can enhance the ability of multiscale features in complex scenes, and realizes improvements of 1.56% mean intersection over union (MIoU) on the Gaofen Image Dataset and 1.93% MIoU on the ISPRS Potsdam Dataset, respectively.",Feature extraction,Remote sensing,Correlation,Complexity theory,Context modeling,"Cheng, Jianmei",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Computational modeling,Deep learning,Context,remote sensing,self-attention,semantic segmentation,,,,,,,,,,,,,,,,
Row_261,"Dong, Xingjun","Zhang, Changsheng","Fang, Lei","Yan, Yuxiao",A deep learning based framework for remote sensing image ground object segmentation,,NOV 2022,7,"Semantic segmentation of very-high-resolution (VHR) remote sensing images is of great significance, in which remote sensing can be applied to numerous fields. However, VHR remote sensing images are taken at different seasons and regions, causing the large intra-class and low inter-class variations of pixels. Thus, the state-of-the-art semantic segmentation network have considerable misclassifications and blurring of object boundaries. To solve the above problems, a deep learning-based semantic segmentation framework (DLSS) of VHR remote sensing images is proposed in this study, which comprising three stages. At the pre-processing stage, a novel data pre-processing method named Image Block Segmentation (IBS) is proposed to coarse segmentation of VHR remote sensing images at the image block scale. At the image segmentation stage, the different strategies are adopted to segment image blocks of different categories for fine segmentation. Through these two stages, this study implements a coarse-to-fine segmentation strategy, which reduces the phenomenon of misclassification by using different network models for low inter class variations of pixels. At the post-processing stage, a novel post-processing method termed Superpixel Cluster (SPC) is proposed to modify the segmentation results. SPC can capture fine details of objects and aggregating continuous pixels with similar characteristics into a set of superpixels, so as to ensure the boundary accuracy and internal consistency of ground object. Extensive experiments, including a comprehensive ablation study, confirm that IBS is capable of effectively reducing the misclassification, and SPC can correct the segmentation results significantly. The experimental results on the Gaofen Image Dataset (GID) suggest that the overall accuracy (OA) of the commonly utilized models combined with DLSS framework can increase, and the average is 2.05%. The code of DLSS is available at https://github.com/dxj620/Deep-learning-semantic-segementation.(c) 2022 Elsevier B.V. All rights reserved.",Convolutional Neural Networks,Semantic segmentation,Remote sensing images,Superpixel segmentation,Landcover classification,,,,,APPLIED SOFT COMPUTING,,,,,,,,,,,,,,,,,,,,,,,
Row_262,"Gu, Yuhang","Hao, Jie","Chen, Bing","Deng, Hai",Top-Down Pyramid Fusion Network for High-Resolution Remote Sensing Semantic Segmentation,,OCT 2021,3,"In recent years, high-resolution remote sensing semantic segmentation based on data fusion has gradually become a research focus in the field of land classification, which is an indispensable task of a smart city. However, the existing feature fusion methods with bottom-up structures can achieve limited fusion results. Alternatively, various auxiliary fusion modules significantly increase the complexity of the models and make the training process intolerably expensive. In this paper, we propose a new lightweight model called top-down pyramid fusion network (TdPFNet) including a multi-source feature extractor, a top-down pyramid fusion module and a decoder. It can deeply fuse features from different sources in a top-down structure using high-level semantic knowledge guiding the fusion of low-level texture information. Digital surface model (DSM) data and open street map (OSM) data are used as auxiliary inputs to the Potsdam dataset for the proposed model evaluation. Experimental results show that the network proposed in this paper not only notably improves the segmentation accuracy, but also reduces the complexity of the multi-source semantic segmentation model.",semantic segmentation,data fusion,deep learning,open street map,digital surface model,,,,,REMOTE SENSING,,high-resolution remote sensing image,,,,,,,,,,,,,,,,,,,,,
Row_263,"Liang, Min","Wang, Xili",,,A bidirectional semantic segmentation method for remote sensing image based on super-resolution and domain adaptation,,JAN 17 2023,3,"Image semantic segmentation methods based on convolutional neural networks rely on supervised learning with labels, and their performance often drops significantly when applied to unlabelled datasets from different sources. The domain adaptation methods can reduce the inconsistency of feature distribution between the unlabelled target domain data used for testing and the labelled source domain data used for training, thus improve the segmentation performance and have more practical applications. However, in the field of remote sensing image processing, if the spatial resolutions of the source domain and the target domain are different and this problem is not to be solved, the performance of the transferred model will be affected. In this paper, we propose a bidirectional semantic segmentation method based on super-resolution and domain adaption (BSSM-SRDA), which is suitable for the transfer learning task of a semantic segmentation model from a low-resolution source domain data to a high-resolution target domain data. BSSM-SRDA mainly consists of three parts: a shared feature extraction network; a super-resolution image translation module, which incorporates a super-resolution approach to reduce spatial resolution differences and visual style differences of the two domains; a domain-adaptive semantic segmentation module, which combines an adversarial domain adaptation approach to reduce differences at the output level. At the same time, we design a new bidirectional self-supervised learning algorithm for BSSM-SRDA that facilitates mutually beneficial learning of the super-resolution image translation module and the domain-adaptive semantic segmentation module. The experiments demonstrate the superiority of the proposed method over other state-of-the-art methods on two remote-sensing image datasets, with mIoU improvements of 2.5% and 3.2%, respectively.",remote-sensing image,semantic segmentation,domain adaptation,super resolution,self-supervised learning,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_264,Niu Mengjia,Zhang Yongjun,Li Zhi,Yang Gang,Remote Sensing Image Segmentation Network Based on Adaptive Multiscale and Contour Gradient,,JAN 2023,1,"Remote sensing image segmentation algorithms are susceptible to interference from environmental factors, such as object occlusion and uneven illumination. Existing deep learning remote sensing image semantic segmentation methods usually adopt an end-to-end codec structure. However, they still suffer from inaccurate segmentation for the structure and contours of high similarity objects. Therefore, to improve the algorithm robustness and classification accuracy, a deep convolutional neural network remote sensing image semantic segmentation algorithm based on contour gradient learning is proposed. To improve the quality of the predicted feature maps, the adaptive attention-based multichannel multiscale feature fusion network (D-MMA Net) is proposed based on the SegNet model network. The D-MA block uses an attention-based adaptive multiscale module to adaptively extract different scale features according to the learned weights to obtain more effective high level semantic features. To further refine the extracted object boundaries, the contour extraction module, a learnable contour extraction module, is proposed based on the principle of the Sobel edge detection operator. Finally, the contour information is combined with multi-scale semantic features to enhance the robustness of the spatial resolution of the image. The experimental results show that the proposed method improves the segmentation accuracy and produces good segmentation results for irregular object boundaries.",remote sensing,remote sensing image,multi-channel feature extraction,contour gradient,feature fusion,Cui Zhongwei,Liu Junwen,,,LASER & OPTOELECTRONICS PROGRESS,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_265,"Ma, Bifang","Chang, Chih-Yung",,,Semantic Segmentation of High-Resolution Remote Sensing Images Using Multiscale Skip Connection Network,,FEB 15 2022,15,"Semantic segmentation of remote sensing images plays a vital role in land resource management, yield estimation, and economic evaluation. Therefore, this paper proposes a multi-scale skip connection network with the Atrous convolution to deal with the segmentation problems of the multi-modal and multi-scale high-resolution remote sensing images. Firstly, we applied the Atrous convolution in the encoder to enlarge the convolution kernel's receptive field. Secondly, based on the U-Net network, we merged the light and deep features of different scales by redesigning the skip connection and combining multi-scale features in each U-Net layer. Finally, we applied a pixel-by-pixel classification method and obtained the semantic segmentation results of remote sensing images. The effectiveness of the proposed algorithm is verified. The experimental results show that the mF1 scores are 89.4% and 90.3% on the open dataset of ISPRS Vaihingen and ISPRS Potsdam, respectively, which are better than the state-of-the-art algorithms.",Image segmentation,Semantics,Remote sensing,Feature extraction,Sensors,,,,,IEEE SENSORS JOURNAL,,Convolution,Task analysis,Deep convolutional neural network,high-resolution remote sensing image,multi-scale skip connection,semantic segmentation,,,,,,,,,,,,,,,,
Row_266,"Li, Chunhua","Li, Xin","Xia, Runliang","Li, Tao",Hierarchical Self-Attention Embedded Neural Network With Dense Connection for Remote-Sensing Image Semantic Segmentation,,2021,1,"Semantic segmentation of remote-sensing imagery strives to assign a pixel-wise semantic label. Since encoder-decoder networks have demonstrated tremendous success in natural image semantic segmentation, the adoption and extension of this kind of method are transferring such superior performance for the problems in remote-sensing. Facing the high-altitude angle of imaging and complex and diverse ground objects of remote-sensing data, it is necessary to strengthen the features' distinguishability by enhancing the network's capability. Nevertheless, the existing methods suffer from the structural stereotype, leveraging the short-range and long-range contextual information insufficiently. Attempting to address the problems mentioned above, a hierarchical self-attention embedded neural network with dense connection for remote sensing image semantic segmentation (HSDCN) is proposed. In the encoder stage, multiple self-attention modules (SAM) are embedded to model pixel-wise and channel-wise relationships at various scales hierarchically, making the representations more refined and discriminative. Then the dense connections are used to fuse the heterogeneous features. Thus, the network could produce logical and reasonable clues for labeling pixels. The extensive experiments are conducted on ISPRS Vaihingen and Potsdam benchmarks. And the results reveal significant improvements in comparison with other state-of-the-art methods.",Semantics,Image segmentation,Remote sensing,Task analysis,Decoding,"Lyu, Xin","Tong, Yao","Zhao, Liancheng","Wang, Xinyuan",IEEE ACCESS,,Computer architecture,Costs,Semantic segmentation,remote-sensing imagery,self-attention,dense connection,ISPRS benchmarks,,,,,,,,,,,,,,,
Row_267,"Xu, Zhiyong","Zhang, Weicun","Zhang, Tianxiang","Yang, Zhifang",Efficient Transformer for Remote Sensing Image Segmentation,,SEP 2021,128,"Semantic segmentation for remote sensing images (RSIs) is widely applied in geological surveys, urban resources management, and disaster monitoring. Recent solutions on remote sensing segmentation tasks are generally addressed by CNN-based models and transformer-based models. In particular, transformer-based architecture generally struggles with two main problems: a high computation load and inaccurate edge classification. Therefore, to overcome these problems, we propose a novel transformer model to realize lightweight edge classification. First, based on a Swin transformer backbone, a pure Efficient transformer with mlphead is proposed to accelerate the inference speed. Moreover, explicit and implicit edge enhancement methods are proposed to cope with object edge problems. The experimental results evaluated on the Potsdam and Vaihingen datasets present that the proposed approach significantly improved the final accuracy, achieving a trade-off between computational complexity (Flops) and accuracy (Efficient-L obtaining 3.23% mIoU improvement on Vaihingen and 2.46% mIoU improvement on Potsdam compared with HRCNet_W48). As a result, it is believed that the proposed Efficient transformer will have an advantage in dealing with remote sensing image segmentation problems.",semantic segmentation,remote sensing,deep learning,pure Efficient transformer,Swin transformer,"Li, Jiangyun",,,,REMOTE SENSING,,edge,,,,,,,,,,,,,,,,,,,,,
Row_268,"Lu, Yujie","Zhang, Yongjun","Cui, Zhongwei","Long, Wei",Multi-Dimensional Manifolds Consistency Regularization for semi-supervised remote sensing semantic segmentation,,SEP 5 2024,2,"Semi -supervised semantic segmentation in remote sensing is critical for urban planning, environmental monitoring and disaster response. The high cost and time required for high -quality data annotation limits its wider application. Traditional semi -supervised deep learning methods, which operate in a single dimension, limit model robustness and generalization. Our study addresses this issue by proposing an effective semisupervised learning method. This method improves model robustness and generalization in remote sensing semantic segmentation. We introduce the Multi -Dimensional Manifolds Consistency Regularization (MDMCR) approach. It applies multi -dimensional perturbations to input images and features, expanding the sample library and improving learning efficiency. Our method has been rigorously tested on various datasets. With only 1/8 of the data labeled, it achieved mean Intersection over Union (mIoU) scores of 74.48% on ISPRS Vaihingen and 78.80% on Potsdam. With only 5% labeled data, it reached 49.93% mIoU on DeepGlobe Roads and 57.90% on Massachusetts Roads. These results show the superiority of our method over existing techniques.",Semi-supervised remote sensing semantic,segmentation,Consistency regularization,Manifold hypothesis,Multi-dimensional manifolds,"Chen, Ziyang",,,,KNOWLEDGE-BASED SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,
Row_269,"Xiong, Shun","Ma, Chao","Yang, Guang","Song, Yaodong",Semantic segmentation of remote sensing imagery for road extraction via joint angle prediction: comparisons to deep learning,,DEC 28 2023,1,"Accurate road network information is required to study and analyze the relationship between land usage type and land subsidence, and road extraction from remote sensing images is an important data source for updating road networks. This task has been considered a significant semantic segmentation problem, given the many road extraction methods developed for remote sensing images in recent years. Although impressive results have been achieved by classifying each pixel in the remote sensing image using a semantic segmentation network, traditional semantic segmentation methods often lack clear constraints of road features. Consequently, the geometric features of the results might deviate from actual roads, leading to issues like road fractures, rough edges, inconsistent road widths, and more, which hinder their effectiveness in road updates. This paper proposes a novel road semantic segmentation algorithm for remote sensing images based on the joint road angle prediction. By incorporating the angle prediction module and the angle feature fusion module, constraints are added to the angle features of the road. Through the angle prediction and angle feature fusion, the information contained in the remote sensing images can be better utilized. The experimental results show that the proposed method outperforms existing semantic segmentation methods in both quantitative evaluation and visual effects. Furthermore, the extracted roads were consecutive with distinct edges, making them more suitable for mapping road updates.",angle prediction,semantic segmentation,road extraction,remote sensing image,map cartography,"Liang, Shuaizhe","Feng, Jing",,,FRONTIERS IN EARTH SCIENCE,,,,,,,,,,,,,,,,,,,,,,,
Row_270,"Lyu, Xinran","Zheng, Ruohui","Zhang, Libao",,Semantic Segmentation of Weakly Annotated Remote Sensing Images Based on Feature Adversary and Uncertainty Perception,,2024,0,"The rich details in remote sensing images require the expertise of domain experts for annotation, making precise labeling across multiple scenarios challenging. Currently, there is an increasing interest in low-precision image-level annotations. However, errors and omissions are difficult to avoid during the labeling process. How to achieve accurate semantic segmentation under noisy annotations has become an urgent problem that requires resolution. In this letter, we consider the rich features of remote sensing targets and design a weakly labeled semantic segmentation model for remote sensing images based on feature adversary and uncertainty perception. First, we introduce a confidence model voting method to handle missing or incorrect image-level labels. Subsequently, we perform multilevel feature fusion to obtain initial pixel-level pseudo labels. Finally, leveraging the significant differences in image features across different categories, we design a feature adversarial model and introduce an uncertainty analysis method, improving the utilization of remote sensing image features and enhancing the accuracy of semantic segmentation. The effectiveness of this approach is validated through a comprehensive evaluation of four datasets.",Remote sensing,Semantic segmentation,Annotations,Uncertainty,Training,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Accuracy,Feature extraction,Feature adversary,noise cleaning,remote sensing,uncertainty perception,weak annotation,,,,,,,,,,,,,,,
Row_271,"Kang, Yuhan","Ji, Jian","Xu, Hekai","Yang, Yong",Swin-CDSA: The Semantic Segmentation of Remote Sensing Images Based on Cascaded Depthwise Convolution and Spatial Attention Mechanism,,2024,1,"As an important task in remote sensing image processing, semantic segmentation of remote sensing images has broad application prospects in many fields such as disaster warning and rescue, environmental protection, and road planning. Research on semantic segmentation of remote sensing images based on deep learning has made some progress, but there are still problems such as poor perception of small object features, loss of detailed information in deep feature extraction, and imprecise segmentation contours of small objects. To this end, we propose a new remote sensing semantic segmentation model Swin-CDSA, which copes these problems to some extent by designing cascaded deep convolutional modules (CDCMs) and spatial attention mechanisms (SAMs). CDCM extracts multiscale features by using multilayer convolutions with different layers but parallel fixed small-sized kernels, while SAM supplements the model's understanding of local and global information through a dual attention mechanism. We conducted experiments on the Potsdam and LoveDA datasets and achieved good results.",Convolution,Remote sensing,Feature extraction,Semantic segmentation,Attention mechanisms,"Chen, Peng","Zhao, Hui",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Transformers,Semantics,Attention mechanism,remote sensing,semantic segmentation,transformer,,,,,,,,,,,,,,,,
Row_272,Zhang Zhehan,Fang Wei,Du Lili,Qiao Yanli,Semantic Segmentation of Remote Sensing Image Based on Encoder-Decoder Convolutional Neural Network,,FEB 10 2020,19,"The remote sensing image semantic segmentation in rural areas is the basis for urban and rural planning, vegetation and agricultural land detection. Segmentation of a high-resolution remote sensing image of rural areas is difficult because of the complex image information. Herein, we designed a complete symmetric network structure that includes a pooled index and a convolution used to fuse semantic information and image features. The Bottleneck layer is constructed using 1x1 convolution and employed to extract the details and reduce the parameter quantity, deepen the filter depth to build an end-to-end semantic segmentation network, and improve the activation function to further enhance network performance. The experimental results show that the accuracies of the proposed method and the classical semantic segmentation networks U-Net and SegNet are 98.4%, 80.3%, and 98.1%, respectively on the CCF dataset. Thus, the proposed method achieves better performance than the other two methods.",image processing,agricultural land detection,remote sensing images,semantic segmentation,encoder-decoder network,Zhang Dongying,Ding Guoshen,,,ACTA OPTICA SINICA,,deep learning,,,,,,,,,,,,,,,,,,,,,
Row_273,"Liao, Lingcen","Liu, Wei","Liu, Shibin",,Effect of Bit Depth on Cloud Segmentation of Remote-Sensing Images,,MAY 12 2023,2,"Due to the cloud coverage of remote-sensing images, the ground object information will be attenuated or even lost, and the texture and spectral information of the image will be changed at the same time. Accurately detecting clouds from remote-sensing images is of great significance to the field of remote sensing. Cloud detection utilizes semantic segmentation to classify remote-sensing images at the pixel level. However, previous studies have focused on the improvement of algorithm performance, and little attention has been paid to the impact of bit depth of remote-sensing images on cloud detection. In this paper, the deep semantic segmentation algorithm UNet is taken as an example, and a set of widely used cloud labeling dataset ""L8 Biome"" is used as the verification data to explore the relationship between bit depth and segmentation accuracy on different surface landscapes when the algorithm is used for cloud detection. The research results show that when the image is normalized, the effect of cloud detection with a 16-bit remote-sensing image is slightly better than that of an 8-bit remote sensing image; when the image is not normalized, the gap will be widened. However, using 16-bit remote-sensing images for training will take longer. This means data selection and classification do not always need to follow the highest possible bit depth when doing cloud detection but should consider the balance of efficiency and accuracy.",bit depth,remote sensing,semantic segmentation,cloud,deep learning,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_274,"Wu, Honglin","Huang, Peng","Zhang, Min","Tang, Wenlong",CMTFNet: CNN and Multiscale Transformer Fusion Network for Remote-Sensing Image Semantic Segmentation,,2023,52,"Convolutional neural networks (CNNs) are powerful in extracting local information but lack the ability to model long-range dependencies. In contrast, the transformer relies on multihead self-attention mechanisms to effectively extract the global contextual information and thus model long-range dependencies. In this article, we propose a novel encoder-decoder structured semantic segmentation network, named CNN and multiscale transformer fusion network (CMTFNet), to extract and fuse local information and multiscale global contextual information of high-resolution remote-sensing images. Specifically, to further process the output features from the CNN encoder, we build a transformer decoder based on the multiscale multihead self-attention (M2SA) module for extracting rich multiscale global contextual information and channel information. Additionally, the transformer block introduces an efficient feed-forward network (E-FFN) to enhance the information interaction between different channels of the feature. Finally, the multiscale attention fusion (MAF) module fully fuses the feature information from different levels. We have conducted extensive comparison experiments and ablation experiments on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam datasets. The extensive experimental results demonstrate that our proposed CMTFNet can obtain superior performance compared to the currently popular methods. The codes will be available at https://github.com/DrWuHonglin/CMTFNet.",Global contextual information,multiscale transformer,remote-sensing image,semantic segmentation,,"Yu, Xinyu",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_275,"Su, Zhongbin","Li, Wei","Ma, Zheng","Gao, Rui",An improved U-Net method for the semantic segmentation of remote sensing images,,FEB 2022,35,"Foremost deep neural network models trained in natural scenes cannot transfer and apply to remote sensing image semantic segmentation well. Studies have shown that fine-tuning methods containing model fusion can alleviate this dilemma. In this paper, we provide an approach used to improve U-Net and propose an end-to-end deep convolutional neural network (DCNN) combining the superiorities of DenseNet, U-Net, dilated convolution, and DeconvNet. We evaluated the proposed method and model on the Potsdam orthophoto data set. Compared with U-Net, our approach increases the PA, mPA, and mIoU evaluation indexes by 11.1%, 14.0%, and 13.5%, respectively; the segmentation speed increases by approximately 1.18 times and the number of parameters is 59.0% that of U-Net. The experiments demonstrate that for the semantic segmentation of high-resolution remote sensing images, using the combined dilated convolutions as the primary feature extractor, using the transposed convolution to restore the size of the feature maps, and reducing the number of layers is an effective method to improve the comprehensive performance of U-Net. This research enriches the models based on DCNNs and the modes of using DCNNs in a specific scene.",Deep learning,Artificial neural network,Remote sensing,U-Net,Semantic segmentation,,,,,APPLIED INTELLIGENCE,,,,,,,,,,,,,,,,,,,,,,,
Row_276,"Yi, Yaning","Zhang, Zhijie","Zhang, Wanchang","Zhang, Chuanrong",Semantic Segmentation of Urban Buildings from VHR Remote Sensing Imagery Using a Deep Convolutional Neural Network,,AUG 2019,165,"Urban building segmentation is a prevalent research domain for very high resolution (VHR) remote sensing; however, various appearances and complicated background of VHR remote sensing imagery make accurate semantic segmentation of urban buildings a challenge in relevant applications. Following the basic architecture of U-Net, an end-to-end deep convolutional neural network (denoted as DeepResUnet) was proposed, which can effectively perform urban building segmentation at pixel scale from VHR imagery and generate accurate segmentation results. The method contains two sub-networks: One is a cascade down-sampling network for extracting feature maps of buildings from the VHR image, and the other is an up-sampling network for reconstructing those extracted feature maps back to the same size of the input VHR image. The deep residual learning approach was adopted to facilitate training in order to alleviate the degradation problem that often occurred in the model training process. The proposed DeepResUnet was tested with aerial images with a spatial resolution of 0.075 m and was compared in performance under the exact same conditions with six other state-of-the-art networks-FCN-8s, SegNet, DeconvNet, U-Net, ResUNet and DeepUNet. Results of extensive experiments indicated that the proposed DeepResUnet outperformed the other six existing networks in semantic segmentation of urban buildings in terms of visual and quantitative evaluation, especially in labeling irregular-shape and small-size buildings with higher accuracy and entirety. Compared with the U-Net, the F1 score, Kappa coefficient and overall accuracy of DeepResUnet were improved by 3.52%, 4.67% and 1.72%, respectively. Moreover, the proposed DeepResUnet required much fewer parameters than the U-Net, highlighting its significant improvement among U-Net applications. Nevertheless, the inference time of DeepResUnet is slightly longer than that of the U-Net, which is subject to further improvement.",semantic segmentation,urban building extraction,deep convolutional neural network,VHR remote sensing imagery,U-Net,"Li, Weidong","Zhao, Tian",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_277,"Li, Xin","Xu, Feng","Tao, Feifei","Tong, Yao",A Cross-Domain Coupling Network for Semantic Segmentation of Remote Sensing Images,,2024,0,"Semantic segmentation of remote sensing images (RSIs) is critical for various applications, including urban planning, agriculture, and disaster management. Existing methods often fail to capture fine-grained textures and periodic patterns in RSIs, leading to suboptimal results in complex terrains. To address these challenges, we propose a cross-domain coupling network (CDCNet) that leverages both domain-specific extraction and cross-domain coupling (CDC) to enrich contextual cues for semantic inference. Our CDCNet integrates a CDC layer within the encoder-decoder architecture to simultaneously refine representations in the frequency and spatial domains. This approach effectively models fine-grained textures and periodic patterns in the frequency domain, as well as edges, shapes, and broad structural elements in the spatial domain. Extensive experiments on the ISPRS Potsdam and LoveDA datasets demonstrate the superiority of CDCNet over several state-of-the-art methods. Ablation studies confirm the significant impact of the CDC layer, validating the effectiveness of our approach in handling RSIs.",Cross-domain coupling (CDC),frequency domain,remote sensing images (RSIs),semantic segmentation,Cross-domain coupling (CDC),"Gao, Hongmin","Liu, Fan","Chen, Ziqi","Lyu, Xin",IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,frequency domain,remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,,,,
Row_278,"Li, Rui","Zheng, Shunyi","Duan, Chenxi","Su, Jianlin",Multistage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images,,2022,131,"The attention mechanism can refine the extracted feature maps and boost the classification performance of the deep network, which has become an essential technique in computer vision and natural language processing. However, the memory and computational costs of the dot-product attention mechanism increase quadratically with the spatiotemporal size of the input. Such growth hinders the usage of attention mechanisms considerably in application scenarios with large-scale inputs. In this letter, we propose a linear attention mechanism (LAM) to address this issue, which is approximately equivalent to dot-product attention with computational efficiency. Such a design makes the incorporation between attention mechanisms and deep networks much more flexible and versatile. Based on the proposed LAM, we refactor the skip connections in the raw U-Net and design a multistage attention ResU-Net (MAResU-Net) for semantic segmentation from fine-resolution remote sensing images. Experiments conducted on the Vaihingen data set demonstrated the effectiveness and efficiency of our MAResU-Net. Our code is available at https://github.com/lironui/MAResU-Net.",Semantics,Complexity theory,Remote sensing,Task analysis,Image segmentation,"Zhang, Ce",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Feature extraction,Decoding,Fine-resolution remote sensing images,linear attention mechanism (LAM),semantic segmentation,,,,,,,,,,,,,,,,,
Row_279,"Xiang, Xuyang","Gong, Wenping","Li, Shuailong","Chen, Jun",TCNet: Multiscale Fusion of Transformer and CNN for Semantic Segmentation of Remote Sensing Images,,2024,10,"Semantic segmentation of remote sensing images plays a critical role in areas such as urban change detection, environmental protection, and geohazard identification. Convolutional Neural Networks (CNNs) have been excessively employed for semantic segmentation over the past few years; however, a limitation of the CNN is that there exists a challenge in extracting the global context of remote sensing images, which is vital for semantic segmentation, due to the locality of the convolution operation. It is informed that the recently developed Transformer is equipped with powerful global modeling capabilities. A network called TCNet is proposed in this article, and a parallel-in-branch architecture of the Transformer and the CNN is adopted in the TCNet. As such, the TCNet takes advantage of both Transformer and CNN, and both global context and low-level spatial details could be captured in a much shallower manner. In addition, a novel fusion technique called Interactive Self-attention is advanced to fuse the multilevel features extracted from both branches. To bridge the semantic gap between regions, a skip connection module called Windowed Self-attention Gating is further developed and added to the progressive upsampling network. Experiments on three public datasets (i.e., Bijie Landslide Dataset, WHU Building Dataset, and Massachusetts Buildings Dataset) depict that TCNet yields superior performance over state-of-the-art models. The IoU values obtained by TCNet for these three datasets are 75.34% (ranked first among 10 models compared), 91.16% (ranked first among 13 models compared), and 76.21% (ranked first among 13 models compared), respectively.",Convolutional Neural Network (CNN),feature fusion,remote sensing images,semantic segmentation,Transformer,"Ren, Tianhe",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_280,"Li, Xin","Yong, Xi","Li, Tao","Tong, Yao",A Spectral-Spatial Context-Boosted Network for Semantic Segmentation of Remote Sensing Images,,APR 2024,6,"Semantic segmentation of remote sensing images (RSIs) is pivotal for numerous applications in urban planning, agricultural monitoring, and environmental conservation. However, traditional approaches have primarily emphasized learning within the spatial domain, which frequently leads to less than optimal discrimination of features. Considering the inherent spectral qualities of RSIs, it is essential to bolster these representations by incorporating the spectral context in conjunction with spatial information to improve discriminative capacity. In this paper, we introduce the spectral-spatial context-boosted network (SSCBNet), an innovative network designed to enhance the accuracy semantic segmentation in RSIs. SSCBNet integrates synergetic attention (SYA) layers and cross-fusion modules (CFMs) to harness both spectral and spatial information, addressing the intrinsic complexities of urban and natural landscapes within RSIs. Extensive experiments on the ISPRS Potsdam and LoveDA datasets reveal that SSCBNet surpasses existing state-of-the-art models, achieving remarkable results in F1-scores, overall accuracy (OA), and mean intersection over union (mIoU). Ablation studies confirm the significant contribution of SYA layers and CFMs to the model's performance, emphasizing the effectiveness of these components in capturing detailed contextual cues.",semantic segmentation,remote sensing images,spectral-spatial context,synergetic attention,cross-fusion module,"Gao, Hongmin","Wang, Xinyuan","Xu, Zhennan","Fang, Yiwei",REMOTE SENSING,"You, Qian",,,,,,,,,,"Lyu, Xin",,,,,,,,,,,,
Row_281,"Liu, Bing","Chen, Xiaohui","Yu, Anzhu","Feng, Fan",Large multimodal model for open vocabulary semantic segmentation of remote sensing images,,DEC 31 2025,0,"Conventional remote sensing image semantic segmentation tasks require training specialized models for specific categories of ground objects, which often fail to recognize unseen ground object categories during the training process. The generalization ability of the model is the key to achieving open vocabulary semantic segmentation of remote sensing images. Recently, large multimodal models that have been pre-trained with massive amounts of image and text data have demonstrated strong generalization capabilities. Inspired by the success of large multimodal models, we propose an open vocabulary segmentation method for remote sensing images. The proposed method uses the large multimodal model LLAVA and the vision large model SAM to achieve segmentation of open vocabulary. Specifically, LLAVA is used to understand the remote sensing images and the open vocabulary, and SAM is used to extract visual features of the remote sensing images. Finally, the features extracted from SAM and LLAVA are input into the mask decoder to complete semantic segmentation tasks. In order to verify the effectiveness of the proposed method, we conducted a large number of experiments on multiple ground object categories such as airplane, ship, river and lake. The qualitative and quantitative evaluation results fully verified the effectiveness of our proposed method.",Open vocabulary Semantic Segmentation,large multimodal model,deep learning,segment anything,,"Yue, Jiaying","Yu, Xuchu",,,EUROPEAN JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_282,"Jiang, Na","Li, Jiyuan",,,An Improved Semantic Segmentation Method for Remote Sensing Images Based on Neural Network,,APR 2020,8,"Traditional semantic segmentation methods cannot accurately classify high-resolution remote sensing images, due to the difficulty in acquiring the correlations between geophysical objects in these images. To solve the problem, this paper proposes an improved semantic segmentation method for remote sensing images based on neural network. Based on residual network, the proposed algorithm changes the dilated convolution kernels in the dilated spatial pyramid pooling (SPP) module before extracting the correlations between geophysical objects, thus improving the accuracy of segmentation. Next, the high resolution of the input image was maintained through deconvolution, and the semantic segmentation was realized by the pixel-level method. To enhance the robustness of our algorithm, the dataset was expanded through random cropping and stitching of images. Finally, our algorithm was trained and tested on the Potsdam dataset provided by the International Society for Photogrammetry and Remote Sensing (ISPRS). The results show that our algorithm was 1.4% more accurate than the DeepLab v3 Plus. The research results shed new light on the semantic segmentation of high-resolution remote sensing images.",remote sensing images,pixel-level method,residual network (ResNet),dilated spatial pyramid pooling (SPP),sub pixel up-sampling,,,,,TRAITEMENT DU SIGNAL,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_283,"Ni, Yue","Liu, Jiahang","Cui, Jian","Yang, Yuze",Edge Guidance Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,2023,5,"With the improvement of spatial resolution, the conveyed information of remote sensing images has become increasingly intricate. The semantic content of pixels within the same object exhibits considerable variability, whereas the semantic content of pixels between different objects exhibits significant overlap. However, most existing approaches focus solely on establishing the internal consistency of objects by aggregating global or multiscale contextual information without adequately considering the orientation and spatially detailed features of the target. Moreover, these methods often overlook the potential of edge information in achieving accurate edge positioning. These defects will adversely affect the accuracy of segmentation. In this article, we present an edge information guided network, which leverages edge information to guide the aggregation of rich contextual information for semantic segmentation to improve the segmentation accuracy of high-resolution remote sensing images. Specifically, an orientation convolution module is proposed to construct a spatial detail branch for acquiring precise edge information and spatial detail information. To effectively guide the aggregation of spatial detail features and semantic features, we propose a spatial-semantic feature aggregation module. Moreover, to enhance the extraction of long-range dependencies of irregular objects, we propose the orientation atrous convolution module, which facilitates the extraction of multiphase long-range dependencies of objects. The ISPRS Vaihingen and Potsdam datasets are employed to validate the efficacy of the proposed methodology and draw comparisons with various state-of-the-art techniques. The experimental results demonstrate that the proposed method offers distinct advantages.",Feature extraction,Semantics,Semantic segmentation,Image edge detection,Remote sensing,"Wang, Xiaozhen",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Convolution,Data mining,Edge information,orientation astrous convolution,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,
Row_284,"Liu, Yan","Ren, Qirui","Geng, Jiahui","Ding, Meng",Efficient Patch-Wise Semantic Segmentation for Large-Scale Remote Sensing Images,,OCT 2018,60,"Efficient and accurate semantic segmentation is the key technique for automatic remote sensing image analysis. While there have been many segmentation methods based on traditional hand-craft feature extractors, it is still challenging to process high-resolution and large-scale remote sensing images. In this work, a novel patch-wise semantic segmentation method with a new training strategy based on fully convolutional networks is presented to segment common land resources. First, to handle the high-resolution image, the images are split as local patches and then a patch-wise network is built. Second, training data is preprocessed in several ways to meet the specific characteristics of remote sensing images, i.e., color imbalance, object rotation variations and lens distortion. Third, a multi-scale training strategy is developed to solve the severe scale variation problem. In addition, the impact of conditional random field (CRF) is studied to improve the precision. The proposed method was evaluated on a dataset collected from a capital city in West China with the Gaofen-2 satellite. The dataset contains ten common land resources (Grassland, Road, etc.). The experimental results show that the proposed algorithm achieves 54.96% in terms of mean intersection over union (MIoU) and outperforms other state-of-the-art methods in remote sensing image segmentation.",remote sensing,image segmentation,fully convolutional network,patch-wise,multi-scale,"Li, Jiangyun",,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,
Row_285,"Fan, Junyu","Li, Jinjiang","Liu, Yepeng","Zhang, Fan",Frequency-aware robust multidimensional information fusion framework for remote sensing image segmentation,,MAR 2024,7,"Urban scene image segmentation is an important research area in high-resolution remote sensing image processing. However, due to its complex three-dimensional structure, interference factors such as occlusion, shadow, intra-class inconsistency, and inter-class indistinction affect segmentation performance. Many methods have combined local and global information using CNNs and Transformers to achieve high performance in remote sensing image segmentation tasks. However, these methods are not stable when dealing with these interference factors. Recent studies have found that semantic segmentation is highly sensitive to frequency information, so we introduced frequency information to make the model learn more comprehensively about different categories of targets from multiple dimensions. By modeling the target with local features, global information, and frequency information, the target features can be learned in multiple dimensions to reduce the impact of interference factors on the model and improve its robustness. In this paper, we consider frequency information in addition to combining CNNs and Transformers for modeling and propose a Multidimensional Information Fusion Network (MIFNet) for high-resolution remote sensing image segmentation of urban scenes. Specifically, we design an information fusion Transformer module that can adaptively associate local features, global semantic information, and frequency information and a relevant semantic aggregation module for aggregating features at different scales to construct the decoder. By aggregating image features at different depths, the specific representation of the target and the correlation between targets can be modeled in multiple dimensions, allowing the network to better recognize and understand the features of each class of targets to resist various interference factors that affect segmentation performance. We conducted extensive ablation experiments and comparative experiments on the ISPRS Vaihingen and ISPRS Potsdam benchmarks to verify our proposed method. In a large number of experiments, our method achieved the best results, with 84.53% and 87.3% mIoU scores on the Vaihingen and Potsdam datasets, respectively, proving the superiority of our method. The source code will be available at https://github.com/JunyuFan/MIFNet.",Remote sensing,Semantic segmentation,Frequency information,Transformer,,,,,,ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,,,,,,,,,,,,,,,,,,,,,,,
Row_286,"Dong, He","Yu, Baoguo","Wu, Wanqing","He, Chenglong",Enhanced Lightweight End-to-End Semantic Segmentation for High-Resolution Remote Sensing Images,,2022,4,"Deep learning based methods have shown promising performance in semantic segmentation of high-resolution remote sensing (HRRS) images. However, due to the multi-scale property and complexity of HRRS images, it still faces many challenges in tackling the scale variance problem and obtaining global context information. In this paper, we propose an enhanced lightweight end-to-end semantic segmentation (ELES2) framework for HRRS images, where a superpixel segmentation pooling (SSP) module is embedded with the framework for result refinement, leading to a more accurate end-to-end semantic segmentation. Besides, compensation connections (CC) are applied between encoder blocks to establish long-range dependencies. In addition, a dense dilated convolutional pyramid (DDCP) module is proposed to generate dense features under different scales and capture global context information. Experiments conducted showed that our ELES2 respectively achieves the mean pixel intersection-over-union (mIoU) values of 80.16% and 73.20% on the ISPRS Potsdam and Vaihingen benchmark datasets using only 12.62M parameters and 13.09G floating-point operations (FLOPs). Experimental results prove that our method achieves a promising balance between segmentation accuracy and computational efficiency compared with the state-of-the-art semantic segmentation models.",Image segmentation,Semantics,Remote sensing,Computational efficiency,Feature extraction,,,,,IEEE ACCESS,,Deep learning,Decoding,High-resolution remote sensing images,semantic segmentation,superpixel pooling,,,,,,,,,,,,,,,,,
Row_287,"Chen, Xi","Li, Zhiqiang","Jiang, Jie","Han, Zhen",Adaptive Effective Receptive Field Convolution for Semantic Segmentation of VHR Remote Sensing Images,,APR 2021,48,"Convolutional neural networks (CNNs) have facilitated impressive improvements in the semantic segmentation of very high-resolution (VHR) remote sensing images. The success of semantic segmentation depends on an effective receptive field (RF) large enough to cover the entire object. Popular methods to enlarge the effective RF include dilated filters, subsampling operations, and stacking layers. Unfortunately, the methods are inefficient or able to cause grid artifacts. Moreover, although the object sizes vary greatly in remote sensing images, the size of the RF cannot reach a compromise between small and large objects. To tackle these problems, we propose adaptive effective receptive convolution (AERFC) for VHR remote sensing images. AERFC adaptively controls the sampling location of convolution and automatically adjusts the effective RF without significantly increasing the parameter number and computational cost. Thus, AERFC reduces the training difficulty, decreases overfitting risk, and reserves details in VHR images. AERFC is also integrated with spatial pyramid pooling (SPP) to aggregate diverse multiscale features for exploring contextual information. Experimental results of the quantitative and qualitative evaluation over four benchmark data sets show that AERFC outperforms state-of-the-art methods.",Radio frequency,Convolution,Feature extraction,Image segmentation,Semantics,"Deng, Shiyi","Li, Zhihong","Fang, Tao","Huo, Hong",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Li, Qingli",Remote sensing,Shape,Field of view,filter,kernel,semantic contextual information,,,,"Liu, Min",,,,,,,,,,,,
Row_288,"Dong, Sijun","Chen, Zhengchao",,,A Multi-Level Feature Fusion Network for Remote Sensing Image Segmentation,,FEB 2021,16,"High-resolution remote sensing image segmentation is a mature application in many industrial-level image applications and it also has military and civil applications. The scene analysis needs to be automated as much as possible with high-resolution remote sensing images. This plays a significant role in environmental disaster monitoring, forestry industry, agricultural farming, urban planning, and road analysis. This study proposes a multi-level feature fusion network (MFNet) that can integrate the multi-level features in the backbone to obtain different types of image information. Finally, the experiments in this study demonstrate that the proposed network can achieve good segmentation results in the Vaihingen and Potsdam datasets. By aiming to achieve a large difference in the scale of the target objects in remote sensing images and achieving a poor recognition result for small objects, a multi-level feature fusion solution is proposed in this study. This investigation improves the recognition results of the remote sensing image segmentation to a certain extent.",remote sensing image,image semantic segmentation,scale difference,feature fusion,,,,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,
Row_289,"Xi, Zhihao","Meng, Yu","Chen, Jingbo","Deng, Yupeng",Learning to Adapt Adversarial Perturbation Consistency for Domain Adaptive Semantic Segmentation of Remote Sensing Images,,DEC 2023,1,"Semantic segmentation techniques for remote sensing images (RSIs) have been widely developed and applied. However, most segmentation methods depend on sufficiently annotated data for specific scenarios. When a large change occurs in the target scenes, model performance drops significantly. Therefore, unsupervised domain adaptation (UDA) for semantic segmentation is proposed to alleviate the reliance on expensive per-pixel densely labeled data. In this paper, two key issues of existing domain adaptive (DA) methods are considered: (1) the factors that cause data distribution shifts in RSIs may be complex and diverse, and existing DA approaches cannot adaptively optimize for different domain discrepancy scenarios; (2) domain-invariant feature alignment, based on adversarial training (AT), is prone to excessive feature perturbation, leading to over robust models. To address these issues, we propose an AdvCDA method that guides the model to adapt adversarial perturbation consistency. We combine consistency regularization to consider interdomain feature alignment as perturbation information in the feature space, and thus propose a joint AT and self-training (ST) DA method to further promote the generalization performance of the model. Additionally, we propose a confidence estimation mechanism that determines network stream training weights so that the model can adaptively adjust the optimization direction. Extensive experiments have been conducted on Potsdam, Vaihingen, and LoveDA remote sensing datasets, and the results demonstrate that the proposed method can significantly improve the UDA performance in various cross-domain scenarios.",unsupervised domain adaptation,adversarial perturbation consistency,self-training,semantic segmentation,remote sensing,"Liu, Diyou","Kong, Yunlong","Yue, Anzhi",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_290,"Zhou, Nan","Hong, Jin","Cui, Wenyu","Wu, Shichao",A Multiscale Attention Segment Network-Based Semantic Segmentation Model for Landslide Remote Sensing Images,,MAY 2024,5,"Landslide disasters have garnered significant attention due to their extensive devastating impact, leading to a growing emphasis on the prompt and precise identification and detection of landslides as a prominent area of research. Previous research has primarily relied on human-computer interactions and visual interpretation from remote sensing to identify landslides. However, these methods are time-consuming, labor-intensive, subjective, and have a low level of accuracy in extracting data. An essential task in deep learning, semantic segmentation, has been crucial to automated remote sensing image recognition tasks because of its end-to-end pixel-level classification capability. In this study, to mitigate the disadvantages of existing landslide detection methods, we propose a multiscale attention segment network (MsASNet) that acquires different scales of remote sensing image features, designs an encoder-decoder structure to strengthen the landslide boundary, and combines the channel attention mechanism to strengthen the feature extraction capability. The MsASNet model exhibited an average accuracy of 95.13% on the test set from Bijie's landslide dataset, a mean accuracy of 91.45% on the test set from Chongqing's landslide dataset, and a mean accuracy of 90.17% on the test set from Tianshui's landslide dataset, signifying its ability to extract landslide information efficiently and accurately in real time. Our proposed model may be used in efforts toward the prevention and control of geological disasters.",deep learning,remote sensing images,landslide identification,semantic segmentation,feature extraction,"Zhang, Ziheng",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_291,"Sun, Wei","Zhou, Rong","Nie, Congchong","Wang, Livan",Farmland Segmentation from Remote Sensing Images Using Deep Learning Methods,,2020,6,"Farmland segmentation is crucial and getting increasingly important role in the field of agricultural insurance and digital agriculture. To accurately achieve insured crop area and disaster loss assessment, precision smallholders' farmland segmentation and mapping are necessary. Deep learning technology has demonstrated its strength and has out-performed state-of-the-art alternatives in many fields. In this study, we aim to explore the effectiveness of five classic deep semantic segmentation models on the smallholder-wise farmland segmentation from remote sensing images. Five FCN-based segmentation models were selected including U-Net, PSPNet, DeepLabV3+, DANet, and CCNet, which were originally proposed for natural or medical image segmentation. The study area locates in Jiaxiang County in the north China. We used GF-1 at 2m spatial resolution with 4-band multispectral images (red, green, blue, and near-infrared) acquired based on image fusion. Comparative experiment results showed that DeepLabv3+ achieved the best mIoU of 89.82%. PSPNet, DANet, CCNet and U-Net obtained lower but similar mIoU with: 89.63%, 89.66%, 89.59%, and 89.15%. The OA metric of the all the five models were above 94.8%. The results indicate that the FCN-based deep semantic segmentation networks are effective for larger-scale smallholder-wise farmland segmentation with high spatial resolution multispectral satellite images.",Farmland segmentation,remote sensing,satellite imagery,deep semantic segmentation,agriculture insurance,"Sun, Jun",,,,"MEDICAL IMAGE COMPUTING AND COMPUTER-ASSISTED INTERVENTION, PT III",,,,,,,,,,,,,,,,,,,,,,,
Row_292,"Zhang, Wenbo","Wang, Achuan",,,Research on Semantic Segmentation Method of Remote Sensing Image Based on Self-supervised Learning,,AUG 2023,0,"To address the challenge of requiring a large amount of manually annotated data for semantic segmentation of remote sensing images using deep learning, a method based on self-supervised learning is proposed. Firstly, to simultaneously learn the global and local features of remote sensing images, a self-supervised learning network structure called TBSNet (Triple-Branch Self-supervised Network) is constructed. This network comprises an image transformation prediction branch, a global contrastive learning branch, and a local contrastive learning branch. The contrastive learning part of the network employs a novel data augmentation method to simulate positive pairs of the same remote sensing images under different weather conditions, enhancing the model's performance. Meanwhile, the model integrates channel attention and spatial attention mechanisms in the projection head structure of the global contrastive learning branch, and replaces a fully connected layer with a convolutional layer in the local contrastive learning branch, thus improving the model's feature extraction ability. Secondly, to mitigate the high computational cost during the pre-training phase, an algorithm optimization strategy is proposed using the TracIn method and sequential optimization theory, which increases the efficiency of pre-training. Lastly, by fine-tuning the model with a small amount of annotated data, effective semantic segmentation of remote sensing images is achieved even with limited annotated data. The experimental results indicate that with only 10% annotated data, the overall accuracy (OA) and recall of this model have improved by 4.60% and 4.88% respectively, compared to the traditional self-supervised model SimCLR (A Simple Framework for Contrastive Learning of Visual Representations). This provides significant application value for tasks such as semantic segmentation in remote sensing imagery and other computer vision domains.",Computer vision,deep learning,self-supervised learning,remote sensing image,semantic segmentation,,,,,INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS,,,,,,,,,,,,,,,,,,,,,,,
Row_293,"Cui, Wei","Wang, Fei","He, Xin","Zhang, Dongyou",Multi-Scale Semantic Segmentation and Spatial Relationship Recognition of Remote Sensing Images Based on an Attention Model,,MAY 1 2019,46,"A comprehensive interpretation of remote sensing images involves not only remote sensing object recognition but also the recognition of spatial relations between objects. Especially in the case of different objects with the same spectrum, the spatial relationship can help interpret remote sensing objects more accurately. Compared with traditional remote sensing object recognition methods, deep learning has the advantages of high accuracy and strong generalizability regarding scene classification and semantic segmentation. However, it is difficult to simultaneously recognize remote sensing objects and their spatial relationship from end-to-end only relying on present deep learning networks. To address this problem, we propose a multi-scale remote sensing image interpretation network, called the MSRIN. The architecture of the MSRIN is a parallel deep neural network based on a fully convolutional network (FCN), a U-Net, and a long short-term memory network (LSTM). The MSRIN recognizes remote sensing objects and their spatial relationship through three processes. First, the MSRIN defines a multi-scale remote sensing image caption strategy and simultaneously segments the same image using the FCN and U-Net on different spatial scales so that a two-scale hierarchy is formed. The output of the FCN and U-Net are masked to obtain the location and boundaries of remote sensing objects. Second, using an attention-based LSTM, the remote sensing image captions include the remote sensing objects (nouns) and their spatial relationships described with natural language. Finally, we designed a remote sensing object recognition and correction mechanism to build the relationship between nouns in captions and object mask graphs using an attention weight matrix to transfer the spatial relationship from captions to objects mask graphs. In other words, the MSRIN simultaneously realizes the semantic segmentation of the remote sensing objects and their spatial relationship identification end-to-end. Experimental results demonstrated that the matching rate between samples and the mask graph increased by 67.37 percentage points, and the matching rate between nouns and the mask graph increased by 41.78 percentage points compared to before correction. The proposed MSRIN has achieved remarkable results.",multi-scale,semantic segmentation,image caption,remote sensing,LSTM,"Xu, Xuxiang","Yao, Meng","Wang, Ziwei","Huang, Jiejun",REMOTE SENSING,,U-Net,upscaling,downscaling,,,,,,,,,,,,,,,,,,,
Row_294,"Wang, Yiqin","Dong, Yunyun",,,A Semantic Segmentation Method of Remote Sensing Image Based on Feature Fusion and Attention Mechanism,,OCT 2024,0,"Current methods for semantic segmentation of remote-sensing images, especially for irregular and small targets, often result in low precision and incomplete feature extraction. To address this issue, an improved semantic segmentation method was developed utilizing DeepLabv3+. First, DeepLabv3+ is combined with the proposed feature fusion module to make full use of the complementary information of low- and high-level features. Second, the channel attention module helps extract effective features while suppressing irrelevant features, thereby enabling the extraction of more meaningful global information from high-level features. Finally, rich spatial information is selected using guided spatial attention, which improves the accuracy of edge segmentation of target objects. The results of the comparison show that the mean F1 score (MF1) and overall accuracy (OA) of the proposed method on the ISPRS Potsdam dataset are 89.81% and 88.45%, respectively. The MF1 of the proposed method is 89.90% and the OA is 89.14% for the UAVid dataset, which are higher than those of the other comparison algorithms. The proposed method exhibits superior semantic segmentation capabilities for remote-sensing images.",Channel Attention,DeepLabv3+,Feature Fusion Module,Remote-Sensing Images,Semantic Segmentation,,,,,JOURNAL OF INFORMATION PROCESSING SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,
Row_295,"Liu, Bin","Li, Bing","Sreeram, Victor","Li, Shuofeng",MBT-UNet: Multi-Branch Transform Combined with UNet for Semantic Segmentation of Remote Sensing Images,,AUG 2024,0,"Remote sensing (RS) images play an indispensable role in many key fields such as environmental monitoring, precision agriculture, and urban resource management. Traditional deep convolutional neural networks have the problem of limited receptive fields. To address this problem, this paper introduces a hybrid network model that combines the advantages of CNN and Transformer, called MBT-UNet. First, a multi-branch encoder design based on the pyramid vision transformer (PVT) is proposed to effectively capture multi-scale feature information; second, an efficient feature fusion module (FFM) is proposed to optimize the collaboration and integration of features at different scales; finally, in the decoder stage, a multi-scale upsampling module (MSUM) is proposed to further refine the segmentation results and enhance segmentation accuracy. We conduct experiments on the ISPRS Vaihingen dataset, the Potsdam dataset, the LoveDA dataset, and the UAVid dataset. Experimental results show that MBT-UNet surpasses state-of-the-art algorithms in key performance indicators, confirming its superior performance in high-precision remote sensing image segmentation tasks.",transformer,semantic segmentation,convolutional neural network,remote sensing,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_296,"Dong, Rongsheng","Pan, Xiaoquan","Li, Fengying",,DenseU-Net-Based Semantic Segmentation of Objects in Urban Remote Sensing Images,,2019,108,"Class imbalance is a serious problem that plagues the semantic segmentation task in urban remote sensing images. Since large object classes dominate the segmentation task, small object classes are usually suppressed, so the solutions based on optimizing the overall accuracy are often unsatisfactory. In the light of the class imbalance of the semantic segmentation in urban remote sensing images, we developed the concept of the Down-sampling Block (DownBlock) for obtaining context information and the Up-sampling Block (UpBlock) for restoring the original resolution. We proposed an end-to-end deep convolutional neural network (DenseU-Net) architecture for pixel-wise urban remote sensing image segmentation. The main idea of the DenseU-Net is to connect convolutional neural network features through cascade operations and use its symmetrical structure to fuse the detail features in shallow layers and the abstract semantic features in deep layers. A focal loss function weighted by the median frequency balancing (MFB_Focal(loss)) is proposed; the accuracy of the small object classes and the overall accuracy are improved effectively with our approach. Our experiments were based on the 2016 ISPRS Vaihingen 2D semantic labeling dataset and demonstrated the following outcomes. In the case where boundary pixels were considered (GT), MFB_Focal(loss) achieved a good overall segmentation performance using the same U-Net model, and the F1-score of the small object class ""car"" was improved by 9.28% compared with the cross-entropy loss function. Using the same MFB_Focal(loss) loss function, the overall accuracy of the DenseU-Net was better than that of U-Net, where the F1-score of the ""car"" class was 6.71% higher. Finally, without any post-processing, the DenseU-Net+MFB_Focal(loss) achieved the overall accuracy of 85.63%, and the F1-score of the ""car"" class was 83.23%, which is superior to HSN+OI+WBP both numerically and visually.",Class imbalance,deep convolutional neural networks,median frequency balancing,semantic segmentation,urban remote sensing images,,,,,IEEE ACCESS,,,,,,,,,,,,,,,,,,,,,,,
Row_297,"Li, Yansheng","Shi, Te","Zhang, Yongjun","Chen, Wei",Learning deep semantic segmentation network under multiple weakly-supervised constraints for cross-domain remote sensing image semantic segmentation,,MAY 2021,151,"Due to its wide applications, remote sensing (RS) image semantic segmentation has attracted increasing research interest in recent years. Benefiting from its hierarchical abstract ability, the deep semantic segmentation network (DSSN) has achieved tremendous success on RS image semantic segmentation and has gradually become the mainstream technology. However, the superior performance of DSSN highly depends on two conditions: (I) massive quantities of labeled training data exist; (II) the testing data seriously resemble the training data. In actual RS applications, it is difficult to fully meet these conditions due to the RS sensor variation and the distinct landscape variation in different geographic locations. To make DSSN fit the actual RS scenario, this paper exploits the cross-domain RS image semantic segmentation task, which means that DSSN is trained on one labeled dataset (i.e., the source domain) but is tested on another varied dataset (i.e., the target domain). In this setting, the performance of DSSN is inevitably very limited due to the data shift between the source and target domains. To reduce the disadvantageous influence of data shift, this paper proposes a novel objective function with multiple weakly-supervised constraints to learn DSSN for cross-domain RS image semantic segmentation. Through carefully examining the characteristics of cross-domain RS image semantic segmentation, multiple weakly-supervised constraints include the weakly-supervised transfer invariant constraint (WTIC), weakly-supervised pseudo-label constraint (WPLC) and weakly-supervised rotation consistency constraint (WRCC). Specifically, DualGAN is recommended to conduct unsupervised style transfer between the source and target domains to carry out WTIC. To make full use of the merits of multiple constraints, this paper presents a dynamic optimization strategy that dynamically adjusts the constraint weights of the objective function during the training process. With full consideration of the characteristics of the cross-domain RS image semantic segmentation task, this paper gives two cross-domain RS image semantic segmentation settings: (I) variation in geographic location and (II) variation in both geographic location and imaging mode. Extensive experiments demonstrate that our proposed method remarkably outperforms the state-of-the-art methods under both of these settings. The collected datasets and evaluation benchmarks have been made publicly available online (htt ps://github.com/te-shi/MUCSS).",Cross-domain remote sensing (RS) image semantic segmentation,Weakly-supervised transfer invariant constraint (WTIC),Weakly-supervised pseudo-label constraint (WPLC),Weakly-supervised rotation consistency constraint (WRCC),DualGAN,"Wang, Zhibin","Li, Hao",,,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,,Dynamic optimization strategy,,,,,,,,,,,,,,,,,,,,,
Row_298,"Dang, Yuanyuan","Gao, Yu","Liu, Bing",,MFAFNet: A Multiscale Fully Attention Fusion Network for Remote Sensing Image Semantic Segmentation,,2024,0,"The semantic segmentation of high-resolution remote sensing images is widely used in various precision agriculture, urban planning, and environmental detection are some examples of these industries. Convolutional neural networks (CNNs) are excellent in the semantic segmentation of remote sensing images. CNN excels in extracting local feature details but lacks the ability to model global context data. Therefore, to obtain rich local-global information about context, we describe in this work a semantic segmentation network design technique for remote sensing, based on an encoder-decoder structure, which is named Multiscale Fully Attention Fusion Network for Remote Sensing Image Semantic Segmentation (MFAFNet). In particular, to improve the segmentation efficiency, the encoder's extractor of features was ResNet18, after which the explicit visual center module EVC and the full attention network FANB are intended to retrieve the detailed global context data. Finally, the gated channel attention fusion module (GCF) tries to augment channel interaction information in the decoder stage while fusing low-level characteristics for efficient aggregation. During our research and testing, we used the publicly available Vaihingen and Potsdam datasets from the International Society for Photogrammetry and Remote Sensing (ISPRS), as well as the LoveDA dataset. Meanwhile, it demonstrates that MFAFNet outperforms other well-liked methods in terms of competition. We further validated the efficiency of the network components in the study by conducting ablation experiments on the Vaihingen dataset.",Feature extraction,Semantic segmentation,Remote sensing,Convolutional neural networks,Decoding,,,,,IEEE ACCESS,,Semantics,Data mining,Attention mechanisms,remote sensing,global-local context,attention mechanism,,,,,,,,,,,,,,,,
Row_299,"Chen, Yan","Dong, Quan","Wang, Xiaofeng","Zhang, Qianchuan",Hybrid Attention Fusion Embedded in Transformer for Remote Sensing Image Semantic Segmentation,,2024,6,"In the context of fast progress in deep learning, convolutional neural networks have been extensively applied to the semantic segmentation of remote sensing images and have achieved significant progress. However, certain limitations exist in capturing global contextual information due to the characteristics of convolutional local properties. Recently, Transformer has become a focus of research in computer vision and has shown great potential in extracting global contextual information, further promoting the development of semantic segmentation tasks. In this article, we use ResNet50 as an encoder, embed the hybrid attention mechanism into Transformer, and propose a Transformer-based decoder. The Channel-Spatial Transformer Block further aggregates features by integrating the local feature maps extracted by the encoder with their associated global dependencies. At the same time, an adaptive approach is employed to reweight the interdependent channel maps to enhance the feature fusion. The global cross-fusion module combines the extracted complementary features to obtain more comprehensive semantic information. Extensive comparative experiments were conducted on the ISPRS Potsdam and Vaihingen datasets, where mIoU reached 78.06% and 76.37%, respectively. The outcomes of multiple ablation experiments also validate the effectiveness of the proposed method.",Feature extraction,Semantic segmentation,Semantics,Transformers,Remote sensing,"Kang, Menglei","Jiang, Wenxiang","Wang, Mengyuan","Xu, Lixiang",IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Zhang, Chen",Task analysis,Decoding,Global cross fusion,hybrid attention,remote sensing image,semantic segmentation,Transformer,,,,,,,,,,,,,,,
Row_300,"Kemker, Ronald","Luu, Ryan","Kanan, Christopher",,Low-Shot Learning for the Semantic Segmentation of Remote Sensing Imagery,,OCT 2018,46,"Recent advances in computer vision using deep learning with RGB imagery (e.g., object recognition and detection) have been made possible thanks to the development of large annotated RGB image data sets. In contrast, multispectral image (MSI) and hyperspectral image (HSI) data sets contain far fewer labeled images, in part due to the wide variety of sensors used. These annotations are especially limited for semantic segmentation, or pixelwise classification, of remote sensing imagery because it is labor intensive to generate image annotations. Low-shot learning algorithms can make effective inferences despite smaller amounts of annotated data. In this paper, we study low-shot learning using self-taught feature learning for semantic segmentation. We introduce: 1) an improved self-taught feature learning framework for HSI and MSI data and 2) a semisupervised classification algorithm. When these are combined, they achieve the state-of-the-art performance on remote sensing data sets that have little annotated training data available. These low-shot learning frameworks will reduce the manual image annotation burden and improve semantic segmentation performance for remote sensing imagery.",Deep learning,feature learning,hyperspectral imaging,self-taught learning,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,semisupervised,,,,,,,,,,,,,,,,,,,,,
Row_301,"Liu, Zhuoran","Li, Zizhen","Liang, Ying","Persello, Claudio",RSPS-SAM: A Remote Sensing Image Panoptic Segmentation Method Based on SAM,,NOV 2024,0,"Satellite remote sensing images contain complex and diverse ground object information and the images exhibit spatial multi-scale characteristics, making the panoptic segmentation of satellite remote sensing images a highly challenging task. Due to the lack of large-scale annotated datasets for panoramic segmentation, existing methods still suffer from weak model generalization capabilities. To mitigate this issue, this paper leverages the advantages of the Segment Anything Model (SAM), which can segment any object in remote sensing images without requiring any annotations and proposes a high-resolution remote sensing image panoptic segmentation method called Remote Sensing Panoptic Segmentation SAM (RSPS-SAM). Firstly, to address the problem of global information loss caused by cropping large remote sensing images for training, a Batch Attention Pyramid was designed to extract multi-scale features from remote sensing images and capture long-range contextual information between cropped patches, thereby enhancing the semantic understanding of remote sensing images. Secondly, we constructed a Mask Decoder to address the limitation of SAM requiring manual input prompts and its inability to output category information. This decoder utilized mask-based attention for mask segmentation, enabling automatic prompt generation and category prediction of segmented objects. Finally, the effectiveness of the proposed method was validated on the high-resolution remote sensing image airport scene dataset RSAPS-ASD. The results demonstrate that the proposed method achieves segmentation and recognition of foreground instances and background regions in high-resolution remote sensing images without the need for prompt input, while providing smooth segmentation boundaries with a panoptic segmentation quality (PQ) of 57.2, outperforming current mainstream methods.",panoptic segmentation,segment anything model,remote sensing,deep learning,,"Sun, Bo","He, Guangjun","Ma, Lei",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_302,"He, Sheng","Liu, Jin",,,Semantic segmentation of very high resolution remote sensing images with residual logic deep fully convolutional networks,"MIPPR 2019: REMOTE SENSING IMAGE PROCESSING, GEOGRAPHIC INFORMATION SYSTEMS, AND OTHER APPLICATIONS",2020,0,"This paper describes a deep learning approach to semantic segmentation of very high resolution remote sensing images. We introduce RLFCN, a fully convolutional architecture based on residual logic blocks, to model the ambiguous mapping between remote sensing images and classification maps. In order to recover the output resolution to the original size, we adopt a special way to efficiently learn feature map up-sampling within the network. For optimization, we employ the equally-weighted focal loss which is particularly suitable for the task for it reduces the impact of class imbalance. Our framework consists of only one single architecture which is trained end-to-end and doesn't rely on any post-processing techniques and needs no extra data except images. Based on our framework, we conducted experiments on a ISPRS dataset: Vaihingen. The results indicate that our framework achieves better performance than the current state of the art, while containing fewer parameters and requires fewer training data.",deep learning,remote sensing,semantic segmentation,fully convolutional networks,RLFCN,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_303,"Ding, Lei","Bruzzone, Lorenzo",,,A DEEP ARCHITECTURE BASED ON A TWO-STAGE LEARNING FOR SEMANTIC SEGMENTATION OF LARGE-SIZE REMOTE SENSING IMAGES,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),2019,4,"Remote sensing images (RSIs) usually have much larger size compared to typical natural images used in computer vision applications. This makes the computational cost of training convolutional neural networks with full-size images unaffordable. Commonly used methodologies for semantic segmentation of RSIs perform training and prediction on cropped local image patches. Thus they fail to model the potential dependencies between ground objects at a higher level of abstraction. In order to better exploit global context information in RSIs, a deep architecture based on a two-stage training approach that is specially tailored to training large-size RSIs is proposed. In the first training stage, down-scaled images are used as input to learn high-level features from a large image area. In the second training stage, a local feature extraction network is designed to extract low-level information from cropped image patches. The complementary information learned from different levels is fused to make the prediction. As a result, the proposed two-stage training approach is able to exploit the context information of RSIs from a larger perspective without losing spatial details. Experimental results on a benchmark remote sensing dataset demonstrate the effectiveness of the proposed approach.",Semantic Segmentation,Convolutional Neural Network,Deep Learning,Remote Sensing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_304,"Xiao, Sining","Wang, Peijin","Diao, Wenhui","Rong, Xuee",MoCG: Modality Characteristics-Guided Semantic Segmentation in Multimodal Remote Sensing Images,,2023,3,"The rapid development of satellite platforms has yielded copious and diverse multisource data for earth observation, greatly facilitating the growth of multimodal semantic segmentation (MSS) in remote sensing. However, MSS also suffers from numerous challenges: 1) existing inherent defects in each modality due to the different imaging mechanisms; 2) insufficient exploration of the intrinsic characteristics of modalities; and 3) the existence of the huge semantic gap between heterogeneous data causes difficulties in feature fusion. The inability to effectively utilize the rich and diverse information provided by each modality and ignorance of the heterogeneity between modalities will hinder the feature enhancement, and further significantly impacts the semantic segmentation accuracy. Furthermore, neglecting the huge gap makes feature fusion challenging. In this study, we introduce a novel framework for MSS that effectively mitigates the aforementioned problems. Our approach employs a pseudo-Siamese structure for feature extraction. Specifically, we propose a simple yet effective geometric topology structure modeling (GTSM) module to extract geometric relationships and texture information from optical data. Additionally, we present a modality intrinsic noise suppression (MINS) module to fully exploit radiation information and alleviate the effects of unique geometric distortions for synthetic aperture radar (SAR). Furthermore, we present an adaptive multimodal feature fusion (AMFF) module for fully fusing different modality features. Extensive experiments on both WHU-OPT-SAR and DFC23 datasets validate the robustness and effectiveness of the proposed Modality Characteristics-Guided (MoCG) semantic segmentation network compared to other state-of-the-art semantic segmentation methods, including multimodal and single-modal approaches. Our approach achieves the best performance on both datasets, resulting in mean intersection over union (mIoU)/overall accuracy (OA) gains 69.1%/87.5% on WHU-OPT-SAR and 86.7%/97.3% on DFC23.",Optical sensors,Optical imaging,Semantic segmentation,Feature extraction,Semantics,"Li, Xuexue","Fu, Kun","Sun, Xian",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Adaptive optics,Optical distortion,Cross-modal feature fusion,multimodal semantic segmentation (MSS),optical,remote sensing,synthetic aperture radar (SAR),,,,,,,,,,,,,,,
Row_305,"Che, Rui","Ma, Xiaowen","Hong, Tingfeng","Wang, Xinyu",DBDAN: Dual-Branch Dynamic Attention Network for Semantic Segmentation of Remote Sensing Images,,2024,0,"Attention mechanism is capable to capture long-range dependence. However, its independent calculation of correlations can hardly consider the complex background of remote sensing images, which causes noisy and ambiguous attention weights. To address this issue, we design a correlation attention module (CAM) to enhance appropriate correlations and suppress erroneous ones by seeking consensus among all correlation vectors, which facilitates feature aggregation. Simultaneously, we introduce the CAM into a local dynamic attention (LDA) branch and a global dynamic attention (GDA) branch to obtain the information on local texture details and global context, respectively. In addition, considering the different demands of complex and diverse geographical objects for both local texture details and global context, we devise a dynamic weighting mechanism to adaptively adjust the contributions of both branches, thereby constructing a more discriminative feature representation. Experimental results on three datasets suggest that the proposed dual-branch dynamic attention network (DBDAN), which integrates the CAM and both branches, can considerably improve the performance for semantic segmentation of remote sensing images and outperform representative state-of-the-art methods.",Remote Sensing,Semantic Segmentation,Attention Mechanism,Deep Learning,,"Feng, Tian","Zhang, Wei",,,"PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2023, PT IV",,,,,,,,,,,,,,,,,,,,,,,
Row_306,Wu Shengwei,Fang Jiaoli,Zhu Daming,,Semantic Segmentation of Remote Sensing Imagery Based on Improved Squeeze and Excitaion Block,,JUN 2024,0,"Aiming to solve the semantic recognition error of traditional methods in semantic segmentation of remote sensing imagery with complex background, we propose a simple but effective convolutional attention module, region squeeze and excitation block (RSE-block), based on squeeze and excitation block (SE-block). This block can squeeze regional context information of features, guides the network to screen more important features and excite features expression in both spatial and channel dimensions. In addition, it can be added to any convolutional neural network and trained end-to-end with the network. Meanwhile, we propose a multi-scale integration method supported by this block to solve the recognition problem of different size ground objects, and a new semantic segmentation network, RSENet, is constructed on these bases. The experimental results show that RSENet is superior to the baseline in terms of mean F1-score and mean intersection over union by 0. 028 and 0. 021 respectively on the Potsdam dataset, and is more competitive with some current advanced methods.",remote sensing imagery,semantic segmentation,attention mechanism,convolutional neural network,,,,,,LASER & OPTOELECTRONICS PROGRESS,,,,,,,,,,,,,,,,,,,,,,,
Row_307,"Yuan, Wei","Xu, Wenbo",,,NeighborLoss: A Loss Function Considering Spatial Correlation for Semantic Segmentation of Remote Sensing Image,,2021,15,"House segmentation of remote sensing image based on deep learning has become the main segmentation method because it can automatically extract features. However, the accuracy of image segmentation is affected not only by the network model, but also by the loss function, but the existing loss functions, except Binary Cross Entropy, are designed to deal with imbalanced dataset, no new research on improving Binary Cross Entropy for balanced dataset, and all loss function treat each pixel in isolation, without considering the spatial correlation between pixel and its neighbor pixels. To solve this problem, a new loss function, named NeighborLoss function, is proposed. Firstly, the deep learning network is used to get the prediction results of each pixel. According to whether the prediction results of the eight neighboring pixels of each pixel are consistent with the each pixel prediction, different weights are given to each pixel. Finally, the weighted average value of cross entropy of all pixels in the batch is taken as the final loss function value. We use the main deep learning semantic segmentation networks SegNet, PSPNet, UNET ++, MUNet with both NeighborLoss and cross entropy loss respectively to extract houses on the open data set named WHU dataset for remote sensing. The results show that compared with cross entropy loss functions, the MIoU, Precision, Recall, and Accuracy of NeighborLoss function are improved. From the predicted graph, the NeighborLoss function is more accurate to extract the edge of the house, especially in the corner of the house. NeighborLoss function is a more effective loss function for remote sensing image segmentation.",Entropy,Image segmentation,Semantics,Remote sensing,Deep learning,,,,,IEEE ACCESS,,Feature extraction,Correlation,Deep learning,loss function,remote sensing image,semantic segmentation,,,,,,,,,,,,,,,,
Row_308,"Lopez, Josue","Santos, Stewart","Atzberger, Clement","Torres, Deni",Convolutional Neural Networks for Semantic Segmentation of Multispectral Remote Sensing Images,2018 IEEE 10TH LATIN-AMERICAN CONFERENCE ON COMMUNICATIONS (IEEE LATINCOM),2018,15,"The recent impulse in development of artificial intelligence (AI) methodologies has simplified the application of this in multiple research areas. This simplification was not favorable before, due to the limitations in dimensionality, processing time, computational resources, among others. Working with multispectral remote sensing (RS) images, in an artificial neural network (NN) was quite complex. Due the methods used required millions of processes that took a long time to be executed and produce competitive results compared with the state of the art (SoA). Deep learning (DL) strategies have been applied to alleviate these limitations and have greatly improved the use of neural networks. Therefore, this paper presents the analysis of DL-NNs to perform semantic segmentation of multispectral RS images. Images are captured by the constellation of satellites Sentinel-2 from the European Space Agency. The objective of this research is to classify each pixel of a scene into five categories: 1-vegetation, 2-soil, 3-water, 4-clouds and 5-cloud shadows. The selection of spectral bands for the formation of input datasets for segmentation of these classes is very important. The spectral signatures of each material aid to discern among several classes. Results presented in this work, show that the AI strategy proposed offer better accuracy segmentation than other methods of the SoA in competitive processing time.",Convolutional neural networks,multispectral images,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_309,"Lu, Xiao","Jiang, Zhiguo","Zhang, Haopeng",,Weakly Supervised Remote Sensing Image Semantic Segmentation With Pseudo-Label Noise Suppression,,2024,1,"Semantic segmentation of remote sensing images (RSIs) plays a crucial role in various applications, including urban planning and environmental monitoring. However, the high cost and complexity of obtaining detailed annotations for RSIs pose significant challenge. This issue necessitates the exploration of weakly supervised learning as an effective alternative, which utilizes more readily available, less granular forms of labeling. Yet, weakly supervised approaches face their own set of challenges, primarily due to scarcity of precise pixel-level labels which significantly hampers the model's ability to learn accurate representations. In this article, we introduce a weakly supervised semantic segmentation (WSSS) approach for RSIs that leverages self-supervised learning (SSL) and pseudo-label noise mitigation to address these challenges. Our method leverages a self-supervised encoder for providing similarity information, which enhances feature representation in RSIs and enables the generation of more accurate pseudo-labels, thus reducing the noise in the pseudo-labels. Furthermore, we propose a refined loss function that incorporates gradient clipping and label smoothing to mitigate the impact of noisy labels, thereby improving the robustness and accuracy of the segmentation results. Extensive experiments on the ISPRS Potsdam, ISPRS Vaihingen, and iSAID datasets demonstrate that our approach achieves state-of-the-art (SOTA) performance, closely matching that of fully supervised methods. Our method not only reduces the dependency on expensive pixel-level annotations but also showcases the potential of SSL in enhancing WSSS tasks.",Accuracy,Remote sensing,Semantics,Semantic segmentation,Task analysis,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Training,Noise,Image-level label,pseudo-label noise suppression,remote sensing images (RSIs),self-supervised learning (SSL),weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,
Row_310,"Lu, Xiaoqiang","Jiao, Licheng","Liu, Fang","Yang, Shuyuan",Simple and Efficient: A Semisupervised Learning Framework for Remote Sensing Image Semantic Segmentation,,2022,19,"Semantic segmentation based on deep learning has achieved impressive results in recent years, but these results are supported by a large amount of labeled data, which requires intensive annotation at the pixel level, particularly for high-resolution remote sensing (RS) images. In this work, we propose a simple yet efficient semisupervised learning framework based on linear sampling (LS) self-training, named LSST, to improve the performance of RS image semantic segmentation. Specifically, the classical pseudolabeling-based self-training paradigm is enhanced by injecting strong data augmentations (SDAs) applicable to RS images, based on which a powerful baseline is constructed. Nevertheless, the problem of insufficient data training to generate pseudolabels with a high level of noise persists, and the noisy pseudolabels will continue to accumulate and impede model improvement during the retraining phase. Previous works commonly employ a predefined threshold to remove noise, but it will lead to overfitting the model to easily identified classes. To address it, a method using LS is presented for assigning thresholds to different classes in an adaptive manner, which provides noiseless regions for retraining. Experiments prove that the proposed pixelwise selection is more available for segmentation than image-level selection in RS images. Finally, LSST achieves state of the art on several datasets and different evaluation metrics.",Data augmentation,remote sensing (RS) images,self-training,semantic segmentation,semisupervised learning (SSL),"Liu, Xu","Feng, Zhixi","Li, Lingling","Chen, Puhua",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_311,"Zhang, Li","Tan, Zhenshan","Zhang, Guo","Zhang, Wen",Learn More and Learn Usefully: Truncation Compensation Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,2024,3,"Semantic segmentation of high-resolution remote-sensing images (HR-RSIs) focuses on classifying each pixel of input images. Recent methods have incorporated a downscaled global image as supplementary input to alleviate global context loss from cropping. Nonetheless, these methods encounter two key challenges: diminished detail in features due to downsampling of the global auxiliary image (GAI) and noise from the same image that reduces the network's discriminability of useful and useless information. To overcome these challenges, we propose a truncation compensation network (TCNet) for HR-RSI semantic segmentation. TCNet features three pivotal modules: the guidance feature extraction module (GFM), the related-category semantic enhancement module (RSEM), and the global-local contextual cross-fusion module (CFM). GFM focuses on compensating for truncated features in the local image and minimizing noise to emphasize learning of useful information. RSEM enhances discernment of global semantic information by predicting spatial positions of related categories and establishing spatial mappings for each. CFM facilitates local image semantic segmentation with extensive contextual information by transferring information from global to local feature maps. Extensive testing on the ISPRS, BLU, and GID datasets confirms the superior efficiency of TCNet over other approaches.",Semantic segmentation,Semantics,Remote sensing,Feature extraction,Aggregates,"Li, Zhijiang",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Transformers,Decoding,Context fusion,remote-sensing images,semantic enhancement,semantic segmentation,useful information,,,,,,,,,,,,,,,
Row_312,"Fan, Junyu","Li, Jinjiang","Hua, Zhen","Zhang, Fan",Elevation Information-Guided Multimodal Fusion Robust Framework for Remote Sensing Image Segmentation,,2024,1,"Currently, the task of remote sensing image segmentation still faces some challenges, such as variations in illumination, shadows, and occlusions present in remote sensing images. In addition, there may be similarities and confusions between different types of terrain features. In this letter, we aim to explore how to use information exchange between multiple modalities to reduce the impact of interfering factors. To fully exploit the complementary information between different modalities, we establish an information exchange mechanism between optical images (visible light + infrared) features and digital surface model (DSM) features. This allows them to interact and express themselves in a shared feature space, facilitating the acquisition of complementary information from different modalities. Furthermore, through a multimodal fusion encoder and decoder based on transformer design, the optical features and DSM features are integrated, enabling the learning of high-level semantic representations in different dimensions. Extensive subjective, objective comparative experiments, and ablation experiments are conducted on the ISPRS Vaihingen and Potsdam datasets to evaluate the proposed method. The mIoU on the Vaihingen and Potsdam datasets reached 85.06% and 87.6%, respectively, while the OA reached 92.01% and 91.92%, respectively. The source code will be available at https://github.com/JunyuFan/MIEFNet.",Multimodal,remote sensing,semantic segmentation,transformer,,"Zhang, Caiming",,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,,,,,,,,,,,,,,,,,,,,,,
Row_313,"Ding, Lei","Zhang, Jing","Bruzzone, Lorenzo",,Semantic Segmentation of Large-Size VHR Remote Sensing Images Using a Two-Stage Multiscale Training Architecture,,AUG 2020,106,"Very-high resolution (VHR) remote sensing images (RSIs) have significantly larger spatial size compared to typical natural images used in computer vision applications. Therefore, it is computationally unaffordable to train and test classifiers on these images at a full-size scale. Commonly used methodologies for semantic segmentation of RSIs perform training and prediction on cropped image patches. Thus, they have the limitation of failing to incorporate enough context information. In order to better exploit the correlations between ground objects, we propose a deep architecture with a two-stage multiscale training strategy that is tailored to the semantic segmentation of large-size VHR RSIs. In the first stage of the training strategy, a semantic embedding network is designed to learn high-level features from downscaled images covering a large area. In the second training stage, a local feature extraction network is designed to introduce low-level information from cropped image patches. The resulting training strategy is able to fuse complementary information learned from multiple levels to make predictions. Experimental results on two data sets show that it outperforms local-patch-based training models in terms of both accuracy and stability.",Convolutional neural network,deep learning,remote sensing,semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_314,"Wang, Xin","Jing, Shihan","Dai, Huifeng","Shi, Aiye",High-resolution remote sensing images semantic segmentation using improved UNet and SegNet,,MAY 2023,10,"Semantic segmentation for high-resolution (HR) remote sensing (RS) images under the condition of a small number of training samples with imprecise labels is a challenging task. In this paper, we propose a novel method based on improved UNet and SegNet. First, a batch normalization layer is introduced into the original UNet to accelerate the convergence speed and an ELU activation function is selected instead of ReLU to avoid the neuron death. Second, to enhance the conventional SegNet, the encoder is reconstructed and a skip connection is designed to reuse the deep features, which is also beneficial for the network convergence. Third, a joint model is constructed by combining the improved UNet and SegNet and meanwhile a voting strategy is proposed to compute the final results. Experiments on the real-world HR RS images verify the effectiveness and superiority of the proposed method.",Remote sensing,Semantic segmentation,Deep learning,UNet,SegNet,,,,,COMPUTERS & ELECTRICAL ENGINEERING,,,,,,,,,,,,,,,,,,,,,,,
Row_315,"Chang, Zhanyuan","Xu, Mingyu","Wei, Yuwen","Lian, Jie",UNeXt: An Efficient Network for the Semantic Segmentation of High-Resolution Remote Sensing Images,,OCT 2024,0,"The application of deep neural networks for the semantic segmentation of remote sensing images is a significant research area within the field of the intelligent interpretation of remote sensing data. The semantic segmentation of remote sensing images holds great practical value in urban planning, disaster assessment, the estimation of carbon sinks, and other related fields. With the continuous advancement of remote sensing technology, the spatial resolution of remote sensing images is gradually increasing. This increase in resolution brings about challenges such as significant changes in the scale of ground objects, redundant information, and irregular shapes within remote sensing images. Current methods leverage Transformers to capture global long-range dependencies. However, the use of Transformers introduces higher computational complexity and is prone to losing local details. In this paper, we propose UNeXt (UNet+ConvNeXt+Transformer), a real-time semantic segmentation model tailored for high-resolution remote sensing images. To achieve efficient segmentation, UNeXt uses the lightweight ConvNeXt-T as the encoder and a lightweight decoder, Transnext, which combines a Transformer and CNN (Convolutional Neural Networks) to capture global information while avoiding the loss of local details. Furthermore, in order to more effectively utilize spatial and channel information, we propose a SCFB (SC Feature Fuse Block) to reduce computational complexity while enhancing the model's recognition of complex scenes. A series of ablation experiments and comprehensive comparative experiments demonstrate that our method not only runs faster than state-of-the-art (SOTA) lightweight models but also achieves higher accuracy. Specifically, our proposed UNeXt achieves 85.2% and 82.9% mIoUs on the Vaihingen and Gaofen5 (GID5) datasets, respectively, while maintaining 97 fps for 512 x 512 inputs on a single NVIDIA GTX 4090 GPU, outperforming other SOTA methods.",high-resolution remote sensing images,real-time semantic segmentation,convolutional attention,global-local context,transformer,"Zhang, Chongming","Li, Chuanjiang",,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,
Row_316,"Zhou, Shunping","Feng, Yuting","Li, Shengwen","Zheng, Daoyuan",DSM-Assisted Unsupervised Domain Adaptive Network for Semantic Segmentation of Remote Sensing Imagery,,2023,11,"The semantic segmentation of high-resolution remote sensing imagery (RSI) is an essential task for many applications. As a promising unsupervised learning method, unsupervised domain adaptation (UDA) methods remarkably contribute to the advancement of high-resolution RSI semantic segmentation. Previous methods focus on reducing the domain shift of orthophotos, suffering from some limitations because the available information in orthophotos is relatively homogeneous. This article proposes a framework to introduce digital surface model (DSM) data for the unsupervised semantic segmentation of RSI. The proposed method combines RSI with DSM through two modules, namely, multipath encoder (MPE) and multitask decoder (MTD), and aligns global data distribution in the source and target domains with a UDA module. A refined postfusion (RPF) module is proposed in the inference phase to exploit the height information fully for refining the segmentation results. Specifically, MPE is designed to utilize RSI and DSM to train the segmentation network jointly, which iteratively fuses RSI and DSM features at multiple levels to enhance their feature representations. MTD is designed to produce fusion prediction maps by filtering interference information of DSM and yielding accurate segmentation masks of DSM and RSI. Experimental results show that the proposed method substantially improves the semantic segmentation performance on high-resolution RSI and outperforms state-of-the-art methods. This article provides a methodological reference for fusing multimodal data in various RSI-based unsupervised tasks.",Semantic segmentation,Task analysis,Semantics,Remote sensing,Geology,"Fang, Fang","Liu, Yuanyuan","Wan, Bo",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Feature extraction,Data models,High-resolution remote sensing imagery (RSI),refined postfusion (RPF),semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,
Row_317,"Liu, Yuheng","Wang, Ye","Zhang, Yifan","Mei, Shaohui",Cross-City Semantic Segmentation (C2Seg) in Multimodal Remote Sensing: Outcome of the 2023 IEEE WHISPERS C2Seg Challenge,,2024,1,"Given the ever-growing availability of remote sensing data (e.g., Gaofen in China, Sentinel in the EU, and Landsat in the USA), multimodal remote sensing techniques have been garnering increasing attention and have made extraordinary progress in various Earth observation (EO)-related tasks. The data acquired by different platforms can provide diverse and complementary information. The joint exploitation of multimodal remote sensing has been proven effective in improving the existing methods of land-use/land-cover segmentation in urban environments. To boost technical breakthroughs and accelerate the development of EO applications across cities and regions, one important task is to build novel cross-city semantic segmentation models based on modern artificial intelligence technologies and emerging multimodal remote sensing data. This leads to the development of better semantic segmentation models with high transferability among different cities and regions. The Cross-City Semantic Segmentation contest is organized in conjunction with the 13th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS).",Artificial intelligence (AI),cross-city,deep learning,hyperspectral,land cover,"Zou, Jiaqi","Li, Zhuohong","Lu, Fangxiao","He, Wei",IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Zhang, Hongyan",multimodal benchmark datasets,remote sensing,semantic segmentation,,,,,,,"Zhao, Huilin","Chen, Chuan","Xia, Cong","Li, Hao","Vivone, Gemine","Haensch, Ronny","Taskin, Gulsen","Yao, Jing","Qin, A. K.","Zhang, Bing","Chanussot, Jocelyn","Hong, Danfeng",
Row_318,"Lu, Junzhe","He, Guangjun","Dou, Hongkun","Gao, Qing",ScoreSeg: Leveraging Score-Based Generative Model for Self-Supervised Semantic Segmentation of Remote Sensing,,2023,3,"The performance of semantic segmentation of remote sensing images (RSIs) heavily depends on the number of pixel-level annotations. In practice, the accumulation of pixel-level annotations for large RSIs is quite expensive or even impossible under certain scenarios. Here, we try to solve this data-intensive problem from the novel aspect of score-based self-supervise learning (SSL) and introduce a robust RSI semantic segmentation model called ScoreSeg. Unlike traditional pixel-level SSL paradigms, the generative SSL mechanism in ScoreSeg is simple in loss design and stable in pretraining, granting it an indispensable ability in dense feature learning from very large RSIs. In the model implementation, ScoreSeg first extracts pixelwise representations of RSIs by pretraining a time-dependent score-based model on abundant off-the-shelf unlabeled RSIs. Then, to address the sparse feature problem in RSIs, the collected features from different timesteps and resolutions are aggregated together forming a rich feature map for downstream semantic segmentation. Experimental results on three datasets show that our proposed ScoreSeg outperforms state-of-the-art (SOTA) SSL methods and alternative pretraining models on ImageNet by nontrivial margins, especially with very limited annotations.",Remote sensing,score-based generative model,self-supervised learning,semantic segmentation,,"Fang, Leyuan","Deng, Yue",,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_319,"Zhang, Hua","Jiang, Zhengang","Zheng, Guoxun","Yao, Xuekun",Semantic Segmentation of High-Resolution Remote Sensing Images with Improved U-Net Based on Transfer Learning,,NOV 14 2023,5,"Semantic segmentation of high-resolution remote sensing images has emerged as one of the foci of research in the remote sensing field, which can accurately identify objects on the ground and determine their localization. In contrast, the traditional deep learning-based semantic segmentation, on the other hand, requires a large amount of annotated data, which is unsuitable for high-resolution remote sensing tasks with limited resources. It is therefore important to build a semantic segmentation method for high-resolution remote sensing images. In this paper, it is proposed an improved U-Net model based on transfer learning to solve the semantic segmentation problem of high-resolution remote sensing images. The model is based on the symmetric encoder-decoder structure of U-Net. For the encoder, transfer learning is applied and VGG16 is used as the backbone of the feature extraction network, and in the decoder, after upsampling using bilinear interpolation, it is performed multiscale fusion with the feature maps of the corresponding layers of the encoder in turn and is finally obtained the predicted value of each pixel to achieve precise localization. To verify the efficacy of the proposed network, experiments are performed on the ISPRS Vaihingen dataset. The experiments show that the applied method has achieved high-quality semantic segmentation results on the high-resolution remote sensing dataset, and the MIoU is 1.70%, 2.20%, and 2.33% higher on the training, validation, and test sets, respectively, and the IoU is 4.26%, 6.89%, and 5.44% higher for the automotive category compared to the traditional U-Net.",Deep learning,Convolutional neural network,Semantic segmentation,Transfer learning,High-resolution remote sensing images,,,,,INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS,,,,,,,,,,,,,,,,,,,,,,,
Row_320,"Li, Zhenshi","Zhang, Xueliang","Xiao, Pengfeng","Zheng, Zixian",On the Effectiveness of Weakly Supervised Semantic Segmentation for Building Extraction From High-Resolution Remote Sensing Imagery,,2021,48,"A critical obstacle to achieve semantic segmentation of remote sensing images by the deep convolutional neural network is the requirement of huge pixel-level labels. Taking building extraction as an example, this study focuses on how to effectively apply weakly supervised semantic segmentation (WSSS) to high-resolution remote sensing (HR) images with image-level labels, which is a prominent solution for the huge labeling challenge. The widely used two-step WSSS framework is adopted, in which the pseudo-masks are first produced from image-level labels and followed by a segmentation network trained by the pseudo-masks. In addition, the fully connected conditional random field (CRF) is utilized to explore spatial context in both training and prediction stages. Detailed analyzes are implemented on applying WSSS on HR images in terms of producing pseudo-masks, training segmentation network, and optimizing predictions. We show that the tradeoff between precision and recall of pseudo-masks, as well as the boundary accuracy and the background, needs to be carefully considered. The benefits of the segmentation network in the two-step framework are demonstrated in comparison to using classification network only for WSSS, and the effects of CRF-loss are identified to be powerful for improving the segmentation network while it is not appropriate for dense buildings. An overlapping strategy and CRF postprocessing are further demonstrated to be effective for optimizing the segmentation results during inferencing. Through deliberate settings, we can generate results comparable to fully supervised on the ISPRS Potsdam and Vaihingen dataset, which is meaningful for promoting WSSS applications for extracting geographic information from HR images.",Image segmentation,Training,Buildings,Remote sensing,Semantics,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Feature extraction,Data mining,Building extraction,fully convolutional network,high-resolution remote sensing imagery,weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,,
Row_321,"Gao, Zhi","Sun, Wenbo","Lu, Yao","Zhang, Yichen",Joint Learning of Semantic Segmentation and Height Estimation for Remote Sensing Image Leveraging Contrastive Learning,,2023,9,"Semantic segmentation (SS) and height estimation (HE) are two critical tasks in remote sensing scene understanding that are highly correlated with each other. To address both the tasks simultaneously, it is natural to consider designing a unified deep learning model that aims to improve performance by jointly learning complementary information among the associated tasks. In this article, we learn the two tasks jointly under a deep multitask learning (MTL) framework and propose two novel objective functions, called cross-task contrastive (CTC) loss and cross-pixel contrastive (CPC) loss, respectively, to enhance MTL performance through contrastive learning. Specifically, the CTC loss is designed to maximize the mutual information of different task features and enforce the model to learn the consistency between SS and height estimation. In addition, our method goes beyond previous approaches that only apply contrastive learning at the instance level. Instead, we design a pixelwise contrastive loss function that pulls together pixel embeddings belonging to the same semantic class, while pushing apart pixel embeddings from different semantic classes. Furthermore, we find that this semantic-guided contrastive loss simultaneously improves the performance of the HE task. Our proposed approach is simple and effective and does not introduce any additional overhead to the model during the testing phase. We extensively evaluate our method on the Vaihingen and Potsdam datasets, and the experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods in both HE and SS.",Contrastive learning,height estimation,multi-task learning (MTL),remote sensing,semantic segmentation (SS),"Song, Weiwei","Zhang, Yongjun","Zhai, Ruifang",,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_322,"Tan, Xiaowei","Xiao, Zhifeng","Zhang, Yanru","Wang, Zhenjiang",Context-Driven Feature-Focusing Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,MAR 2023,0,"High-resolution remote sensing images (HRRSIs) cover a broad range of geographic regions and contain a wide variety of artificial objects and natural elements at various scales that comprise different image contexts. In semantic segmentation tasks based on deep convolutional neural networks (DCNNs), different resolution features are not equally effective for extracting ground objects with different scales. In this article, we propose a novel context-driven feature-focusing network (CFFNet) aimed at focusing on the multi-scale ground object in fused features of different resolutions. The CFFNet consists of three components: a depth-residual encoder, a context-driven feature-focusing (CFF) decoder, and a classifier. First, features with different resolutions are extracted using the depth-residual encoder to construct a feature pyramid. The multi-scale information in the fused features is then extracted using the feature-focusing (FF) module in the CFF decoder, followed by computing the focus weights of different scale features adaptively using the context-focusing (CF) module to obtain the weighted multi-scale fused feature representation. Finally, the final results are obtained using the classifier. The experiments are conducted on the public LoveDA and GID datasets. Quantitative and qualitative analyses of state-of-the-art (SOTA) segmentation benchmarks demonstrate the rationality and effectiveness of the proposed approach.",deep learning,feature-focusing,semantic segmentation,remote sensing,attention,"Qi, Xiaole","Li, Deren",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_323,"Hu, Lei","Zhou, Xun","Ruan, Jiachen","Li, Supeng",ASPP+-LANet: A Multi-Scale Context Extraction Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,MAR 2024,3,"Semantic segmentation of remote sensing (RS) images is a pivotal branch in the realm of RS image processing, which plays a significant role in urban planning, building extraction, vegetation extraction, etc. With the continuous advancement of remote sensing technology, the spatial resolution of remote sensing images is progressively improving. This escalation in resolution gives rise to challenges like imbalanced class distributions among ground objects in RS images, the significant variations of ground object scales, as well as the presence of redundant information and noise interference. In this paper, we propose a multi-scale context extraction network, ASPP+-LANet, based on the LANet for semantic segmentation of high-resolution RS images. Firstly, we design an ASPP+ module, expanding upon the ASPP module by incorporating an additional feature extraction channel, redesigning the dilation rates, and introducing the Coordinate Attention (CA) mechanism so that it can effectively improve the segmentation performance of ground object targets at different scales. Secondly, we introduce the Funnel ReLU (FReLU) activation function for enhancing the segmentation effect of slender ground object targets and refining the segmentation edges. The experimental results show that our network model demonstrates superior segmentation performance on both Potsdam and Vaihingen datasets, outperforming other state-of-the-art (SOTA) methods.",high-resolution remote sensing images,semantic segmentation,ASPP module,local attention network model,activation function,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_324,"Chen, Chao","Qian, Yurong","Liu, Hui","Yang, Guangqi",CLANET: a cross-linear attention network for semantic segmentation of urban scenes remote sensing images,,DEC 2 2023,0,"Semantic segmentation of high-resolution remote sensing images is important in land cover classification, road extraction, building extraction, water extraction, etc. However, high-resolution remote-sensing images have a lot of details. Due to the fixed receptive field of convolution blocks, it is impossible to model the correlation of global features. In addition, complex fusion methods cannot integrate spatial and global context information. In order to solve these problems, this paper proposes a cross-linear attention network (CLANet) to capture spatial and context information in images. The structure consists of a spatial branch and a context branch. The spatial branch is constructed by stacked convolution to better capture spatial information. The context branch models the global information based on the transformer deformation module. In addition, to effectively fuse spatial and context information, this paper also designs a feature fusion module (FFM), which uses a cross-linear attention mechanism for feature aggregation. Finally, this paper conducts many experiments on the ISPRS Vaihingen and the ISPRS Potsdam datasets. Among them, 82.28% of mIoU achieves on the ISPRS Vaihingen dataset. The experimental results show that CLANet has better performance and effect than the methods in recent years.",Remote sensing,semantic segmentation,cross attention,convolutional neural network,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_325,"He, Shumeng","Yang, Houqun","Zhang, Xiaoying","Li, Xuanyu",MFTransNet: A Multi-Modal Fusion with CNN-Transformer Network for Semantic Segmentation of HSR Remote Sensing Images,,FEB 2023,9,"Due to the inherent inter-class similarity and class imbalance of remote sensing images, it is difficult to obtain effective results in single-source semantic segmentation. We consider applying multi-modal data to the task of the semantic segmentation of HSR (high spatial resolution) remote sensing images, and obtain richer semantic information by data fusion to improve the accuracy and efficiency of segmentation. However, it is still a great challenge to discover how to achieve efficient and useful information complementarity based on multi-modal remote sensing image semantic segmentation, so we have to seriously examine the numerous models. Transformer has made remarkable progress in decreasing model complexity and improving scalability and training efficiency in computer vision tasks. Therefore, we introduce Transformer into multi-modal semantic segmentation. In order to cope with the issue that the Transformer model requires a large amount of computing resources, we propose a model, MFTransNet, which combines a CNN (convolutional neural network) and Transformer to realize a lightweight multi-modal semantic segmentation structure. To do this, a small convolutional network is first used for performing preliminary feature extraction. Subsequently, these features are sent to the multi-head feature fusion module to achieve adaptive feature fusion. Finally, the features of different scales are integrated together through a multi-scale decoder. The experimental results demonstrate that MFTransNet achieves the best balance among segmentation accuracy, memory-usage efficiency and inference speed.",semantic segmentation,high spatial resolution remote sensing images,transformer,multi-modal,,,,,,MATHEMATICS,,,,,,,,,,,,,,,,,,,,,,,
Row_326,"Chen, Junsong","Yi, Jizheng","Chen, Aibin","Lin, Hui",SRCBTFusion-Net: An Efficient Fusion Architecture via Stacked Residual Convolution Blocks and Transformer for Remote Sensing Image Semantic Segmentation,,2023,4,"Convolutional neural network (CNN) and transformer-based self-attention models have their advantages in extracting local information and global semantic information, and it is a trend to design a model combining stacked residual convolution blocks (SRCBs) and transformer. How to efficiently integrate the two mechanisms to improve the segmentation effect of remote sensing (RS) images is an urgent problem to be solved. An efficient fusion via SRCB and transformer (SRCBTFusion-Net) is proposed as a new semantic segmentation architecture for RS images. The SRCBTFusion-Net adopts an encoder-decoder structure and the Transformer is embedded into SRCB to form a double coding structure, then the coding features are upsampled and fused with multiscale features of SRCB to form a decoding structure. First, a semantic information enhancement module (SIEM) is proposed to get global clues for enhancing deep semantic information. Subsequently, the relationship guidance module (RGM) is incorporated to reencode the decoder's upsampled feature maps, enhancing the edge segmentation performance. Second, a multipath atrous self-attention module (MASM) is developed to enhance the effective selection and weighting of low-level features, effectively reducing the potential confusion introduced by the skip connections between low- and high-level features. Finally, a multiscale feature aggregation module (MFAM) is developed to enhance the extraction of semantic and contextual information, thus alleviating the loss of image feature information and improving the ability to identify similar categories. The proposed SRCBTFusion-Net's performance on the Vaihingen and Potsdam datasets is superior to the state-of-the-art methods. The code will be freely available at https://github.com/js257/SRCBTFusion-Net.",Multiscale feature,remote sensing (RS),semantic segmentation,transformer,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_327,"Dong, Zhe","Gao, Guoming","Liu, Tianzhu","Gu, Yanfeng",Distilling Segmenters From CNNs and Transformers for Remote Sensing Images' Semantic Segmentation,,2023,16,"Semantic segmentation is a crucial task in remote sensing and has been predominantly performed using convolutional neural networks (CNNs) for the past decade. Recently, transformers with self-attention mechanisms have demonstrated superior performance compared with CNNs. However, due to the locality of CNN and the high computational complexity and massive data resource requirements of transformer, neither of them can be well applied in resource-constrained practical remote sensing scenarios. Motivated by the limitations of using either CNNs or transformers alone in the task of semantic segmentation of remote sensing images, a novel cross-model knowledge distillation (KD) framework, named distilling segmenters from CNNs and transformers (DSCTs), is proposed in this article to harness the complementary advantages of both the models. The framework uses a channel-weighted attention-guided feature distillation (CAFD) module to condense the feature from the teacher model and enhance the student model's focus on the teacher-focused regions. In addition, a target-nontarget KD (TNKD) module is proposed that decouples logit distillation into target and nontarget KD to guide the student model in learning the underlying representations and decision boundaries from the teacher model. By learning the complementary knowledge from the teacher, our proposed DSCT framework improves the student's segmentation performance without adding trainable parameters. Experiments on four available remote sensing datasets (ISPRS Potsdam, Vaihingen, GID, and LoveDA) indicate that the proposed DSCT outperforms the state-of-the-art KD methods and demonstrates its effectiveness and robustness.",Convolutional neural networks (CNNs),knowl-edge distillation (KD),remote sensing images,semantic segmen-tation,transformer,"Zhang, Xiangrong",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_328,"Chen, Xiaoshu","Pan, Shaoming","Chong, Yanwen",,Unsupervised Domain Adaptation for Remote Sensing Image Semantic Segmentation Using Region and Category Adaptive Domain Discriminator,,2022,23,"By reason of factors such as terrains, weather conditions, sensor imaging methods, and cultural and economic development, there is a large shift between the remote sensing imagery collected from different geographic locations and different sensors, which makes the state-of-the-art semantic segmentation models trained on source domain (an image set gathered from specific geographic locations and sensors) difficult to generalize to target domain (another image set collected from other geographic locations and sensors). Currently, unsupervised domain adaptation (UDA) using adversarial training, whose purpose is to align the marginal distribution in the output space between the source and target domains, is the most explored and practical approach to address this issue. However, this global alignment approach does not take into account diversities of different regions in a specific image nor the category-level distribution, which leads to the consequence that some regions and categories which are already well aligned between the source and target domains may be incorrectly remapped. Therefore, we propose a region and category adaptive domain discriminator (RCA-DD), aiming to emphasize the differences in regions and categories during the process of alignment. Specifically, on the one hand, we propose an entropy-based regional attention module (ERAM) in domain discriminator to emphasize the importance of difficult-to-align regions. On the other hand, we propose a class-clear module (CCM) to update only the distribution of existing categories in one iteration without affecting all categories. Finally, a lot of experiments are introduced to indicate that the proposed method can obtain better results when compared with other state-of-the-art UDA methods using adversarial training.",Image segmentation,Semantics,Remote sensing,Training,Adaptation models,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Entropy,Predictive models,Remote sensing image processing,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,
Row_329,"Bousbih, Safa","Chan-Hon-Tong, Adrien","Lenczner, Gaston",,WHAT COULD WE LEARN FROM MANY DATASETS IN REMOTE SENSING ROOF SEMANTIC SEGMENTATION?,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,1,"Deep learning models trained for roof semantic segmentation from remote sensing images may suffer from low performance when applied in new cities as roof appearance may be different depending on the geographic areas. However, we claim in this paper that models learned on many different cities have better performances on a new city than a model learned on few data from this targeted city. This result may interest the community for industrial purpose: annotating few tiles of an area for simple supervised training is worthless in our experiments.",semantic segmentation,geographic robustness,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_330,"Wu, Zhipeng","Liu, Chang","Song, Bingze","Pei, Huaxin",Diff-HRNet: A Diffusion Model-Based High-Resolution Network for Remote Sensing Semantic Segmentation,,2025,0,"The semantic segmentation methods based on deep neural networks predominantly employ supervised learning, relying heavily on the quantity and quality of annotated samples. Due to the complexity of high-resolution remote sensing imagery, obtaining sufficient and precise pixel-level labeled data is highly challenging. This letter introduces a novel self-supervised learning method using a pretrained denoising diffusion probabilistic model (DDPM) to leverage semantic information from large-scale unlabeled remote sensing imageries. Building on this, a multistage fusion scheme between pretrained features and high-resolution features is proposed, enabling the network to learn more effective strategies to leverage prior information provided by the pretrained model while preserving the rich semantic details of high-resolution images. Experimental results on two remote sensing semantic segmentation datasets show that the proposed Diff-HRNet outperforms all compared methods, demonstrating the potential of pretrained diffusion models in extracting crucial feature representations for semantic segmentation tasks.",Feature extraction,Remote sensing,Semantics,Spatial resolution,Diffusion models,"Li, Pinjie","Chen, Mengshuo",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Noise reduction,Noise measurement,Convolution,Supervised learning,Denoising diffusion probabilistic model (DDPM),self-supervised learning,semantic segmentation,semantic segmentation,,,,,,,,,,,,,,
Row_331,"Zhang, Shijie","Zhang, Bin","Wu, Yuntao","Zhou, Huabing",SegCLIP: Multimodal Visual-Language and Prompt Learning for High-Resolution Remote Sensing Semantic Segmentation,,2024,0,"Remote sensing semantic segmentation is considered a key step in the intelligent interpretation of high-resolution remote sensing (HRRS) images, with widespread applications in fields such as hazard assessment, environmental monitoring, and urban planning. Recently, numerous deep learning-based semantic segmentation methods have emerged, achieving significant breakthroughs. However, the majority of current research still concentrates on representation learning in the visual feature space, with the potential of multimodal data sources yet to be fully explored. In recent years, the foundational visual language model, namely contrastive language-image pretraining (CLIP), has established a new paradigm in the visual field, demonstrating excellent generalization capabilities and deep semantic understanding across a variety of tasks. Inspired by prompt learning, we propose a prompting approach based on linguistic descriptions to enable CLIP to generate semantically distinct contextual information for remote sensing images. We introduce the SegCLIP network architecture, a novel framework specifically designed for semantic segmentation of HRRS images. Specifically, we have adapted CLIP to extract text information, thereby guiding the visual model in distinguishing among classes. Additionally, we have designed a cross-modal feature fusion (CFF) module that integrates linguistic and visual semantic features, ensuring semantic consistency across modalities. Finally, we have fully exploited the potential of text data and have used additional real text to refine ambiguous query features. Experimental evaluations confirm that the method exhibits superior performance on the LoveDA, iSAID, and UAVid public semantic segmentation datasets.",Semantic segmentation,Remote sensing,Semantics,Visualization,Transformers,"Jiang, Junjun","Ma, Jiayi",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Linguistics,Feature extraction,Sensors,Accuracy,Laser radar,Attention mechanism,contrastive language-image pretraining (CLIP),prompt learning,remote sensing,,,,,,,,,,,,,semantic segmentation
Row_332,"Liu, Kuan-Hsien","Lin, Bo-Yen",,,MSCSA-Net: Multi-Scale Channel Spatial Attention Network for Semantic Segmentation of Remote Sensing Images,,SEP 2023,13,"Although deep learning-based methods for semantic segmentation have achieved prominent performance in the general image domain, semantic segmentation for high-resolution remote sensing images remains highly challenging. One challenge is the large image size. High-resolution remote sensing images can have very high spatial resolution, resulting in images with hundreds of millions of pixels. This makes it difficult for deep learning models to process the images efficiently, as they typically require large amounts of memory and computational resources. Another challenge is the complexity of the objects and scenes in the images. High-resolution remote sensing images often contain a wide variety of objects, such as buildings, roads, trees, and water bodies, with complex shapes and textures. This requires deep learning models to be able to capture a wide range of features and patterns to segment the objects accurately. Moreover, remote sensing images can suffer from various types of noise and distortions, such as atmospheric effects, shadows, and sensor noises, which can also increase difficulty in segmentation tasks. To deal with the aforementioned challenges, we propose a new, mixed deep learning model for semantic segmentation on high-resolution remote sensing images. Our proposed model adopts our newly designed local channel spatial attention, multi-scale attention, and 16-piece local channel spatial attention to effectively extract informative multi-scale features and improve object boundary discrimination. Experimental results with two public benchmark datasets show that our model can indeed improve overall accuracy and compete with several state-of-the-art methods.",channel spatial attention,encoder-decoder,multi-scale attention,remote sensing image,semantic segmentation,,,,,APPLIED SCIENCES-BASEL,,,,,,,,,,,,,,,,,,,,,,,
Row_333,"Chen, Yan","Zhang, Qianchuan","Wang, Xiaofeng","Dong, Quan",AANet: Adaptive Attention Networks for Semantic Segmentation of High-Resolution Remote Sensing Imagery,,2024,0,"Contextual information can effectively aid deep-learning models in extracting interclass and intraclass difference features in remote sensing images. This article presents a novel approach called the adaptive attention network (AANet) for semantic segmentation in high-resolution remote sensing images. The proposed AANet aims to enhance the segmentation performance while minimizing the network's computational and parametric aspects. Furthermore, the AANet is designed to facilitate real-time segmentation. The AANet involves the construction of three distinct modules, namely the multiscale channel attention module (MCAM), the multidimensional spatial attention module (MSAM), and the contextual information adaptive fusion module (CIAFM). MCAM enhances a multiscale approach to effectively capture contextual information from neighboring channels and category information. MSAM is designed to extract and combine detailed information from each dimension of the spatial domain. CIAFM focuses on the complementary nature of channel and spatial context information and the correlation between pixels and categories. The methodology employed in this article involved conducting experiments on the ISPRS Vaihingen, ISPRS Potsdam, and multiobject coastal supervision semantic segmentation dataset (MO-CSSSD) datasets alongside a comparative analysis with conventional semantic segmentation models. The results of the article indicate that our approach demonstrates exceptional performance on the ISPRS Vaihingen dataset, ISPRS Potsdam dataset, and MO-CSSSD dataset, achieving mean intersection over union scores of 83.17%, 85.67%, and 89.68%, respectively.",Feature extraction,Data mining,Remote sensing,Semantic segmentation,Convolution,"Kang, Menglei","Jiang, Wenxiang","Wang, Mengyuan","Xu, Lixiang",IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,"Zhang, Chen",Semantics,Attention mechanisms,Adaptive attention,contextual information,high-resolution remote sensing imagery,multidimensional spatial attention,multiscale channel attention,,,,,,,,,,,,,,,
Row_334,"Zhang, Cheng","Jiang, Wanshou","Zhang, Yuan","Wang, Wei",Transformer and CNN Hybrid Deep Neural Network for Semantic Segmentation of Very-High-Resolution Remote Sensing Imagery,,2022,178,"This article presents a transformer and convolutional neural network (CNN) hybrid deep neural network for semantic segmentation of very high resolution (VHR) remote sensing imagery. The model follows an encoder-decoder structure. The encoder module uses a new universal backbone Swin transformer to extract features to achieve better long-range spatial dependencies modeling. The decoder module draws on some effective blocks and successful strategies of CNN-based models in remote sensing image segmentation. In the middle of the framework, an atrous spatial pyramid pooling block based on depthwise separable convolution (SASPP) is applied to obtain a multiscale context. A U-shaped decoder is used to gradually restore the size of the feature maps. Three skip connections are built between the encoder and decoder feature maps of the same size to maintain the transmission of local details and enhance the communication of multiscale features. A squeeze-and-excitation (SE) channel attention block is added before segmentation for feature augmentation. An auxiliary boundary detection branch is combined to provide edge constraints for semantic segmentation. Extensive ablation experiments were conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam benchmarks to test the effectiveness of multiple components of the network. At the same time, the proposed method is compared with the current state-of-the-art methods on the two benchmarks. The proposed hybrid network achieved the second highest overall accuracy (OA) on both the Potsdam and Vaihingen benchmarks (code and models are available at https://github.com/zq7734509/mmsegmentation- multilayer).",Transformers,Semantics,Image segmentation,Feature extraction,Remote sensing,"Zhao, Qing","Wang, Chenjie",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Decoding,Convolutional neural networks,Boundary detection,semantic segmentation,squeeze-and-excitation (SE) block,Swin transformer,very high resolution (VHR) remote sensing imagery,,,,,,,,,,,,,,,
Row_335,"Sun, Xian","Shi, Aijun","Huang, Hai","Mayer, Helmut",BAS4Net: Boundary-Aware Semi-Supervised Semantic Segmentation Network for Very High Resolution Remote Sensing Images,,2020,91,"Semantic segmentation is a fundamental task in remote sensing image understanding. Recently, Deep Convolutional Neural Networks (DCNNs) have considerably improved the performance of the semantic segmentation of natural scenes. However, it is still challenging for very high-resolution remote sensing images. Due to the large and complex scenes as well as the influence of illumination and imaging angle, it is particularly difficult for the existing methods to accurately obtain the category of pixels at object boundaries-the so-called boundary blur. We propose a framework called Boundary-Aware Semi-Supervised Semantic Segmentation Network (BAS(4)Net), which obtains more accurate segmentation results without additional annotation workload, especially at the object boundaries. The Channel-weighted Multi-scale Feature (CMF) module balances semantic and spatial information, and the Boundary Attention Module (BAM) weights the features with rich semantic boundary information to alleviate the boundary blur. Additionally, to decrease the amount of difficult and tedious manual labeling of remote sensing images, a discriminator network infers pseudolabels from unlabeled images to assist semisupervised learning, and further improves the performance of the segmentation network. To validate the effectiveness of the proposed framework, extensive experiments have been performed on both the ISPRS Vaihingen dataset, and the novel remote sensing dataset AIR-SEG with more categories, and complex boundaries. The results demonstrate a significant improvement of accuracy especially on boundaries and for small objects.",Hospitals,Collaboration,Lenses,Cybernetics,Systems thinking,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Economics,Boundary-aware,fully convolutional networks (FCNs),remote sensing images,semantic segmentation,semisupervised learning,,,,,,,,,,,,,,,,
Row_336,"Lang, FengKai","Zhang, Ming","Zhao, JinQi","Zheng, NanShan",Semantic segmentation for multisource remote sensing images incorporating feature slice reconstruction and attention upsampling,,APR 17 2024,0,"Multisource remote sensing images have rich features and high interpretability and are widely employed in many applications. However, highly unbalanced category distributions and complex backgrounds have created some difficulties in the application of remote sensing image semantic segmentation tasks, such as low accuracy of small target segmentation and inaccurate edge extraction. To solve these problems, in this paper, a feature map segmentation reconstruction module and an attention upsampling module are proposed. In the encoder part, the input feature map is equally segmented, and the segmented feature map is enlarged to effectively improve the small target feature information expression ability in the model. In the decoder part, the key segmentation and location information of shallow features are obtained using the global view. The deep semantic information and shallow spatial location information are fully combined to achieve a more refined upsampling operation. In addition, the attention mechanism of the spatial and channel squeeze and excitation block (scSE) is applied to pay more attention to important features and to suppress irrelevant background and redundant information. To verify the effectiveness of the proposed method, the WHU-OPT-SAR dataset and six state-of-the-art algorithms are utilized in comparative experiments. The experimental results show that our model has demonstrated the best performance and low computational complexity. With only approximately half the floating-point operation count and the number of model parameters of the MCANet model, which is specially designed for the dataset, our model surpasses MCANet by 1.52% and 1.53% in terms of mean intersection over union (mIoU) and F1 score, respectively. In particular, for small object regions such as roads and other categories, compared to the baseline model, the IoU and F1 score of our model are improved by 5.27% and 3.99% and by 5.68% and 5.65%, respectively. These results demonstrate the superior performance of our model in terms of accuracy and efficiency.",Remote sensing images,feature slice reconstruction,attention upsampling,attention mechanism,semantic segmentation,"Shi, Hongtao",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_337,"Cui, Binge","Chen, Xin","Lu, Yan",,Semantic Segmentation of Remote Sensing Images Using Transfer Learning and Deep Convolutional Neural Network With Dense Connection,,2020,56,"Semantic segmentation is an important approach in remote sensing image analysis. However, when segmenting multiobject from remote sensing images with insufficient labeled data and imbalanced data classes, the performances of the current semantic segmentation models were often unsatisfactory. In this paper, we try to solve this problem with transfer learning and a novel deep convolutional neural network with dense connection. We designed a UNet-based deep convolutional neural network, which is called TL-DenseUNet, for the semantic segmentation of remote sensing images. The proposed TL-DenseUNet contains two subnetworks. Among them, the encoder subnetwork uses a transferring DenseNet pretrained on three-band ImageNet images to extract multilevel semantic features, and the decoder subnetwork adopts dense connection to fuse the multiscale information in each layer, which can strengthen the expressive capability of the features. We carried out comprehensive experiments on remote sensing image datasets with 11 classes of ground objects. The experimental results demonstrate that both transfer learning and dense connection are effective for the multiobject semantic segmentation of remote sensing images with insufficient labeled data and imbalanced data classes. Compared with several other state-of-the-art models, the kappa coefficient of TL-DenseUNet is improved by more than 0.0752. TL-DenseUNet achieves better performance and more accurate segmentation results than the state-of-the-art models.",Dense connection,transfer learning,remote sensing image,multiscale feature fusion,semantic segmentation,,,,,IEEE ACCESS,,UNet,,,,,,,,,,,,,,,,,,,,,
Row_338,"Lv, Liang","Guo, Yiyou","Bao, Tengfei","Fu, Chenqin",MFALNet: A Multiscale Feature Aggregation Lightweight Network for Semantic Segmentation of High-Resolution Remote Sensing Images,,DEC 2021,12,"Semantic segmentation labels each pixel in high-resolution remote sensing (HRRS) images with a category. To tackle with the large size and complexity of HRRS images, this letter presents a novel multiscale feature aggregation lightweight network (MFALNet) for semantic segmentation. Unlike standard convolution, asymmetric depth-wise separable convolution residual (ADCR) unit is used to reduce the parameter size of the network and makes the optimized structure deeper but lightweight and less complex. The proposed network is an encoder-decoder structure, where multiscale feature aggregation is implemented in both the encoder and the decoder. The spatial self-attention block helps to capture long-range contextual information, and the gated convolution modules are further used for refining features when aggerating high- and low-level feature maps in the decoder. The proposed MFALNet has evaluated on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam 2-D semantic labeling contest open benchmark data set, and the experimental results prove that the scheme can obtain a better tradeoff between segmentation accuracy and computational efficiency compared with the state-of-the-art semantic segmentation models.",Semantics,Convolution,Image segmentation,Kernel,Decoding,"Huo, Hong","Fang, Tao",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Remote sensing,Feature extraction,Aggregate,gated convolution (GC),lightweight network,remote sensing,semantic segmentation,spatial attention,,,,,,,,,,,,,,
Row_339,"Liu, Wenjie","Zhang, Yongjun","Yan, Jun","Zou, Yongjie",Semantic Segmentation Network of Remote Sensing Images With Dynamic Loss Fusion Strategy,,2021,4,"The remote sensing (RS) images are widely used in various industries, among which semantic segmentation of RS images is a common research direction. At the same time, because of the complexity of target information and the high similarity of features between the classes, this task is very challenging. In recent years, semantic segmentation algorithms of RS images have emerged in an endless stream, but most of them are improved around the scale features of the target, and the accuracy has great room for improvement. In this case, we propose a semantic segmentation framework for RS images with dynamic perceptual loss. The framework is improved based on the InceptionV-4 network to form a network that includes contextual semantic fusion and dual-channel atrous spatial pyramid pooling (ASPP). The semantic segmentation network is an encoder-decoder structure. In addition, we design a dynamic perceptual loss module and a dynamic loss fusion strategy by further observing the loss changes of the network, so as to better improve the classified details. Finally, experiment on the ISPRS 2D Semantic Labeling Contest Vaihingen Dataset and Massachusetts Building Dataset. Compared with some segmentation networks, our model has excellent performance.",Image segmentation,Semantics,Convolution,Feature extraction,Task analysis,"Cui, Zhongwei",,,,IEEE ACCESS,,Deep learning,Training,Remote sensing,semantic segmentation,perceptual loss,loss fusion,,,,,,,,,,,,,,,,
Row_340,"Zheng, Xiaoxiong","Chen, Tao",,,SEGMENTATION OF HIGH SPATIAL RESOLUTION REMOTE SENSING IMAGE BASED ON U-NET CONVOLUTIONAL NETWORKS,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,10,"With the rapid development of deep learning in recent years, the field of remote sensing image processing has gradually started to use some deep learning algorithms to achieve intelligent and fast processing of images, and the results have improved to a certain extent compared to traditional methods. The U-Net convolutional neural network was proposed used in medical image segmentation in 2015. Based on the previous work, we transferred the U-Net to remote sensing image segmentation to realize the pixel level semantic segmentation of remote sensing image end-to-end. Through UNet training and learning on GF-2 remote sensing image, the overall accuracy of training sets is 93.83%, while the overall accuracy of the test data is 82.27%, the kappa coefficient is 0.7721, and the Mean intersection Over Union (MiOU) is 0.6405. The results showed that the experiment has high segmentation accuracy and generalization ability.",High spatial resolution remote sensing image,Semantic segmentation,U-Net,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_341,"Barbato, Mirko Paolo","Piccoli, Flavio","Napoletano, Paolo",,Ticino: A multi-modal remote sensing dataset for semantic segmentation,,SEP 1 2024,2,"Multi-modal remote sensing (RS) involves the fusion of data from multiple sensors, such as RGB, Multispectral, Hyperspectral, Light Detection and Ranging, Synthetic Aperture Radar, etc., each capturing unique information across different regions of the electromagnetic spectrum. The fusion of different modalities can provide complementary information, allowing for a comprehensive understanding of the Earth's surface. Multi-modal RS image segmentation leverages various RS modalities to achieve pixel-level semantics classification. While deep learning has demonstrated promise in this domain, the limited availability of labeled multi-modal data poses a constraint on leveraging data-intensive techniques like deep learning to their full potential. To address this gap, we present Ticino, a novel multi-modal remote sensing dataset tailored for semantic segmentation. Ticino includes five modalities, including RGB, Digital Terrain Model, Panchromatic, and Hyperspectral images within the visual-near and short-wave infrared spectrum. Specifically annotated for Land Cover and Soil Agricultural Use, the dataset serves as a valuable resource for researchers in the field. Additionally, we conduct a comparative analysis, comparing single-modality with multi-modality deep learning techniques and evaluating the effectiveness of early fusion versus middle fusion approaches. This work aims to facilitate future research efforts in the domain by providing a robust benchmark dataset and insights into the effectiveness of various segmentation approaches.",Multi -modal remote sensing,Image semantic segmentation,Deep learning,Multi -modal dataset,Data fusion,,,,,EXPERT SYSTEMS WITH APPLICATIONS,,Hyperspectral imaging,,,,,,,,,,,,,,,,,,,,,
Row_342,"Wang, Chendan","Chen, Bowen","Zou, Zhengxia","Shi, Zhenwei",Remote Sensing Image Synthesis via Semantic Embedding Generative Adversarial Networks,,2023,5,"Generating photo-realistic remote sensing images conditioned on semantic masks has many practical applications like image editing, detecting deep fake geography, and data augmentation. Although previous methods achieved high-quality synthesis results for natural images like faces and everyday objects, they still underperform in remote sensing scenarios in terms of both visual fidelity and diversity. The high data imbalance and high semantic similarity of remote-sensing object categories make the semantic synthesis of remote sensing images more challenging than natural images. To tackle these challenges, we propose a novel method named conducted semantic embedding GAN (CSEBGAN) for semantic-controllable remote sensing image synthesis. The proposed method decouples different semantic classes into independent semantic embeddings, which explores the regularities between classes to improve visual fidelity and naturally supports semantic-level. We further introduce a novel tripartite cooperation adversarial training scheme that involves a conductor network to provide fine-grained semantic feedback for the generator. We also show that the proposed semantic image synthesis method can be utilized as an effective data augmentation approach on improving the performance of the downstream remote sensing image segmentation tasks. Extensive experiments show the superiority of our method compared with the state-of-the-art image synthesis methods.",Semantics,Remote sensing,Image synthesis,Generators,Training,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Image segmentation,Visualization,Generative adversarial networks,image segmentation,remote sensing images,semantic image synthesis,,,,,,,,,,,,,,,,
Row_343,"Liu, Siyu","He, Changtao","Bai, Haiwei","Zhang, Yijie",LIGHT-WEIGHT ATTENTION SEMANTIC SEGMENTATION NETWORK FOR HIGH-RESOLUTION REMOTE SENSING IMAGES,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2020,12,"Semantic segmentation of high-resolution remote sensing (HRRS) images becomes more and more important at present. Popular approaches use deep learning to solve this task, which depends on a large amount of labeled data and powerful computing resources. When computing resources or the labeled data are insufficient, their performance will be severely degraded. To deal with this problem, we proposed a light-weight network with attention modules for semantic segmentation of HRRS images. The depth and width of the network are designed, which has a small number of parameters to ensure the efficiency of training. The network adopts an encoder-decoder architecture. The feature maps of different scales from the encoder are concatenated together after resizing to carry out multi-scale feature fusion. To capture the global semantic information from the context, the attention mechanism is employed in the decoder. With one GTX2080Ti GPU and only 15 MB parameters the model owns, our light-weight network has quality results evaluated on ISPRS Vaihingen Dataset with fewer parameters compared to other popular approaches.",Light-weight network,attention mechanism,semantic segmentation,,,"Cheng, Jian",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_344,"Milosavljevic, Aleksandar",,,,Automated Processing of Remote Sensing Imagery Using Deep Semantic Segmentation: A Building Footprint Extraction Case,,AUG 2020,17,"The proliferation of high-resolution remote sensing sensors and platforms imposes the need for effective analyses and automated processing of high volumes of aerial imagery. The recent advance of artificial intelligence (AI) in the form of deep learning (DL) and convolutional neural networks (CNN) showed remarkable results in several image-related tasks, and naturally, gain the focus of the remote sensing community. In this paper, we focus on specifying the processing pipeline that relies on existing state-of-the-art DL segmentation models to automate building footprint extraction. The proposed pipeline is organized in three stages: image preparation, model implementation and training, and predictions fusion. For the first and third stages, we introduced several techniques that leverage remote sensing imagery specifics, while for the selection of the segmentation model, we relied on empirical examination. In the paper, we presented and discussed several experiments that we conducted on Inria Aerial Image Labeling Dataset. Our findings confirmed that automatic processing of remote sensing imagery using DL semantic segmentation is both possible and can provide applicable results. The proposed pipeline can be potentially transferred to any other remote sensing imagery segmentation task if the corresponding dataset is available.",remote sensing imagery,deep learning,semantic segmentation,building extraction,convolutional neural networks,,,,,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,,,,,,,,,,,,,,,,,,,,,,,
Row_345,"Guo, Xuejun","Chen, Zehua","Wang, Chengyi",,Fully convolutional DenseNet with adversarial training for semantic segmentation of high-resolution remote sensing images,,MAR 17 2021,9,"Semantic segmentation is an important and foundational task in the application of high-resolution remote sensing images (HRRSIs). However, HRRSIs feature large differences within categories and minor variances across categories, posing a significant challenge to the high-accuracy semantic segmentation of HRRSIs. To address this issue and obtain powerful feature expressiveness, a deep conditional generative adversarial network (DCGAN), integrating fully convolutional DenseNet (FC-DenseNet) and Pix2pix, is proposed. The DCGAN is composed of a generator-discriminator pair, which is built on a modified downsampling unit of FC-DenseNet. The proposed method possesses strong feature expression ability because of its skip connections, the very deep network structure and multiscale supervision introduced by FC-DenseNet, and the supervision from the discriminator. Experiments on a Deep Globe Land Cover dataset demonstrate the feasibility and effectiveness of this approach for the semantic segmentation of HRRSIs. The results also reveal that our method can mitigate the influence of class imbalance. Our approach for precise semantic segmentation can effectively facilitate the application of HRRSIs. (C) 2021 Society of Photo-Optical Instrumentation Engineers (SPIE)",semantic segmentation,generative adversarial network,fully convolutional neural network,high resolution remote sensing,DenseNets,,,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_346,"Cai, Yuxiang","Yang, Yingchun","Shang, Yongheng","Shen, Zhengwei",DASRSNet: Multitask Domain Adaptation for Super-Resolution-Aided Semantic Segmentation of Remote Sensing Images,,2023,7,"Unsupervised domain adaptation (UDA) has become an important technique for cross-domain semantic segmentation (SS) in the remote sensing community and obtained remarkable results. However, when transferring from high-resolution (HR) remote sensing images to low-resolution (LR) images, the existing UDA frameworks always fail to segment the LR target images, especially for small objects (e.g., cars), due to the severe spatial resolution shift problem. In this article, to improve the segmentation ability of UDA models for LR target images and small objects, we propose a novel multitask domain adaptation network (DASRSNet) for SS of remote sensing images with the aid of super-resolution (SR). The proposed DASRSNet contains domain adaptation for SS (DASS) branch, domain adaptation for SR (DASR) branch, and feature affinity (FA) module. Specifically, the DASS and DASR branches share the same encoder to extract the domain-invariant features for the target and source domains, and these two branches utilize different decoders and discriminators to conduct cross-domain SS task and SR task, which align the domain shift in output space and image space, respectively. Finally, the FA module, which involves the proposed FA loss, is applied to enhance the affinity of SS features and SR features for both source and target domains. The experimental results on the cross-city aerial datasets demonstrate the effectiveness and superiority of our DASRSNet against the recent UDA models.",Remote sensing,Spatial resolution,Feature extraction,Adaptation models,Task analysis,"Yin, Jianwei",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantic segmentation,Multitasking,Adversarial learning,multitask learning (MTL),remote sensing images,semantic segmentation (SS),super-resolution (SR),unsupervised domain adaptation (UDA),,,,,,,,,,,,,,
Row_347,"Ma, Xianping","Xu, Xichen","Zhang, Xiaokang","Pun, Man-On",Adjacent-Scale Multimodal Fusion Networks for Semantic Segmentation of Remote Sensing Data,,2024,0,"Semantic segmentation is a fundamental task in remote sensing image analysis. The accurate delineation of objects within such imagery serves as the cornerstone for a wide range of applications. To address this issue, edge detection, cross-modal data, large intraclass variability, and limited interclass variance must be considered. Traditional convolutional-neural-network-based models are notably constrained by their local receptive fields, Nowadays, transformer-based methods show great potential to learn features globally, while they ignore positional cues easily and are still unable to cope with multimodal data. Therefore, this work proposes an adjacent-scale multimodal fusion network (ASMFNet) for semantic segmentation of remote sensing data. ASMFNet stands out not only for its innovative interaction mechanism across adjacent-scale features, effectively capturing contextual cues while maintaining low computational complexity but also for its remarkable cross-modal capability. It seamlessly integrates different modalities, enriching feature representation. Its hierarchical scale attention (HSA) module bolsters the association between ground objects and their surrounding scenes through learning discriminative features at higher level abstractions, thereby linking the broad structural information. Adaptive modality fusion module is equipped by HSA with valuable insights into the interrelationships between cross-model data, and it assigns spatial weights at the pixel level and seamlessly integrates them into channel features to enhance fusion representation through an evaluation of modality importance via feature concatenation and filtering. Extensive experiments on representative remote sensing semantic segmentation datasets, including the ISPRS Vaihingen and Potsdam datasets, confirm the impressive performance of the proposed ASMFNet.",Transformers,Remote sensing,Semantics,Semantic segmentation,Feature extraction,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Decoding,Data models,Convolutional neural networks,Stacking,Optical sensors,Adjacent-scale,multimodal fusion,remote sensing,semantic segmentation,,,,,,,,,,,,,
Row_348,"Bai, Lin","Lin, Xiangyuan","Ye, Zhen","Xue, Dongling",MsanlfNet: Semantic Segmentation Network With Multiscale Attention and Nonlocal Filters for High-Resolution Remote Sensing Images,,2022,15,"With the development of deep learning, remote sensing image (RSI) semantic segmentation has produced significant advances. The majority of existing methods use fully convolutional network (FCN) that lacks fine-grained multiscale representation and fails to extract global context information. Thus, we improve FCN by adding two modules-multiscale attention (MSA) and nonlocal filter (NLF). The MSA module enhances the network's fine-grained multiscale representation capability and allows modeling the interdependencies of feature maps among different channels. The NLF module can capture global context information by sequential using fast Fourier transform (FFT), parameter learnable filters, and inverse FFT. By using MSA module for encoder and NLF module for decoder in the FCN framework, MSA and NLF network (MsanlfNet) can obtain both fine-grained multiscale spatial feature and global context information, thus achieving a balance between performance and computational effort. Experimental results on the remote sensing semantic segmentation public datasets demonstrate that our method can achieve better performance. The code is available at https://github.com/xyuanLin/MsanlfNet.",Feature extraction,Semantics,Convolution,Remote sensing,Image segmentation,"Yao, Cheng","Hui, Meng",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Frequency-domain analysis,Decoding,Multiscale attention (MSA),nonlocal filters (NLFs),remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,
Row_349,"Sun, Li","Zou, Huanxin","Wei, Juan","Cao, Xu",Semantic Segmentation of High-Resolution Remote Sensing Images Based on Sparse Self-Attention and Feature Alignment,,MAR 2023,4,"Semantic segmentation of high-resolution remote sensing images (HRSI) is significant, yet challenging. Recently, several research works have utilized the self-attention operation to capture global dependencies. HRSI have complex scenes and rich details, and the implementation of self-attention on a whole image will introduce redundant information and interfere with semantic segmentation. The detail recovery of HRSI is another challenging aspect of semantic segmentation. Several networks use up-sampling, skip-connections, parallel structure, and enhanced edge features to obtain more precise results. However, the above methods ignore the misalignment of features with different resolutions, which affects the accuracy of the segmentation results. To resolve these problems, this paper proposes a semantic segmentation network based on sparse self-attention and feature alignment (SAANet). Specifically, the sparse position self-attention module (SPAM) divides, rearranges, and resorts the feature maps in the position dimension and performs position attention operations (PAM) in rearranged and restored sub-regions, respectively. Meanwhile, the proposed sparse channel self-attention module (SCAM) groups, rearranges, and resorts the feature maps in the channel dimension and performs channel attention operations (CAM) in the rearranged and restored sub-channels, respectively. SPAM and SCAM effectively model long-range context information and interdependencies between channels, while reducing the introduction of redundant information. Finally, the feature alignment module (FAM) utilizes convolutions to obtain a learnable offset map and aligns feature maps with different resolutions, helping to recover details and refine feature representations. Extensive experiments conducted on the ISPRS Vaihingen, Potsdam, and LoveDA datasets demonstrate that the proposed method precedes general semantic segmentation- and self-attention-based networks.",semantic segmentation,high-resolution remote sensing,self-attention,context modeling,feature alignment,"He, Shitian","Li, Meilin","Liu, Shuo",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_350,"Xu, Yizhe","Jiang, Jie",,,High-Resolution Boundary-Constrained and Context-Enhanced Network for Remote Sensing Image Segmentation,,APR 2022,7,"The technology of remote sensing image segmentation has made great progress in recent years. However, there are still several challenges which need to be addressed (e.g., ground objects blocked by shadows, higher intra-class variance and lower inter-class variance). In this paper, we propose a novel high-resolution boundary-constrained and context-enhanced network (HBCNet), which combines boundary information to supervise network training and utilizes the semantic information of categories with the regional feature presentations to improve final segmentation accuracy. On the one hand, we design the boundary-constrained module (BCM) and form the parallel boundary segmentation branch, which outputs the boundary segmentation results and supervises the network training simultaneously. On the other hand, we also devise a context-enhanced module (CEM), which integrates the self-attention mechanism to advance the semantic correlation between pixels of the same category. The two modules are independent and can be directly embedded in the main segmentation network to promote performance. Extensive experiments were conducted using the ISPRS Vahingen and Potsdam benchmarks. The mean F1 score (m-F1) of our model reached 91.32% and 93.38%, respectively, which exceeds most existing CNN-based models and represents state-of-the-art results.",remote sensing image,semantic segmentation,attention mechanism,boundary information,,,,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_351,"Xin, Yi","Fan, Zide","Qi, Xiyu","Geng, Ying",Enhancing Semi-Supervised Semantic Segmentation of Remote Sensing Images via Feature Perturbation-Based Consistency Regularization Methods,,FEB 2024,4,"In the field of remote sensing technology, the semantic segmentation of remote sensing images carries substantial importance. The creation of high-quality models for this task calls for an extensive collection of image data. However, the manual annotation of these images can be both time-consuming and labor-intensive. This has catalyzed the advent of semi-supervised semantic segmentation methodologies. Yet, the complexities inherent within the foreground categories of these remote sensing images present challenges in preserving prediction consistency. Moreover, remote sensing images possess more complex features, and different categories are confused within the feature space, making optimization based on the feature space challenging. To enhance model consistency and to optimize feature-based class categorization, this paper introduces a novel semi-supervised semantic segmentation framework based on Mean Teacher (MT). Unlike the conventional Mean Teacher that only introduces perturbations at the image level, we incorporate perturbations at the feature level. Simultaneously, to maintain consistency after feature perturbation, we employ contrastive learning for feature-level learning. In response to the complex feature space of remote sensing images, we utilize entropy threshold to assist contrastive learning, selecting feature key-values more precisely, thereby enhancing the accuracy of segmentation. Extensive experimental results on the ISPRS Potsdam dataset and the challenging iSAID dataset substantiate the superior performance of our proposed methodology.",remote sensing,semantic segmentation,semi-supervised learning,consistency regularization,feature perturbation,"Li, Xinming",,,,SENSORS,,contrastive learning,,,,,,,,,,,,,,,,,,,,,
Row_352,"Sui, Baikai","Cao, Yungang","Cheng, Haibo","Zhang, Shuang",Detail-Optimized Super-Resolution Reconstruction-Based Multistage Training Strategy for Remote Sensing Semantic Segmentation,,2024,0,"Low resolution is a major factor that negatively impacts the accuracy of remote sensing (RS) interpretation. High-quality super-resolution reconstruction (SRR) can help alleviate this problem. In this study, we present a multistage semantic segmentation training strategy called SRSSTS, which is based on detail-optimized SRR. SRSSTS addresses the challenges of poor reconstruction quality and low semantic segmentation (SS) accuracy of RS images. Our approach involves constructing a generative adversarial network (GAN)-based perceptual-loss-dominated SRR network, which generates texture-realistic, high-quality high-resolution images from low-resolution RS images. We then input the generated images into an SS network in steps and propose a three-stage joint loss to generate segmentation results at different resolutions. SRSSTS is straightforward and efficient, and it can be applied to any SS backbone networks. Additionally, this strategy does not increase computational resources compared to other SR-based segmentation methods, since the SRR network is trained separately. We evaluated the effectiveness of our designed SRR network and SRSSTS on five RS datasets (ISPRS Potsdam and Vaihingen, WHDLD, LoveDA Rural and Urban) with different resolutions, achieving excellent performance compared to an SS model for a single task and other SR-based segmentation methods. Moreover, we observed that the learned perceptual image patch similarity (LPIPS) of the super-resolution (SR) reconstructed images, i.e., the stronger the perception ability, the more helpful it is for the SS task.",remote sensing (RS),semantic segmentation (SS),super resolution (SR),training strategy (TS),,"Zhu, Jun","Xie, Yakun",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_353,"Song, Wanying","Zhou, Xinwei","Zhang, Shiru","Wu, Yan",GLF-Net: A Semantic Segmentation Model Fusing Global and Local Features for High-Resolution Remote Sensing Images,,OCT 2023,3,"Semantic segmentation of high-resolution remote sensing images holds paramount importance in the field of remote sensing. To better excavate and fully fuse the features in high-resolution remote sensing images, this paper introduces a novel Global and Local Feature Fusion Network, abbreviated as GLF-Net, by incorporating the extensive contextual information and refined fine-grained features. The proposed GLF-Net, devised as an encoder-decoder network, employs the powerful ResNet50 as its baseline model. It incorporates two pivotal components within the encoder phase: a Covariance Attention Module (CAM) and a Local Fine-Grained Extraction Module (LFM). And an additional wavelet self-attention module (WST) is integrated into the decoder stage. The CAM effectively extracts the features of different scales from various stages of the ResNet and then encodes them with graph convolutions. In this way, the proposed GLF-Net model can well capture the global contextual information with both universality and consistency. Additionally, the local feature extraction module refines the feature map by encoding the semantic and spatial information, thereby capturing the local fine-grained features in images. Furthermore, the WST maximizes the synergy between the high-frequency and the low-frequency information, facilitating the fusion of global and local features for better performance in semantic segmentation. The effectiveness of the proposed GLF-Net model is validated through experiments conducted on the ISPRS Potsdam and Vaihingen datasets. The results verify that it can greatly improve segmentation accuracy.",high-resolution remote sensing,semantic segmentation,global context information,fine-grained feature,feature fusion,"Zhang, Peng",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_354,"Chen, Bingyu","Xia, Min","Qian, Ming","Huang, Junqing",MANet: a multi-level aggregation network for semantic segmentation of high-resolution remote sensing images,,AUG 18 2022,65,"With the continuous improvement of the segmentation effect for natural datasets, some studies have gradually been applied to high-resolution remote sensing images (HRRSIs). Due to a large amount of ground object information contained, even objects of the same type present the diversity and complexity of features in different periods or locations. The existing algorithms applied to semantic segmentation of remote sensing images are limited by the short-range context, and the high-resolution details, especially the edges, couldnot be fully recovered. Aiming at the problem, a multi-level aggregation network (MANet) is proposed. Firstly, the proposed global dependency module extracts deep global features by learning the interrelationships of all positions in the context, and filters redundant channel information as well. Secondly, MANet we proposed extends Multi-level Feature Aggregation Network by adding a simple and effective two-path feature refining module before each up-sample module to optimize the segmentation results. The two-path feature refining module uses two independent branches to obtain the features with different depths, which enriches the hierarchical structure of the network. Besides, it is combined with the subsequent up-sample module to effectively enhance MANet's ability to recover detailed information of HRRSI. Experimental results show that the methods proposed in the paper achieve competitive performance.",High-Resolution,remote sensing images,Mosaic data enhancement,semantic segmentation,deep learning,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_355,"Chen, Yan","Jiang, Wenxiang","Wang, Mengyuan","Kang, Menglei",LightFGCNet: A Lightweight and Focusing on Global Context Information Semantic Segmentation Network for Remote Sensing Imagery,,DEC 2022,4,"Convolutional neural networks have attracted much attention for their use in the semantic segmentation of remote sensing imagery. The effectiveness of semantic segmentation of remote sensing images is significantly influenced by contextual information extraction. The traditional convolutional neural network is constrained by the size of the convolution kernel and mainly concentrates on local contextual information. We suggest a new lightweight global context semantic segmentation network, LightFGCNet, to fully utilize the global context data and to further reduce the model parameters. It uses an encoder-decoder architecture and gradually combines feature information from adjacent encoder blocks during the decoding upsampling stage, allowing the network to better extract global context information. Considering that the frequent merging of feature information produces a significant quantity of redundant noise, we build a unique and lightweight parallel channel spatial attention module (PCSAM) for a few critical contextual features. Additionally, we design a multi-scale fusion module (MSFM) to acquire multi-scale feature target information. We conduct comprehensive experiments on the two well-known datasets ISPRS Vaihingen and WHU Building. The findings demonstrate that our suggested strategy can efficiently decrease the number of parameters. Separately, the number of parameters and FLOPs are 3.12 M and 23.5 G, respectively, and the mIoU and IoU of our model on the two datasets are 70.45% and 89.87%, respectively, which is significantly better than what the conventional convolutional neural networks for semantic segmentation can deliver.",remote sensing imagery,semantic segmentation,attention mechanism,global contextual information,multi-scale fusion,"Weise, Thomas","Wang, Xiaofeng","Tan, Ming","Xu, Lixiang",REMOTE SENSING,"Li, Xinlu",lightweight model,,,,,,,,,"Zhang, Chen",,,,,,,,,,,,
Row_356,"Guo, Shichen","Jin, Qizhao","Wang, Hongzhen","Wang, Xuezhi",Learnable Gated Convolutional Neural Network for Semantic Segmentation in Remote-Sensing Images,,AUG 2019,29,"Semantic segmentation in high-resolution remote-sensing (RS) images is a fundamental task for RS-based urban understanding and planning. However, various types of artificial objects in urban areas make this task quite challenging. Recently, the use of Deep Convolutional Neural Networks (DCNNs) with multiscale information fusion has demonstrated great potential in enhancing performance. Technically, however, existing fusions are usually implemented by summing or concatenating feature maps in a straightforward way. Seldom do works consider the spatial importance for global-to-local context-information aggregation. This paper proposes a Learnable-Gated CNN (L-GCNN) to address this issue. Methodologically, the Taylor expression of the information-entropy function is first parameterized to design the gate function, which is employed to generate pixelwise weights for coarse-to-fine refinement in the L-GCNN. Accordingly, a Parameterized Gate Module (PGM) was designed to achieve this goal. Then, the single PGM and its densely connected extension were embedded into different levels of the encoder in the L-GCNN to help identify the discriminative feature maps at different scales. With the above designs, the L-GCNN is finally organized as a self-cascaded end-to-end architecture that is able to sequentially aggregate context information for fine segmentation. The proposed model was evaluated on two public challenging benchmarks, the ISPRS 2Dsemantic segmentation challenge Potsdam dataset and the Massachusetts building dataset. The experiment results demonstrate that the proposed method exhibited significant improvement compared with several related segmentation networks, including the FCN, SegNet, RefineNet, PSPNet, DeepLab and GSN.For example, on the Potsdam dataset, our method achieved a 93.65% F1 score and 88.06% IoU score for the segmentation of tiny cars in high-resolution RS images. As a conclusion, the proposed model showed potential for object segmentation from the RS images of buildings, impervious surfaces, low vegetation, trees and cars in urban settings, which largely varies in size and have confusing appearances.",semantic segmentation,CNN,deep learning,remote sensing,gate function,"Wang, Yangang","Xiang, Shiming",,,REMOTE SENSING,,multiscale feature fusion,,,,,,,,,,,,,,,,,,,,,
Row_357,"Li, Zhenshi","Zhang, Xueliang","Xiao, Pengfeng",,One Model Is Enough: Toward Multiclass Weakly Supervised Remote Sensing Image Semantic Segmentation,,2023,25,"Semantic segmentation of remote sensing images (RSIs) is effective for large-scale land cover mapping, which heavily relies on a large amount of training data with laborious pixel-level labeling. Due to the easy availability of image-level labels, weakly supervised semantic segmentation (WSSS) based on them has attracted intensive attention. However, existing image-level WSSS methods for RSIs mainly focus on binary segmentation, which are difficult to apply to multiclass scenarios. This study proposes a comprehensive framework for image-level multiclass WSSS of RSIs, consisting of appropriate image-level label generation, high-quality pixel-level pseudo mask generation, and segmentation network iterative training. Specifically, a training sample filtering method, as well as a dataset co-occurrence evaluation metric, is proposed to demonstrate proper image-level training samples. Leveraging multiclass class activation maps (CAMs), an uncertainty-driven pixel-level weighted mask is proposed to relieve the overfitting of labeling noise in pseudo masks when training the segmentation network. Extensive experiments demonstrate that the proposed framework can achieve high-quality multiclass WSSS performance with image-level labels, which can attain 94.23% and 90.77% of the mean intersection over union (mIoU) from pixel-level labels for the ISPRS Potsdam and Vaihingen datasets, respectively. Beyond that, for the DeepGlobe dataset with more complex landscapes, the WSSS framework can achieve an accuracy close to 99% of the fully supervised case. In addition, we further demonstrate that compared to adopting multiple binary WSSS models, directly training a multiclass WSSS model can achieve better results, which can provide new thoughts to achieve WSSS of RSIs for multiclass application scenarios. Our code is publically available at https://github.com/NJU-LHRS/OME.",Class activation map (CAM),image-level label,multiclass,pixel-level uncertainty,remote sensing image (RSI),,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,,,,,,,
Row_358,"Zheng, Chen","Zhang, Yun","Wang, Leiguang",,Semantic Segmentation of Remote Sensing Imagery Using an Object-Based Markov Random Field Model With Auxiliary Label Fields,,MAY 2017,37,"The Markov random field (MRF) model has attracted great attention in the field of image segmentation. However, most MRF-based methods fail to resolve segmentation misclassification problems for high spatial resolution remote sensing images due to insufficiently using the hierarchical semantic information. In order to solve such a problem, this paper proposes an object-based MRF model with auxiliary label fields that can capture more macro and detailed information and apply it to the semantic segmentation of high spatial resolution remote sensing images. Specifically, apart from the label field, two auxiliary label fields are first introduced into the proposed model for interpreting remote sensing images from different perspectives, which are implemented by setting a different number of auxiliary classes. Then, the multilevel logistic model is used to describe the interactions within each label field, and a conditional probability distribution is developed to model the interactions between label fields. A net context structure is established among them to model the interactions of classes within and between label fields. A principled probabilistic inference is suggested to solve the proposed model by iteratively renewing the label field and auxiliary label fields, in which different information of auxiliary label fields can be integrated into the label field during iterations. Experiments on different remote sensing images demonstrate that our model produces more accurate segmentation than the state-of-the-art MRF-based methods. If some prior information is added, the proposed model can produce accurate results even in complex areas.",Auxiliary label field,object-based Markov random field,remote sensing image,semantic segmentation,,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_359,"Zhang, Zhen","Huang, Jue","Jiang, Tao","Sui, Baikai",Semantic segmentation of very high-resolution remote sensing image based on multiple band combinations and patchwise scene analysis,,JAN 6 2020,19,"Large intraclass variance and low interclass variance are among the most challenging problems in very high-resolution (VHR) image classification. Semantic segmentation constructed in a deep convolution neural network is used as a classification algorithm conducted via end-to-end training, which combines spectral-spatial features and context information. However, large-scale remote sensing images cannot be directly processed because they are limited by GPU memory and segmentation algorithm. At the same time, classification using single band combinations is also unsatisfactory due to the extraordinary complex features of VHR images. Therefore, a method is proposed based on multiple band combinations and patchwise scene analysis. A complex remote sensing image can be considered as into a combination of simple scenes from multiple patchwise images. And optimal band combinations of each patchwise image are selected according to their scene. The segmentation results of each patchwise image are merged to get the desired results according to geographical coordinates. Our method is validated on the ISPRS 2-D Semantic Labeling dataset of Potsdam, on which results competitive with the state-of-the-art are obtained. The proposed scheme has strong universality and can be used for large-scale high-resolution remote sensing image classification. (C) 2020 Society of Photo-Optical Instrumentation Engineers (SPIE)",very high-resolution remote sensing,semantic segmentation,multiple band combination,scene analysis,,"Pan, Xinliang",,,,JOURNAL OF APPLIED REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_360,"Peng, Cheng","Li, Yangyang","Jiao, Licheng","Chen, Yanqiao",Densely Based Multi-Scale and Multi-Modal Fully Convolutional Networks for High-Resolution Remote-Sensing Image Semantic Segmentation,,AUG 2019,96,"Automatic and accurate semantic segmentation from high-resolution remote-sensing images plays an important role in the field of aerial images analysis. The task of dense semantic segmentation requires that semantic labels be assigned to each pixel in the image. Recently, convolutional neural networks (CNNs) have proven to be powerful tools for image classification, and they have been adopted in the remote-sensing community. But many limitations still exist when modern CNN architectures are directly applied to remote-sensing images, such as gradient explosion when the depth of the network increases, over-fitting with limited labeled remote-sensing data, and special differences between remote-sensing images and natural images. In this paper, we present a novel architecture that combines the thought of dense connection and fully convolutional networks, referred as DFCN, to automatically provide fine-grained semantic segmentation maps. In addition, we improve DFCN with multi-scale filters to widen the network and to increase the richness and diversity of extracted information, making the network more powerful and expressive than the naive convolution layer. Furthermore, we investigate a multi-modal network that incorporates digital surface models (DSMs) into a DFCN structure, and then we propose dual-path densely convolutional networks where the encoder consists of two paths that, respectively, extract features from spectral data and DSMs data and then fuse them. Finally, through conducting comprehensive experimental evaluations on two remote sensing benchmark datasets, we test our proposed models and compare them with other deep networks. The results demonstrate the effectiveness of proposed approaches; they can achieve competitive performance compared with the current state-of-the-art methods.",Dense connection,fully convolutional networks (FCN),high-resolution remote-sensing image,semantic segmentation,,"Shang, Ronghua",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_361,"Wang, Jiaxin","Ding, Chris H. Q.","Chen, Sibao","He, Chenggang",Semi-Supervised Remote Sensing Image Semantic Segmentation via Consistency Regularization and Average Update of Pseudo-Label,,NOV 2020,57,"Image segmentation has made great progress in recent years, but the annotation required for image segmentation is usually expensive, especially for remote sensing images. To solve this problem, we explore semi-supervised learning methods and appropriately utilize a large amount of unlabeled data to improve the performance of remote sensing image segmentation. This paper proposes a method for remote sensing image segmentation based on semi-supervised learning. We first design a Consistency Regularization (CR) training method for semi-supervised training, then employ the new learned model for Average Update of Pseudo-label (AUP), and finally combine pseudo labels and strong labels to train semantic segmentation network. We demonstrate the effectiveness of the proposed method on three remote sensing datasets, achieving better performance without more labeled data. Extensive experiments show that our semi-supervised method can learn the latent information from the unlabeled data to improve the segmentation performance.",semi-supervised learning,remote sensing image segmentation,consistency training,pseudo label,,"Luo, Bin",,,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_362,"Li, Yunbo","Yi, Zhiyu","Wang, Yuebin","Zhang, Liqiang",Adaptive Context Transformer for Semisupervised Remote Sensing Image Segmentation,,2023,2,"Current deep learning methods for semantic seg-mentation in remote sensing heavily depend on a substantial amount of labeled data. However, obtaining pixel-level labeled data in this field is both time-consuming and laborious. To address this challenge, semi supervised learning (SSL) method shave been introduced. Pseudo supervision is one of the most effective methods, which can be adopted to enhance the performance of semsupervised semantic segmentation of remotesensing images. But incorrect pseudolabels can cause substantialdistortions to the segmentation model in SSL. Moreover, it isdifficult for conventional semantic segmentation methods to dealwith global-local features of the remote sensing image withoutadaptive context feature. In this article, we propose a novel learn-ing approach based on an adaptive context transformer (ACT)and pseudolabeling, called ACT for semisupervised (ACTSS)remote sensing image segmentation. We propose an adaptivecontext attention model with adjustable sliding windows. A smallwindow is used to capture Query (Q) for local feature, andbigger windows are used to capture Key (K) and Value(V)forglobal feature. Then, we combine them and get the global-local features. And we propose a point-line-plane (PLP) pseudolabelfilter mechanism based on clustering and boundary extraction, which can filter unreliable pseudolabels from three angles: point,line, and plane. To validate the effectiveness of the model, we carried out extensive experiments on the LOVEDA, Potsdam, and Vaihingen datasets and compared ACTSS with other methods. These experiments demonstrate that ACTSS achieves state-of-the-art performance for semisupervised semantic segmentation on all tested datasets.",Adaptive context transformer (ACT),pseudolabel filter,semantic segmentation,semisupervised learning (SSL),,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_363,"Su, Hao","Wei, Shunjun","Liu, Shan","Liang, Jiadian",HQ-ISNet: High-Quality Instance Segmentation for Remote Sensing Imagery,,MAR 2020,76,"Instance segmentation in high-resolution (HR) remote sensing imagery is one of the most challenging tasks and is more difficult than object detection and semantic segmentation tasks. It aims to predict class labels and pixel-wise instance masks to locate instances in an image. However, there are rare methods currently suitable for instance segmentation in the HR remote sensing images. Meanwhile, it is more difficult to implement instance segmentation due to the complex background of remote sensing images. In this article, a novel instance segmentation approach of HR remote sensing imagery based on Cascade Mask R-CNN is proposed, which is called a high-quality instance segmentation network (HQ-ISNet). In this scheme, the HQ-ISNet exploits a HR feature pyramid network (HRFPN) to fully utilize multi-level feature maps and maintain HR feature maps for remote sensing images' instance segmentation. Next, to refine mask information flow between mask branches, the instance segmentation network version 2 (ISNetV2) is proposed to promote further improvements in mask prediction accuracy. Then, we construct a new, more challenging dataset based on the synthetic aperture radar (SAR) ship detection dataset (SSDD) and the Northwestern Polytechnical University very-high-resolution 10-class geospatial object detection dataset (NWPU VHR-10) for remote sensing images instance segmentation which can be used as a benchmark for evaluating instance segmentation algorithms in the high-resolution remote sensing images. Finally, extensive experimental analyses and comparisons on the SSDD and the NWPU VHR-10 dataset show that (1) the HRFPN makes the predicted instance masks more accurate, which can effectively enhance the instance segmentation performance of the high-resolution remote sensing imagery; (2) the ISNetV2 is effective and promotes further improvements in mask prediction accuracy; (3) our proposed framework HQ-ISNet is effective and more accurate for instance segmentation in the remote sensing imagery than the existing algorithms.",instance segmentation,HRFPN,ISNetV2,SSDD,NWPU VHR-10,"Wang, Chen","Shi, Jun","Zhang, Xiaoling",,REMOTE SENSING,,remote sensing images,,,,,,,,,,,,,,,,,,,,,
Row_364,"Zheng, Zhiyu","Lv, Liang","Zhang, Lefei",,Enhancing the Semi-Supervised Semantic Segmentation With Prototype-Based Supervision for Remote Sensing Images,,2024,0,"While image semantic segmentation is a fundamental and well-studied task in remote sensing (RS) society, it usually depends on large amounts of pixel-level annotations. RS image semi-supervised semantic segmentation (RSIS4) tries to improve performance by exploring the unlabeled data, thus significantly reducing the label costs. The core idea of RSIS4 is to transfer the prior information from the labeled to unlabeled pixels, which is commonly achieved by considering the confident part of the softmax prediction as pseudolabels for further supervised learning. However, such pixel-level instruction could inevitably involve uncertainty (e.g., noise and error) due to the extremely limited annotated data at the initialization. To address this issue, in this letter, we employ the prototypes, which contain inbuilt resistance to potentially inaccurate pixels, to bring substantial supervision directly from the embedded feature space. Specifically, we project deep features into the embedding space to generate prototypes, each of which can be regarded as the category-level feature representation of a certain semantic category. These prototypes are then used to perform the pixelwise classification, with the advantage of capturing the global similarity throughout the whole pixels within the category. Moreover, to ensure accurate prototypes, we further introduce pixel-prototype contrast to better explore the discriminative category-level feature embedding. By integrating the guidance from the above pixel-level and category-level feature representations, the proposed algorithm obtains high-quality pseudolabels and extracts effective features. Extensive experiments on four RS image segmentation datasets have demonstrated the effectiveness of the proposed method. The code is available at https://github.com/Duckyee728/PCSSS.git.",Prototype learning,remote sensing (RS) images,semi-supervised semantic segmentation (S4),Prototype learning,remote sensing (RS) images,,,,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,semi-supervised semantic segmentation (S4),,,,,,,,,,,,,,,,,,,,,
Row_365,"Wu, Linshan","Fang, Leyuan","Yue, Jun","Zhang, Bob",Deep Bilateral Filtering Network for Point-Supervised Semantic Segmentation in Remote Sensing Images,,2022,43,"Semantic segmentation methods based on deep neural networks have achieved great success in recent years. However, training such deep neural networks relies heavily on a large number of images with accurate pixel-level labels, which requires a huge amount of human effort, especially for large-scale remote sensing images. In this paper, we propose a point-based weakly supervised learning framework called the deep bilateral filtering network (DBFNet) for the semantic segmentation of remote sensing images. Compared with pixel-level labels, point annotations are usually sparse and cannot reveal the complete structure of the objects; they also lack boundary information, thus resulting in incomplete prediction within the object and the loss of object boundaries. To address these problems, we incorporate the bilateral filtering technique into deeply learned representations in two respects. First, since a target object contains smooth regions that always belong to the same category, we perform deep bilateral filtering (DBF) to filter the deep features by a nonlinear combination of nearby feature values, which encourages the nearby and similar features to become closer, thus achieving a consistent prediction in the smooth region. In addition, the DBF can distinguish the boundary by enlarging the distance between the features on different sides of the edge, thus preserving the boundary information well. Experimental results on two widely used datasets, the ISPRS 2-D semantic labeling Potsdam and Vaihingen datasets, demonstrate that our proposed DBFNet can achieve a highly competitive performance compared with state-of-the-art fully-supervised methods. Code is available at https://github.com/Luffy03/DBFNet.",Bilateral filtering,point annotations,remote sensing,semantic segmentation,weakly-supervised learning,"Ghamisi, Pedram","He, Min",,,IEEE TRANSACTIONS ON IMAGE PROCESSING,,,,,,,,,,,,,,,,,,,,,,,
Row_366,"Sun, Le","Cheng, Shiwei","Zheng, Yuhui","Wu, Zebin",SPANet: Successive Pooling Attention Network for Semantic Segmentation of Remote Sensing Images,,2022,54,"In the convolutional neural network, the precise segmentation of small-scale objects and object boundaries in remote sensing images is a great challenge. As the model gets deeper, low-level features with geometric information and high-level features with semantic information cannot be obtained simultaneously. To alleviate this problem, a successive pooling attention network (SPANet) was proposed. The SPANet mainly consists of ResNet50 as the backbone, successive pooling attention module (SPAM), and feature fusion module (FFM). Specifically, the SPANet uses two parallel branches to extract high-level features by ResNet50 and low-level features by the first 11 layers of ResNet50. Then, both the high- and low-level features are fed to the SPAM, which is mainly composed of a successive pooling operator and a self-attention submodule, for further extracting deeper multiscale and salient features. In addition, the low- and high-level features after the SPAM are fused by the FFM to achieve the complementarity of spatial and geometric information. This fusion module alleviates the problem of the accurate segmentation of object edges. Finally, the high-level features and enhanced low-level features of the two branches are fused to obtain the final prediction results. Experiments show that the proposed SPANet achieves a good segmentation effect compared with other models on two remotely sensed datasets.",Feature extraction,Semantics,Image segmentation,Remote sensing,Data mining,"Zhang, Jianwei",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Context modeling,Decoding,Attention mechanism,convolutional neural network,remote sensing images,semantic segmentation,successive pooling,,,,,,,,,,,,,,,
Row_367,"Zhang, Jinglin","Li, Yuxia","Zhang, Bowei","He, Lei",CD-MQANet: Enhancing Multi-Objective Semantic Segmentation of Remote Sensing Images through Channel Creation and Dual-Path Encoding,,SEP 2023,0,"As a crucial computer vision task, multi-objective semantic segmentation has attracted widespread attention and research in the field of remote sensing image analysis. This technology has important application value in fields such as land resource surveys, global change monitoring, urban planning, and environmental monitoring. However, multi-target semantic segmentation of remote sensing images faces challenges such as complex surface features, complex spectral features, and a wide spatial range, resulting in differences in spatial and spectral dimensions among target features. To fully exploit and utilize spectral feature information, focusing on the information contained in spatial and spectral dimensions of multi-spectral images, and integrating external information, this paper constructs the CD-MQANet network structure, where C represents the Channel Creator module and D represents the Dual-Path Encoder. The Channel Creator module (CCM) mainly includes two parts: a generator block and a spectral attention module. The generator block aims to generate spectral channels that can expand different ground target types, while the spectral attention module can enhance useful spectral information. Dual-Path Encoders include channel encoders and spatial encoders, intended to fully utilize spectrally enhanced images while maintaining the spatial information of the original feature map. The decoder of CD-MQANet is a multitasking decoder composed of four types of attention, enhancing decoding capabilities. The loss function used in the CD-MQANet consists of three parts, which are generated by the intermediate results of the CCM, the intermediate results of the decoder, and the final segmentation results and label calculation. We performed experiments on the Potsdam dataset and the Vaihingen dataset. Compared to the baseline MQANet model, the CD-MQANet network improved mean F1 and OA by 2.03% and 2.49%, respectively, on the Potsdam dataset, and improved mean F1 and OA by 1.42% and 1.25%, respectively, on the Vaihingen dataset. The effectiveness of CD-MQANet was also proven by comparative experiments with other studies. We also conducted a thermographic analysis of the attention mechanism used in CD-MQANet and analyzed the intermediate results generated by CCM and LAM. Both modules generated intermediate results that had a significant positive impact on segmentation.",deep learning,remote sensing,semantic segmentation,attention mechanism,multispectral remote sensing data,"He, Yuan","Deng, Wantao","Si, Yu","Tong, Zhonggui",REMOTE SENSING,"Gong, Yushu",,,,,,,,,,"Liao, Kunwei",,,,,,,,,,,,
Row_368,"Yang, Liangzhe","Chen, Hao","Yang, Anran","Li, Jun",EasySeg: An Error-Aware Domain Adaptation Framework for Remote Sensing Imagery Semantic Segmentation via Interactive Learning and Active Learning,,2024,2,"Semantic segmentation of remote sensing images has attracted much attention for its wide applications. While deep learning models have shown impressive performance in this task, challenges arise when applying them to data from other domains without fine-tuning, due to domain gaps. Domain adaptation (DA) has emerged as a solution to bridge this gap. Existing works mainly focus on unsupervised DA (UDA), which lags far behind fully supervised models. However, active DA (ADA) methods focused on natural images face challenges when applied to remote sensing images due to pronounced domain gaps and error unawareness problems. In this work, we propose a novel error-aware DA framework for remote sensing imagery semantic segmentation, called EasySeg, via interactive learning and active learning. First, we introduce a point-level labeling strategy, named ""See-First-Ask-Later"" (SFAL), combining both interactive and active learning manners, allowing obvious errors and information-rich pixels to be annotated easily and efficiently. Then, we introduce an interactive semantic segmentation network (ISS-Net), which can perform automatic semantic segmentation and interactive refinement. Based on the acquired target point-level labels, ISS-Net generates dense and accurate pseudo-labels to enhance DA performance through retraining with consistency regularization. Comprehensive experiments on two tasks demonstrate that our method outperforms the state-of-the-art ADA methods in terms of overall accuracy (OA), mean accuracy (MA), $F1$ score, and mean intersection of union (mIoU) with lower labeling costs, even surpassing some fully supervised models. The source code of EasySeg is freely available at https://github.com/Yangliangzhe/EasySeg.",Active learning,domain adaptation (DA),interactive learning,remote sensing images,semantic segmentation,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_369,"Li, Yuxia","Si, Yu","Tong, Zhonggui","He, Lei",MQANet: Multi-Task Quadruple Attention Network of Multi-Object Semantic Segmentation from Remote Sensing Images,,DEC 2022,9,"Multi-object semantic segmentation from remote sensing images has gained significant attention in land resource surveying, global change monitoring, and disaster detection. Compared to other application scenarios, the objects in the remote sensing field are larger and have a wider range of distribution. In addition, some similar targets, such as roads and concrete-roofed buildings, are easily misjudged. However, existing convolutional neural networks operate only in the local receptive field, and this limits their capacity to represent the potential association between different objects and surrounding features. This paper develops a Multi-task Quadruple Attention Network (MQANet) to address the above-mentioned issues and increase segmentation accuracy. The MQANet contains four attention modules: position attention module (PAM), channel attention module (CAM), label attention module (LAM), and edge attention module (EAM). The quadruple attention modules obtain global features by expanding the receptive fields of the network and introducing spatial context information in the label. Then, a multi-tasking mechanism which splits a multi-category segmentation task into several binary-classification segmentation tasks is introduced to improve the ability to identify similar objects. The proposed MQANet network was applied to the Potsdam dataset, the Vaihingen dataset and self-annotated images from Chongzhou and Wuzhen (CZ-WZ), representative cities in China. Our MQANet performs better over the baseline net by a large margin of +6.33 OA and +7.05 Mean F1-score on the Vaihingen dataset, +3.57 OA and +2.83 Mean F1-score on the Potsdam dataset, and +3.88 OA and +8.65 Mean F1-score on the self-annotated dataset (CZ-WZ dataset). In addition, each image execution time of the MQANet model is reduced 66.6 ms compared to UNet. Moreover, the effectiveness of MQANet was also proven by comparative experiments with other studies.",deep learning,remote sensing,semantic segmentation,multi-task learning,attention mechanism,"Zhang, Jinglin","Luo, Shiyu","Gong, Yushu",,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_370,"Li, Xin","Xu, Feng","Yong, Xi","Chen, Deqing",SSCNet: A Spectrum-Space Collaborative Network for Semantic Segmentation of Remote Sensing Images,,DEC 2023,21,"Semantic segmentation plays a pivotal role in the intelligent interpretation of remote sensing images (RSIs). However, conventional methods predominantly focus on learning representations within the spatial domain, often resulting in suboptimal discriminative capabilities. Given the intrinsic spectral characteristics of RSIs, it becomes imperative to enhance the discriminative potential of these representations by integrating spectral context alongside spatial information. In this paper, we introduce the spectrum-space collaborative network (SSCNet), which is designed to capture both spectral and spatial dependencies, thereby elevating the quality of semantic segmentation in RSIs. Our innovative approach features a joint spectral-spatial attention module (JSSA) that concurrently employs spectral attention (SpeA) and spatial attention (SpaA). Instead of feature-level aggregation, we propose the fusion of attention maps to gather spectral and spatial contexts from their respective branches. Within SpeA, we calculate the position-wise spectral similarity using the complex spectral Euclidean distance (CSED) of the real and imaginary components of projected feature maps in the frequency domain. To comprehensively calculate both spectral and spatial losses, we introduce edge loss, Dice loss, and cross-entropy loss, subsequently merging them with appropriate weighting. Extensive experiments on the ISPRS Potsdam and LoveDA datasets underscore SSCNet's superior performance compared with several state-of-the-art methods. Furthermore, an ablation study confirms the efficacy of SpeA.",semantic segmentation,remote sensing images,spectral attention,spectral and spatial contexts,loss function,"Xia, Runliang","Ye, Baoliu","Gao, Hongmin","Chen, Ziqi",REMOTE SENSING,"Lyu, Xin",,,,,,,,,,,,,,,,,,,,,,
Row_371,"He, Yongjun","Wang, Jinfei","Liao, Chunhua","Shan, Bo",ClassHyPer: ClassMix-Based Hybrid Perturbations for Deep Semi-Supervised Semantic Segmentation of Remote Sensing Imagery,,FEB 2022,28,"Inspired by the tremendous success of deep learning (DL) and the increased availability of remote sensing data, DL-based image semantic segmentation has attracted growing interest in the remote sensing community. The ideal scenario of DL application requires a vast number of annotation data with the same feature distribution as the area of interest. However, obtaining such enormous training sets that suit the data distribution of the target area is highly time-consuming and costly. Consistency-regularization-based semi-supervised learning (SSL) methods have gained growing popularity thanks to their ease of implementation and remarkable performance. However, there have been limited applications of SSL in remote sensing. This study comprehensively analyzed several advanced SSL methods based on consistency regularization from the perspective of data- and model-level perturbation. Then, an end-to-end SSL approach based on a hybrid perturbation paradigm was introduced to improve the DL model's performance with a limited number of labels. The proposed method integrates the semantic boundary information to generate more meaningful mixing images when performing data-level perturbation. Additionally, by using implicit pseudo-supervision based on model-level perturbation, it eliminates the need to set extra threshold parameters in training. Furthermore, it can be flexibly paired with the DL model in an end-to-end manner, as opposed to the separated training stages used in the traditional pseudo-labeling. Experimental results for five remote sensing benchmark datasets in the application of segmentation of roads, buildings, and land cover demonstrated the effectiveness and robustness of the proposed approach. It is particularly encouraging that the ratio of accuracy obtained using the proposed method with 5% labels to that using the purely supervised method with 100% labels was more than 89% on all benchmark datasets.",deep learning,remote sensing semantic segmentation,transfer learning,semi-supervised learning,consistency regularization,"Zhou, Xin",,,,REMOTE SENSING,,hybrid perturbation,,,,,,,,,,,,,,,,,,,,,
Row_372,"Amine, Hadir","Olga, Assainova","Mohamed, Adjou","Gaetan, Palka",Bridging the gap: deep learning in the semantic segmentation of remote sensing data,PATTERN RECOGNITION AND PREDICTION XXXV,2024,0,"Semantic segmentation has crucial importance in various domains due to its ability to recognize and categorize objects within an image at a pixel level. This task enables a wide range of applications, such as autonomous vehicles, environmental monitoring, and remote sensing (RS). In RS, semantic segmentation plays a crucial role, acting as the basis for applications including land cover classification. Following the success of deep learning (DL) methods in computer vision, our paper addresses the intersection between DL and RS imagery. We focus on improving the efficiency of some baseline and backbone models to ensure their adaptability to the challenges posed by RS imagery. Therefore, we evaluate state-of-the-art models on two datasets and investigate their ability to accurately segment objects in RS imagery. Our research aims to open the way for more accurate and reliable semantic segmentation methods in geospatial analysis.",Remote sensing images,deep learning,semantic segmentation,land cover or crop type classification,,"Marwa, Elbouz","Ayman, Al Falou",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_373,"Cai, Jian","Ma, Lei","Li, Feimo","Yang, Yiping",JOINT FEATURE NETWORK FOR BRIDGE SEGMENTATION IN REMOTE SENSING IMAGES,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,2018,1,"This paper proposes a novel convolutional neural network architecture for semantic segmentation of bridges with various scales in optical remote sensing images. In the context of RSI analysis on objects with irregular shapes, it is necessary to get dense, pixelwise classification maps. To address the issue, a new network architecture for producing refined shapes is required instead of image categorization labels. In our end-to-end framework, a ResNet is used as a backbone model to extract semantic features, then a cascaded top-down path is added to fuse these features as different scales. Joint features are obtained by stacking different layers of feature maps. Experiments show our proposed architecture has the ability to combine rich multi-scale contextual information to produce semantic segmentation maps with high accuracy.",convolutional neural networks (CNNs),semantic segmentation,pixelwise classification,remote sensing images (RSIs),,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_374,"Chen, Jie","Zhu, Jingru","Guo, Ya","Sun, Geng",Unsupervised Domain Adaptation for Semantic Segmentation of High-Resolution Remote Sensing Imagery Driven by Category-Certainty Attention,,2022,48,"Semantic segmentation is an important task of analysis and understanding of high-resolution remote sensing images (HRSIs). The deep convolutional neural network (DCNN)-based model shows their excellent performance in remote sensing image semantic segmentation. Most of the existing HRSI semantic segmentation methods are only designed for a very limited data domain, that is, the training and test images are from the same dataset. The accuracy drops sharply once a model trained on a certain dataset is used for cross-domain prediction due to the difference in feature distribution of the dataset. To this end, this article proposes an unsupervised domain adaptation framework based on adversarial learning for HRSI semantic segmentation. This framework uses high-level feature alignment to narrow the difference between the source and target domains at the semantic level. It uses the category-certainty attention module to reduce the attention of the classifier on category-level aligned features and increase the attention on category-level unaligned features. Experimental results show that the proposed method performs favorably against the state-of-the-art methods in cross-domain segmentation.",Semantics,Image segmentation,Feature extraction,Adaptation models,Task analysis,"Zhang, Yi","Deng, Min",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Remote sensing,Training,Category-certainty attention,domain adaptation,generative adversarial networks,semantic segmentation,,,,,,,,,,,,,,,,
Row_375,"Chen, Yuncheng","Wang, Leiguang","Li, Jingying","Zheng, Chen",Hierarchical Self-Learning Knowledge Inference Based on Markov Random Field for Semantic Segmentation of Remote Sensing Images,,2024,0,"Semantic segmentation is one of the most important tasks in the field of remote sensing. As the spatial resolution increases, the remote sensing images can capture more detailed information and make hierarchical semantic interpretation possible. However, hierarchical semantic segmentation encounters high heterogeneity not only within the intra-layer classes but also among inter-layer classes. It brings challenges to semantic segmentation methods such as the convolutional neural network (CNN). In this article, a hierarchical self-learning knowledge inference model (HSKIM) based on the Markov random field (MRF) model is proposed for hierarchical semantic segmentation of remote sensing images. The HSKIM model introduces a new framework that integrates the advantages of CNN-based data feature learning and MRF-based hierarchical semantic inference. It contains three modules: data learning module ( $\boldsymbol {D}$ ), inference units generation module ( $\boldsymbol {I}$ ), and self-learning knowledge inference module ( $\boldsymbol {S}$ ). The module $\boldsymbol {D}$ uses CNN to learn specific data features layer by layer and extract preliminary geographical objects as the initial results. The module I refines the geographical objects using a novel boundary-preservation trick to generate more accurate inference units with clear geographical meaning. The module S introduces a hierarchical object-based MRF model to implement semantic inference among intra-layer and inter-layer inference units, guided by the spatial interactions and geographical criteria. This module can self-learn and update the relationship between classes iteratively and provide the final result. Experiments on the GID dataset with hierarchical classes, alongside 12 state-of-the-art CNN-based methods, validate the effectiveness and robustness of the proposed HSKIM model. The code of this article is available at https://github.com/iichengzi/HSKIM.",Semantics,Semantic segmentation,Remote sensing,Feature extraction,Spatial resolution,,,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Data mining,Convolutional neural networks,Convolutional neural network (CNN),hierarchical semantic inference,hierarchical semantic segmentation,Markov random field model (MRF),remote sensing,,,,,,,,,,,,,,,
Row_376,"Feng, Jiangfan","Zheng, Wei","Gu, Zhujun","Guo, Dongen",A position-aware attention network with progressive detailing for land use semantic segmentation of Remote Sensing images,,NOV 2 2023,1,"Deep learning has achieved remarkable success in the semantic segmentation of remote sensing images (RSIs).In the domain of semantic segmentation, where classification and localization tasks need to be performed simultaneously, it is crucial to consider both global and local spatial relationships in RSIs. This is especially important for the recognition of ground objects that have a slim and elongated appearance. However, existing methods for land use semantic segmentation lack an effective mechanism to coordinate and address these two aspects, resulting in limitations on the recognition of slim targets and the continuity of land object identification. Here, a specific attention-based network called PaANet is developed for semantic segmentation. Our proposed framework builds upon the Swin transformer by incorporating two key modules: the position-aware attention (PaA) module and the pyramid pooling expectation-maximization (PPEM) module. These modules provide significant improvements in recognition accuracy and the continuity of ground object recognition while preserving structural classification details. Furthermore, we propose a multiresolution data augmentation method that utilizes scale-related information to guide the encoder. This approach leads to improved performance and generalization ability for the model. In experiments, the mIoU of our approach for the BLU and GID datasets is 2.37% and 3.94% higher than that of the baseline model, respectively. Our results also show significant superior to those of other methods regarding the continuity of ground object recognition.",Semantic Segmentation,Remote Sensing Images,Attention mechanism,Land-use Classification,,"Qin, Rui",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_377,"Zhang, Lili","Lu, Yushi","Shi, Rui","Shen, Yipin",Towards interpretability lightweight semantic segmentation model for waterbody extraction in large-scale high resolution remote sensing images,,APR 17 2024,2,"In recent years, high resolution (HR) remote sensing images have brought significant changes to land cover monitoring. Monitoring water resources, a vital and scarce commodity for human survival, is a crucial aspect of land cover monitoring and forms the foundation for water resource allocation in regions facing shortages. Hence, a universal method is necessary to address this problem and handle large-scale water resources monitoring. We explore the network structure via improving the interpretability to reduce the network layers and propose a lightweight pixel-wise semantic segmentation model, which achieves waterbody extraction from GF-1 remote sensing images with high accuracy and high speed. The proposed model structure addresses both semantic segmentation for large and small waterbodies, extracting fine-grained edge and contour information from high-resolution feature maps. Additionally, it efficiently extracts multi-scale high-level semantic information using residual convolutional blocks and dilated convolutional blocks. The multi-scale feature maps are fused, and binary classification is predicted through Support Vector Machines (SVM). Furthermore, the paper introduces a model training method with an adaptive learning rate, reducing the overall training time. To validate the model's performance, remote sensing images from GF-1 are utilized to construct a dataset. Experimental results, compared with five models, demonstrate that the proposed method achieves the highest accuracy and least training time.",High resolution (HR) remote sensing images,lightweight semantic segmentation,pixel-wise waterbody extraction,,,"Shao, Yehong",,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_378,"Liang, Zhengyin","Wang, Xili",,,SEMANTIC SEGMENTATION NETWORK WITH BAND-LOCATION ADAPTIVE SELECTION MECHANISM FOR MULTISPECTRAL REMOTE SENSING IMAGES,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,1,"How to acquire effective information from multispectral remote sensing images is a challenging task in semantic segmentation of remote sensing images. In this paper, an end-to-end semantic segmentation network (BLASeNet) is proposed. The model adopts an encoder-decoder structure. In the encoder phase, to exploit the band correlation of multispectral remote sensing images, we propose an effective 3D Residual block to encode the spectral-spatial features of images. In order to extract more discriminative features from multispectral images, a band-location adaptive selection mechanism is proposed to adaptively learn the weights of different bands and different spatial locations within a single band, enhancing the expression of features. In the decoder phase, we introduce two trainable parameter matrices W-alpha and W-beta in the skip connections, adaptively adjusting the fusion ratio of low-level detail features and high-level semantic features by network learning, improving the image segmentation accuracy. In addition, we extend the channel attention to 3D data, further refining the fused feature maps. Experimental results on ISPRS Potsdam and Qinghai datasets demonstrate the effectiveness of BLASeNet.",Multispectral images,semantic segmentation,3D convolution,band-location adaptive selection mechanism,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_379,"Wang, Lijun","Li, Bicao","Wang, Bei","Li, Chunlei",PSR-Net: A Dual-Branch Pyramid Semantic Reasoning Network for Segmentation of Remote Sensing Images,,2023,0,"The long-range context information in the semantic segmentation network for remote sensing images (RSIs) plays an important role in the improvement of segmentation performance. However, in large RSIs, the interaction between local information and global information is limited. In order to solve the problem, we propose a dual-branch pyramid semantic reasoning segmentation network. Our dual-branch network consists of a global and local branch. The traditional CNN network is employed on the global branch, and a lightweight multi-scale hierarchical feature aggregation (MHFA) module is introduced into the local branch. In addition, the Feature Semantic Reasoning (FSR) module is proposed to enhance the valuable features and weaken the useless features to improve the semantic representation of RSIs, and then the double branch transformer is embedded. The ablation experiment on the Beijing Land-Use (BLU) dataset illustrates the effectiveness of the added modules, and the results presented by comparison with other traditional networks also confirm the superiority of our proposed network. The proposed network can achieve better segmentation accuracy on large-scale RSI datasets.",Remote Sensing Images,Semantic Segmentation,Multi-scale Hierarchical Feature Aggregation,Feature Semantic Reasoning,,"Huang, Jie","Song, Mengxing",,,"ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2023, PT II",,,,,,,,,,,,,,,,,,,,,,,
Row_380,"Ma, Nan","Sun, Lin","Zhou, Chenghu","He, Yawen",CLOUD DETECTION FOR REMOTE SENSING IMAGES BASED ON DIFFERENCE FEATURES AND SEMANTIC SEGMENTATION NETWORK,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),2022,0,"High-precision cloud detection for remote sensing image is of great significance for applications in agriculture, environment, meteorology and other fields. Cloud detection in bright surface environments and thin clouds identification have always been a challenge in cloud detection research. Aiming at this problem, a cloud detection method for remote sensing images based on difference features and semantic segmentation network is proposed in this paper. Cloudy and cloudless images of the same area are used to obtain difference features, and cloudless images are used as the surface reference. The multi-band cloud detection network based on U-network (U-net) fully learns the difference feature between clouds and the surface, and combines the feature information of the shallow and deep layers to accurately identify the cloud and the surface. Experiments were conducted on the Landsat 8 validation dataset. The results show that the proposed method achieves good performance. It improves the detection accuracy and reduces the misclassification over the bright surface underlying compared with the method based on top of atmosphere reflectance (TOA).",Difference feature,segmentation networks,cloud detection,remote sensing image,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_381,"Pang, Shiyan","Shi, Yepeng","Hu, Hanchun","Ye, Lizhi",PTRSegNet: A Patch-to-Region Bottom–Up Pyramid Framework for the Semantic Segmentation of Large-Format Remote Sensing Images,,2024,1,"Semantic segmentation is a basic task in the interpretation of remote sensing images. Mainstream deep-learning-based semantic segmentation algorithms typically process images with small sizes. However, remote sensing images typically involve large areas with buildings and water, which have weak textures. Because of the limited range of receptive fields, the semantic segmentation of such areas from small images may lead to problems, such as loss of spatial features and inaccurate boundary extraction. To address these problems, this article devises a patch-to-region framework for the semantic segmentation of large-format remote sensing images. This framework has a bottom-up pyramid structure, where the bottom layer is a small image patch, referred to as a ""patch,"" and the upper layer is a large image region, referred to as a ""region."" The classical semantic segmentation network is first used to process small image patches to obtain pixel-by-pixel semantic features. Then, the pixel-by-pixel semantic features are sparsely reduced into patch-level semantic feature vectors, and the semantic feature vectors of the entire image region are processed through the contextual information extractor to extract the global semantic feature vectors. Subsequently, an information aggregation module is used to integrate the global semantic feature vectors and semantic features to obtain new semantic features with both global and local information. Finally, a lightweight decoding module is used to process the new semantic features to obtain the final semantic segmentation result. The developed framework is evaluated over three public datasets. The results of extensive experiments show that the framework can achieve more accurate and reliable semantic segmentation of high-resolution remote sensing images than state-of-the-art semantic segmentation algorithms. Moreover, ablation studies are performed to verify the effectiveness of each module of the framework.",Feature extraction,Semantics,Transformers,Semantic segmentation,Remote sensing,"Chen, Jia",,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Data mining,Convolution,Building extraction,high-resolution remote sensing images,large-format image processing,semantic segmentation,transformer,,,,,,,,,,,,,,,
Row_382,"Zhang, Ronghuan","Zhao, Jing","Li, Ming","Zou, Qingzhi",FEST: Feature Enhancement Swin Transformer for Remote Sensing Image Semantic Segmentation,"PROCEEDINGS OF THE 2024 27 TH INTERNATIONAL CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK IN DESIGN, CSCWD 2024",2024,0,"The global context is crucial for the precise segmentation of remote sensing images. However, the large volumes and high spatial resolutions of remote sensing images make efficient analysis of the entire scene challenging for most convolutional neural network (CNN)-based methods. To address this issue, we propose to design an innovative framework for semantic segmentation of remote sensing images called Feature Enhancement Swin Transformer (FEST). Firstly, we utilize the Swin Transformer as the encoder and incorporates a Global Information Enhancement Model (GIEM) within each Swin Transformer block to reduce information loss and enable encoding of more accurate spatial information. Secondly, we introduce an enhanced decoding structure called Enhanced Feature Fusion Module (EFFM) with added enhanced channel and spatial attention modules to retain localized information while obtaining extensive contextual information. Finally, for loss calculation, we utilize the dice and cross-entropy loss to jointly supervise the model, aiming to achieve a competitive performance. We comprehensively evaluated FEST on the ISPRS-Vaihingen and Potsdam datasets. The results indicate that our approach has achieved significant improvements in semantic segmentation tasks compared to existing methods.",global information,semantic segmentation,Swin Transformer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_383,"Yang, Ruiqi","Dai, Qinlin","Cheng, Haiyan","Zhang, Yue",IMPROVING SEMANTIC SEGMENTATION PERFORMANCE BY JOINTLY USING HIGH RESOLUTION REMOTE SENSING IMAGE AND NDSM,"XXIV ISPRS CONGRESS: IMAGING TODAY, FORESEEING TOMORROW, COMMISSION III",2022,2,"Semantic segmentation algorithms based on full convolutional neural network have greatly improved segmentation accuracy of highresolution remote sensing (RS) images. However, the interpretation of RS images from single sensor is still challenging due to the variety and complexity of land objects, the extreme imbalance distributions of land objects on size and numbers. In contrast, multiple sensors can provide complementary information on the land classes, and thus benefit the interpretation. In this context, this research explores the joint use of RGB optical bands and normalized DSM (nDSM) to analyze an urban scene. The method firstly concatenated three channels RGB image and one channel nDSM band into a four-channel image. Thereafter, ResNet-101 network with fine adjustment were utilized as the backbone network to retain multiple feature information by residual blocks. Then the augmented RGB and nDSM images were used to training the network. The established model was evaluated on the Postdam test set. Results show that the proposed method achieves 86.85% on Overall Accuracy (OA), 77.42% Mean Intersection Over Union (MIOU), which is 6.88% and 11.39% higher than the result achieved by single RGB images. Especially, small targets, such as car and tree, are higher. The experimental results show that the simple structure adjustment of ResNet-101 network can achieve good segmentation performance on RS images (especially small targets) after the combination of twice augmented RGB channels and nDSM channels respectively. In addition, with the addition of nDSM, the accuracy of buildings and trees with height information has been improved.",Semantic Segmentation,Deep Learning,nDSM,ResNet,Resolution Remote Sensing,"Chen, Nan","Wang, Leiguang",,,,,Augmentation,,,,,,,,,,,,,,,,,,,,,
Row_384,"Lv, Xiaowei","Wang, Rui","Zheng, Chengyu","Yang, Qicheng",Multi-representation decoupled joint network for semantic segmentation of remote sensing images,,FEB 2024,0,"In recent years, semantic segmentation has become an important means of processing remote sensing images, and it is widely used in various fields such as natural disaster detection, environmental protection, and land resource management. In response to this, the mainstream method of the deep convolutional network is constantly innovating and iterating. However, previous methods usually do not fully exploit the information associations between different representations, and the information of low-level representations is usually not well applied. In response to this, we propose a multi-representation decoupled joint network (MDJN) based on a three-branch architecture to improve the performance of semantic segmentation on remote sensing images, which utilizes multi-representation decoupling (MRD) to decouple the original single-branch network into the main branch, body branch and edge branch to enhance information fusion for different representations. Specifically, based on representation learning, we first propose a cross-representation graph convolution (CGC) module to fully mine and learn the context information between different representations with the help of graph convolutional networks (GCN). Secondly, we propose a new three-branch information interaction (TII) module to perform three-way interaction for the information of the three branches, so that the intra-class consistency and inter-class expressivity between different representations can fully play a role. The mean intersection over union (mIoU) of MDJN reaches 78.19% and 81.26% respectively on on both International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam datasets.",Graph convolution,Multi-representation,Decoupling module,Three-way information interaction,Remote sensing image,"Wang, Zhaoxin","Nie, Jie",,,MULTIMEDIA TOOLS AND APPLICATIONS,,Semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_385,"Xu, Penglei","Tang, Hong","Ge, Jiayi","Feng, Lin",ESPC_NASUnet: An End-to-End Super-Resolution Semantic Segmentation Network for Mapping Buildings From Remote Sensing Images,,2021,24,"Higher resolution building mapping from lower resolution remote sensing images is in great demand due to the lack of higher resolution data access, especially in the context of disaster assessment. High resolution building layout map is crucial for emergency rescue after the disaster. The emergency response time would be reduced if detailed building footprints were delineated from more easily available low-resolution data. To achieve this goal, we propose a super-resolution semantic segmentation network calledESPC_NASUnet, which consists of a feature super-resolution module and a semantic segmentation module. To the best of authors' knowledge, this is the first work to systematically explore a deep learning-based approach to generate semantic maps with higher spatial resolution fromlower spatial resolution remote sensing images in an end-to-end fashion. The experimental results for two datasets suggest that the proposed network is the best among four different end-to-end architectures in terms of both pixel-level metrics and object-level metrics. In terms of pixel-level F1-score, the improvements are greater than 0.068 and 0.055. Regarding the object-levelF1-score, the disparities between ESPC_NASUnet and other end-to-end methods are more than 0.083 and 0.161 in the two datasets, respectively. Compared with stage-wise methods, our end-to-end network is less impacted by low-resolution input images. Finally, the proposed network produces building semantic maps comparable to those generated by semantic segmentation networks trained with high-resolution images and the ground truth utilizing the two datasets.",Buildings,Semantics,Remote sensing,Spatial resolution,Image segmentation,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,Convolution,Superresolution,Building extraction,end-to-end network,remote sensing,super-resolution semantic segmentation (SRSS),,,,,,,,,,,,,,,,
Row_386,"Li, Haifeng","Qiu, Kaijian","Chen, Li","Mei, Xiaoming",SCAttNet: Semantic Segmentation Network With Spatial and Channel Attention Mechanism for High-Resolution Remote Sensing Images,,MAY 2021,163,"High-resolution remote sensing images (HRRSIs) contain substantial ground object information, such as texture, shape, and spatial location. Semantic segmentation, which is an important task for element extraction, has been widely used in processing mass HRRSIs. However, HRRSIs often exhibit large intraclass variance and small interclass variance due to the diversity and complexity of ground objects, thereby bringing great challenges to a semantic segmentation task. In this letter, we propose a new end-to-end semantic segmentation network, which integrates lightweight spatial and channel attention modules that can refine features adaptively. We compare our method with several classic methods on the ISPRS Vaihingen and Potsdam data sets. Experimental results show that our method can achieve better semantic segmentation results. The source codes are available at https://github.com/lehaifeng/SCAttNet.",Semantics,Image segmentation,Feature extraction,Remote sensing,Task analysis,"Hong, Liang","Tao, Chao",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Computational modeling,Training,Attention module,convolutional neural network,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,
Row_387,"Zhu, Enze","Chen, Zhan","Wang, Dingkai","Shi, Hanru",UNetMamba: An Efficient UNet-Like Mamba for Semantic Segmentation of High-Resolution Remote Sensing Images,,2025,0,"Semantic segmentation of high-resolution remote sensing images is vital in downstream applications such as land-cover mapping, urban planning, and disaster assessment. Existing Transformer-based methods suffer from the constraint between accuracy and efficiency, while the recently proposed Mamba is renowned for being efficient. Therefore, to overcome the dilemma, we propose UNetMamba, a UNet-like semantic segmentation model based on Mamba. It incorporates a Mamba segmentation decoder (MSD) that can efficiently decode the complex information within high-resolution images, and a local supervision module (LSM), which is train-only but can significantly enhance the perception of local contents. Extensive experiments demonstrate that UNetMamba outperforms the state-of-the-art (SOTA) methods with mIoU increased by 0.87% on LoveDA and 0.39% on ISPRS Vaihingen while achieving high efficiency through the lightweight design, less memory footprint, and reduced computational cost. The source code is available at https://github.com/EnzeZhu2001/UNetMamba.",Semantic segmentation,Decoding,Remote sensing,Semantics,Transformers,"Liu, Xiaoxuan","Wang, Lei",,,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,,Accuracy,Visualization,Buildings,Training,Sensors,Mamba,remote sensing,semantic segmentation,,,,,,,,,,,,,,
Row_388,"Zheng, Chen","Wang, Leiguang","Chen, Xiaohui",,A Hybrid Markov Random Field Model With Multi-Granularity Information for Semantic Segmentation of Remote Sensing Imagery,,AUG 2019,14,"High-spatial-resolution (HSR) remote sensing images usually contain rich hierarchical semantic information. However, many methods fail to solve the segmentation misclassification problems for HSR images due to just considering one layer of granularity information, such as the pixel granularity layer or the object granularity layer. This paper presents a hybrid Markov random field model for the semantic segmentation of HSR images by paying closer attention to capture the multi-granularity information. In this model, a probability graph with a multilayer structure is first built to represent different granularities information. Then, a hybrid label field is developed to model the multi-granularity classes with the form of a vector, and a new joint distribution is designed to capture the isotropic spatial interactions within each layer of the hybrid label field and the anisotropic spatial interactions between different layers. A generative probabilistic inference is proposed to realize the synergy between the multi-granularity information and the hybrid label interactions by iteratively updating the likelihood function and the joint probability of the hybrid label. The final semantic segmentation result can be achieved when the probabilistic inference converges. Experimental results over different HSR remote sensing images show that the proposed method can achieve more accurate segmentation than other state-of-the-art methods.",Markov random field (MRF),multi-granularity information,remote sensing image,semantic segmentation,,,,,,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_389,"Xiu, Xiaochen","Ma, Xianping","Pun, Man-On","Liu, Ming",AMBNet: Adaptive Multi-feature Balanced Network for Multimodal Remote Sensing Semantic Segmentation,,2024,0,"This work proposes an Adaptive Multi-feature Balanced network (AMBNet) for semantic segmentation in complex urban remote sensing scenarios. To fully exploit optical images and Digital Surface Models (DSM) data obtained from remote sensing sensors, a Depth Feature Extraction and Balancer (DFEB) module is devised to estimate and balance the depth information of all pixels by capturing detailed structural compositions of the ground surface. After that, a Parallel Multi-Stage Segmentator (PMSS) comprised of a dual-branch Encoder and Decoder with skip connections is constructed to perform effective segmentation by exploiting the balanced DSM (BDSM) and optical information. As a result, the proposed AMBNet can make effective use of optical images to complete depth information, so as to achieve multimodal information-assisted semantic segmentation for complex remote sensing scenes. Comprehensive experiments performed on the ISPRS Vaihingen and Potsdam remote sensing datasets confirm the segmentation performance of the proposed method.",Height estimation,Feature fusion,Multimodal semantic segmentation,,,,,,,APSIPA TRANSACTIONS ON SIGNAL AND INFORMATION PROCESSING,,,,,,,,,,,,,,,,,,,,,,,
Row_390,"Ma, Xianping","Wu, Qianqian","Zhao, Xingyu","Zhang, Xiaokang",SAM-Assisted Remote Sensing Imagery Semantic Segmentation With Object and Boundary Constraints,,2024,1,"Semantic segmentation of remote sensing imagery plays a pivotal role in extracting precise information for diverse downstream applications. Recent development of the segment anything model (SAM), an advanced general-purpose segmentation model, has revolutionized this field, presenting new avenues for accurate and efficient segmentation. However, SAM is limited to generating segmentation results without class information. Meanwhile, the segmentation map predicted by current methods generally exhibits excessive fragmentation and inaccuracy of boundary. This article introduces a streamlined framework designed to leverage the raw output of SAM by exploiting two novel concepts called SAM-generated object (SGO) and SAM-generated boundary (SGB). More specifically, we propose a novel object consistency loss and further introduce a boundary preservation loss in this work. Considering the content characteristics of SGO, we introduce the concept of object consistency to leverage segmented regions lacking semantic information. By imposing constraints on the consistency of predicted values within objects, the object consistency loss aims to enhance semantic segmentation performance. Furthermore, the boundary preservation loss capitalizes on the distinctive features of SGB by directing the model's attention to the boundary information of the object. Experimental results on two well-known datasets, ISPRS Vaihingen and LoveDA Urban, demonstrate the effectiveness and broad applicability of the proposed method. The source code for this work is accessible at https://github.com/sstary/ SSRS.",Semantic segmentation,Remote sensing,Task analysis,Decoding,Transformers,"Pun, Man-On","Huang, Bo",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Semantics,Computational modeling,Boundary preservation loss,object consistency loss,remote sensing,segment anything model (SAM),semantic segmentation,,,,,,,,,,,,,,,
Row_391,"Zhang, Zhili","Hu, Xiangyun","Yang, Bingnan","Deng, Kai",Enhanced semantic-positional feature fusion network via diverse pre-trained encoders for remote sensing image water-body segmentation,,OCT 2024,0,"In the era of increasingly advanced Earth Observation (EO) technologies, extracting pertinent information (such as water-bodies) from the Earth's surface has become a crucial task. Deep Learning, especially via pre-trained models, currently offers a highly promising approach for the semantic segmentation of Remote Sensing Imagery (RSI). However, effectively adapting these pre-trained models to RSI tasks remains challenging. Typically, these models undergo fine-tuning for specialized tasks, involving modifications to their parameters or structure of the original architecture, which may impact their inherent generalization capabilities. Furthermore, robust pre-trained models on nature images are not specifically designed for RSI, presenting challenges in their direct application to RSI tasks. To alleviate these problems, our study introduces a light-weight Enhanced Semantic-positional Feature Fusion Network (ESFFNet), leveraging diverse pre-trained image encoders alongside extensive EO data. The proposed method begins by leveraging pre-trained encoders, specifically Vision Transformer (ViT)-based and Convolutional Neural Network (CNN)-based models, to extract deep semantic and precise positional features respectively, without additional training. Following this, we introduce the Enhanced Semantic-positional Feature Fusion Module (ESFFM). This module adeptly merges semantic features derived from the ViT-based encoder with spatial features extracted from the CNN-based encoder. Such integration is realized via multi-scale feature fusion, local and long-distance feature integration, and dense connectivity strategies, leading to a robust feature representation. Finally, the Primary Segmentation-guided Fine Extraction Module (PSFEM) further bolsters the precision of remote sensing image segmentation. Collectively, these two modules constitute our light-weight decoder, with a parameter size of less than 4 M. Our approach is evaluated on two distinct water-body datasets, indicating superiority over other leading segmentation techniques. In addition, our method also demonstrates exemplary efficacy in diverse remote sensing segmentation tasks, such as building extraction and land cover classification. The source codes will be available at https://github.com/zhilyzhang/ESFFNet.",Pre-trained image encoders,extensive remote sensing imagery,feature fusion,deep learning,semantic segmentation,"Zhang, Mi","Zhu, Dehui",,,GEO-SPATIAL INFORMATION SCIENCE,,,,,,,,,,,,,,,,,,,,,,,
Row_392,"Wu, Boyang","Cui, Jianyong","Cui, Wenkai","Yuan, Yirong",Fast Semantic Segmentation of Remote Sensing Images Using a Network That Integrates Global and Local Information,,JUN 3 2023,1,"Efficient processing of ultra-high-resolution images is increasingly sought after with the continuous advancement of photography and sensor technology. However, the semantic segmentation of remote sensing images lacks a satisfactory solution to optimize GPU memory utilization and the feature extraction speed. To tackle this challenge, Chen et al. introduced GLNet, a network designed to strike a better balance between GPU memory usage and segmentation accuracy when processing high-resolution images. Building upon GLNet and PFNet, our proposed method, Fast-GLNet, further enhances the feature fusion and segmentation processes. It incorporates the double feature pyramid aggregation (DFPA) module and IFS module for local and global branches, respectively, resulting in superior feature maps and optimized segmentation speed. Extensive experimentation demonstrates that Fast-GLNet achieves faster semantic segmentation while maintaining segmentation quality. Additionally, it effectively optimizes GPU memory utilization. For example, compared to GLNet, Fast-GLNet's mIoU on the Deepglobe dataset increased from 71.6% to 72.1%, and GPU memory usage decreased from 1865 MB to 1639 MB. Notably, Fast-GLNet surpasses existing general-purpose methods, offering a superior trade-off between speed and accuracy in semantic segmentation.",remote sensing images,semantic segmentation,global and local information,,,"Ren, Xiancong",,,,SENSORS,,,,,,,,,,,,,,,,,,,,,,,
Row_393,"Zheng, Aihua","He, Jinbo","Wang, Ming","Li, Chenglong",Category-Wise Fusion and Enhancement Learning for Multimodal Remote Sensing Image Semantic Segmentation,,2022,9,"This article presents a simple yet effective method called Category-wise Fusion and Enhancement learning (CaFE), which leverages the category priors to achieve effective feature fusion and imbalance learning, for multimodal remote sensing image semantic segmentation. In particular, we disentangle the feature fusion process via the categories to achieve the category-wise fusion based on the fact that the feature fusion in the same category regions tends to have similar characteristics. The disentangled fusion would also increase the fusion capacity with a small number of parameters while reducing the dependence on large-scale training data. For the sample imbalance problem, we design a simple yet effective category-wise enhancement learning scheme. In particular, we assign the weight for each category region based on the proportion of samples in this region over the whole image. By this way, the learning algorithm would focus more on the regions with smaller proportion. Note that both category-wise feature fusion and imbalance learning are only performed in the training stage, and the segmentation efficiency is thus not affected. Experimental results on two benchmark datasets demonstrate the effectiveness of our CaFE against other state-of-the-art methods.",Category-wise enhancement learning (CEL),category-wise fusion,imbalance learning,multimodal remote sensing,semantic segmentation,"Luo, Bin",,,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_394,"Li, Jinsong","Zhang, Shujun","Han, Qi","Sun, Yuanyuan",CSRL-Net: contextual self-rasterization learning network with joint weight loss for remote sensing image semantic segmentation,,DEC 2 2023,1,"Semantic segmentation plays a vital role in the intelligent comprehension of remote sensing images (RSIs). However, research on semantic segmentation of RSIs still faces the following challenges: 1) The complexity of ground object structures, including variations in scale and shading environments, poses difficulties for current methods in capturing global context. 2) In long-tail distributed remote sensing data, the scarcity of tail classes makes it difficult for their features to be effectively learned, as features of head classes often overshadow them. To address these issues, we propose a contextual self-rasterization learning network (CSRL-Net) for the semantic segmentation of RSIs. Our approach comprises the following two key components. Firstly, a grid context perception mechanism is developed to collaboratively establish context dependencies within and among multi-scale grids, capturing long-range spatial correlations. Secondly, a joint weight loss function is designed to convert the prior knowledge into weight factors. This loss function combines re-weighting and logit adjustment, giving more attention to tail classes and integrally balancing learning bias. To evaluate the effectiveness of our proposed method, we apply it to the Potsdam, Vaihingen, and GID datasets and compare its performance with current advanced models. Experimental results demonstrate that our method achieves excellent performance in terms of MIoU, mean F1 and OA, with improvements ranging from 0.364% to 1.764% compared to the 17 comparison models. Notably, the proposed joint weight loss significantly improves IoU and F1 for tail classes, resulting in increases of 2.909% (IoU) and 2.085% (F1) on the Vaihingen dataset and 4.697% (IoU) and 4.043% (F1) on the GID dataset.",Semantic segmentation,remote sensing images,grid context perception,joint weight loss,,,,,,INTERNATIONAL JOURNAL OF REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
Row_395,"Feng, Dongdong","Zhang, Zhihua","Yan, Kun",,A Semantic Segmentation Method for Remote Sensing Images Based on the Swin Transformer Fusion Gabor Filter,,2022,16,"Semantic segmentation of remote sensing images is increasingly important in urban planning, autonomous driving, disaster monitoring, and land cover classification. With the development of high-resolution remote sensing satellite technology, multilevel, large-scale, and high-precision segmentation has become the focus of current research. High-resolution remote sensing images have high intraclass diversity and low interclass separability, which pose challenges to the precision of the detailed representation of multiscale information. In this paper, a semantic segmentation method for remote sensing images based on Swin Transformer fusion with a Gabor filter is proposed. First, a Swin Transformer is used as the backbone network to extract image information at different levels. Then, the texture and edge features of the input image are extracted with a Gabor filter, and the multilevel features are merged by introducing a feature aggregation module (FAM) and an attentional embedding module (AEM). Finally, the segmentation result is optimized with the fully connected conditional random field (FC-CRF). Our proposed method, called Swin-S-GF, its mean Intersection over Union (mIoU) scored 80.14%, 66.50%, and 70.61% on the large-scale classification set, the fine land-cover classification set, and the ""AI + Remote Sensing imaging dataset"" (AI+RS dataset), respectively. Compared with DeepLabV3, mIoU increased by 0.67%, 3.43%, and 3.80%, respectively. Therefore, we believe that this model provides a good tool for the semantic segmentation of high-precision remote sensing images.",Image segmentation,Feature extraction,Transformers,Remote sensing,Convolution,,,,,IEEE ACCESS,,Semantics,Image edge detection,FAM,Gabor filter,remote sensing,semantic segmentation,Swin transformer,,,,,,,,,,,,,,,
Row_396,"Liu, Jiamin","Wang, Ziyi","Luo, Fulin","Guo, Tan",ESMS-Net: Enhancing Semantic-Mask Segmentation Network With Pyramid Atrousformer for Remote Sensing Image,,2024,0,"Transformers has gained widespread adoption in remote sensing image (RSI) segmentation. However, RSI has densely overlapping terrain and significant shadow, making it challenging to segment the blended boundaries of terrains that are the hard classes. Currently, most transformer-based methods construct the self-attention with a sliding window, which influences the feature receptive fields to conquer the intersecting and overlapping objects. Additionally, they often rarely focus specifically on the representation of these hard segmentation objects. To overcome these challenges, we propose a novel Enhancing Semantic Mask Segmentation Network (ESMS-Net) framework including a local-global joint encoder, an auxiliary enhanced encoder, and a multiscale dense decoder. In the local-global joint encoder, we construct a Pyramid Pooling AtrousFormer (PPAFormer) that performs the self-attention with a pyramid-structured atrous sliding window, which enhances the range of receptive fields and the global representation performance. Meanwhile, we construct the dual-feature fusion module (DFFM) and multilevel feature weighted fusion (MFWF) in the multiscale dense decoder to reduce information loss and facilitate the interaction of deep semantic information. For the auxiliary enhanced encoder, we develop a semantic mask based on the predicted results to maintain the hard segmentation classes, and then use the same structure as the first two stages of the local-global joint encoder to learn the hard regions again. Extensive experiments demonstrate the proposed ESMS-Net can achieve significant improvements for segmentation performance compared with the state-of-the-art methods on the ISPRS-Vaihingen and Potsdam datasets.",Feature extraction,Transformers,Semantics,Decoding,Semantic segmentation,"Yang, Feng","Gao, Xinbo",,,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,,Remote sensing,Fuses,Data mining,Convolutional codes,Computer vision,Pyramid atrous structure,remote sensing image (RSI),semantic mask,semantic segmentation,,,,,,,,,,,,,transformer
Row_397,"Bello, Inuwa Mamuda","Zhang, Ke","Wang, Jingyu","Aslam, Muhammad Azeem",A Multiscale Segmentation Framework for Uncompleted Building Footprint Extraction from Remote Sensing Imagery,"2021 IEEE ASIA-PACIFIC CONFERENCE ON GEOSCIENCE, ELECTRONICS AND REMOTE SENSING TECHNOLOGY (AGERS-2021)",2021,1,"Building extraction from aerial and satellite images has been playing a significant role in urban development. The deep neural networks' automatic feature extraction capability provides the ease to infer building footprint from remote sensing imagery with greater accuracy. However, designing a classifier that can infer salient features such as the building category remains a challenging task This article proposes a parameter- efficient, multiscale segmentation network for uncompleted building structure extraction. The proposed network was designed based on the architectural framework of the inception module that allows feature learning at multiscale level. Our proposed framework consists of three types of modules known as the subnets that form the encoder, the decoder, and the bottleneck of the network that allow multiscale semantic learning for segmentation application. The experimental result indicates that our proposed network required less training time to attain the best accuracy than state-of-the-art networks. We also present an approach to determine the precise geographical coordinates of the uncompleted building segment's using the georeferencing technique.",Remote sensing,image segmentation,neural networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_398,"Wang, Kaiyue","Fan, Xiaoye","Wang, Qi",,FPB-UNet plus plus : Semantic Segmentation for Remote Sensing Images of reservoir area via Improved UNet plus plus with FPN,"6TH INTERNATIONAL CONFERENCE ON INNOVATION IN ARTIFICIAL INTELLIGENCE, ICIAI2022",2022,0,"In order to improve the accuracy of semantic segmentation of remote sensing images in the reservoir area, this paper improves UNet ++, and proposes a UNet ++ semantic segmentation network model fused with feature pyramid network, called FPB-UNet ++. First, in order to fully extract the semantic information of different scales and enhance the recovery ability of the spatial information of remote sensing images, this paper uses the improved feature pyramid structure as the basic unit of the UNet ++ coding structure. Then, the pooling of position information will be lost between each coding unit To remove the layer, use convolution instead. Finally, in order to make full use of multi-scale feature information in the multi-sided output part, all the side output feature maps are stitched and fused in the channel dimension. Through experiments on the open and self-built remote sensing image semantic segmentation data set of Xiaolangdi Reservoir area, the results show that the network model has a good segmentation effect on feature information.",Remote Sensing Image,Semantic Segmentation,UNet plus,Deep Neural Network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_399,"Sun, Long","Li, Lingling","Shao, Yilin","Jiao, Licheng",Which Target to Focus on: Class-Perception for Semantic Segmentation of Remote Sensing,,2023,9,"Deep-learning-based (DL) methods have dominated the task of semantic segmentation of remote sensing images. However, the sizes of different objects vary widely, and there is a great deal of label noise due to the inevitable shadows. Therefore, there is an urgent need for a method that can precisely handle complex ground data. In this article, we propose an interclass enhanced network (ICEN) for representing features of varying sizes. It comprises two branches: sparse representation network (SPN) and feature extraction network (FEN). Then, a class-perception block (CPB) is inserted between the two branches to instruct the SPN's low-level semantic features to be merged into the deeper network. Such a block can reduce label noise in remote sensing image segmentation. In addition, the proposed EIRI provides a more precise classification process for target edges containing many misclassified points without requiring excessive computational overhead. The experimental results of our proposed class-perception network (C-PNet) achieve competitive performance on the Vaihingen, Potsdam, LoveDA, and UAVid datasets.",Feature extraction,Remote sensing,Semantics,Deep learning,Computer architecture,"Liu, Xu","Chen, Puhua","Liu, Fang","Yang, Shuyuan",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,"Hou, Biao",Semantic segmentation,Neural networks,Class-perception,computational overhead,label noise,multiresolution,remote sensing,sparse representation,,,,,,,,,,,,,,
Row_400,"Shi, Wenxu","Meng, Qingyan","Zhang, Linlin","Zhao, Maofan",DSANet: A Deep Supervision-Based Simple Attention Network for Efficient Semantic Segmentation in Remote Sensing Imagery,,NOV 2022,11,"Semantic segmentation for remote sensing images (RSIs) plays an important role in many applications, such as urban planning, environmental protection, agricultural valuation, and military reconnaissance. With the boom in remote sensing technology, numerous RSIs are generated; this is difficult for current complex networks to handle. Efficient networks are the key to solving this challenge. Many previous works aimed at designing lightweight networks or utilizing pruning and knowledge distillation methods to obtain efficient networks, but these methods inevitably reduce the ability of the resulting models to characterize spatial and semantic features. We propose an effective deep supervision-based simple attention network (DSANet) with spatial and semantic enhancement losses to handle these problems. In the network, (1) a lightweight architecture is used as the backbone; (2) deep supervision modules with improved multiscale spatial detail (MSD) and hierarchical semantic enhancement (HSE) losses synergistically strengthen the obtained feature representations; and (3) a simple embedding attention module (EAM) with linear complexity performs long-range relationship modeling. Experiments conducted on two public RSI datasets (the ISPRS Potsdam dataset and Vaihingen dataset) exhibit the substantial advantages of the proposed approach. Our method achieves 79.19% mean intersection over union (mIoU) on the ISPRS Potsdam test set and 72.26% mIoU on the Vaihingen test set with speeds of 470.07 FPS on 512 x 512 images and 5.46 FPS on 6000 x 6000 images using an RTX 3090 GPU.",convolutional neural network (CNN),deep supervision,lightweight model,remote sensing,semantic segmentation,"Su, Chen","Jancso, Tamas",,,REMOTE SENSING,,,,,,,,,,,,,,,,,,,,,,,
