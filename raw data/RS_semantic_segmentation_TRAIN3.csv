,author1,author2,author3,title,journal/book,publish time,citation,abstract,keyword1,keyword2,keyword3,keyword4,keyword5,author4,conference,author5,keyword6,author6,author7,keyword7,keyword8,keyword9,keyword10,keyword11,keyword12,author8,author9,author10,author11,keyword13,keyword14,keyword15,keyword16,author12
Row_1001,"Veljanovski, Tatiana","Kanjir, Ursa","Ostir, Kristof",OBJECT-BASED IMAGE ANALYSIS OF REMOTE SENSING DATA,GEODETSKI VESTNIK,DEC 2011,6,"Remote sensing has developed various methods and technologies for con tactless and cost-effective mapping of large area land cover/land use maps and other thematic maps. The key factor for the availability and reliability of these maps for use in Earth sciences is the development of effective procedures for satellite data analysis and classification. The most appropriate approach for classifying low and medium resolution satellite images (pixel size is coarser than, or at best similar to, the size of geographical objects) is pixel-based classification in which an individual pixel is classified into the closest class based on its spectral similarity.With increasing spatial resolution, pixel-based classification methods became less effective, since the relationship between the pixel size and the dimension of the observed objects on the Earth's surface has changed significantly. Therefore object-oriented classification has become increasingly popular over the past decade. This combines segmentation (which is a fundamental phase of the approach) and contextual classification. Segmentation divides the image into homogeneous pixel groups (segments), which are - during the Semantic classification process - arranged into classes based on their spectral, geometric, textural and other features during. The intent of this paper is to present the theoretical argumentation and methodology of object-bused image analysis of remote sensing data, provide an overview of the field and point out certain restrictions as regards the current operational solutions.",remote sensing,object-based image analysis,segmentation,object-based classification,semantic classification,,,,,,,,,,,,,,,,,,,,,
Row_1002,"Liu, Bin","Li, Bing","Liu, Haiming",ST-MDAMNet: Swin transformer combines multi-dimensional attention mechanism for semantic segmentation of high-resolution earth surface images,ADVANCES IN SPACE RESEARCH,OCT 15 2024,0,"In the derection of remote sensing (RS) image analysis, semantic segmentation, as an important technology, is of key significance for the identification and analysis of land surface cover types. In recent years, applying deep learning models to tasks such as road extraction, water distribution extraction, building classification and building segmentation from RS images has become an important research hot- spot. Due to its limited receptive field, traditional convolutional neural networks (CNN) cannot effectively capture global context information. Transformer uses the multi-head self-attention mechanism to capture a wide range of information and can solve this problem well. Therefore, we proposed ST-MDAMNet based on Swin Transformer and combined with the multi-dimensional attention mechanism. First, a feature enhancement module (FAM) is introduced after each stage of the Swin Transformer encoder to effectively enhance the model's proficiency in identifying essential information. Secondly, a feature fusion module (FFM) is proposed to effectively fuse the multi-scale information of the encoder part. It further improves the expression ability of different dimensional features and effectively improves the detection effect of small targets. Ultimately, the fused features are input into the multi-dimensional attention module (MDAM) to carefully optimize the features, which greatly increases the effect of semantic segmentation of RS images. We demonstrate the effectiveness of each module through ablation experiments. Comparative experiments are completed on two publicly large-scale data- sets, and the proposed method shows excellent results compared with state-of-the-art methods. (c) 2024 COSPAR. Published by Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",Swin Transfromer,Semantic segmentation,Remote sensing,Attention mechanism,,"Li, Shuofeng",,,,,,,,,,,,,,,,,,,,
Row_1003,"Tasar, Onur","Tarabalka, Yuliya","Alliez, Pierre",Incremental Learning for Semantic Segmentation of Large-Scale Remote Sensing Data,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,SEP 2019,86,"In spite of remarkable success of the convolutional neural networks on semantic segmentation, they suffer from catastrophic forgetting: a significant performance drop for the already learned classes when new classes are added on the data having no annotations for the old classes. We propose an incremental learning methodology, enabling to learn segmenting new classes without hindering dense labeling abilities for the previous classes, although the entire previous data are not accessible. The key points of the proposed approach are adapting the network to learn new as well as old classes on the new training data, and allowing it to remember the previously learned information for the old classes. For adaptation, we keep a frozen copy of the previously trained network, which is used as a memory for the updated network in the absence of annotations for the former classes. The updated network minimizes a loss function, which balances the discrepancy between outputs for the previous classes from the memory and updated networks, and the misclassification rate between outputs for the new classes from the updated network and the new ground-truth. For remembering, we either regularly feed samples from the stored, little fraction of the previous data or use the memory network, depending on whether the new data are collected from completely different geographic areas or from the same city. Our experimental results prove that it is possible to add new classes to the network, while maintaining its performance for the previous classes, despite the whole previous training data are not available.",Catastrophic forgetting,convolutional neural networks (CNNs),incremental learning,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,
Row_1004,"Kniaz, Vladimir V.",,,Conditional GANs for Semantic Segmentation of Multispectral Satellite Images,,2018,12,"Algorithms for automatic semantic segmentation of the satellite images provide an effective approach for the generation of vector maps. Convolutional neural networks (CNN) have achieved the state-of-the-art quality of the output segmentation on the satellite images-to-semantic labels task. However, the generalization ability of such methods is not sufficient to process the satellite images that were captured in the different area or during the different season. Recently, the Generative Adversarial Networks (GAN) were introduced that can overcome the overfitting using the adversarial loss. This paper is focused on the development of the new GAN model for effective semantic segmentation of multispectral satellite images. The pix2pix(1) model is used as the starting point of the research. It is trained in the semi-supervised setting on the aligned pairs of images. The perceptual validation has demonstrated the high quality of the output labels. The evaluation on the independent test dataset has proved the robustness of GANs on the task of semantic segmentation of multispectral satellite images.",Generative Adversarial Networks,Semantic segmentation,Remote sensing,Multispectral satellite images,Convolutional Neural Networks,,IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXIV,,,,,,,,,,,,,,,,,,,
Row_1005,"Ji, Xun","Tang, Longbin","Liu, Tianhe",HeteroNet: a heterogeneous encoder-decoder network for sea-land segmentation of remote sensing images,JOURNAL OF ELECTRONIC IMAGING,SEP 1 2023,1,"The research on sea-land segmentation of remote sensing images has received tremendous attention, which is of great significance to coastline extraction and ocean monitoring. In recent years, various convolutional neural networks (CNNs) have been presented to achieve precise and efficient sea-land segmentation effect. However, existing CNNs typically adopt the symmetric encoder-decoder structure, which is inefficient for feature extraction, feature fusion, and information transmission. To address these problems, this work develops a CNN for pixel-level sea-land segmentation, termed HeteroNet. The proposed HeteroNet constructs a heterogeneous encoder-decoder structure consisting of successive dense-connected encoding modules and squeeze-and-excitation-connected decoding modules that can effectively enhance the feature extraction and fusion capabilities of the network. In addition, an easy-to-embed global context enhanced module is designed to further facilitate information transmission efficiency. Comparative experiments with state-of-the-art methods are conducted to reveal that the HeteroNet can exhibit superior sea-land segmentation performance in different scenarios, and the ablation study is performed to demonstrate the effectiveness of each component in the network.(c) 2023 SPIE and IS&T",sea-land segmentation,remote sensing images,semantic segmentation,deep learning,convolutional neural network,"Guo, Hui",,,,,,,,,,,,,,,,,,,,
Row_1006,"Zhang, Lianjun","Zhang, Jing","Zhang, Dapeng",Urban Road Extraction from High-resolution Remote Sensing Images Based on Semantic Model,,2010,1,"From the perspective of semantic network model, this paper does research on the urban road extraction from high-resolution remote sensing images. First, we analyze spatial features and contextual information of road in high resolution remote sensing images. By using the method of regional segmentation edge detection, area filter and Hough transform methods respectively, we obtain the candidate nodes for the semantic network model of road. And with the application of space semantic model theory, this paper establishes the semantic network model. Finally, through the experiment of road extraction from Quick Bird images of Beijing urban area, it represents that this method is feasible to extract road information automatically by use of the semantic model.",high-resolution,remote sensing images,road feature extraction,area filter,Hough transform,"Hou, Xiaohui",2010 18TH INTERNATIONAL CONFERENCE ON GEOINFORMATICS,"Yang, Gang",,,,,,,,,,,,,,,,,,
Row_1007,"Shi, Zongwen","Fan, Junfu","Du, Yujie",LULC-SegNet: Enhancing Land Use and Land Cover Semantic Segmentation with Denoising Diffusion Feature Fusion,REMOTE SENSING,DEC 2024,0,"Deep convolutional networks often encounter information bottlenecks when extracting land object features, resulting in critical geometric information loss, which impedes semantic segmentation capabilities in complex geospatial backgrounds. We developed LULC-SegNet, a semantic segmentation network for land use and land cover (LULC), which integrates features from the denoising diffusion probabilistic model (DDPM). This network enhances the clarity of the edge segmentation, detail resolution, and the visualization and accuracy of the contours by delving into the spatial details of the remote sensing images. The LULC-SegNet incorporates DDPM decoder features into the LULC segmentation task, utilizing machine learning clustering algorithms and spatial attention to extract continuous DDPM semantic features. The network addresses the potential loss of spatial details during feature extraction in convolutional neural network (CNN), and the integration of the DDPM features with the CNN feature extraction network improves the accuracy of the segmentation boundaries of the geographical features. Ablation and comparison experiments conducted on the Circum-Tarim Basin Region LULC Dataset demonstrate that the LULC-SegNet improved the LULC semantic segmentation. The LULC-SegNet excels in multiple key performance indicators compared to existing advanced semantic segmentation methods. Specifically, the network achieved remarkable scores of 80.25% in the mean intersection over union (MIOU) and 93.92% in the F1 score, surpassing current technologies. The LULC-SegNet demonstrated an IOU score of 73.67%, particularly in segmenting the small-sample river class. Our method adapts to the complex geophysical characteristics of remote sensing datasets, enhancing the performance of automatic semantic segmentation tasks for land use and land cover changes and making critical advancements.",land use and land cover,semantic segmentation,remote sensing images,denoising diffusion probabilistic model,feature fusion,"Zhou, Yuke",,"Zhang, Yi",K-means clustering algorithms,,,,,,,,,,,,,,,,,
Row_1008,"Wang, Yan","Yang, Ling","Liu, Xinzhan",An improved semantic segmentation algorithm for high-resolution remote sensing images based on DeepLabv3+,SCIENTIFIC REPORTS,APR 27 2024,3,"High-precision and high-efficiency Semantic segmentation of high-resolution remote sensing images is a challenge. Existing models typically require a significant amount of training data to achieve good classification results and have numerous training parameters. A novel model called MST-DeepLabv3+ was suggested in this paper for remote sensing image classification. It's based on the DeepLabv3+ and can produce better results with fewer train parameters. MST-DeepLabv3+ made three improvements: (1) Reducing the number of model parameters by substituting MobileNetV2 for the Xception in the DeepLabv3+'s backbone network. (2) Adding the attention mechanism module SENet to increase the precision of semantic segmentation. (3) Increasing Transfer Learning to enhance the model's capacity to recognize features, and raise the segmentation accuracy. MST-DeepLabv3+ was tested on international society for photogrammetry and remote sensing (ISPRS) dataset, Gaofen image dataset (GID), and practically applied to the Taikang cultivated land dataset. On the ISPRS dataset, the mean intersection over union (MIoU), overall accuracy (OA), Precision, Recall, and F1-score are 82.47%, 92.13%, 90.34%, 90.12%, and 90.23%, respectively. On the GID dataset, these values are 73.44%, 85.58%, 84.10%, 84.86%, and 84.48%, respectively. The results were as high as 90.77%, 95.47%, 95.28%, 95.02%, and 95.15% on the Taikang cultivated land dataset. The experimental results indicate that MST-DeepLabv3+ effectively improves the accuracy of semantic segmentation of remote sensing images, recognizes the edge information with more completeness, and significantly reduces the parameter size.",,,,,,"Yan, Pengfei",,,,,,,,,,,,,,,,,,,,
Row_1009,"Pan, Xin","Zhao, Jian","Xu, Jun",An End-to-End and Localized Post-Processing Method for Correcting High-Resolution Remote Sensing Classification Result Images,REMOTE SENSING,MAR 2020,14,"Since the result images obtained by deep semantic segmentation neural networks are usually not perfect, especially at object borders, the conditional random field (CRF) method is frequently utilized in the result post-processing stage to obtain the corrected classification result image. The CRF method has achieved many successes in the field of computer vision, but when it is applied to remote sensing images, overcorrection phenomena may occur. This paper proposes an end-to-end and localized post-processing method (ELP) to correct the result images of high-resolution remote sensing image classification methods. ELP has two advantages. (1) End-to-end evaluation: ELP can identify which locations of the result image are highly suspected of having errors without requiring samples. This characteristic allows ELP to be adapted to an end-to-end classification process. (2) Localization: Based on the suspect areas, ELP limits the CRF analysis and update area to a small range and controls the iteration termination condition. This characteristic avoids the overcorrections caused by the global processing of the CRF. In the experiments, ELP is used to correct the classification results obtained by various deep semantic segmentation neural networks. Compared with traditional methods, the proposed method more effectively corrects the classification result and improves classification accuracy.",semantic segmentation,high-resolution remote sensing image,pixel-wise classification,result correction,conditional random field (CRF),,,,,,,,,,,,,,,,,,,,,
Row_1010,"Liu, Yansong","Piramanayagam, Sankaranarayanan","Monteiro, Sildomar T.",Semantic segmentation of multisensor remote sensing imagery with deep ConvNets and higher-order conditional random fields,JOURNAL OF APPLIED REMOTE SENSING,JAN 11 2019,35,"Aerial images acquired by multiple sensors provide comprehensive and diverse information of materials and objects within a surveyed area. The current use of pretrained deep convolutional neural networks (DCNNs) is usually constrained to three-band images (i.e., RGB) obtained from a single optical sensor. Additional spectral bands from a multiple sensor setup introduce challenges for the use of DCNN. We fuse the RGB feature information obtained from a deep learning framework with light detection and ranging (LiDAR) features to obtain semantic labeling. Specifically, we propose a decision-level multisensor fusion technique for semantic labeling of the very-high-resolution optical imagery and LiDAR data. Our approach first obtains initial probabilistic predictions from two different sources: one from a pretrained neural network fine-tuned on a three-band optical image, and another from a probabilistic classifier trained on LiDAR data. These two predictions are then combined as the unary potential using a higher-order conditional random field (CRF) framework, which resolves fusion ambiguities by exploiting the spatial-contextual information. We utilize graph cut to efficiently infer the final semantic labeling for our proposed higher-order CRF framework. Experiments performed on three benchmarking multisensor datasets demonstrate the performance advantages of our proposed method. (C) The Authors. Published by SPIE under a Creative Commons Attribution 3.0 Unported License.",semantic segmentation,multisensor remote sensing,light detection and ranging,deep convolutional neural networks,conditional random fields,"Saber, Eli",,,,,,,,,,,,,,,,,,,,
Row_1011,"Zeng, Junying","Gu, Yajin","Qin, Chuanbo",Unsupervised domain adaptation for remote sensing semantic segmentation with the 2D discrete wavelet transform,SCIENTIFIC REPORTS,OCT 9 2024,0,"There would be the differences in spectra, scale and resolution between the Remote Sensing datasets of the source and target domains, which would lead to the degradation of the cross-domain segmentation performance of the model. Image transfer faced two problems in the process of domain-adaptive learning: overly focusing on style features while ignoring semantic information, leading to biased transformation results, and easily overlooking the true transfer characteristics of remote sensing images, resulting in unstable model training. To address these issues, we proposes a novel dual-space generative adversarial domain adaptation segmentation framework, DS-DWTGAN, to minimize the differences between the source domain and the target domain. DS-DWTGAN aims to mitigate the distinctions between the source and target domains, thereby rectifying the imbalances in style and semantic representation.The framework introduces a network branch leveraging wavelet transform to capture comprehensive frequency domain and semantic information. It aims to preserve semantic details within the frequency domain space, mitigating image conversion deviations. Furthermore, our proposed method integrates output adaptation and data enhancement training strategies to reinforce the acquisition of domain-invariant features. This approach effectively diminishes noise interference during the migration process, bolsters model stability, and elevates the model's adaptability to remote sensing images within different domains. Experimental validation was conducted on the publicly available Potsdam and Vaihingen datasets. The findings reveal that in the PotsdamIRRG to Vaihingen task, the proposed method attains outstanding performance with mIoU and mF1 values reaching 56.04% and 67.28%, respectively. Notably, these metrics surpass the corresponding values achieved by state-of-the-art (SOTA) methods, registering an increase of 2.81% and 2.08%. In comparison to alternative approaches, our proposed framework exhibits superior efficacy in the domain of unsupervised semantic segmentation for UAV remote sensing images.",,,,,,"Jia, Xudong",,"Deng, Senyao",,"Xu, Jiahua","Tian, Huiming",,,,,,,,,,,,,,,
Row_1012,"Li, Qingyu","Mou, Lichao","Sun, Yao",A Review of Building Extraction From Remote Sensing Imagery: Geometrical Structures and Semantic Attributes,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,12,"In the remote sensing community, extracting buildings from remote sensing imagery has triggered great interest. While many studies have been conducted, a comprehensive review of these approaches that are applied to optical and synthetic aperture radar (SAR) imagery is still lacking. Therefore, we provide an in-depth review of both early efforts and recent advances, which are aimed at extracting geometrical structures or semantic attributes of buildings, including building footprint generation, building facade segmentation, roof segment and superstructure segmentation, building height retrieval, building-type classification, building change detection, and annotation data correction. Furthermore, a list of corresponding benchmark datasets is given. Finally, challenges and outlooks of existing approaches as well as promising applications are discussed to enhance comprehension within this realm of research.",Buildings,Remote sensing,Optical sensors,Optical imaging,Task analysis,"Hua, Yuansheng",,"Shi, Yilei",Radar polarimetry,"Zhu, Xiao Xiang",,Semantics,Building extraction,deep learning,optical imagery,review,synthetic aperture radar (SAR),,,,,,,,,
Row_1013,"Sun, Ziyu","Jiang, Weiguo","Ling, Ziyan",Using Multisource High-Resolution Remote Sensing Data (2 m) with a Habitat-Tide-Semantic Segmentation Approach for Mangrove Mapping,REMOTE SENSING,NOV 2023,9,"Mangrove wetlands are hotspots of global biodiversity and blue carbon reserves in coastal wetlands, with unique ecological functions and significant socioeconomic value. Annual fine-scale monitoring of mangroves is crucial for evaluating national conservation programs and implementing sustainable mangrove management strategies. However, annual fine-scale mapping of mangroves over large areas using remote sensing remains a challenge due to spectral similarities with coastal vegetation, tidal periodic fluctuations, and the need for consistent and dependable samples across different years. In previous research, there has been a lack of strategies that simultaneously consider spatial, temporal, and methodological aspects of mangrove extraction. Therefore, based on an approach that considers mangrove habitat, tides, and a semantic segmentation approach, we propose a method for fine-scale mangrove mapping suitable for long time-series data. This is an optimized hybrid model that integrates spatial, temporal, and methodological considerations. The model uses five sensors (GF-1, GF-2, GF-6, ZY-301, ZY-302) to combine deep learning U-Net models with mangrove habitat information and algorithms during low-tide periods. This method produces a mangrove map with a spatial resolution of 2 m. We applied this algorithm to three typical mangrove regions in the Beibu Gulf of Guangxi Province. The results showed the following: (1) The model scored above 0.9 in terms of its F1-score in all three study areas at the time of training, with an average accuracy of 92.54% for mangrove extraction. (2) The average overall accuracy (OA) for the extraction of mangrove distribution in three typical areas in the Beibu Gulf was 93.29%. When comparing the validation of different regions and years, the overall OA accuracy exceeded 89.84% and the Kappa coefficient exceeded 0.74. (3) The model results are reliable for extracting sparse and slow-growing young mangroves and narrow mangrove belts along roadsides. In some areas where tidal flooding occurs, the existing dataset underestimates mangrove extraction to a certain extent. The fine-scale mangrove extraction method provides a foundation for the implementation of fine-scale management of mangrove ecosystems, support for species diversity conservation, blue carbon recovery, and sustainable development goals related to coastal development.",mangrove forest,U-Net,multisource high resolution,remote sensing,mangrove habitat,"Zhong, Shiquan",,"Zhang, Ze",,"Song, Jie","Xiao, Zhijie",,,,,,,,,,,,,,,
Row_1014,"Lee, Kyungsu","Lee, Haeyun","Park, Juhum",Fine-Grained Binary Object Segmentation in Remote Sensing Imagery via Path-Selective Test-Time Adaptation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"For several decades, the significance of geospatial object segmentation in remote sensing (RS) images has been emphasized for both scientific and industrial purposes. Object segmentation plays a pivotal role in the analysis of urban and rural area expansion, as well as in advancing sustainable development within the realm of RS. Deep learning (DL)-based segmentation methodologies, overcoming the limitations of the conventional vision-based analysis, have yielded precise predictions by utilizing convolutional neural networks (CNNs). However, CNNs classify images at the pixel level and generate outputs based on probability distributions derived from the SoftMax function. This approach precludes the reflection of morphological properties, such as shape and object density, during predictions in RS imagery, leading to imprecise results. In addition, due to the intrinsic attributes of probability-based segmentation, fine-grained segmentation may not be achieved, leading to coarse predictions in the boundaries of geospatial objects. To address this issue, this article introduces a novel DL framework, the density-based guide network (DG-Net), which incorporates the density of segmentation targets into pixel-wise classification through a test-time adaptation learning methodology. DG-Net first discerns the density of segmentation targets in the input images, then fine-tunes the baseline network to reflect this density, thereby generating precise segmentation outputs. The effectiveness of DG-Net is demonstrated through various multitarget segmentation benchmarks in RS imagery. Experimental results demonstrate the superior performance of the DG-Net in object segmentation when compared to state-of-the-art (SotA) models across numerous aerial image and satellite image datasets.",Image segmentation,Predictive models,Feature extraction,Deep learning,Adaptation models,"Hwang, Jae Youn",,,Training,,,Geospatial analysis,Fine-grained segmentation,object segmentation,remote sensing (RS) imagery,test-time adaptation,,,,,,,,,,
Row_1015,"Yu, Anzhu","Quan, Yujun","Yu, Ru",Deep Learning Methods for Semantic Segmentation in Remote Sensing with Small Data: A Survey,REMOTE SENSING,OCT 2023,9,"The annotations used during the training process are crucial for the inference results of remote sensing images (RSIs) based on a deep learning framework. Unlabeled RSIs can be obtained relatively easily. However, pixel-level annotation is a process that necessitates a high level of expertise and experience. Consequently, the use of small sample training methods has attracted widespread attention as they help alleviate reliance on large amounts of high-quality labeled data and current deep learning methods. Moreover, research on small sample learning is still in its infancy owing to the unique challenges faced when completing semantic segmentation tasks with RSI. To better understand and stimulate future research that utilizes semantic segmentation tasks with small data, we summarized the supervised learning methods and challenges they face. We also reviewed the supervised approaches with data that are currently popular to help elucidate how to efficiently utilize a limited number of samples to address issues with semantic segmentation in RSI. The main methods discussed are self-supervised learning, semi-supervised learning, weakly supervised learning and few-shot methods. The solution of cross-domain challenges has also been discussed. Furthermore, multi-modal methods, prior knowledge constrained methods, and future research required to help optimize deep learning models for various downstream tasks in relation to RSI have been identified.",self-supervised learning,semi-supervised learning,weakly supervised learning,few-shot approaches,,"Guo, Wenyue",,"Wang, Xin",,"Hong, Danyang","Zhang, Haodi",,,,,,,"Chen, Junming","Hu, Qingfeng","He, Peipei",,,,,,
Row_1016,"Amirgan, Burcu","Erener, Arzu",,Semantic segmentation of satellite images with different building types using deep learning methods,REMOTE SENSING APPLICATIONS-SOCIETY AND ENVIRONMENT,APR 2024,2,"In this study, using deep learning-based semantic segmentation methods, an automatic building segmentation application was carried out with a remote sensing image on a sample area covering a small part of Istanbul. In this context, firstly, fully convolutional networks, semantic segmentation inference principles, and open-source building datasets presented to the public were examined. Within the scope of the study, the IST building dataset containing examples from 5 different building type classes were created using very high resolution Pleiades satellite images. Then, building segmentation training was carried out on UNet and UNet++ architectures with this dataset. Segmentation success was compared between the models obtained after the training and the building classes according to their types. Experimental results showed that the UNET and UNet++ architecture IOU metric achieved segmentation success of 0.9167 and 0.9150 for Industrial class, 0.8124 and 0.8175 for Adjacent class, 0.8459 and 0.8446 for Housing-Villa class, 0.7629 and 0.7477 for Slum class, 0.6697 and 0.6140 for Other class. Finally, building segmentation difficulties arising from the types of buildings have been identified, and suggestions have been made to overcome this problem.",Building extraction,Building segmentation,Different building types,Deep learning,Semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_1017,"Zhang, Enwei","Hu, Kai","Xia, Min",Multilevel feature context semantic fusion network for cloud and cloud shadow segmentation,JOURNAL OF APPLIED REMOTE SENSING,OCT 2022,7,"Cloud detection is an important prerequisite for remote sensing image application. Any remote sensing image from which the information of ground object could be obtained will inevitably be preprocessed on cloud occlusion. In the traditional method, the segmentation of cloud and its shadow will be affected by the complex background. In the detection process, due to insufficient information extraction, misjudgment often occurs, and the cloud boundary processing is also very rough. In order to improve the accuracy of cloud and cloud shadow segmentation, we propose a multilevel feature context semantic fusion network. The network takes the residual network as the backbone networkand adopts the structure of encoding and decoding as a whole. In the model, we introduce the multibranch residual context semantic module, the multiscale convolution subchannel attention module, and the feature fusion upsampling module to strengthen the feature extraction, refine the cloud and cloud shadow edge information, and enhance the actual segmentation ability of the model. The experimental results show that the proposed method is more accurate than the previous network in the segmentation of cloud and cloud shadow and has good generalization ability on other datasets, which is of great significance for the study of cloud. (c) 2022 Society of Photo-Optical Instrumentation Engineers (SPIE)",cloud,cloud shadow,multilevel feature fusion,remote sensing,image,"Weng, Liguo",,"Lin, Haifeng",segmentation,,,,,,,,,,,,,,,,,
Row_1018,"Cao, Yong","Huo, Chunlei","Xu, Nuo",HENet: Head-Level Ensemble Network for Very High Resolution Remote Sensing Images Semantic Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,4,"Semantic segmentation plays an important role in very high resolution (VHR) image understanding. Despite the potentials of the deep convolutional network in improving performance by end-to-end feature learning, each model has its limitations, and it is hard to discriminate complex features purely by a single model. Ensemble learning is promising for integrating the strengths of different models, however, the ensemble of deep models is challenging due to the huge amount of parameters and computation of the deep model itself as well as the difficulty in capturing complementarity between different models. To tackle these problems, a head-level ensemble network (HENet) is proposed in this letter, which reduces model complexity by sharing feature extraction networks and improves complementarity between models by novel cooperative learning (CL). Experiments on ISPRS 2-D semantic labeling benchmark demonstrate the effectiveness and advantage of the proposed method.",Head,Computational modeling,Semantics,Image segmentation,Feature extraction,"Zhang, Xin",,"Xiang, Shiming",Correlation,"Pan, Chunhong",,Mathematical models,Cooperative learning (CL),ensemble learning,semantic segmentation,,,,,,,,,,,
Row_1019,"Li, Xin","Xu, Feng","Lyu, Xin",Dual attention deep fusion semantic segmentation networks of large-scale satellite remote-sensing images,INTERNATIONAL JOURNAL OF REMOTE SENSING,MAY 3 2021,53,"Since DCNNs (deep convolutional neural networks) have been successfully applied to various academic and industrial fields, semantic segmentation methods, based on DCNNs, are increasingly explored for remote-sensing image interpreting and information extracting. It is still highly challenging due to the presence of irregular target shapes, and similarities of inter - and intra-class objects in large-scale high-resolution satellite images. A majority of existing methods fuse the multi-scale features that always fail to provide satisfactory results. In this paper, a dual attention deep fusion semantic segmentation network of large-scale satellite remote-sensing images is proposed (DASSN_RSI). The framework consists of novel encoder-decoder architecture, and a weight-adaptive loss function based on focal loss. To refine high-level semantic and low-level spatial feature maps, the deep layer channel attention module (DLCAM) and shallow layer spatial attention module (SLSAM) are designed and appended with specific blocks. Then the DUpsampling is incorporated to fuse feature maps in a lossless way. Peculiarly, the weight-adaptive focal loss (W-AFL) is inferred and embedded successfully, alleviating the class-imbalanced issue as much as possible. The extensive experiments are conducted on Gaofen image dataset (GID) datasets (Gaofen-2 satellite images, coarse set with five categories and refined set with fifteen categories). And the results show that our approach achieves state-of-the-art performance compared to other typical variants of encoder-decoder networks in the numerical evaluation and visual inspection. Besides, the necessary ablation studies are carried out for a comprehensive evaluation.",,,,,,"Gao, Hongmin",,"Tong, Yao",,"Cai, Sujin","Li, Shengyang",,,,,,,"Liu, Daofang",,,,,,,,
Row_1020,"Shen, Xu","Weng, Liguo","Xia, Min",Multi-Scale Feature Aggregation Network for Semantic Segmentation of Land Cover,REMOTE SENSING,DEC 2022,7,"Land cover semantic segmentation is an important technique in land. It is very practical in land resource protection planning, geographical classification, surveying and mapping analysis. Deep learning shows excellent performance in picture segmentation in recent years, but there are few semantic segmentation algorithms for land cover. When dealing with land cover segmentation tasks, traditional semantic segmentation networks often have disadvantages such as low segmentation precision and weak generalization due to the loss of image detail information and the limitation of weight distribution. In order to achieve high-precision land cover segmentation, this article develops a multi-scale feature aggregation network. Traditional convolutional neural network downsampling procedure has problems of detail information loss and resolution degradation; to fix these problems, a multi-scale feature extraction spatial pyramid module is made to assemble regional context data from different areas. In order to address the issue of incomplete information of traditional convolutional neural networks at multiple sizes, a multi-scale feature fusion module is developed to fuse attributes from various layers and several sizes to boost segmentation accuracy. Finally, a multi-scale convolutional attention module is presented to enhance the segmentation's attention to the target in order to address the issue that the classic convolutional neural network has low attention capacity to the building waters in land cover segmentation. Through the contrast experiment and generalization experiment, it can be clearly demonstrated that the segmentation algorithm proposed in this paper realizes the high precision segmentation of land cover.",land cover,remote sensing image,deep learning,semantic segmentation,,"Lin, Haifeng",,,,,,,,,,,,,,,,,,,,
Row_1021,Mao Feng,Liu Ze,Zhou Wensheng,Extracting of Urban features from high resolution remote sensing data based on multiscale segmentation,,2009,0,"A multiscale segmentation method is proposed for multispectral imagery of high resolution by combining an adapted watershed algorithm and a region merging algorithm. Before the preliminary segmentation by the adapted watershed algorithm, a filtering method and a method for getting rid of local minimum areas are imposed to avoid over-segmentation. The whole process can be divided into five steps as follows. A case study is conducted with a high resolution image, QuikBird, of Beijing city acquired in 2007. From the segmentation results it can be found most of urban features could be extracted correctly and the segmentation edge is accurate and smooth. And it can be concluded that the method can have more semantic information, reduce the 'Pepper and Salt Phenomenon' effectively, and improve the overall classification accuracy of QuikBird image with improved computing efficiency.",multiscale segmentation,high resolution remote sensing image,urban areas,watershed transformation,region merging algorithm,Li Qiang,"2009 JOINT URBAN REMOTE SENSING EVENT, VOLS 1-3",,,,,,,,,,,,,,,,,,,
Row_1022,"Bauer, Adrian","Krabbe, Jan-Christoph","Ibrahim, Mohaned",REFINING SEMANTIC GRANULARITY OF AERIAL IMAGE SEGMENTATION DATASETS BASED ON GROUND INFORMATION,,2023,0,"This paper presents a novel approach for refining the semantic granularity of remote sensing image segmentation datasets using openly available cadastral data, thus minimizing the annotation effort. A two-step method is proposed: (1) a supervised learning step for training a semantic segmentation model on fine-grained cadastre labels to extract class prototypes, and (2) an unsupervised learning step utilizing hierarchical clustering on the extracted prototypes to generate a class hierarchy. The method is demonstrated on building segmentation in aerial imagery data, resulting in models capable of predicting new aggregated semantic classes without extra effort in dataset annotation. Evaluation of the proposed method shows its ability to generate meaningful hierarchical relationships among labels and achieve high segmentation performance.",Automatic annotation,Supervised learning,Hierarchical clustering,Remote sensing data,,"Kummert, Anton",IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,,,,,
Row_1023,"Dallaqua, F. B. J. R.","Rosa, R. A. S.","Schultz, B.",FOREST PLANTATION DETECTION THROUGH DEEP SEMANTIC SEGMENTATION,,2022,0,"Forest plantations play an important role ecologically, contribute to carbon sequestration and support billions of dollars of economic activity each year through sustainable forest management and forest sector value chains. As the global demand for forest products and services increases, the marketplace is seeking more reliable data on forest plantations. Remote sensing technologies allied with machine learning, and most recently deep learning techniques, provide valuable data for inventorying forest plantations and related valuation products. In this work, deep semantic segmentation with U-net architecture was used to detect forest plantation areas using Sentinel-2 and CBERS-4A images of different areas of Brazil. First, the U-net models were built from an area of the Centre-East of Parana State, and then the best models were tested in 3 new areas that present different characteristics. The U-net models built with Sentinel-2 images achieved promising results for areas similar to the ones used in the training set, with F1-score ranging from 0.9171 to 0.9499 and with Kappa values between 0.8712 to 0.9272, demonstrating the feasibility of deep semantic segmentation to detect forest plantations.",Deep learning,Forest plantation,Semantic segmentation,U-net,Sentinel-2,"Faria, L. R.","XXIV ISPRS CONGRESS: IMAGING TODAY, FORESEEING TOMORROW, COMMISSION III","Rodrigues, T. G.",CBERS-4A,"Oliveira, C. G.","Kieser, M. E. J.",Forest inventory,,,,,,"Malhotra, V","Dwyer, T.","Wolfe, D. S.",,,,,,
Row_1024,"Li, Shengfu","Liao, Cheng","Ding, Yulin",Cascaded Residual Attention Enhanced Road Extraction from Remote Sensing Images,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,JAN 2022,35,"Efficient and accurate road extraction from remote sensing imagery is important for applications related to navigation and Geographic Information System updating. Existing data-driven methods based on semantic segmentation recognize roads from images pixel by pixel, which generally uses only local spatial information and causes issues of discontinuous extraction and jagged boundary recognition. To address these problems, we propose a cascaded attention-enhanced architecture to extract boundary-refined roads from remote sensing images. Our proposed architecture uses spatial attention residual blocks on multi-scale features to capture long-distance relations and introduce channel attention layers to optimize the multi-scale features fusion. Furthermore, a lightweight encoder-decoder network is connected to adaptively optimize the boundaries of the extracted roads. Our experiments showed that the proposed method outperformed existing methods and achieved state-of-the-art results on the Massachusetts dataset. In addition, our method achieved competitive results on more recent benchmark datasets, e.g., the DeepGlobe and the Huawei Cloud road extraction challenge.",road extraction,remote sensing imagery,semantic segmentation,deep learning,attention mechanism,"Hu, Han",,"Jia, Yang",,"Chen, Min","Xu, Bo",,,,,,,"Ge, Xuming","Liu, Tianyang","Wu, Di",,,,,,
Row_1025,"Yuan, Wei","Xu, Wenbo",,MSST-Net: A Multi-Scale Adaptive Network for Building Extraction from Remote Sensing Images Based on Swin Transformer,REMOTE SENSING,DEC 2021,38,"The segmentation of remote sensing images by deep learning technology is the main method for remote sensing image interpretation. However, the segmentation model based on a convolutional neural network cannot capture the global features very well. A transformer, whose self-attention mechanism can supply each pixel with a global feature, makes up for the deficiency of the convolutional neural network. Therefore, a multi-scale adaptive segmentation network model (MSST-Net) based on a Swin Transformer is proposed in this paper. Firstly, a Swin Transformer is used as the backbone to encode the input image. Then, the feature maps of different levels are decoded separately. Thirdly, the convolution is used for fusion, so that the network can automatically learn the weight of the decoding results of each level. Finally, we adjust the channels to obtain the final prediction map by using the convolution with a kernel of 1 x 1. By comparing this with other segmentation network models on a WHU building data set, the evaluation metrics, mIoU, F1-score and accuracy are all improved. The network model proposed in this paper is a multi-scale adaptive network model that pays more attention to the global features for remote sensing segmentation.",deep learning,remote sensing,transformer,semantic segmentation,multi-scale adaptive,,,,,,,,,,,,,,,,,,,,,
Row_1026,"Wu, Peng","Fu, Junjie","Yi, Xiaomei",Research on water extraction from high resolution remote sensing images based on deep learning,FRONTIERS IN REMOTE SENSING,DEC 4 2023,2,"Introduction: Monitoring surface water through the extraction of water bodies from high-resolution remote sensing images is of significant importance. With the advancements in deep learning, deep neural networks have been increasingly applied to high-resolution remote sensing image segmentation. However, conventional convolutional models face challenges in water body extraction, including issues like unclear water boundaries and a high number of training parameters.Methods: In this study, we employed the DeeplabV3+ network for water body extraction in high-resolution remote sensing images. However, the traditional DeeplabV3+ network exhibited limitations in segmentation accuracy for high-resolution remote sensing images and incurred high training costs due to a large number of parameters. To address these issues, we made several improvements to the traditional DeeplabV3+ network: Replaced the backbone network with MobileNetV2. Added a Channel Attention (CA) module to the MobileNetV2 feature extraction network. Introduced an Atrous Spatial Pyramid Pooling (ASPP) module. Implemented Focal loss for balanced loss computation.Results: Our proposed method yielded significant enhancements. It not only improved the segmentation accuracy of water bodies in high-resolution remote sensing images but also effectively reduced the number of network parameters and training time. Experimental results on the Water dataset demonstrated superior performance compared to other networks: Outperformed the U-Net network by 3.06% in terms of mean Intersection over Union (mIoU). Outperformed the MACU-Net network by 1.03%. Outperformed the traditional DeeplabV3+ network by 2.05%. The proposed method surpassed not only the traditional DeeplabV3+ but also U-Net, PSP-Net, and MACU-Net networks.Discussion: These results highlight the effectiveness of our modified DeeplabV3+ network with MobileNetV2 backbone, CA module, ASPP module, and Focal loss for water body extraction in high-resolution remote sensing images. The reduction in training time and parameters makes our approach a promising solution for accurate and efficient water body segmentation in remote sensing applications.",remote sensing image,semantic segmentation,deep learning,water extraction,DeepLabV3+,"Wang, Guoying",,"Mo, Lufeng",,"Maponde, Brian Tapiwanashe","Liang, Hao",,,,,,,"Tao, Chunling","Ge, WenYing","Jiang, TengTeng","Ren, Zhen",,,,,
Row_1027,"Li, Xiujuan","Li, Junhuai",,MFCA-Net: a deep learning method for semantic segmentation of remote sensing images,SCIENTIFIC REPORTS,MAR 8 2024,1,"Semantic segmentation of remote sensing images (RSI) is an important research direction in remote sensing technology. This paper proposes a multi-feature fusion and channel attention network, MFCA-Net, aiming to improve the segmentation accuracy of remote sensing images and the recognition performance of small target objects. The architecture is built on an encoding-decoding structure. The encoding structure includes the improved MobileNet V2 (IMV2) and multi-feature dense fusion (MFDF). In IMV2, the attention mechanism is introduced twice to enhance the feature extraction capability, and the design of MFDF can obtain more dense feature sampling points and larger receptive fields. In the decoding section, three branches of shallow features of the backbone network are fused with deep features, and upsampling is performed to achieve the pixel-level classification. Comparative experimental results of the six most advanced methods effectively prove that the segmentation accuracy of the proposed network has been significantly improved. Furthermore, the recognition degree of small target objects is higher. For example, the proposed MFCA-Net achieves about 3.65-23.55% MIoU improvement on the dataset Vaihingen.",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_1028,"Li, Jialin","Hua, Li","Li, Lu",Construction of Improved Semantic Segmentation Model and Application to Extraction of Anthropogenically Disturbed Parcels With Soil Erosion From Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,3,"With the rapid socioeconomic development in China, increasing soil erosion caused by anthropogenic production and construction activities is taking place, which is characterized by short duration, high frequency, and great damage to its surrounding environment. Therefore, the regulation and control of soil erosion of anthropogenically disturbed parcels is an urgent task. This study proposes an improved model that combines the boundary constraint and jagged hybrid dilated convolution channel shuffling module (BCJHDC and the polarized self-attention (PSA) module for extracting anthropogenically disturbed parcels with soil erosion from high-resolution remote sensing images in Hubei Province. First, the PSA module is added to the encoder to better extract the feature information of the target object. Second, the BCJHDC module is used to extract multiscale semantic information from images and improve the boundary segmentation quality. Precision, recall, intersection over union (IOU), and F1 score (F1) are calculated to evaluate the model accuracy. The results indicate that our improved model performs well on the human-perturbed parcel extraction task, with an F1 of 87.92% and an IOU of 78.44%. Ablation experiments and application experiments suggest the validity of the applicability and the portability of our proposed improved model, respectively. Compared with the other seven advanced semantic segmentation models, our improved model has significant advantages. Overall, this study provides a valuable reference for policy formulation of water and soil conservation.",Remote sensing,Soil,Feature extraction,Semantic segmentation,Water conservation,"Zhang, Zijing",,"Cai, Chongfa",Roads,,,Convolution,Anthropogenically disturbed parcels with soil erosion (ADPSE),boundary constraints and jagged hybrid dilated convolution channel shuffling module (BCJHDC),deep learning,remote sensing,self-attention,,,,,,,,,
Row_1029,"Chen, De-Yue","Peng, Ling","Zhang, Wen-Yue",Research on Self-Supervised Building Information Extraction with High-Resolution Remote Sensing Images for Photovoltaic Potential Evaluation,REMOTE SENSING,NOV 2022,4,"With the rapid development of the energy industry and the growth of the global energy demand in recent years, the development of the photovoltaic industry has become increasingly significant. However, the development of the PV industry is constrained by high land costs, and land in central cities and industrial areas is often very expensive and unsuitable for the installation of PV equipment in large areas. With this background knowledge, the key to evaluating the PV potential is by counting the rooftop information of buildings, and an ideal solution for extracting building rooftop information is from remote sensing satellite images using the deep learning method; however, the deep learning method often requires large-scale labeled samples, and the labeling of remote sensing images is often time-consuming and expensive. To reduce the burden of data labeling, models trained on large datasets can be used as pre-trained models (e.g., ImageNet) to provide prior knowledge for training. However, most of the existing pre-trained model parameters are not suitable for direct transfer to remote sensing tasks. In this paper, we design a pseudo-label-guided self-supervised learning (PGSSL) semantic segmentation network structure based on high-resolution remote sensing images to extract building information. The pseudo-label-guided learning method allows the feature results extracted by the pretext task to be more applicable to the target task and ultimately improves segmentation accuracy. Our proposed method achieves better results than current contrastive learning methods in most experiments and uses only about 20-50% of the labeled data to achieve comparable performance with random initialization. In addition, a more accurate statistical method for building density distribution is designed based on the semantic segmentation results. This method addresses the last step of the extraction results oriented to the PV potential assessment, and this paper is validated in Beijing, China, to demonstrate the effectiveness of the proposed method.",remote sensing building extraction,building photovoltaic,self-supervised learning,semantic segmentation,,"Wang, Yin-Da",,"Yang, Li-Na",,,,,,,,,,,,,,,,,,
Row_1030,"Du, Ruiqi","Tang, Xu","Ma, Jingjing",Semantic-Assisted Feature Integration Network for Multilabel Remote Sensing Scene Classification,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2025,0,"With remote sensing (RS) images' resolution increasing, a single scene label cannot adequately represent RS scenes' contents. Therefore, multilabel RS scene classification (MLRSSC) is gradually attracting the researchers' attention. Many methods have been proposed recently, and most use deep features or semantic connections to complete MLRSSC. However, they ignore the combination of these two aspects. In addition, the high interclass similarity and low intraclass similarity of RS images limit the robustness of these methods. In this article, we propose a semantic-assisted feature integration network (SFIN) to overcome the above limitations. It contains a dual-scale feature extractor module (DFEM), a local semantic enhance module (LSEM), a cross-scale interactive attention module (CIAM), and a classifier module (CM). DFEM utilizes the convolutional neural networks (CNNs) to extract multiscale features from RS images. LSEM extracts semantic information and establishes their relationships at different scales. CIAM enhances the feature representation by interacting with the clues across different scales. CM completes the prediction of classification (CLA) results. Integrating them into an end-to-end framework, SFIN can discover the diverse and complex land covers hidden in RS images. Furthermore, to ensure the accuracy of explored semantics and enhance the SFIN's feature extraction ability, we design a semantic supervision (SS) loss and a semantic-based contrastive learning (SB-CL) loss. They are in charge of the correctness and discrimination of the mined semantics. Along with the typical CLA loss, SFIN can be adequately trained. Extensive experiments have been conducted on four MLRSSC datasets, and the positive results demonstrate that SFIN outperforms many existing methods in MLRSSC tasks. Our source codes are available at: https://github.com/TangXu-Group/multilabelRSSC/tree/main/SFIN.",Multilabel,remote sensing (RS) scene classification (CLA),remote sensing (RS) scene classification (CLA),,,"Zhang, Xiangrong",,"Liu, Fang",,"Jiao, Licheng",,,,,,,,,,,,,,,,
Row_1031,"Yuan, Hao","Zhang, Zhihua","Rong, Xing",MPFFNet: LULC classification model for high-resolution remote sensing images with multi-path feature fusion,INTERNATIONAL JOURNAL OF REMOTE SENSING,OCT 2 2023,7,"Land Use/Land Cover (LULC) classification has become increasingly important in various fields, including ecological and environmental protection, urban planning, and geological disaster monitoring. With the development of high-resolution remote sensing satellite technology, there is a growing focus on achieving precise LULC classification. However, the accuracy of fine-grained LULC classification is challenged by the high intra-class diversity and low inter-class separability inherent in high-resolution remote sensing images. To address this challenge, this paper proposes a novel multi-path feature fusion semantic segmentation model, called MPFFNet, which combines the segmentation results of convolutional neural networks with traditional filtering processes to achieve finer LULC classification. MPFFNet consists of three modules: the Improved Encoder Module (IEM) extracts contextual and spatial detail information through the backbone network, DASPP, and MFEAM; the Improved Decoder Module (IDM) utilizes the Cascade Feature Fusion (CFF) module to effectively merge shallow and deep information; and the Feature Fusion Module (FAM) enables dual-path feature fusion using a convolutional neural network and Gabor Filter. Experimental results on the large-scale classification set and the fine land-cover classification set of the Gaofen Image Dataset (GID) demonstrate the effectiveness of the proposed method, achieving mIoU scores of 81.02% and 77.83%, respectively. These scores outperform U-Net by 7.95% and 3.28%, respectively. Therefore, we believe that our model can deliver superior results in the task of LULC classification.",Semantic segmentation,land use/land cover,high-resolution remote sensing images,multi-path feature fusion,,"Feng, Dongdong",,"Zhang, Shaobin",,"Yang, Shuwen",,,,,,,,,,,,,,,,
Row_1032,"Zhang, Ziwen","Liu, Qi","Liu, Xiaodong",PMNet: a multi-branch and multi-scale semantic segmentation approach to water extraction from high-resolution remote sensing images with edge-cloud computing,JOURNAL OF CLOUD COMPUTING-ADVANCES SYSTEMS AND APPLICATIONS,MAR 27 2024,0,"In the field of remote sensing image interpretation, automatically extracting water body information from high-resolution images is a key task. However, facing the complex multi-scale features in high-resolution remote sensing images, traditional methods and basic deep convolutional neural networks are difficult to effectively capture the global spatial relationship of the target objects, resulting in incomplete, rough shape and blurred edges of the extracted water body information. Meanwhile, massive image data processing usually leads to computational resource overload and inefficiency. Fortunately, the local data processing capability of edge computing combined with the powerful computational resources of cloud centres can provide timely and efficient computation and storage for high-resolution remote sensing image segmentation. In this regard, this paper proposes PMNet, a lightweight deep learning network for edge-cloud collaboration, which utilises a pipelined multi-step aggregation method to capture image information at different scales and understand the relationships between remote pixels through horizontal and vertical spatial dimensions. Also, it adopts a combination of multiple decoding branches in the decoding stage instead of the traditional single decoding branch. The accuracy of the results is improved while reducing the consumption of system resources. The model obtained F1-score of 90.22 and 88.57 on Landsat-8 and GID remote sensing image datasets with low model complexity, which is better than other semantic segmentation models, highlighting the potential of mobile edge computing in processing massive high-resolution remote sensing image data.",Mobile edge computing,Deep learning,Light-weight computing,Image semantic segmentation,,"Zhang, Yonghong",,"Du, Zihao",,"Cao, Xuefei",,,,,,,,,,,,,,,,
Row_1033,"He, Qibin","Yan, Zhiyuan","Diao, Wenhui",DLC: Dynamic Loss Correction for Cross-Domain Remotely Sensed Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Due to the diversity of acquisition conditions and imaging mechanisms in remote sensing, the generalization of semantic segmentation models trained with labeled data in the source domain to other unlabeled target domains is hindered. Existing mainstream self-training-based methods provide pseudo-labels to target data as ground truth to utilize target domain evidence for unsupervised domain adaptation (UDA). However, the label shift and domain gap between different domains inevitably introduce noise into pseudo-labeled target data, that is, misclassified pixels. As a consequence, we present a dynamic loss correction (DLC) framework for cross-domain semantic segmentation, which mitigates domain discrepancy by formally modeling the noise distribution of pseudo-labels in the target domain with noise transition matrix (NTM). Specifically, to promote the model output to fit the true label distribution, we employ the high-order consistency information of neighbor representations to estimate NTM and correct the supervision signal without heuristically setting anchors. Furthermore, smooth geometric constraints are introduced to regularize the mutual improvement of NTM derivation and segmentation model optimization in a data-driven manner, thereby compensating for the lack of target domain knowledge. Extensive experimental results on four cross-domain remotely sensed segmentation tasks highlight the generalization capability and competitiveness of the presented method, including cross-scene, cross-band, and cross-modal transfer. Our results and code are available at https://github.com/heqibin/dlc.",Cross-domain semantic segmentation,high-order consistency,noise transition matrix (NTM),remote sensing,smooth geometric constraint,"Sun, Xian",,,,,,,,,,,,,,,,,,,,
Row_1034,"Li, Jiankun","Ding, Wenrui","Li, Hongguang",Semantic Segmentation for High-Resolution Aerial Imagery Using Multi-Skip Network and Markov Random Fields,,2017,22,"Semantic segmentation for aerial imagery is a significant work for remote sensing applications especially for unmanned aerial vehicles (CAI's). In recent years, with the success of deep learning methods, convolutional neural network (CNN) based model plays an important role in both image classification and segmentation. However, due to the presence of small objects in the imagery and imbalance of classes distribution, the pixel-wise semantic segmentation remains a challenge for high-resolution remote sensing imagery. In this paper, a novel CNN based semantic segmentation method is proposed to solve the mentioned problem. To proside more context information for the decoding stage, multi-scale skip connections are designed to feed the pooling layers output from encoding stage to the decoding part. Inception modules are also used to replace the convolutional layers providing multi-scale reception areas. Finally, to enhance the result visually, we re-correct the result basing on prediction confidence in post processing procedure, and then a Markov random fields model is built to refine the label map using simulated annealing algorithm. Experiments on Vaihingen dataset show accuracy improvement on overall performance and car class segmentations.",semantic segmentation,convolutional neural network,Markov random fields,remote sensing,unmanned aerial vehicles,"Liu, Chunlei",PROCEEDINGS OF 2017 IEEE INTERNATIONAL CONFERENCE ON UNMANNED SYSTEMS (ICUS),,,,,,,,,,,,,,,,,,,
Row_1035,"Li, Donghui","Liu, Jia","Liu, Fang",A DUAL-FUSION SEMANTIC SEGMENTATION FRAMEWORK WITH GAN FOR SAR IMAGES,,2022,1,"Deep learning based semantic segmentation is one of the popular methods in remote sensing image segmentation. In this paper, a network based on the widely used encoder-decoder architecture is proposed to accomplish the synthetic aperture radar (SAR) images segmentation. With the better representation capability of optical images, we propose to enrich SAR images with generated optical images via the generative adversative network (GAN) trained by numerous SAR and optical images. These optical images can be used as expansions of original SAR images, thus ensuring robust result of segmentation. Then the optical images generated by the GAN are stitched together with the corresponding real images. An attention module following the stitched data is used to strengthen the representation of the objects. Experiments indicate that our method is efficient compared to other commonly used methods.",Semantic segmentation,SAR images,encoder-decoder framework,,,"Zhang, Wenhua",2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),"Zhang, Andi",,"Gao, Wefei","Shi, Jiao",,,,,,,,,,,,,,,
Row_1036,"Liang, Chenbin","Cheng, Bo","Xiao, Baihua",Unsupervised Domain Adaptation for Remote Sensing Image Segmentation Based on Adversarial Learning and Self-Training,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,10,"There is a large amount of out-of-distribution (OOD) data in remote sensing, which hinders high-accuracy segmentation models under the assumption of independent identical distribution (i.i.d.) from stable and reliable performance in real-world remote sensing applications. And domain adaptation (DA) is presented to seamlessly extend classifiers to the label-scarce target domain in the presence of the label-sufficient source domain with different data distributions. However, given that the domain shift, i.e., the distribution difference between the two domains, is more serious in remote sensing images, the current DA methods for image segmentation in computer vision (CV) typically perform unsatisfactorily in remote sensing, even suffering from the negative domain alignment. To this end, this letter proposes the self-training adversarial DA (STADA) method for remote sensing image segmentation, which not only performs adversarial learning to extract domain-invariant features but also implements self-training using pseudo-labels in the target domain denoised by the conditional adversarial loss for classifier adaptation. The International Society for Photogrammetry and Remote Sensing (ISPRS) and Wuhan University (WHU) datasets are employed to conduct extensive experiments to investigate the effectiveness of STADA and the specific effect of each DA component. And the experimental results demonstrate that STADA outperforms other state-of-the-art DA methods in the remote sensing image segmentation task.",Adversarial learning,domain adaptation (DA),self-training,semantic segmentation,,"Dong, Yunyun",,,,,,,,,,,,,,,,,,,,
Row_1037,"Huang, Junqing","Weng, Liguo","Chen, Bingyu",DFFAN: Dual Function Feature Aggregation Network for Semantic Segmentation of Land Cover,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,MAR 2021,17,"Analyzing land cover using remote sensing images has broad prospects, the precise segmentation of land cover is the key to the application of this technology. Nowadays, the Convolution Neural Network (CNN) is widely used in many image semantic segmentation tasks. However, existing CNN models often exhibit poor generalization ability and low segmentation accuracy when dealing with land cover segmentation tasks. To solve this problem, this paper proposes Dual Function Feature Aggregation Network (DFFAN). This method combines image context information, gathers image spatial information, and extracts and fuses features. DFFAN uses residual neural networks as backbone to obtain different dimensional feature information of remote sensing images through multiple downsamplings. This work designs Affinity Matrix Module (AMM) to obtain the context of each feature map and proposes Boundary Feature Fusion Module (BFF) to fuse the context information and spatial information of an image to determine the location distribution of each image's category. Compared with existing methods, the proposed method is significantly improved in accuracy. Its mean intersection over union (MIoU) on the LandCover dataset reaches 84.81%.",land cover,semantic segmentation,convolution neural network,,,"Xia, Min",,,,,,,,,,,,,,,,,,,,
Row_1038,"Wang, Jianxun","Chen, Xin","Jiang, Weicheng",PVNet: A novel semantic segmentation model for extracting high-quality photovoltaic panels in large-scale systems from high-resolution remote sensing imagery,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,MAY 2023,13,"Timely extraction of high-quality photovoltaic (PV) panels from high-resolution remote sensing imagery can contribute to a comprehensive understanding of energy production, and supports global carbon neutrality and Sustainable Development Goals (SDGs). Existing studies for extracting PV panels only focus on small-scale rooftop PV systems but not on large-scale PV systems composed of single or several PV panels or arrays. Furthermore, the currently available public datasets related to large-scale PV systems are limited by the coarse resolution of the imagery used or annotation scale and do not provide highly detailed footprints for PV panels or attributes such as the location, quantity, or area of these panels. To fill this gap, a novel semantic segmentation model (PVNet) for extracting high-quality PV panels from the densely distributed and regularly shaped PV panels in large-scale PV systems is proposed. PVNet consists of two modules, a Coarse Prediction Module (CPM) and a Fine Optimization Module (FOM). The CPM extracts complete regions of individual PV panels by fusing low-level local features with high-level global features, while the FOM optimizes the output of CPM by residual refinement to match extracted boundaries to ground truth. High-quality details from region to boundary of PV panels are obtained through joint supervision of CPM and FOM results. PVNet was trained on a newly annotated PV Panel Dataset and tested under four scenario conditions in China where the large-scale PV industry is growing rapidly. Qualitative and quantitative results show that PVNet can achieve the highest accuracy for PV panel extraction with F1socre higher than 0.88 and IoU higher than 0.79. Area-wide PV panel mapping and comparisons with existing PV footprint datasets demonstrate that PVNet is a feasible solution for obtaining high-quality geo-spatial databases of large-scale PV systems.",Photovoltaic panels,Remote sensing,Semantic segmentation,Coarse prediction,Fine optimization,"Hua, Li",,"Liu, Junyi",Area -wide Mapping,"Sui, Haigang",,,,,,,,,,,,,,,,
Row_1039,"Arhant, Yoann","Tellez, Olga Lopera","Neyt, Xavier",D4SC: DEEP SUPERVISED SEMANTIC SEGMENTATION FOR SEABED CHARACTERISATION IN LOW-LABEL REGIME,,2023,1,"Seabed characterisation consists in the study of the physical and biological properties of the bottom of the oceans. It is effectively achieved with sonar, a remote sensing method that captures acoustic backscatter of the seabed. Classical Machine Learning (ML) and Deep Learning (DL) research have failed to successfully address the automatic mapping of the seabed from noisy sonar data. This work introduces the Deep Supervised Semantic Segmentation model for Seabed Characterisation (D4SC), a novel U-Net-like model tailored to such data and low-label regime, and proposes a new end-to-end processing pipeline for seabed semantic segmentation. That dual contribution achieves state-of-the-art results on a high resolution Synthetic Aperture Sonar (SAS) survey dataset.",Seabed,Semantic Segmentation,Synthetic Aperture Sonar,Deep Learning,,"Pizurica, Aleksandra",IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,,,,,
Row_1040,"Ouadou, Anes","Huangal, David","Hurt, J. Alex",SEMANTIC SEGMENTATION OF BURNED AREAS IN SENTINEL-2 SATELLITE IMAGES USING DEEP LEARNING MODELS,,2023,2,"Earth Observation (EO) data have become abundantly available and at low prices or for free, opening many possibilities to tackle problems such as the mapping of burned areas after wildfires have been extinguished. This task can be completed using deep learning semantic segmentation techniques. In this paper, we first present our methodology for adapting burn area polygons into semantic segmentation training data for deep neural models. Then, we evaluate four deep semantic segmentation models, U-Net, U-Net++, DeepLabV3, and DeepLabV3+ on Sentinel-2 satellite imagery data obtained from the Copernicus program over Canada in 2019. Our results show that DeepLab models outperformed U-Net models with the DeepLabV3 model achieving the best recall of 83.78%, while the DeepLabV3+ achieved the best precision of 84.35%. EO data allows us a faster and more accurate assessment of burned areas, which can accelerate the restoration planning and processing of insurance claims.",Burned area mapping,deep learning,remote sensing,semantic segmentation,wildfires,"Scott, Grant J.",IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,,,,,
Row_1041,"Zhao, Danpei","Lu, Jiankai","Yuan, Bo","See, Perceive, and Answer: A Unified Benchmark for High-Resolution Postdisaster Evaluation in Remote Sensing Images",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"Visual-language generation for remote sensing image (RSI) is an emerging and challenging research area that requires multitask learning to achieve a comprehensive understanding. However, most existing models are limited to single-level tasks and do not leverage the advantages of the visual-language pretraining (VLP) model. In this article, we present a unified benchmark that learns multiple tasks, including interpretation, perception, and question answering. Specifically, a model is designed to perform semantic segmentation, image captioning, and visual question answering (VQA) for high-resolution RSIs simultaneously. Our model not only attains pixel-level segmentation accuracy and global semantic comprehension but also responds to user-defined queries of interest. Moreover, to address the challenges of multitask perception, we construct a novel multitask dataset called FloodNet+, which provides a new solution for the comprehensive postdisaster assessment. The experimental results demonstrate that our approach surpasses existing methods or baseline in all three tasks. This is the first attempt to simultaneously consider multiple remote sensing perception tasks in an integrated framework, which lays a solid foundation for future research in this area. Our datasets are publicly available at: https://github.com/LDS614705356/FloodNet-plus.",Task analysis,Semantic segmentation,Multitasking,Remote sensing,Transformers,,,,Training,,,Feature extraction,Disaster detection,image captioning,multitask learning,semantic segmentation,visual question answering (VQA),,,,,,,,,
Row_1042,"Wang, Di","Zhang, Jing","Xu, Minqiang",MTP: Advancing Remote Sensing Foundation Model via Multitask Pretraining,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,5,"Foundation models have reshaped the landscape of remote sensing (RS) by enhancing various image interpretation tasks. Pretraining is an active research topic, encompassing supervised and self-supervised learning methods to initialize model weights effectively. However, transferring the pretrained models to downstream tasks may encounter task discrepancy due to their formulation of pretraining as image classification or object discrimination tasks. In this study, we explore the multitask pretraining (MTP) paradigm for RS foundation models to address this issue. Using a shared encoder and task-specific decoder architecture, we conduct multitask supervised pretraining on the segment anything model annotated remote sensing segmentation dataset, encompassing semantic segmentation, instance segmentation, and rotated object detection. MTP supports both convolutional neural networks and vision transformer foundation models with over 300 million parameters. The pretrained models are finetuned on various RS downstream tasks, such as scene classification, horizontal, and rotated object detection, semantic segmentation, and change detection. Extensive experiments across 14 datasets demonstrate the superiority of our models over existing ones of similar size and their competitive performance compared to larger state-of-the-art models, thus validating the effectiveness of MTP.",Task analysis,Image segmentation,Data models,Multitasking,Object detection,"Liu, Lin",,"Wang, Dongsheng",Semantic segmentation,"Gao, Erzhong","Han, Chengxi",Labeling,Change detection,foundation model,multitask pretraining (MTP),object detection,remote sensing (RS),"Guo, Haonan","Du, Bo","Tao, Dacheng","Zhang, Liangpei",scene classification,semantic segmentation,,,
Row_1043,"Liu, Jiabin","Huang, Huaigang","Sun, Hanxiao",LRAD-Net: An Improved Lightweight Network for Building Extraction From Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,14,"The building extraction method of remote sensing images that uses deep learning algorithms can solve the problems of low efficiency and poor effect of traditional methods during feature extraction. Although some semantic segmentation networks proposed recently can achieve good segmentation performance in extracting buildings, their huge parameters and large amount of calculation lead to great obstacles in practical application. Therefore, we propose a lightweight network (named LRAD-Net) for building extraction from remote sensing images. LRAD-Net can be divided into two stages: encoding and decoding. In the encoding stage, the lightweight RegNet network with 600 million flop (600 MF) is finally selected as our feature extraction backbone net though lots of experimental comparisons. Then, a multiscale depthwise separable atrous spatial pyramid pooling structure is proposed to extract more comprehensive and important details of buildings. In the decoding stage, the squeeze-and-excitation attention mechanism is applied innovatively to redistribute the channel weights before fusing feature maps with low-level details and high-level semantics, thus can enrich the local and global information of the buildings. What's more, a lightweight residual block with polarized self-attention is proposed, it can incorporate features extracted from the space of maps and different channels with a small number of parameters, and improve the accuracy of recovering building boundary. In order to verify the effectiveness and robustness of proposed LRAD-Net, we conduct experiments on a self-annotated UAV dataset with higher resolution and three public datasets (the WHU aerial image dataset, the WHU satellite image dataset and the Inria aerial image dataset). Compared with several representative networks, LRAD-Net can extract more details of building, and has smaller number of parameters, faster computing speed, stronger generalization ability, which can improve the training speed of the network without affecting the building extraction effect and accuracy.",Building extraction,channel attentional mechanism,high-resolution remote sensing image,semantic segmentation,,"Wu, Zhifeng",,"Luo, Renbo",,,,,,,,,,,,,,,,,,
Row_1044,"He, Wei","Li, Lianfa","Gao, Xilin",Geocomplexity Statistical Indicator to Enhance Multiclass Semantic Segmentation of Remotely Sensed Data with Less Sampling Bias,REMOTE SENSING,JUN 2024,0,"Challenges in enhancing the multiclass segmentation of remotely sensed data include expensive and scarce labeled samples, complex geo-surface scenes, and resulting biases. The intricate nature of geographical surfaces, comprising varying elements and features, introduces significant complexity to the task of segmentation. The limited label data used to train segmentation models may exhibit biases due to imbalances or the inadequate representation of certain surface types or features. For applications like land use/cover monitoring, the assumption of evenly distributed simple random sampling may be not satisfied due to spatial stratified heterogeneity, introducing biases that can adversely impact the model's ability to generalize effectively across diverse geographical areas. We introduced two statistical indicators to encode the complexity of geo-features under multiclass scenes and designed a corresponding optimal sampling scheme to select representative samples to reduce sampling bias during machine learning model training, especially that of deep learning models. The results of the complexity scores showed that the entropy-based and gray-based indicators effectively detected the complexity from geo-surface scenes: the entropy-based indicator was sensitive to the boundaries of different classes and the contours of geographical objects, while the Moran's I indicator had a better performance in identifying the spatial structure information of geographical objects in remote sensing images. According to the complexity scores, the optimal sampling methods appropriately adapted the distribution of the training samples to the geo-context and enhanced their representativeness relative to the population. The single-score optimal sampling method achieved the highest improvement in DeepLab-V3 (increasing pixel accuracy by 0.3% and MIoU by 5.5%), and the multi-score optimal sampling method achieved the highest improvement in SegFormer (increasing ACC by 0.2% and MIoU by 2.4%). These findings carry significant implications for quantifying the complexity of geo-surface scenes and hence can enhance the semantic segmentation of high-resolution remote sensing images with less sampling bias.",sampling bias,optimal sampling,semantic segmentation,deep learning,,,,,,,,,,,,,,,,,,,,,,
Row_1045,"Liang, Shike","Hua, Zhen","Li, Jinjiang",Hybrid transformer-CNN networks using superpixel segmentation for remote sensing building change detection,INTERNATIONAL JOURNAL OF REMOTE SENSING,APR 18 2023,7,"Convolution in convolutional neural network(CNN) essentially uses a filter (kernel) with shared parameters to achieve feature extraction by computing the weighted sum of the centre pixel and adjacent pixels. The transformer divides the input image into patches and adds position encodings, then learns global semantic information and performs remote modelling through a self-attentive mechanism. However, CNNs are good at extracting local features but have difficulty in capturing global cues; the Transformer uses the self-attention mechanism for remote modelling. However, relative to CNN, local feature details are ignored to a certain extent. We believe that CNN and Transformer are complementary and will show better results if they are fused. Therefore, in this work, we propose a Hybrid Transformer-CNN Networks based on the fusion of CNN and Transformer branches for remote sensing change detection. In the CNN branch, we use the classical U-Net architecture to learn local semantic features. In the Transformer branch, we use Transformer-based progressive sampling to focus the model's attention on objects of interest and prevent corrupting object structure. Subsequently, we propose an adaptive feature merging module to fully fuse the features of CNN and Transformer to enhance feature representation. At the same time, we introduce a differentiable superpixel branch to take advantage of the superpixel segmentation algorithm to accurately identify object boundaries, preserve boundary information and reduce noise in pixel-level features. We supplement the fused enhanced features into the superpixel branch features using a feature refinement module. After our experiments, we demonstrate the superiority of our model over other State of the art methods.",High-resolution optical remote sensing image,superpixel segmentation,change detection(CD),transformer,convolutional neural network (CNN),,,,,,,,,,,,,,,,,,,,,
Row_1046,"Zhou, Wujie","Li, Yangzhen","Huan, Juan",MSTNet-KD: Multilevel Transfer Networks Using Knowledge Distillation for the Dense Prediction of Remote-Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,8,"Recently, methods based on convolutional neural networks have achieved good results in the dense prediction of remote-sensing images, particularly when employing normalized digital surface models (nDSMs). However, most existing methods use multiscale convolution and attention methods to mine multimodal feature information without considering the differences and complementarities between the two features. Moreover, previous studies have prioritized model segmentation performance and ignored parametric issues, which makes it difficult to deploy the model in practical applications. To address this challenge, we designed a multilevel semantic transfer network (MSTNet) for the dense prediction of remote-sensing images using a knowledge-distillation approach to adaptively select useful semantic information for the transfer network. We designed a multilevel semantic knowledge alignment distillation framework (MSKA) to enable a compact student model to learn the semantic information extracted from a complex model. The MSKA framework comprises three main components: cross-layer semantic alignment, dynamic semantic aggregation (DSA), and softening learning (SL) for semantic information transfer and predictive label softening. Experiments on the Vaihingen and Potsdam datasets showed that the student network employing the MSKA framework achieved excellent segmentation performance with only 8.88M parameters and 2.09 gigaFLOPs in terms of computational costs compared with current state-of-the-art (SOTA) methods. The source code and results are available at https://github.com/LYZ00918/MSKANet.",Dense prediction,hierarchical context aggregation (HCA),knowledge distillation (KD),multilevel semantic knowledge alignment,remote-sensing image,"Liu, Yuanyuan",,"Jiang, Qiuping",,,,,,,,,,,,,,,,,,
Row_1047,"Li, Jianfeng","Huang, Zhenghong","Wang, Yongling",Sea and Land Segmentation of Optical Remote Sensing Images Based on U-Net Optimization,REMOTE SENSING,SEP 2022,10,"At present, some related studies on semantic segmentation are becoming complicated, adding a lot of feature layers and various jump splicing to improve the level of refined segmentation, which often requires a large number of parameters to ensure a better segmentation effect. When faced with lightweight tasks, such as sea and land segmentation, the modeling capabilities of these models far exceed the complexity of the task, and reducing the size of the model can easily weaken the original effect of the model. In response to this problem, this paper proposes a U-net optimization structure combining Atrous Spatial Pyramid Pooling (ASPP) and FReLU, namely ACU-Net. ACU-Net replaces the two-layer continuous convolution in the feature extraction part of U-Net with a lightweight ASPP module, retains the symmetric U-shaped structure of the original U-Net structure, and splices the output of the ASPP module with the upsampling part. Use FReLU to improve the modeling ability between pixels, and at the same time cooperate with the attention mechanism to improve the perception ability and receptive field of the network, reduce the training difficulty of the model, and fully tap the hidden information of the samples to capture more effective features. The experimental results show that the ACU-Net in this paper surpasses the reduced U-Net and its optimized improved network U-Net++ in terms of segmentation accuracy and IoU with a smaller volume.",optical remote sensing image,semantic segmentation,deep learning,image feature extraction,U-Net,"Luo, Qinghua",,,,,,,,,,,,,,,,,,,,
Row_1048,"Wang, Xiaoyu","Liang, Longxue","Yan, Haowen",Duplex Restricted Network With Guided Upsampling for the Semantic Segmentation of Remotely Sensed Images,IEEE ACCESS,2021,2,"Deep convolutional networks are of great significance for the automatic semantic annotation of remotely sensed images. Object position and semantic labeling are equally important in semantic segmentation tasks. However, the convolution and pooling operations of the convolutional network will affect the image resolution when extracting semantic information, which makes acquiring semantics and capturing positions contradictory. We design a duplex restricted network with guided upsampling. The detachable enhancement structure to separate opposing features on the same level. In this way, the network can adaptively choose how to trade-off classification and localization tasks. To optimize the detailed information obtained by encoding, a concentration-aware guided upsampling module is further introduced to replace the traditional upsampling operation for resolution restoration. We also add a content capture normalization module to enhance the features extracted in the encoding stage. Our approach uses fewer parameters and significantly outperforms previous results on two very high resolution (VHR) datasets: 84.81% (vs 82.42%) on the Potsdam dataset and 86.76% (vs 82.74%) on the Jiage dataset.",Feature extraction,Semantics,Convolution,Image segmentation,Remote sensing,"Wu, Xiaosuo",,"Lu, Wanzhen",Task analysis,"Cai, Jiali",,Data mining,Information distinction,remote sensing,semantic segmentation,convolutional networks,,,,,,,,,,
Row_1049,"Feng, Jiangfan","Wang, Shiyu","Gu, Zhujun",A novel sea-land segmentation network for enhanced coastline extraction using satellite remote sensing images,ADVANCES IN SPACE RESEARCH,SEP 1 2024,0,"The extraction of coastlines from remote sensing images is vital for promoting sustainable development in coastal areas, conserving marine environments, strengthening disaster response capabilities, and supporting scientific research. However, current coastline detection approaches using remote sensing face challenges related to resolution, terrain, boundary, and data, requiring accurate solutions for reliability. Here, we introduce the Collaborative Supervision and Attention Fusion (CSAFNet) model for pixel-level sea-land segmentation, with a primary goal of improving the accuracy of coastline extraction. The model integrates the Edge Deep Supervision (EDS) module to enhance coastline edge detail fitting. Additionally, the Collaborative Semantic Deep Supervision (CSDS) module and Attention Fusion Module (AFM) collaborate to bridge the semantic gap between different hierarchical features, resulting in a more precise and detailed delineation of coastlines. Experimental validation on the publicly available SLSD 1 dataset has demonstrated superiority over various advanced methods, with an impressive mIoU value of 96.72%. Through simple optimization, detailed and rich coastlines can be extracted, validating the feasibility of coastline extraction. (c) 2024 COSPAR. Published by Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",Semantic segmentation,Edge detection,Feature fusion,Coastline detection,,,,,,,,,,,,,,,,,,,,,,
Row_1050,"Liu, Wenjie","Sun, Xian","Zhang, Wenkai",Associatively Segmenting Semantics and Estimating Height From Monocular Remote-Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,7,"Numerous deep-learning methods have been successfully applied to semantic segmentation (SS) and height estimation (EH) of remote-sensing imagery. It has also been proved that such a framework can be reusable for multiple tasks to reduce computational resource overhead. However, there are still some technical limitations due to the semantic inconsistency between 3-D and 2-D features and the strong interference of different objects with similar spectral-spatial properties. Previous works have sought to address these issues through hard (HPS) or soft parameter sharing (SPS) schemes. But due to unintentional integration, the specific information transmitted between multiple tasks is not clear or in a lot of redundancy. Furthermore, tuning the weights by hand between classification and regression loss functions is challenging. In this article, a novel multitask learning (MTL) method, termed associatively segmenting semantics and estimating height (ASSEH), is proposed to associatively segment semantics and estimate height from monocular remote-sensing imagery. First, considering semantic inconsistency across tasks, we design a task-specific distillation (TSD) module containing a set of task-specific gating units (TSGUs) for each task at the cost of fewer parameters. The module allows for task-specific features to be tailored from the backbone while allowing for task-shared features to be transmitted. Second, we leverage the proposed cross-task propagation (CTP) module to construct and diffuse the local pattern graphlets at the common positions across tasks. Such a high-order recursive method can bridge two tasks explicitly to effectively settle semantic ambiguities caused by similar spectral characteristics with less computational burden and memory requirements. Third, a dynamic weighted geometric mean (DWGeoMean) strategy is introduced to dynamically learn the weights of each task and be more robust to the magnitude of the loss function. Finally, the results of the ISPRS Vaihingen dataset and the urban semantic 3-D (US3D) dataset well demonstrate that our ASSEH achieves state-of-the-art performance.",Task analysis,Semantics,Image segmentation,Remote sensing,Estimation,"Guo, Zhi",,"Fu, Kun",Multitasking,,,Convolution,Height estimation (EH),multitask learning (MTL),remote-sensing imagery,semantic segmentation (SS),,,,,,,,,,
Row_1051,"Zhang, Xiaoqing","Qi, Qingqing","Liu, Weike",Combined Hybrid Neural Networks and Swarm Intelligence Optimization Algorithms for Photovoltaic Panel Segmentation From Remote Sensing Images,IEEE ACCESS,2024,1,"In the context of traditional energy shortage and climate warming, the development of solar energy, as a clean and renewable energy, is crucial. As an effective way to utilize solar energy resources, photovoltaic (PV) power generation technology has been widely used around the world. Using remote sensing images to extract PV panel information, including location, area, has a positive effect on understanding the development status, planning and construction of regional PV new energy. In this study, a semantic segmentation network called HCT-Net, combined with the hybrid neural networks and the swarm intelligence optimization algorithms, is designed to segment solar PV panels from remote sensing images automatically and accurately. To address the problem of inconsistent segmentation within PV regions, a hybrid encoder, which combines a convolutional neural network and a Transformer, is designed to extract local features with rich detail information and global features with global context dependencies, resulting in enhanced feature representations. The foreground relation module is designed to solve the problem of mis-segmentation of the background into PVs. This module strengthens the model's focus on the target object and suppresses the feature representations of non-PVs by explicitly learning the similarity relationship between the global PV feature representation and the feature representations of other objects, and by adaptively assigning weights according to the similarity. The swarm intelligence optimization algorithm is applied to adjust the learning rate and the balance coefficient of the composite loss function of HCT-Net during training. Experimental results show that compared with the current mainstream semantic segmentation network, the method in this study effectively alleviates the problem of inconsistent segmentation within PV regions and mis-segmentation and has advantages in the complete and accurate extraction of PV panels.",Feature extraction,Optimization,Particle swarm optimization,Transformers,Semantics,,,,Photovoltaic systems,,,Remote sensing,Semantic segmentation,Convolutional neural networks,Photovoltaic panel extraction,remote sensing image,semantic segmentation,,,,,swarm intelligence optimization algorithm,CNN,transformer,,
Row_1052,"Xing, Ziyao","Yang, Shuai","Zan, Xuli",Flood vulnerability assessment of urban buildings based on integrating high-resolution remote sensing and street view images,SUSTAINABLE CITIES AND SOCIETY,MAY 2023,32,"Urban flood risk management requires an extensive investigation of the vulnerability characteristics of buildings. Large-scale field surveys usually cost a lot of time and money, while satellite remote sensing and street view images can provide information on the tops and facades of buildings respectively. Thereupon, this paper develops a building vulnerability assessment framework using remote sensing and street view features. Specifically, a UNet-based semantic segmentation model, FSA-UNet (Fusion-Self-Attention-UNet) is proposed to integrate remote sensing and street view features and the vulnerability information contained in the images is fully exploited. And the building vulnerability index is generated to provide the spatial distribution characteristics of urban building vulnerability. The experiment shows that the mIoU of the proposed model can reach 82% for building vulnerability classification in Hefei, China, which is more accurate than the traditional semantic seg-mentation models. The results indicate that the integration of street view and remote sensing image features can improve the ability of building vulnerability assessment, and the model proposed in this study can better capture the correlation features of multi-angle images through the self-attention mechanism and combines hierarchy features and edge information to improve the classification effect. This study can support for disaster manage-ment and urban planning.",Sreet view image,Remote sensing,Flood,Vulnerability assessment,Deep learning,"Dong, Xinrui",,"Yao, Yu",Semantic segmentation,"Liu, Zhe","Zhang, Xiaodong",,,,,,,,,,,,,,,
Row_1053,"Li, Jiayi","Hu, Yuping","Huang, Xin",CaSaFormer: A cross- and self-attention based lightweight network for large-scale building semantic segmentation,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,JUN 2024,1,"Buildings play a crucial role in geographic information systems, and advancements in the resolution of remote sensing imagery have facilitated their extraction on a larger scale. However, this progress has simultaneously heightened the requirements for methods to demonstrate efficiency and enhanced generalization performance. For this purpose, we propose a lightweight building semantic segmentation network, named CaSaFormer. Specifically, we propose an efficient module composed of Cross-attention and Self-attention Blocks connected in series (CaSa Block), to extract valuable semantic information from the feature pyramid. Furthermore, a novel Cross-Attention Gate Fusion (CAGF) module was developed to effectively integrate complementary components from global semantic features and local spatial features. Experiment results have demonstrated that our CaSaFormer outperforms state-of-the-art (SOTA) lightweight methods with best trade-off between accuracy and efficiency, showing a 1.92 % improvement in IoU and 16 % of the computation complexity. When compared to non-lightweight methods under equivalent computational resources, an impressive 1.69 % IoU gain is also achieved with only 1.7 % of the computation complexity. The code is available at: https://github.com/ YpingHu/CaSaFormer.",Large-scale building extraction,Remote sensing,Lightweight network,Attention mechanism,Semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_1054,"Panboonyuen, Teerapong","Charoenphon, Chaiyut","Satirapod, Chalermchon",MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand,REMOTE SENSING,NOV 2023,1,"Semantic segmentation is a fundamental task in remote sensing image analysis that aims to classify each pixel in an image into different land use and land cover (LULC) segmentation tasks. In this paper, we propose MeViT (Medium-Resolution Vision Transformer) on Landsat satellite imagery for the main economic crops in Thailand as follows: (i) para rubber, (ii) corn, and (iii) pineapple. Therefore, our proposed MeViT enhances vision transformers (ViTs), one of the modern deep learning on computer vision tasks, to learn semantically rich and spatially precise multi-scale representations by integrating medium-resolution multi-branch architectures with ViTs. We revised mixed-scale convolutional feedforward networks (MixCFN) by incorporating multiple depth-wise convolution paths to extract multi-scale local information to balance the model's performance and efficiency. To evaluate the effectiveness of our proposed method, we conduct extensive experiments on the publicly available dataset of Thailand scenes and compare the results with several state-of-the-art deep learning methods. The experimental results demonstrate that our proposed MeViT outperforms existing methods and performs better in the semantic segmentation of Thailand scenes. The evaluation metrics used are precision, recall, F1 score, and mean intersection over union (IoU). Among the models compared, MeViT, our proposed model, achieves the best performance in all evaluation metrics. MeViT achieves a precision of 92.22%, a recall of 94.69%, an F1 score of 93.44%, and a mean IoU of 83.63%. These results demonstrate the effectiveness of our proposed approach in accurately segmenting Thai Landsat-8 data. The achieved F1 score overall, using our proposed MeViT, is 93.44%, which is a major significance of this work.",semantic segmentation,deep learning,remote sensing imagery,transformer,Landsat,,,,,,,,,,,,,,,,,,,,,
Row_1055,"Sun, Xiao","Qian, Yurong","Cao, Ruyi",BGFNet: Semantic Segmentation Network Based on Boundary Guidance,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,2,"Over the past few years, there have been significant advancements in deep learning technology, leading to remarkable progress in the field of image analysis. However, when it comes to handling complex remote sensing images, current semantic segmentation methods still face challenges and do not perform as well as desired. How to obtain both spatial detail information and semantic information at the same time is an urgent problem to be solved. This letter proposes a context fusion network based on boundary guidance (BGFNet), which incorporates the patch attention module (PAM), the feature maps are enriched with contextual information, improving their ability to capture spatial dependencies. In order to alleviate boundary ambiguity, a boundary guidance module (BGM) is used to weight features with rich semantic boundary information. Furthermore, the compatible fusion module (CFM) is employed to merge high-order and low-order features, creating novel features. Channel attention is then applied to the obtained features allows us to select the desired features by filtering out irrelevant information. We validate our model on the Vaihingen and Potsdam datasets reached 81.65% and 86.94% mean intersection over union (mIoU), respectively, indicating the superiority of the proposed model.",Semantics,Feature extraction,Remote sensing,Semantic segmentation,Computer architecture,"Tuerxun, Palidan",,"Hu, Zhehao",Image edge detection,,,Convolution,Boundary guidance,compatible fusion,patch attention,remote sensing,semantic segmentation,,,,,,,,,
Row_1056,"He, Zhe","Tao, Yuxiang","Luo, Xiaobo",Road Extraction from Remote Sensing Image Based on an Improved U-Net,LASER & OPTOELECTRONICS PROGRESS,AUG 2023,2,"Road information extracted from remote sensing images is of great value in urban planning, traffic management, and other fields. However, owing to the complex background, obstacles, and numerous similar nonroad areas, high -quality road information extraction from remote sensing images is still challenging. In this work, we propose HSA-UNet, a road information extraction method based on mixed -scale attention and U -Net, for high -quality remote sensing images. First, an attention residual learning unit, composed of a residual structure and an attention feature fusion mechanism, is used in the coding network to improve the extraction ability of global and local features. Second, owing to roads with the characteristics of large spans, narrowness, and continuous distribution, the attention -enhanced atrous spatial pyramid pooling module is added to the bridge network to enhance the ability of road features extraction at different scales. Experiments were performed on Massachusetts roads dataset, and the results showed that HSA-UNet significantly outperformed D-LinkNet, DeepLabV3+ , and other semantic segmentation networks in terms of F1, intersection over union, and other evaluation indicators.",remote sensing,remote sensing image,semantic segmentation,road extraction,scale attention,"Xu, Hao",,,attentional feature fusion,,,,,,,,,,,,,,,,,
Row_1057,"Sun, Jun","Yang, Shiqi","Gao, Xuesong",MASA-SegNet: A Semantic Segmentation Network for PolSAR Images,REMOTE SENSING,JUL 2023,5,"Semantic segmentation of Polarimetric SAR (PolSAR) images is an important research topic in remote sensing. Many deep neural network-based semantic segmentation methods have been applied to PolSAR image segmentation tasks. However, a lack of effective means to deal with the similarity of object features and speckle noise in PolSAR images exists. Thisstudy aims to improve the discriminative capability of neural networks for various intensities of backscattering coefficients while reducing the effects of noise in PolSAR semantic segmentation tasks. Firstly, we propose pre-processing methods for PolSAR image data, which consist of the fusion of multi-source data and false color mapping. Then, we propose a Multi-axis Sequence Attention Segmentation Network (MASA-SegNet) for semantic segmentation of PolSAR data, which is an encoder-decoder framework. Specifically, within the encoder, a feature extractor is designed and implemented by stacking Multi-axis Sequence Attention blocks to efficiently extract PolSAR features at multiple scales while mitigating inter-class similarities and intra-class differences from speckle noise. Moreover, the process of serialized residual connection design enables the propagation of spatial information throughout the network, thereby improving the overall spatial awareness of MASA-SegNet. Within the decoder, it is used to accomplish the semantic segmentation task. The superiority of this algorithm for semantic segmentation will be explored through feature visualization. The experiments show that our proposed spatial sequence attention mechanism can effectively extract features and reduce noise interference and is thus able to obtain the best results on two large-scale public datasets (the AIR-POlSAR-Seg and FUSAR-Map datasets).",PolSAR image,semantic segmentation,multi-axis sequence attention,,,"Ou, Dinghua",,"Tian, Zhaonan",,"Wu, Jing","Wang, Mantao",,,,,,,,,,,,,,,
Row_1058,"Ulku, Irem",,,ResLMFFNet: a real-time semantic segmentation network for precision agriculture,JOURNAL OF REAL-TIME IMAGE PROCESSING,JUL 2024,0,"Lightweight multiscale-feature-fusion network (LMFFNet), a proficient real-time CNN architecture, adeptly achieves a balance between inference time and accuracy. Capturing the intricate details of precision agriculture target objects in remote sensing images requires deep SEM-B blocks in the LMFFNet model design. However, employing numerous SEM-B units leads to instability during backward gradient flow. This work proposes the novel residual-LMFFNet (ResLMFFNet) model for ensuring smooth gradient flow within SEM-B blocks. By incorporating residual connections, ResLMFFNet achieves improved accuracy without affecting the inference speed and the number of trainable parameters. The results of the experiments demonstrate that this architecture has achieved superior performance compared to other real-time architectures across diverse precision agriculture applications involving UAV and satellite images. Compared to LMFFNet, the ResLMFFNet architecture enhances the Jaccard Index values by 2.1% for tree detection, 1.4% for crop detection, and 11.2% for wheat-yellow rust detection. Achieving these remarkable accuracy levels involves maintaining almost identical inference time and computational complexity as the LMFFNet model. The source code is available on GitHub: https://github.com/iremulku/Semantic-Segmentation-in-Precision-Agriculture.",Real-time semantic segmentation,Remote sensing,Precision agriculture,,,,,,,,,,,,,,,,,,,,,,,
Row_1059,"Marmanis, D.","Schindler, K.","Wegner, J. D.",SEMANTIC SEGMENTATION OF AERIAL IMAGES WITH EXPLICIT CLASS-BOUNDARY MODELING,,2017,1,"In this work we propose an end-to-end trainable supervised Deep Convolutional Neural Network (DCNN) targeting the task of semantic-segmentation with the addition of class-aware boundary detection. Through this explicit modeling of the class-boundaries, we enforce the network to extract coherent and complete objects, suppressing the uncertainty influencing these regions. Importantly, we show that class-boundary networks in conjunction with DCNN performs optimally, achieving over 90% overall accuracy (OA) on the challenging ISPRS Vaihingen Semantic Segmentation benchmark.",semantic-segmentation,CNN,FCN,VHSR,aerial imagery,"Datcu, M.",2017 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),"Stilla, U.",,,,,,,,,,,,,,,,,,
Row_1060,"Ayala, Christian","Aranda, Carlos","Galar, Mikel",Multi-Class Strategies for Joint Building Footprint and Road Detection in Remote Sensing,APPLIED SCIENCES-BASEL,SEP 2021,4,"Building footprints and road networks are important inputs for a great deal of services. For instance, building maps are useful for urban planning, whereas road maps are essential for disaster response services. Traditionally, building and road maps are manually generated by remote sensing experts or land surveying, occasionally assisted by semi-automatic tools. In the last decade, deep learning-based approaches have demonstrated their capabilities to extract these elements automatically and accurately from remote sensing imagery. The building footprint and road network detection problem can be considered a multi-class semantic segmentation task, that is, a single model performs a pixel-wise classification on multiple classes, optimizing the overall performance. However, depending on the spatial resolution of the imagery used, both classes may coexist within the same pixel, drastically reducing their separability. In this regard, binary decomposition techniques, which have been widely studied in the machine learning literature, are proved useful for addressing multi-class problems. Accordingly, the multi-class problem can be split into multiple binary semantic segmentation sub-problems, specializing different models for each class. Nevertheless, in these cases, an aggregation step is required to obtain the final output labels. Additionally, other novel approaches, such as multi-task learning, may come in handy to further increase the performance of the binary semantic segmentation models. Since there is no certainty as to which strategy should be carried out to accurately tackle a multi-class remote sensing semantic segmentation problem, this paper performs an in-depth study to shed light on the issue. For this purpose, open-access Sentinel-1 and Sentinel-2 imagery (at 10 m) are considered for extracting buildings and roads, making use of the well-known U-Net convolutional neural network. It is worth stressing that building and road classes may coexist within the same pixel when working at such a low spatial resolution, setting a challenging problem scheme. Accordingly, a robust experimental study is developed to assess the benefits of the decomposition strategies and their combination with a multi-task learning scheme. The obtained results demonstrate that decomposing the considered multi-class remote sensing semantic segmentation problem into multiple binary ones using a One-vs.-All binary decomposition technique leads to better results than the standard direct multi-class approach. Additionally, the benefits of using a multi-task learning scheme for pushing the performance of binary segmentation models are also shown.",Sentinel-1,Sentinel-2,remote sensing,building detection,road detection,,,,deep learning,,,convolutional neural networks,multi-class semantic segmentation,binary semantic segmentation,multi-task semantic segmentation,,,,,,,,,,,
Row_1061,"Akodad, Sara","Gbodjo, Yawogan Jean Eudes","Lassalle, Pierre",Leveraging Physical Augmentations From Multiview Remote Sensing Images for Building Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2025,0,"Building segmentation from remote sensing data has been boosted in recent years by the advances in data processing algorithms and the improved ability of sensors to capture very high spatial resolution (VHSR) images. The prevailing approaches to this task are built upon deep semantic segmentation networks learned on ortho geometry, i.e., orthorectified images. In this study, we propose to deal with the building segmentation using multiview Pl & eacute;iades satellite images at the perfect sensor (PS) geometry instead of the ortho geometry. This has two main advantages: 1) it frees the segmentation workflow from geometric imprecision that may arise after the image rectification step and 2) it allows the physical augmentations of the ground truth (GT) by reprojecting building objects on each of the multiview acquisitions. The GT reprojection process makes use of rational polynomial coefficients (RPCs) provided as image metadata and a fine scale digital surface model (DSM). We assess the benefit of our proposal using a U-Net encoder-decoder learned on a dataset composed of tri-stereo Pl & eacute;iades acquisitions over six French cities. Experimental results demonstrate the significance of the proposal especially an enhanced generalization capability for the building segmentation.",Geometry,Sensors,Buildings,Image segmentation,Image sensors,"Brunet, Pierre-Marie",,,Spatial resolution,,,Remote sensing,Polynomials,Location awareness,Training,Image geometry,perfect sensor,,,,,Pl & eacute;iades,rational polynomial coefficients (RPCs),tri-stereo views,U-Net,
Row_1062,"Miao, Shoukuan","Xia, Min","Qian, Ming",Cloud/shadow segmentation based on multi-level feature enhanced network for remote sensing imagery,INTERNATIONAL JOURNAL OF REMOTE SENSING,AUG 18 2022,47,"In the application of remote sensing, cloud blocking brings trouble to the analysis of surface parameters and atmospheric parameters. Due to the complexity of the background, the influence of some cloud-like interferences (such as ice, snow, buildings, etc.) and the complexity of the cloud shape, the traditional deep learning method is difficult to segment the edge information of cloud and cloud shadow accurately, resulting in misjudgement at the edge. In order to solve these problems, a multilevel feature enhanced network is proposed for cloud/shadow segmentation. In this work, ResNet-18 is used as the backbone to extract all levels of semantic information, and Feature Enhancement Module is proposed to strengthen the feature information to obtain more effective feature information. Multiscale Fusion module is constructed to fuses multiscale features of deep information to obtain global feature information while considering local feature information. Finally, through the Feature Guidance module, low-level features are used to guide the high-level features to guide the recovery of spatial information and improve the efficiency of upsampling. On the data collected by Landsat-8, Sentinel-2, and HRC_WHU data set, the experimental results show that this method is superior to the existing methods.",Cloud,shadow segmentation,Remote-sensing images,Feature enhanced network,Deep learning,"Zhang, Yonghong",,"Liu, Jia",,"Lin, Haifeng",,,,,,,,,,,,,,,,
Row_1063,Yang Xiaoyu,Wang Xili,,Building Segmentation Model of Remote Sensing Image Combining Multiscale Attention and Edge Supervision,LASER & OPTOELECTRONICS PROGRESS,NOV 2022,3,"Remote sensing image segmentation is a crucial application in the field of remote sensing image processing. A semantic segmentation network MAE-Net combining multiscale attention and edge supervision is proposed based on the deep learning network U-Net to address the phenomena of building missing classification, missing segmentation. and inaccurate building contour segmentation in the building segmentation of remote sensing images via convolution neural network. First, a multiscale attention module is introduced into each layer during the coding stage. The module separates the input feature map into equal channels and employs the convolution kernels of various sizes for feature extraction in each group. Thereafter, the channel attention mechanism is used in each group to gain more efficient features through self-learning to solve the problem of inaccurate feature extraction of buildings of various sizes. Second, in the decoding stage, the edge extraction module is introduced to build the edge supervision network. The error between the learning edge label and expected edge is supervised by the loss function to aid the segmentation network in better learning the building edge features and make the building boundary's segmentation result more continuous and smoother. The experimental findings show that MAE-Net can completely segment buildings from remote sensing images with complicated and diverse backgrounds and large-scale changes, and the segmentation accuracy is higher.",image processing,neural network,remote sensing image,multiscale feature extraction,attention mechanism,,,,edge supervision,,,,,,,,,,,,,,,,,
Row_1064,"Zhang, Jun","Qiang, Zhenping","Lin, Hong",Research on Tobacco Field Semantic Segmentation Method Based on Multispectral Unmanned Aerial Vehicle Data and Improved PP-LiteSeg Model,AGRONOMY-BASEL,JUL 2024,2,"In recent years, the estimation of tobacco field areas has become a critical component of precision tobacco cultivation. However, traditional satellite remote sensing methods face challenges such as high costs, low accuracy, and susceptibility to noise, making it difficult to meet the demand for high precision. Additionally, optical remote sensing methods perform poorly in regions with complex terrain. Therefore, Unmanned Aerial Vehicle multispectral remote sensing technology has emerged as a viable solution due to its high resolution and rich spectral information. This study employed a DJI Mavic 3M equipped with high-resolution RGB and multispectral cameras to collect tobacco field data covering five bands: RGB, RED, RED EDGE, NIR, and GREEN in Agang Town, Luoping County, Yunnan Province, China. To ensure the accuracy of the experiment, we used 337, 242, and 215 segmented tobacco field images for model training, targeting both RGB channels and seven-channel data. We developed a tobacco field semantic segmentation method based on PP-LiteSeg and deeply customized the model to adapt to the characteristics of multispectral images. The input layer's channel number was adjusted to multiple channels to fully utilize the information from the multispectral images. The model structure included an encoder, decoder, and SPPM module, which used a multi-layer convolution structure to achieve feature extraction and segmentation of multispectral images. The results indicated that compared to traditional RGB images, multispectral images offered significant advantages in handling edges and complex terrain for semantic segmentation. Specifically, the predicted area using the seven-channel data was 11.43 m(2) larger than that obtained with RGB channels. Additionally, the seven-channel model achieved a prediction accuracy of 98.84%. This study provides an efficient and feasible solution for estimating tobacco field areas based on multispectral images, offering robust support for modern agricultural management.",multispectral,unmanned aerial vehicle,semantic segmentation,remote sensing,,"Chen, Zhuqun",,"Li, Kaibo",,"Zhang, Shuang",,,,,,,,,,,,,,,,
Row_1065,"Zheng, Chen","Chen, Yuncheng","Shao, Jie",An MRF-Based Multigranularity Edge-Preservation Optimization for Semantic Segmentation of Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,8,"Semantic segmentation is one of the most important tasks in the field of remote sensing image processing. Many methods have been proposed to realize it at the pixel granularity or object granularity. Specifically, the pixel-based methods usually can effectively extract the detailed information and edges, and the object-based methods can keep the internal consistency of each land cover or land use. The Markov random field (MRF) model provides a statistical way to combine the advantages of both pixel and object granularities together. However, current MRF-based methods still face a problem, that is, how to ensure that the advantages of different granularities will complement each other, not that disadvantages will affect advantages. To solve this problem, a new multigranularity edge-preservation optimization is proposed in this letter. The proposed method first represents the image with a series of granularities from the object to the pixel by downsampling. Then, the MRF model is defined on each granularity. By defining an edge set for each granularity, during the process of downsampling, the proposed method can continuously correct edges while maintaining intraclass consistency. Experiments of Gaofen-2 and SPOT5 demonstrate the effectiveness of the proposed method. Moreover, the proposed method can be also used as the postprocessing step for deep learning. The experiment of the Pavia University hyperspectral image illustrates it for an instance of DeepLab v3+.",Biological system modeling,Image edge detection,Image segmentation,Markov processes,Data models,"Wang, Leiguang",,,Spatial resolution,,,Semantics,Edge preservation,granularity,Markov random field (MRF),object analysis,segmentation,,,,,,,,,
Row_1066,"Wang, Falin","Ji, Jian","Wang, Yuan",Masked Topology Convolutional Network for Classification and Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Convolutional neural networks have made significant progress in remote sensing image processing. Convolutional networks mostly model the local information of samples based on pixel features, while ignoring the topological relationship among different categories of ground objects. However, the nonlocal topology can better represent the underlying data structure of the image. In order to make up for the insufficiency of Convolutional neural networks in extracting nonlocal topological information, we propose a new masked topology convolutional network (MTC_Net). First, considering the regionality of different categories of ground objects, we use a clustering algorithm to simply cluster the pixels to extract the most obvious area mask, which can strengthen the boundary information among different ground objects. Second, we take the cluster center point as the representative point of the region and use the graph structure to represent the topological relationship, in addition use graph convolution to further extract the topological structure relationship among different categories of ground objects. Finally, we use convolution modules and multilevel feature attention modules to capture important local and global contextual semantic information and reduce category confusion. We conduct a great deal of experiments on four (two classification and two segmentation) public datasets, and the proposed MTC_Net achieves excellent classification and segmentation performance.",Feature extraction,Convolutional neural networks,Data mining,Image segmentation,Topology,"Li, Jingyang",,"Miao, Qiguang",Clustering algorithms,,,Network topology,Classification and segmentation,graph convolutional network (GCN),remote sensing images,topological structure,,,,,,,,,,
Row_1067,"Xia, Junshi","Yokoya, Naoto","Baier, Gerald",DML: Differ-Modality Learning for Building Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,5,"This work critically analyzes the problems arising from differ-modality building semantic segmentation in the remote sensing domain. With the growth of multimodality datasets, such as optical, synthetic aperture radar (SAR), light detection and ranging (LiDAR), and the scarcity of semantic knowledge, the task of learning multimodality information has increasingly become relevant over the last few years. However, multimodality datasets cannot be obtained simultaneously due to many factors. Assume that we have SAR images with reference information in one place and optical images without reference in another; how to learn relevant features of optical images from SAR images? We refer to it as differ-modality learning (DML). To solve the DML problem, we propose novel deep neural network architectures, which include image adaptation, feature adaptation, knowledge distillation, and self-training (SL) modules for different scenarios. We test the proposed methods on the differ-modality remote sensing datasets (very high-resolution SAR and RGB from SpaceNet 6) to build semantic segmentation and to achieve a superior efficiency. The presented approach achieves the best performance when compared with the state-of-the-art methods.",Synthetic aperture radar,Optical imaging,Optical sensors,Semantics,Training,,,,Image segmentation,,,Testing,Building segmentation,differ-modality,optical,synthetic aperture radar (SAR),,,,,,,,,,
Row_1068,"Niu, Ruigang","Sun, Xian","Tian, Yu",Improving Semantic Segmentation in Aerial Imagery via Graph Reasoning and Disentangled Learning,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,10,"Semantic segmentation in aerial imagery is still an important, yet challenging task due to the complex characteristics of remote-sensing data. The critical issues consist of: 1) extreme foreground-background imbalance; 2) large intra-class variance; and 3) arbitrary-oriented, dense, and small objects. The above challenges make it unlikely to model the effective global interdependencies of semantic heterogeneous regions. Besides, general semantic segmentation methods suffer from feature ambiguity due to the joint feature learning paradigm, leading to inferior detail information. In this article, we propose an improved semantic segmentation framework to tackle these problems via graph reasoning (GR) and disentangled learning. On the one hand, a simple, yet effective GR unit is introduced to implement coordinate-interaction space mapping and perform relation reasoning over the graph. It can be deployed on the feature pyramid network (FPN) to exploit cross-stage multi-scale information. On the other hand, we propose a so- called disentangled learning paradigm to explicitly model the foreground and boundary objects, instantiated as foreground prior estimation (FPE) and boundary alignment (BA). The indication of the intermediate feature can be effectively emphasized to enhance the discriminative abilities of the network. Extensive experiments over iSAID, ISPRS Vaihingen, and the general Cityscapes datasets demonstrate the effectiveness and efficiency of the proposed framework over other state-of-the-art semantic segmentation methods.",Semantics,Cognition,Image segmentation,Feature extraction,Context modeling,"Diao, Wenhui",,"Feng, Yingchao",Remote sensing,"Fu, Kun",,Task analysis,Aerial imagery,disentangled learning,feature alignment,graph convolutional networks (GCNs),semantic segmentation,,,,,,,,,
Row_1069,"Wang, Rui","Meng, Yizhuo","Zhang, Wen",Remote Sensing Semantic Segregation for Water Information Extraction: Optimization of Samples via Training Error Performance,IEEE ACCESS,2019,8,"The corrupted information in training samples is an important factor affecting the accuracy and generalizability of the machine learning models. Due to the extremely high memory capacity of deep learning models, the interference of excessive corrupted information makes the model prone to bad generalization behavior. This paper proposes a method of estimating training sample quality using the value calculated by the loss function in the process of gradient descent optimization. The method includes a model accuracy variation degree algorithm and a sample quality analysis algorithm. The model accuracy variation degree algorithm provides a basis for determining the intervention time of the sample quality analysis algorithm by calculating the intensity of the model accuracy variation change. The data error evaluation algorithm analyzes the distribution characteristics of the training error and estimates the error degree of the training samples to control the quality of the input samples. This study includes a water segmentation experiment performed on GF1 remote sensing images, which demonstrates that the optimization method can significantly improve the model accuracy and training stability.",Training error,error analysis,deep learning (DL),semantic segmentation,remote sensing,"Li, Ziyao",,"Hu, Fengmin",water information extraction,"Meng, Lingkui",,generalization accuracy,,,,,,,,,,,,,,
Row_1070,"Liu, Qinghui","Kampffmeyer, Michael","Jenssen, Robert",Self-constructing graph neural networks to model long-range pixel dependencies for semantic segmentation of remote sensing images,INTERNATIONAL JOURNAL OF REMOTE SENSING,JUN 16 2021,11,"Capturing global contextual representations in remote sensing images by exploiting long-range pixel-pixel dependencies has been shown to improve segmentation performance. However, how to do this efficiently is an open question as current approaches of utilizing attention schemes, or very deep models to increase the field of view, increases complexity and memory consumption. Inspired by recent work on graph neural networks, we propose the Self-Constructing Graph (SCG) module that learns a long-range dependency graph directly from the image data and uses it to capture global contextual information efficiently to improve semantic segmentation. The SCG module provides a high degree of flexibility for constructing segmentation networks that seamlessly make use of the benefits of variants of graph neural networks (GNN) and convolutional neural networks (CNN). Our SCG-GCN model, a variant of SCG-Net built upon graph convolutional networks (GCN), performs semantic segmentation in an end-to-end manner with competitive performance on the publicly available ISPRS Potsdam and Vaihingen datasets, achieving a mean F1-scores of 92.0% and 89.8%, respectively. We conclude that the SCG-Net is an attractive architecture for semantic segmentation of remote sensing images since it achieves competitive performance with much fewer parameters and lower computational cost compared to related models based on convolutional neural networks.",,,,,,"Salberg, Arnt-Borre",,,,,,,,,,,,,,,,,,,,
Row_1071,"Chen, Zhong","Cao, Anqi","Deng, He",Accurate contour preservation for semantic segmentation by mitigating the impact of pseudo-boundaries,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,FEB 2024,0,"Accurately extracting the contours of ground objects has been an important research topic in the field of semantic segmentation of remote sensing imagery. However, existing efforts have primarily focused on refining the boundaries of predictive masks, with little consideration given to pseudo boundaries caused by abrupt changes in surface textures. Therefore, this paper addresses this challenge with the contour preservation network (CPNet), a novel semantic segmentation network that effectively mitigates pseudo-boundary effects and produces more precise contours. The key of CPNet is the boundary-guided feature alignment module (BGAM). This module employs supervised boundary guidance to adaptively transfer the model's attention from salient areas to correct semantic boundaries. This adaptive attention transfer mechanism enables the model to suppress the impact of internal pseudo boundaries and refine contours. To further refine boundaries, a boundary point feature rectification module (ReBPM) is designed to rectify the classification of boundary points with neighbor features. Extensive experimental validations have demonstrated the effectiveness and flexibility of the proposed CPNet on ISPRS Potsdam and Vaihingen datasets. The results showed that our model outperforms other state-of-the-art methods in terms of boundary IoU, mean IoU, and mean F1-score, and it exhibits significantly superior contour preservation ability compared to other models, notably in the presence of pseudo-boundaries. The code is available at: https://github.com/angiecao/CPNet.",Semantic segmentation,Contour preservation,Remote sensing images,Feature alignment,,"Mi, Xiaofei",,"Yang, Jian",,,,,,,,,,,,,,,,,,
Row_1072,"Li, Junjie","Meng, Lingkui","Yang, Beibei",LabelRS: An Automated Toolbox to Make Deep Learning Samples from Remote Sensing Images,REMOTE SENSING,JUN 2021,9,"Deep learning technology has achieved great success in the field of remote sensing processing. However, the lack of tools for making deep learning samples with remote sensing images is a problem, so researchers have to rely on a small amount of existing public data sets that may influence the learning effect. Therefore, we developed an add-in (LabelRS) based on ArcGIS to help researchers make their own deep learning samples in a simple way. In this work, we proposed a feature merging strategy that enables LabelRS to automatically adapt to both sparsely distributed and densely distributed scenarios. LabelRS solves the problem of size diversity of the targets in remote sensing images through sliding windows. We have designed and built in multiple band stretching, image resampling, and gray level transformation algorithms for LabelRS to deal with the high spectral remote sensing images. In addition, the attached geographic information helps to achieve seamless conversion between natural samples, and geographic samples. To evaluate the reliability of LabelRS, we used its three sub-tools to make semantic segmentation, object detection and image classification samples, respectively. The experimental results show that LabelRS can produce deep learning samples with remote sensing images automatically and efficiently.",ArcGIS,deep learning,remote sensing,samples,,"Tao, Chongxin",,"Li, Linyi",,"Zhang, Wen",,,,,,,,,,,,,,,,
Row_1073,"Gautam, Swati","Singhai, Jyoti",,Critical review on deep learning methodologies employed for water-body segmentation through remote sensing images,MULTIMEDIA TOOLS AND APPLICATIONS,MAY 2023,4,"In remote sensing along with image interpretation, water body segmentation (WBS) is a significant problem. Over the course of a long period of time, the researchers have been studying about the remotely sensed image's potentiality for researching the natural resources like water. The scientific group has a vast curiosity in learning about water as it is an indispensable natural resource that needs to be preserved. Owing to the critical variations in the water bodies' shape, size, color and texture, the WBS in higher-resolution satellite imagery is highly challenging. To extract the water body from Remote Sensing Images (RSIs), numerous WBS models have been developed in the past. Nevertheless, water boundaries' accurate position was not obtained by fundamental methodologies. Evolvement of Deep Learning (DL) architectures caused a boom in the field of remote sensing image analysis with their magnificent results in terms of accuracy, robustness, speed etc. The emerging DL methodologies signified advanced execution on numerous public data sets, even though phenomenal actions have been dedicated on improving pixel-level accuracy. This study focusses on critically reviewing the deep learning architectures for WBS with the proper enlisting of their results, advantages and disadvantages to make the information available on single platform for readers. Also, how DL models tried to resolve the imperfect extraction faced by fundamental WBS models is discussed. The main difficulty of finding large and benchmarking datasets required for training of DL models is faced by the researchers. Thus, to make the review innovative, a novel idea of enlisting the detailed information on availability of datasets along with their source is presented in this review. Comparison of different DL models on various evaluation parameters provides the researchers with a patterned development on WBS using DL done so far. Finally, the discussion has been described succeeded by the conclusion and future scope.",Water body segmentation,Image segmentation,Remote sensing imagery,Deep learning,Satellite imagery,,,,,,,,,,,,,,,,,,,,,
Row_1074,"Zhang, Wei","Tang, Ping","Corpetti, Thomas",WTS: A Weakly towards Strongly Supervised Learning Framework for Remote Sensing Land Cover Classification Using Segmentation Models,REMOTE SENSING,FEB 2021,19,"Land cover classification is one of the most fundamental tasks in the field of remote sensing. In recent years, fully supervised fully convolutional network (FCN)-based semantic segmentation models have achieved state-of-the-art performance in the semantic segmentation task. However, creating pixel-level annotations is prohibitively expensive and laborious, especially when dealing with remote sensing images. Weakly supervised learning methods from weakly labeled annotations can overcome this difficulty to some extent and achieve impressive segmentation results, but results are limited in accuracy. Inspired by point supervision and the traditional segmentation method of seeded region growing (SRG) algorithm, a weakly towards strongly (WTS) supervised learning framework is proposed in this study for remote sensing land cover classification to handle the absence of well-labeled and abundant pixel-level annotations when using segmentation models. In this framework, only several points with true class labels are required as the training set, which are much less expensive to acquire compared with pixel-level annotations through field survey or visual interpretation using high-resolution images. Firstly, they are used to train a Support Vector Machine (SVM) classifier. Once fully trained, the SVM is used to generate the initial seeded pixel-level training set, in which only the pixels with high confidence are assigned with class labels whereas others are unlabeled. They are used to weakly train the segmentation model. Then, the seeded region growing module and fully connected Conditional Random Fields (CRFs) are used to iteratively update the seeded pixel-level training set for progressively increasing pixel-level supervision of the segmentation model. Sentinel-2 remote sensing images are used to validate the proposed framework, and SVM is selected for comparison. In addition, FROM-GLC10 global land cover map is used as training reference to directly train the segmentation model. Experimental results show that the proposed framework outperforms other methods and can be highly recommended for land cover classification tasks when the pixel-level labeled datasets are insufficient by using segmentation models.",land cover classification,convolutional neural network,segmentation model,weakly supervised,seeded region growing,"Zhao, Lijun",,,fully connected Conditional Random Fields,,,,,,,,,,,,,,,,,
Row_1075,"Zhang, Guangyun","Jia, Xiuping","Hu, Jiankun",Superpixel-Based Graphical Model for Remote Sensing Image Mapping,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,NOV 2015,86,"Object-oriented remote sensing image classification is becoming more and more popular because it can integrate spatial information from neighboring regions of different shapes and sizes into the classification procedure to improve the mapping accuracy. However, object identification itself is difficult and challenging. Superpixels, which are groups of spatially connected similar pixels, have the scale between the pixel level and the object level and can be generated from oversegmentation. In this paper, we establish a new classification framework using a superpixel -based graphical model. Superpixels instead of pixels are applied as the basic unit to the graphical model to capture the contextual information and the spatial dependence between the superpixels. The advantage of this treatment is that it makes the classification less sensitive to noise and segmentation scale. The contribution of this paper is the application of a graphical model to remote sensing image semantic segmentation. It is threefold. 1) Gradient fusion is applied to multispectral images before the watershed segmentation algorithm is used for superpixel generation. 2) A probabilistic fusion method is designed to derive node potential in the superpixel-based graphical model to address the problem of insufficient training samples at the superpixel level. 3) A boundary penalty between the superpixels is introduced in the edge potential evaluation. Experiments on three real data sets were conducted. The results show that the proposed method performs better than the related state-of-the-art methods tested.",Graphical model,remote sensing,superpixel,,,,,,,,,,,,,,,,,,,,,,,
Row_1076,"Xu, Yonghao","Ghamisi, Pedram",,Universal Adversarial Examples in Remote Sensing: Methodology and Benchmark,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,55,"Deep neural networks have achieved great success in many important remote sensing tasks. Nevertheless, their vulnerability to adversarial examples should not be neglected. In this study, we systematically analyze the Universal Adversarial Examples in Remote Sensing (UAE-RS) data for the first time, without any knowledge from the victim model. Specifically, we propose a novel black-box adversarial attack method, namely, Mixup-Attack, and its simple variant Mixcut-Attack, for remote sensing data. The key idea of the proposed methods is to find common vulnerabilities among different networks by attacking the features in the shallow layer of a given surrogate model. Despite their simplicity, the proposed methods can generate transferable adversarial examples that deceive most of the state-of-the-art deep neural networks in both scene classification and semantic segmentation tasks with high success rates. We further provide the generated universal adversarial examples in the dataset named UAE-RS, which is the first dataset that provides black-box adversarial samples in the remote sensing field. We hope UAE-RS may serve as a benchmark that helps researchers design deep neural networks with strong resistance toward adversarial attacks in the remote sensing field. Codes and the UAE-RS dataset are available online (https://github.com/YonghaoXu/UAE-RS).",Remote sensing,Neural networks,Deep learning,Perturbation methods,Task analysis,,,,Forestry,,,Optimization,Adversarial attack,adversarial example,remote sensing,scene classification,semantic segmentation,,,,,,,,,
Row_1077,"Yang, Xuan","Chen, Zhengchao","Zhang, Bing",A Block Shuffle Network with Superpixel Optimization for Landsat Image Semantic Segmentation,REMOTE SENSING,MAR 2022,2,"In recent years, with the development of deep learning in remotely sensed big data, semantic segmentation has been widely used in large-scale landcover classification. Landsat imagery has the advantages of wide coverage, easy acquisition, and good quality. However, there are two significant challenges for the semantic segmentation of mid-resolution remote sensing images: the insufficient feature extraction capability of deep convolutional neural network (DCNN); low edge contour accuracy. In this paper, we propose a block shuffle module to enhance the feature extraction capability of DCNN, a differentiable superpixel branch to optimize the feature of small objects and the accuracy of edge contours, and a self-boosting method to fuse semantic information and edge contour information to further optimize the fine-grained edge contour. We label three sets of Landsat landcover classification datasets, and achieved an overall accuracy of 86.3%, 83.2%, and 73.4% on the three datasets, respectively. Compared with other mainstream semantic segmentation networks, our proposed block shuffle network achieves state-of-the-art performance, and has good generalization ability.",semantic segmentation,superpixel,deep learning,Landsat,block shuffle,"Li, Baipeng",,"Bai, Yongqing",self-boosting,"Chen, Pan",,large scale,,,,,,,,,,,,,,
Row_1078,"Liu, Hui","Xu, Jie","Chen, Wen-Hua",Efficient Semantic Segmentation for Large-Scale Agricultural Nursery Managements via Point Cloud-Based Neural Network,REMOTE SENSING,NOV 2024,0,"Remote sensing technology has found extensive application in agriculture, providing critical data for analysis. The advancement of semantic segmentation models significantly enhances the utilization of point cloud data, offering innovative technical support for modern horticulture in nursery environments, particularly in the area of plant cultivation. Semantic segmentation results aid in obtaining tree components, like canopies and trunks, and detailed data on tree growth environments. However, obtaining precise semantic segmentation results from large-scale areas can be challenging due to the vast number of points involved. Therefore, this paper introduces an improved model aimed at achieving superior performance for large-scale points. The model incorporates direction angles between points to improve local feature extraction and ensure rotational invariance. It also uses geometric and relative distance information for better adjustment of different neighboring point features. An external attention module extracts global spatial features, and an upsampling feature adjustment strategy integrates features from the encoder and decoder. A specialized dataset was created from real nursery environments for experiments. Results show that the improved model surpasses several point-based models, achieving a Mean Intersection over Union (mIoU) of 87.18%. This enhances the precision of nursery environment analysis and supports the advancement of autonomous nursery managements.",remote sensing technology,semantic segmentation task,nursery managements,large-scale point clouds,neural network models,"Shen, Yue",,"Kai, Jinru",,,,,,,,,,,,,,,,,,
Row_1079,"Huang, Bohao","Reichman, Daniel","Collins, Leslie M.",ON THE EXTRACTION OF TRAINING IMAGERY FROM VERY LARGE REMOTE SENSING DATASETS FOR DEEP CONVOLUTIONAL SEGMENATATION NETWORKS,,2018,1,"In this work, we investigate strategies for training convolutional neural networks (CNNs) to perform recognition on remote sensing imagery. In particular we consider the particular problem of semantic segmentation in which the goal is to obtain a dense pixel-wise labeling of the input imagery. Remote sensing imagery is usually stored in the form of very large images, called ""tiles"", which are too big to be segmented directly using most CNNs and their associated hardware. Therefore smaller sub-images, called ""patches"", must be extracted from the available tiles. A popular strategy in the literature is to randomly sample patches from the tiles. However, in this work we demonstrate experimentally that extracting patches randomly from a uniform, non-overlapping spatial grid, leads to more accurate models. Our findings suggest the performance improvements are the result of reducing redundancy within the training dataset. We also find that sampling mini-batches of patches (for stochastic gradient descent) using constraints that maximizes the diversity of images within each batch leads to more accurate models. For example, in this work we constrained patches to come from varying tiles, or cities. These simple strategies contributed to our winning entry (in terms of overall performance) in the first year of the INRIA Building Labeling Challenge.",semantic segmentation,convolutional neural networks,remote sensing data,aerial imagery,building detection,"Bradbury, Kyle",IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,"Malof, Jordan M.",,,,,,,,,,,,,,,,,,
Row_1080,"Li, F","Peng, JX","Zheng, XJ",Object-based and semantic image segmentation using MRF,EURASIP JOURNAL ON APPLIED SIGNAL PROCESSING,JUN 1 2004,2,"The problem that the Markov random field (MRF) model captures the structural as well as the stochastic textures for remote sensing image segmentation is considered. As the one-point clique, namely, the external field, reflects the priori knowledge of the relative likelihood of the different region types which is often unknown, one would like to consider only two-pairwise clique in the texture. To this end, the MRF model cannot satisfactorily capture the structural component of the texture. In order to capture the structural texture, in this paper, a reference image is used as the external field. This reference image is obtained by Wold model decomposition which produces a purely random texture image and structural texture image from the original image. The structural component depicts the periodicity and directionality characteristics of the texture, while the former describes the stochastic. Furthermore, in order to achieve a good result of segmentation, such as improving smoothness of the texture edge, the proportion between the external and internal fields should be estimated by regarding it as a parameter of the MRF model. Due to periodicity of the structural texture, a useful by-product is that some long-range interaction is also taken into account. In addition, in order to reduce computation, a modified version of parameter estimation method is presented. Experimental results on remote sensing image demonstrating the performance of the algorithm are presented.",semantic and structural segmentation,MRF,Wold model,remote sensing image,,,,,,,,,,,,,,,,,,,,,,
Row_1081,"Avenash, R.","Viswanath, P.",,Semantic Segmentation of Satellite Images using a Modified CNN with Hard-Swish Activation Function,,2019,36,"Remote sensing is a key strategy used to obtain information related to the Earth's resources and its usage patterns. Semantic segmentation of a remotely sensed image in the spectral, spatial and temporal domain is an important preprocessing step where different classes of objects like crops, water bodies, roads, buildings are localized by a boundary. The paper proposes to use the Convolutional Neural Network (CNN) called U-HardNet with a new and novel activation function called the Hard-Swish for segmenting remotely sensed images. Along with the CNN, for a precise localization, the paper proposes to use IHS transformed images with binary cross entropy loss minimization. Experiments are done with publicly available images provided by DSTL (Defence Science and Technology Laboratory) for object recognition and a comparison is drawn with some recent relevant techniques.",Semantic Segmentation,Activation Function,Remote Sensing Images,Convolutional Neural Networks,,,"VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4",,,,,,,,,,,,,,,,,,,
Row_1082,"Huang, Bo","He, Boyong","Wu, Liaoni",High-resolution representations and multistage region-based network for ship detection and segmentation from optical remote sensing images,JOURNAL OF APPLIED REMOTE SENSING,JAN 1 2022,2,"Ship detection and segmentation in optical remote sensing (ORS) images is a current concern actively addressed by the academic community owing to its vast applications. Most present methods only detect the location of the ship but do not segment it at the pixel level. Considering the complex background of ORS images, identifying ships from interferences, such as clouds, waves, and some land architectures similar to ships, proves to be difficult. To address this issue, we propose a high-resolution representation and multistage region-based network (HR-MSRN) for ship detection and segmentation from ORS images. HR-MSRN mainly consists of three parts: the high-resolution feature pyramid network (HRFPN), region proposal network (RPN), and multistage detection and segmentation network (MSDSN). First, HRFPN is built as a backbone network to extract and fuse multilevel image feature maps. Second, ship candidate boxes are generated by defining numerous anchors through RPN. Third, using the idea of a cascade mask R-CNN as the reference method, the MSDSN is proposed to obtain the ship localization and mask shape. We utilize the proposed framework to evaluate an Airbus-ship dataset, and the experiments indicate that (1) HRFPN provides better feature representation ability than the ResNet-FPN when maintaining the same detection framework, especially for small ships; (2) the direct flow between mask branches refines the mask information, and the semantic segmentation branch enhances context information, which indicates that MSDSN is effective and promotes further improvements in ship detection and segmentation from ORS images; (3) in comparison to other region-based methods, HR-MSRN obtains superior performance of ship detection and segmentation in the ORS imagery. (C) 2021 Society of Photo-Optical Instrumentation Engineers (SPIE)",ship segmentation,high-resolution representation,cascaded network,optical remote sensing,,"Guo, Zhiming",,,,,,,,,,,,,,,,,,,,
Row_1083,"Hu, Huanjun","Li, Zheng","Li, Lin",Classification of Very High-Resolution Remote Sensing Imagery Using a Fully Convolutional Network With Global and Local Context Information Enhancements,IEEE ACCESS,2020,6,"Deep learning methods for semantic image segmentation can effectively extract geographical features from very high-resolution (VHR) remote sensing images. However, these methods experience over-segmentation in low-level features and a breakdown in the integrity of objects with fixed patch sizes due to the multi-scaled geographical features. In this study, a dual attention mechanism is introduced and embedded into densely connected convolutional networks (DenseNets) to form a dense-global-entropy network (DGEN) for the semantic segmentation of VHR remote sensing images. In the DGEN architecture, a global attention enhancement module is developed for context acquisition, and a local attention fusion module is designed for detail selection. This network presents the improved semantic segmentation performance of test ISPRS 2D datasets. The experimental results indicate an improvement in the overall accuracy (OA), F1, kappa coefficient and mean intersection over union (MIoU). Compared with the DeeplabV3 and SegNet models, the OA improves by 2.79 and 1.19; the mean F1 improves by 3.43 and 0.88; the kappa coefficient improves by 4.04 and 1.82; and the MIoU improves by 5.22 and 1.47, respectively. The experiments showed that the dual attention mechanism presented in this study can improve segmentation and maintain object integrity during the encoding-decoding process.",Attention mechanism,DenseNet,semantic segmentation,very high-resolution remote sensing images,,"Yang, Hui",,"Zhu, Haihong",,,,,,,,,,,,,,,,,,
Row_1084,"Tan, Jiahai","Gao, Ming","Yang, Kai",Remote Sensing Road Extraction by Road Segmentation Network,APPLIED SCIENCES-BASEL,JUN 2021,8,"Road extraction from remote sensing images has attracted much attention in geospatial applications. However, the existing methods do not accurately identify the connectivity of the road. The identification of the road pixels may be interfered with by the abundant ground such as buildings, trees, and shadows. The objective of this paper is to enhance context and strip features of the road by designing UNet-like architecture. The overall method first enhances the context characteristics in the segmentation step and then maintains the stripe characteristics in a refinement step. The segmentation step exploits an attention mechanism to enhance the context information between the adjacent layers. To obtain the strip features of the road, the refinement step introduces the strip pooling in a refinement network to restore the long distance dependent information of the road. Extensive comparative experiments demonstrate that the proposed method outperforms other methods, achieving an overall accuracy of 98.25% on the DeepGlobe dataset, and 97.68% on the Massachusetts dataset.",convolutional neural networks,semantic segmentation,self-attention mechanism,,,"Duan, Tao",,,,,,,,,,,,,,,,,,,,
Row_1085,"Zhang, Yue","Wang, Leiguang","Yang, Ruiqi",Semantic Segmentation of High Spatial Resolution Remote Sensing Imagery Based on Weighted Attention U-Net,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,1,"In recent years, with the development of deep learning and attention mechanism, more research has been carried out to realize semantic image segmentation based on deep learning integrated attention mechanisms. However, the current semantic segmentation methods have low segmentation accuracy, high computation cost, and serious loss of detailed information. In this paper, a lightweight designed attention gate model was introduced to reduce the computation cost. And because it can suppress irrelevant regions in the input image, while highlighting the salient features of specific tasks, the combination of the two weighting factors input features (x(l)) and gating signal (g) in this structure can improve segmentation accuracy and reduce loss of detail. Therefore, this study used the weighted attention U-Net network to perform semantic segmentation on the GID dataset and finally evaluated it on the four indicators of Precision, Recall, F1-Sorce, and mIoU. This result shows that different weight values have a more significant impact on the experimental results. The attention U-Net with the best weight combination compared with the traditional U-Net network, Precision, Recall, F1-Sorce, and mIoU are increased by 0.88%, 1.4%, 1.13%, and 1.2%, respectively. Compared with the original attention U-Net, Precision, Recall, F1-Sorce, and mIoU are increased by 0.86%, 1.24%, 1.04%, and 1.75%, respectively.",Semantic segmentation,deep learning,attention gate model,weighted attention U-Net,GID dataset,"Chen, Nan",,"Zhao, Yili",,"Dai, Qinling",,,,,,,,,,,,,,,,
Row_1086,"Sun, Yi","Tian, Yan","Xu, Yiping",Problems of encoder-decoder frameworks for high-resolution remote sensing image segmentation: Structural stereotype and insufficient learning,NEUROCOMPUTING,FEB 22 2019,61,"This work explores the use of deep convolutional neural networks for high resolution remote sensing imagery segmentation. Encoder-decoder frameworks are popular in semantic image segmentation. However, encoder-decoder models face two main problems. The one is structural stereotype which is receptive fields imbalance rooted in this kind of frameworks. The other is insufficient learning that deeper neural networks tend to encounter the notorious problem of vanishing gradients. Structural stereotype leads to unfair learning and inhomogeneous reasoning. We are the first to reveal the problem and propose ensemble training and inference strategies to suppress the adverse consequences of structural stereotype as far as possible. To alleviate the problem of insufficient learning, we propose a novel residual architecture for encoder-decoder models. The proposed method yields state-of-the-art performances on the ISPRS 2D semantic labeling contest benchmark. (C) 2018 Elsevier B.V. All rights reserved.",Remote sensing,Image segmentation,Encoder-decoder frameworks,Structural stereotype,Insufficient learning,,,,,,,,,,,,,,,,,,,,,
Row_1087,"Huang, Jianfeng","Zhang, Xinchang","Sun, Ying",Attention-Guided Label Refinement Network for Semantic Segmentation of Very High Resolution Aerial Orthoimages,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,17,"The recent applications of fully convolutional networks (FCNs) have shown to improve the semantic segmentation of very high resolution (VHR) remote-sensing images because of the excellent feature representation and end-to-end pixel labeling capabilities. While many FCN-based methods concatenate features from multilevel encoding stages to refine the coarse labeling results, the semantic gap between features of different levels and the selection of representative features are often overlooked, leading to the generation of redundant information and unexpected classification results. In this article, we propose an attention-guided label refinement network (ALRNet) for improved semantic labeling of VHR images. ALRNet follows the paradigm of the encoder-decoder architecture, which progressively refines the coarse labeling maps of different scales by using the channelwise attention mechanism. A novel attention-guided feature fusion module based on the squeeze-and-excitation module is designed to fuse higher level and lower level features. In this way, the semantic gaps among features of different levels are declined, and the category discrimination of each pixel in the lower level features is strengthened, which is helpful for subsequent label refinement. ALRNet is tested on three public datasets, including two ISRPS 2-D labeling datasets and the Wuhan University aerial building dataset. Results demonstrated that ALRNet had shown promising segmentation performance in comparison with state-of-the-art deep learning networks. The source code of ALRNet is made publicly available for further studies.",Semantics,Labeling,Feature extraction,Image segmentation,Remote sensing,"Xin, Qinchuan",,,Decoding,,,Sun,Attention mechanism,convolutional neural networks (CNNs),deep learning,semantic segmentation,urban object extraction,,,,,very high spatial resolution images,,,,
Row_1088,"Chong, Qianpeng","Ni, Mengying","Huang, Jianjun",Pos-DANet: A dual-branch awareness network for small object segmentation within high-resolution remote sensing images,ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,JUL 2024,5,"The more detailed and accurate earth observation has been made driven by the progress of satellites and sensors optical photography technology, which poses both an opportunity and a challenge to small object segmentation task. However, the inherent difficulty and inadequate consideration still make small object segmentation task inevitably encounter a performance gain bottleneck. We analyze the longstanding but underestimated challenges in this task and give a peer-to-peer solution to response them. Specifically, we design a dual-branch awareness structure dedicated to small object segmentation, named Pos-DANet, which is composed with a small object activation branch and a fuzzy refinement branch. The small object activation branch is used to aware the small objects and avoid the negative influence of redundant background. The fuzzy refinement branch utilizes the fuzzy modeling to improve the segmentation accuracy of small objects. These two branches work collaboratively to make the whole structure to focus more on small objects and achieve satisfying segmentation results. Finally, we propose a hierarchical unbiased loss to eliminate the bias against small objects in the regression process. Extensive experiments demonstrated that Pos-DANet exhibits a higher qualitative and quantitative performance than the advanced methods within small objects, which achieves the best results in mIoU (71.12 %, 83.33 %) and sIoU (63.23 %, 68.89 %) on two datasets.",Dual -branch,Remote sensing,Semantic segmentation,Small object,,"Liang, Zongbao",,"Wang, Jie",,"Li, Ziyi","Xu, Jindong",,,,,,,,,,,,,,,
Row_1089,"Zhang, Rongting","Zhang, Guangyun","Yin, Jihao",Mesh-Based DGCNN: Semantic Segmentation of Textured 3-D Urban Scenes,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,7,"Textured 3-D mesh is one of the final user products in photogrammetry and remote sensing. However, research on the semantic segmentation of complex urban scenes represented by textured 3-D meshes is in its infancy. We present a mesh-based dynamic graph convolutional neural network (DGCNN) for the semantic segmentation of textured 3-D meshes. To represent each mesh facet, composite input feature vectors are constructed by concatenating the face-inherent features, i.e., XY Z coordinates of the center of gravity (CoG), texture values, and normal vectors (NVs). A texture fusion module is embedded into the proposed mesh-based DGCNN to generate high-level semantic features of the high-resolution texture information, which is useful for semantic segmentation. We achieve competitive accuracies when the proposed method is applied to the SUM mesh datasets. The overall accuracy (OA), Kappa coefficient (Kap), mean precision (mP), mean recall (mR), mean F1 score (mF1), and mean intersection over union (mIoU) are 93.3%, 88.7%, 79.6%, 83.0%, 80.7%, and 69.6%, respectively. In particular, the OA, mean class accuracy (mAcc), mIoU, and mF1 increase by 0.3%, 12.4%, 3.4%, and 6.9%, respectively, compared with the state-of-the-art method.",Three-dimensional displays,Point cloud compression,Semantics,Semantic segmentation,Remote sensing,"Jia, Xiuping",,"Mian, Ajmal",Deep learning,,,Laser radar,semantic segmentation,textured 3-D mesh,urban scene understanding,,,,,,,,,,,
Row_1090,"Li, Zhengpeng","Hu, Jun","Wu, Kunyang",Comprehensive Attribute Difference Attention Network for Remote Sensing Image Semantic Understanding,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2025,0,"In the task of semantic understanding of remote sensing images, most current research focuses on learning contextual information through attention mechanisms or multiple inductive biases. However, these methods are limited in capturing fine-grained differences within the same attribute, are susceptible to background noise interference, and lack effective modeling capabilities for spatial relationships and long-range dependencies between different remote sensing attributes. To address these issues, we specifically focus on the homogeneous and heterogeneous differences between attributes in remote sensing images. Thus, we propose an innovative comprehensive attribute difference attention network (CADANet) to enhance the performance of understanding remote sensing images. Specifically, we design two key modules: the attribute feature aggregation (AFA) module and the context attribute-aware spatial attention (CAASA) module. The AFA module primarily focuses on global and local domain attribute modeling, reducing the impact of homogeneous attribute differences through fine-grained feature extraction and global context information. The CAASA module integrates pixel-level global background information and relative position priors, employing a self-attention mechanism to capture long-range dependencies, thus addressing heterogeneous attribute differences. Extensive experimental results conducted on the widely used Vaihingen, Potsdam, and WHDLD datasets effectively demonstrate that our proposed method outperforms other recent approaches in performance. Our code is available at https://github.com/lzp-lkd/CADANet.",Feature extraction,Remote sensing,Semantics,Transformers,Context modeling,"Miao, Jiawei",,"Wu, Jiansheng",Land surface,,,Buildings,Data mining,Automobiles,Interference,Attention mechanism,attribute perception,,,,,context modeling,feature extraction,multiattribute scene understanding,,
Row_1091,"Guo, Shibo","Gui, Yuanyuan","Zhang, Mengmeng",MMS:MULTI-SOURCE MUTUAL SUPERVISION SEMANTIC SEGMENTATION,,2023,0,"How to use multi-source data for semantic segmentation is a hot topic. In this article, a new multi-source image data semantic segmentation based on multi-source mutual supervision (MMS) has been proposed. First, multi-source data from the same region are trained separately using identical segmentation networks for initialization; then, MMS performs mutual supervision training on these initialized networks, thus adaptively cooperating differences in information distribution between multiple sources of image data, and combines real label constraints output consistency across different networks. Experimental results demonstrate that the proposed MMS strategy can effectively coordinate information between multi-source data, improve segmentation accuracy, and is effective in different semantic segmentation networks and different types of data sets.",Semantic Segmentation,Multi-source Data,Mutual Supervision,,,"Li, Wei",IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,,,,,
Row_1092,"Ye, Chul-Soo","Ahn, Young-Man","Baek, Tae-Woong",Semantic Building Segmentation Using the Combination of Improved DeepResUNet and Convolutional Block Attention Module,KOREAN JOURNAL OF REMOTE SENSING,DEC 2022,5,"As deep learning technology advances and various high-resolution remote sensing images are available, interest in using deep learning technology and remote sensing big data to detect buildings and change in urban areas is increasing significantly. In this paper, for semantic building segmentation of high-resolution remote sensing images, we propose a new building segmentation model, Convolutional Block Attention Module (CBAM)-DRUNet that uses the DeepResUNet model, which has excellent performance in building segmentation, as the basic structure, improves the residual learning unit and combines a CBAM with the basic structure. In the performance evaluation using WHU dataset and INRIA dataset, the proposed building segmentation model showed excellent performance in terms of F1 score, accuracy and recall compared to ResUNet and DeepResUNet including UNet.",Semantic building segmentation,Convolutional block attention module,Residual learning,,,"Kim, Kyung-Tae",,,,,,,,,,,,,,,,,,,,
Row_1093,"Coelho Vieira da Costa, Marcus Vinicius","Ferreira de Carvalho, Osmar Luiz","Orlandi, Alex Gois",Remote Sensing for Monitoring Photovoltaic Solar Plants in Brazil Using Deep Semantic Segmentation,ENERGIES,MAY 2021,45,"Brazil is a tropical country with continental dimensions and abundant solar resources that are still underutilized. However, solar energy is one of the most promising renewable sources in the country. The proper inspection of Photovoltaic (PV) solar plants is an issue of great interest for the Brazilian territory's energy management agency, and advances in computer vision and deep learning allow automatic, periodic, and low-cost monitoring. The present research aims to identify PV solar plants in Brazil using semantic segmentation and a mosaicking approach for large image classification. We compared four architectures (U-net, DeepLabv3+, Pyramid Scene Parsing Network, and Feature Pyramid Network) with four backbones (Efficient-net-b0, Efficient-net-b7, ResNet-50, and ResNet-101). For mosaicking, we evaluated a sliding window with overlapping pixels using different stride values (8, 16, 32, 64, 128, and 256). We found that: (1) the models presented similar results, showing that the most relevant approach is to acquire high-quality labels rather than models in many scenarios; (2) U-net presented slightly better metrics, and the best configuration was U-net with the Efficient-net-b7 encoder (98% overall accuracy, 91% IoU, and 95% F-score); (3) mosaicking progressively increases results (precision-recall and receiver operating characteristic area under the curve) when decreasing the stride value, at the cost of a higher computational cost. The high trends of solar energy growth in Brazil require rapid mapping, and the proposed study provides a promising approach.",solar panel,deep learning,semantic segmentation,,,"Hirata, Issao",,"de Albuquerque, Anesmar Olino",,"Vilarinho e Silva, Felipe","Guimaraes, Renato Fontes",,,,,,,"Gomes, Roberto Arnaldo Trancoso","de Carvalho Junior, Osmar Abilio",,,,,,,
Row_1094,"Lu, Tingyu","Wan, Luhe","Qi, Shaoqun",Land Cover Classification of UAV Remote Sensing Based on Transformer-CNN Hybrid Architecture,SENSORS,JUN 2 2023,11,"High-precision land cover maps of remote sensing images based on an intelligent extraction method are an important research field for many scholars. In recent years, deep learning represented by convolutional neural networks has been introduced into the field of land cover remote sensing mapping. In view of the problem that a convolution operation is good at extracting local features but has limitations in modeling long-distance dependence relationships, a semantic segmentation network, DE-UNet, with a dual encoder is proposed in this paper. The Swin Transformer and convolutional neural network are used to design the hybrid architecture. The Swin Transformer pays attention to multi-scale global features and learns local features through the convolutional neural network. Integrated features take into account both global and local context information. In the experiment, remote sensing images from UAVs were used to test three deep learning models including DE-UNet. DE-UNet achieved the highest classification accuracy, and the average overall accuracy was 0.28% and 4.81% higher than UNet and UNet++, respectively. It shows that the introduction of a Transformer enhances the model fitting ability.",deep learning,transformer,CNN,semantic segmentation,UAV remote sensing,"Gao, Meixiang",,,,,,,,,,,,,,,,,,,,
Row_1095,"Wang, Ziquan","Zhang, Yongsheng","Zhang, Zhenchao",Exploring Uncertainty-Based Self-Prompt for Test-Time Adaptation Semantic Segmentation in Remote Sensing Images,REMOTE SENSING,APR 2024,0,"Test-time adaptation (TTA) has been proven to effectively improve the adaptability of deep learning semantic segmentation models facing continuous changeable scenes. However, most of the existing TTA algorithms lack an explicit exploration of domain gaps, especially those based on visual domain prompts. To address these issues, this paper proposes a self-prompt strategy based on uncertainty, guiding the model to continuously focus on regions with high uncertainty (i.e., regions with a larger domain gap). Specifically, we still use the Mean-Teacher architecture with the predicted entropy from the teacher network serving as the input to the prompt module. The prompt module processes uncertain maps and guides the student network to focus on regions with higher entropy, enabling continuous adaptation to new scenes. This is a self-prompting strategy that requires no prior knowledge and is tested on widely used benchmarks. In terms of the average performance, our method outperformed the baseline algorithm in TTA and continual TTA settings of Cityscapes-to-ACDC by 3.3% and 3.9%, respectively. Our method also outperformed the baseline algorithm by 4.1% and 3.1% on the more difficult Cityscapes-to-(Foggy and Rainy) Cityscapes setting, which also surpasses six other current TTA methods.",uncertainty-based self-prompt,test-time domain adaptation,semantic segmentation,,,"Jiang, Zhipeng",,"Yu, Ying",,"Li, Lei","Zhang, Lei",,,,,,,,,,,,,,,
Row_1096,"Zhang, Mingwei","Jing, Weipeng","Lin, Jingbo",NAS-HRIS: Automatic Design and Architecture Search of Neural Network for Semantic Segmentation in Remote Sensing Images,SENSORS,SEP 2020,28,"The segmentation of high-resolution (HR) remote sensing images is very important in modern society, especially in the fields of industry, agriculture and urban modelling. Through the neural network, the machine can effectively and accurately extract the surface feature information. However, using the traditional deep learning methods requires plentiful efforts in order to find a robust architecture. In this paper, we introduce a neural network architecture search (NAS) method, called NAS-HRIS, which can automatically search neural network architecture on the dataset. The proposed method embeds a directed acyclic graph (DAG) into the search space and designs the differentiable searching process, which enables it to learn an end-to-end searching rule by using gradient descent optimization. It uses the Gumbel-Max trick to provide an efficient way when drawing samples from a non-continuous probability distribution, and it improves the efficiency of searching and reduces the memory consumption. Compared with other NAS, NAS-HRIS consumes less GPU memory without reducing the accuracy, which corresponds to a large amount of HR remote sensing imagery data. We have carried out experiments on the WHUBuilding dataset and achieved 90.44% MIoU. In order to fully demonstrate the feasibility of the method, we made a new urban Beijing Building dataset, and conducted experiments on satellite images and non-single source images, achieving better results than SegNet, U-Net and Deeplab v3+ models, while the computational complexity of our network architecture is much smaller.",deep learning,high-resolution remote sensing,image segmentation,neural architecture search,neural network optimisation,"Fang, Nengzhen",,"Wei, Wei",urban monitoring,"Wozniak, Marcin","Damasevicius, Robertas",,,,,,,,,,,,,,,
Row_1097,"Audebert, Nicolas","Le Saux, Bertrand","Lefevre, Sebastien",Segment-before-Detect: Vehicle Detection and Classification through Semantic Segmentation of Aerial Images,REMOTE SENSING,APR 2017,196,"Like computer vision before, remote sensing has been radically changed by the introduction of deep learning and, more notably, Convolution Neural Networks. Land cover classification, object detection and scene understanding in aerial images rely more and more on deep networks to achieve new state-of-the-art results. Recent architectures such as Fully Convolutional Networks can even produce pixel level annotations for semantic mapping. In this work, we present a deep-learning based segment-before-detect method for segmentation and subsequent detection and classification of several varieties of wheeled vehicles in high resolution remote sensing images. This allows us to investigate object detection and classification on a complex dataset made up of visually similar classes, and to demonstrate the relevance of such a subclass modeling approach. Especially, we want to show that deep learning is also suitable for object-oriented analysis of Earth Observation data as effective object detection can be obtained as a byproduct of accurate semantic segmentation. First, we train a deep fully convolutional network on the ISPRS Potsdam and the NZAM/ONERA Christchurch datasets and show how the learnt semantic maps can be used to extract precise segmentation of vehicles. Then, we show that those maps are accurate enough to perform vehicle detection by simple connected component extraction. This allows us to study the repartition of vehicles in the city. Finally, we train a Convolutional Neural Network to perform vehicle classification on the VEDAI dataset, and transfer its knowledge to classify the individual vehicle instances that we detected.",deep learning,vehicle detection,semantic segmentation,object classification,,,,,,,,,,,,,,,,,,,,,,
Row_1098,"Dong, Rongsheng","Bai, Lulu","Li, Fengying",SiameseDenseU-Net-based Semantic Segmentation of Urban Remote Sensing Images,MATHEMATICAL PROBLEMS IN ENGINEERING,MAR 23 2020,4,"Boundary pixel blur and category imbalance are common problems that occur during semantic segmentation of urban remote sensing images. Inspired by DenseU-Net, this paper proposes a new end-to-end network-SiameseDenseU-Net. First, the network simultaneously uses both true orthophoto (TOP) images and their corresponding normalized digital surface model (nDSM) as the input of the network structure. The deep image features are extracted in parallel by downsampling blocks. Information such as shallow textures and high-level abstract semantic features are fused throughout the connected channels. The features extracted by the two parallel processing chains are then fused. Finally, a softmax layer is used to perform prediction to generate dense label maps. Experiments on the Vaihingen dataset show that SiameseDenseU-Net improves the F1-score by 8.2% and 7.63% compared with the Hourglass-ShapeNetwork (HSN) model and with the U-Net model. Regarding the boundary pixels, when using the same focus loss function based on median frequency balance weighting, compared with the original DenseU-Net, the small-target ""car"" category F1-score of SiameseDenseU-Net improved by 0.92%. The overall accuracy and the average F1-score also improved to varying degrees. The proposed SiameseDenseU-Net is better at identifying small-target categories and boundary pixels, and it is numerically and visually superior to the contrast model.",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_1099,"Khan, Sultan Daud","Alarabi, Louai","Basalamah, Saleh",Deep Hybrid Network for Land Cover Semantic Segmentation in High-Spatial Resolution Satellite Images,INFORMATION,JUN 2021,23,"Land cover semantic segmentation in high-spatial resolution satellite images plays a vital role in efficient management of land resources, smart agriculture, yield estimation and urban planning. With the recent advancement in remote sensing technologies, such as satellites, drones, UAVs, and airborne vehicles, a large number of high-resolution satellite images are readily available. However, these high-resolution satellite images are complex due to increased spatial resolution and data disruption caused by different factors involved in the acquisition process. Due to these challenges, an efficient land-cover semantic segmentation model is difficult to design and develop. In this paper, we develop a hybrid deep learning model that combines the benefits of two deep models, i.e., DenseNet and U-Net. This is carried out to obtain a pixel-wise classification of land cover. The contraction path of U-Net is replaced with DenseNet to extract features of multiple scales, while long-range connections of U-Net concatenate encoder and decoder paths are used to preserve low-level features. We evaluate the proposed hybrid network on a challenging, publicly available benchmark dataset. From the experimental results, we demonstrate that the proposed hybrid network exhibits a state-of-the-art performance and beats other existing models by a considerable margin.",land cover classification,remote sensing,semantic segmentation,deep learning,,,,,,,,,,,,,,,,,,,,,,
Row_1100,"Maia, Deise Santana","Pham, Minh-Tan","Lefevre, Sebastien",Watershed-Based Attribute Profiles With Semantic Prior Knowledge for Remote Sensing Image Analysis,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,3,"In this article, we develop a novel feature extraction method that combines two well-established mathematical morphology concepts: watersheds and morphological attribute profiles (APs). In order to extract spatial-spectral features from remote sensing data, APs were originally defined as sequences of filtering operators on inclusion trees, i.e., the max- and min-trees, computed from the input image. In this study, we extend the AP paradigm to the more general framework of hierarchical watersheds. Moreover, we explore the semantic knowledge provided by labeled training pixels during different phases of the watershed-AP construction, namely within the construction of hierarchical watersheds from the raw image and later within the filtering of the resulting hierarchy. We illustrate the relevance of the proposed method with two applications including land cover classification and building extraction using optical remote sensing images. Experimental results show that the new profiles outperform various existing features using two public datasets (Zurich and Vaihingen), thus providing another high potential feature extraction method within the AP family.",Image segmentation,Remote sensing,Feature extraction,Training,Semantics,,,,Image edge detection,,,Hyperspectral imaging,Attribute profiles (APs),building extraction,classification,remote sensing,watershed,,,,,,,,,
Row_1101,"Niu, Ruigang","Sun, Xian","Tian, Yu",Hybrid Multiple Attention Network for Semantic Segmentation in Aerial Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,132,"Semantic segmentation in very-high-resolution (VHR) aerial images is one of the most challenging tasks in remote sensing image understanding. Most of the current approaches are based on deep convolutional neural networks (DCNNs). However, standard convolution with local receptive fields fails in modeling global dependencies. Prior research works have indicated that attention-based methods can capture long-range dependencies and further reconstruct the feature maps for better representation. Nevertheless, limited by the mere perspective of spatial and channel attention and huge computation complexity of self-attention (SA) mechanism, it is unlikely to model the effective semantic interdependencies between each pixel pair of remote sensing data with complex spectra. In this work, we propose a novel attention-based framework named hybrid multiple attention network (HMANet) to adaptively capture global correlations from the perspective of space, channel, and category in a more effective and efficient manner. Concretely, a class augmented attention (CAA) module embedded with a class channel attention (CCA) module can be used to compute category-based correlation and recalibrate the class-level information. In addition, we introduce a simple yet effective region shuffle attention (RSA) module to reduce feature redundant and improve the efficiency of SA mechanism via regionwise representations. Extensive experimental results on the ISPRS Vaihingen, Potsdam benchmark, and iSAID data set demonstrate the effectiveness and efficiency of our HMANet over other state-of-the-art methods.",Semantics,Image segmentation,Correlation,Feature extraction,Task analysis,"Diao, Wenhui",,"Chen, Kaiqiang",Remote sensing,"Fu, Kun",,Graphics processing units,Aerial imagery,deep convolution neural networks (DCNNs),self-attention (SA) mechanism,semantic segmentation,,,,,,,,,,
Row_1102,"Liang, Chenbin","Xiao, Baihua","Cheng, Bo",XANet: An Efficient Remote Sensing Image Segmentation Model Using Element-Wise Attention Enhancement and Multi-Scale Attention Fusion,REMOTE SENSING,JAN 2023,4,"Massive and diverse remote sensing data provide opportunities for data-driven tasks in the real world, but also present challenges in terms of data processing and analysis, especially pixel-level image interpretation. However, the existing shallow-learning and deep-learning segmentation methods, bounded by their technical bottlenecks, cannot properly balance accuracy and efficiency, and are thus hardly scalable to the practice scenarios of remote sensing in a successful way. Instead of following the time-consuming deep stacks of local operations as most state-of-the-art segmentation networks, we propose a novel segmentation model with the encoder-decoder structure, dubbed XANet, which leverages the more computationally economical attention mechanism to boost performance. Two novel attention modules in XANet are proposed to strengthen the encoder and decoder, respectively, namely the Attention Recalibration Module (ARM) and Attention Fusion Module (AFM). Unlike current attention modules, which only focus on elevating the feature representation power, and regard the spatial and channel enhancement of a feature map as two independent steps, ARM gathers element-wise semantic descriptors coupling spatial and channel information to directly generate a 3D attention map for feature enhancement, and AFM innovatively utilizes the cross-attention mechanism for the sufficient spatial and channel fusion of multi-scale features. Extensive experiments were conducted on ISPRS and GID datasets to comprehensively analyze XANet and explore the effects of ARM and AFM. Furthermore, the results demonstrate that XANet surpasses other state-of-the-art segmentation methods in both model performance and efficiency, as ARM yields a superior improvement versus existing attention modules with a competitive computational overhead, and AFM achieves the complementary advantages of multi-level features under the sufficient consideration of efficiency.",semantic segmentation,attention mechanism,cross-attention,feature fusion,,"Dong, Yunyun",,,,,,,,,,,,,,,,,,,,
Row_1103,"Niu, Xuerui","Zeng, Qiaolin","Luo, Xiaobo",FCAU-Net for the Semantic Segmentation of Fine-Resolution Remotely Sensed Images,REMOTE SENSING,JAN 2022,10,"The semantic segmentation of fine-resolution remotely sensed images is an urgent issue in satellite image processing. Solving this problem can help overcome various obstacles in urban planning, land cover classification, and environmental protection, paving the way for scene-level landscape pattern analysis and decision making. Encoder-decoder structures based on attention mechanisms have been frequently used for fine-resolution image segmentation. In this paper, we incorporate a coordinate attention (CA) mechanism, adopt an asymmetric convolution block (ACB), and design a refinement fusion block (RFB), forming a network named the fusion coordinate and asymmetry-based U-Net (FCAU-Net). Furthermore, we propose novel convolutional neural network (CNN) architecture to fully capture long-term dependencies and fine-grained details in fine-resolution remotely sensed imagery. This approach has the following advantages: (1) the CA mechanism embeds position information into a channel attention mechanism to enhance the feature representations produced by the network while effectively capturing position information and channel relationships; (2) the ACB enhances the feature representation ability of the standard convolution layer and captures and refines the feature information in each layer of the encoder; and (3) the RFB effectively integrates low-level spatial information and high-level abstract features to eliminate background noise when extracting feature information, reduces the fitting residuals of the fused features, and improves the ability of the network to capture information flows. Extensive experiments conducted on two public datasets (ZY-3 and DeepGlobe) demonstrate the effectiveness of the FCAU-Net. The proposed FCAU-Net transcends U-Net, Attention U-Net, the pyramid scene parsing network (PSPNet), DeepLab v3+, the multistage attention residual U-Net (MAResU-Net), MACU-Net, and the Transformer U-Net (TransUNet). Specifically, the FCAU-Net achieves a 97.97% (95.05%) pixel accuracy (PA), a 98.53% (91.27%) mean PA (mPA), a 95.17% (85.54%) mean intersection over union (mIoU), and a 96.07% (90.74%) frequency-weighted IoU (FWIoU) on the ZY-3 (DeepGlobe) dataset.",semantic segmentation,fine-resolution remotely sensed images,attention mechanism,asymmetric convolution block,refinement fusion block,"Chen, Liangfu",,,,,,,,,,,,,,,,,,,,
Row_1104,"Liang, Yi","Zhang, Chengkun","Han, Min",RaSRNet: An End-to-End Relation-Aware Semantic Reasoning Network for Change Detection in Optical Remote Sensing Images,IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT,2024,7,"Optical remote sensing images (RSIs) are used in surface observation, and one of the most interesting research topics is change detection (CD). The internal problem of RSIs, including multiscales changed objects and cluttered background, still deserves attention. Existing methods make great efforts to solve this problem but inevitably miss detection, which affects the model performance. To address this dilemma, this article proposes a relation-aware semantic reasoning network (RaSRNet) in an end-to-end manner to pop-out change objects in RSIs, where the key point is to perceive contextual semantic information. The relation-aware (Ra) module in RaSRNet combats the lack of contextual information caused by the limited receptive field (RF) of the general convolutional layer, which facilitates all-around changed object detection. The multilevel semantic reasoning encoder-decoder (ED) backbone in RaSRNet extracts and reconstructs pixel semantic information, alleviates the interference of background noise, and improves the integrity recognition of changed objects. In addition, the decoder backend undertakes two semantic segmentation branches and introduces a semantic reasoning loss between the two branches to infer pixel semantic categories, which provides more accurate semantic features for the CD. Extensive experiments are conducted on the three public RSI CD datasets, and the results demonstrate that the proposed RaSRNet can accurately locate changed objects, which consistently outperforms the state-of-the-art CD competitors.",Semantics,Feature extraction,Cognition,Decoding,Semantic segmentation,,,,Head,,,Convolution,Change detection (CD),optical remote sensing images (RSIs),relation-aware (Ra),semantic reasoning,,,,,,,,,,
Row_1105,"Bedawi, Safaa M.","Kamel, Mohamed S.",,Segmentation of Very High Resolution Remote Sensing Imagery of Urban Areas Using Particle Swarm Optimization Algorithm,"IMAGE ANALYSIS AND RECOGNITION, PT I, PROCEEDINGS",2010,4,"As the improvement of the resolution of aerial and satellite remote sensed images, the semantic richness of the image increases which makes image analysis more difficult. Dense urban environment sensed by very high-resolution (VHR) optical sensors is even more challenging. Occlusions and shadows due to buildings and trees hide some objects of the scene. Fast and efficient segmentation of such noisy images (which is essential for their further analysis) has remained a challenging problem for years. It is difficult for traditional methods to deal with such noisy and large volume data. Clustering-based segmentation with swarm-based algorithms is emerging as an alternative to more conventional clustering methods, such as hierarchical clustering and k-means. In this paper, we introduce the use of Particle Swarm Optimization (PSO) clustering algorithm segmenting high resolution remote sensing images. Contrary to the localized searching of the K-means algorithm, the PSO clustering algorithm performs a globalized search in the entire solution space. We applied the PSO and K-means clustering algorithm on thirty images cropped from color aerial images. The results illustrate that PSO algorithm can generate more compact clustering results than the K-means algorithm.",Swarm Intelligence,Particle Swarm Optimization (PSO),Remote Sensing,Aerial Images,and Clustering-Based Segmentation,,,,,,,,,,,,,,,,,,,,,
Row_1106,"Chen, Ming","Jiang, Wanshou",,SCDVit: Semantic Change Detection Based on Sam-Vit and Semantic Consistency,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2025,0,"In recent years, change detection has been a hot research topic in remote sensing. Previous research has focused on binary change detection (BCD), limiting its practical applications. Therefore, semantic change detection (SCD), which can detect multiple change classes, is gradually becoming a more mainstream task. Most existing SCD methods use convolutional neural networks as the backbone to extract multiscale features and use relatively simple decoder structures, leading to unsatisfactory detection accuracy. We propose a multitask network for SCD, and in the encoder, given the great success of segment anything module (SAM) and vision transformer (VIT) in the field of general-purpose segmentation task, we introduce SAM-VIT into the backbone to enhance the encoder's ability to capture long-range contextual semantic relationships. We propose a transformer-based decoder structure for the semantic segmentation branch to extract local and global features effectively. We propose a convolutional attention-based change extractor for the BCD branch to enhance temporal information fusion. Also, we analyze in detail the semantic inconsistency that affects the performance of SCD. First, we introduce contrastive loss to establish the correlation between the output features of the BCD branch and the segmentation branch. Second, we design a bitemporal graph semantic interaction module to maintain semantic consistency between the output features of the two segmentation branches; the module assigns pixels with different land cover types to the corresponding graph nodes based on clustering techniques and then uses cross-attention to model the correlation between bitemporal semantic features in the graph space. Finally, a self-learning training scheme based on pseudolabel further mitigates the problem of semantic inconsistency. SCDVit achieves state-of-the-art performance on two popular high-resolution datasets. Meanwhile, adequate quantitative and qualitative analyses highlight the potential of SAM-VIT for change detection and the effectiveness of the module designed based on semantic consistency.",Change detection (CD),graph convolutional network (GCN),graph learning,remote sensing,semantic segmentation (SS),,,,transformer,,,Change detection (CD),graph convolutional network (GCN),graph learning,remote sensing,semantic segmentation (SS),transformer,,,,,,,,,
Row_1107,"Lu, Min","Liu, Jiayin","Wang, Feng",Multi-Task Learning of Relative Height Estimation and Semantic Segmentation from Single Airborne RGB Images,REMOTE SENSING,JUL 2022,6,"The generation of topographic classification maps or relative heights from aerial or remote sensing images represents a crucial research tool in remote sensing. On the one hand, from auto-driving, three-dimensional city modeling, road design, and resource statistics to smart cities, each task requires relative height data and classification data of objects. On the other hand, most relative height data acquisition methods currently use multiple images. We find that relative height and geographic classification data can be mutually assisted through data distribution. In recent years, with the rapid development of artificial intelligence technology, it has become possible to estimate the relative height from a single image. It learns implicit mapping relationships in a data-driven manner that may not be explicitly available through mathematical modeling. On this basis, we propose a unified, in-depth learning structure that can generate both estimated relative height maps and semantically segmented maps and perform end-to-end training. Compared with the existing methods, our task is to perform both relative height estimation and semantic segmentation tasks simultaneously. We only need one picture to obtain the corresponding semantically segmented images and relative heights simultaneously. The model's performance is much better than that of equivalent computational models. We also designed dynamic weights to enable the model to learn relative height estimation and semantic segmentation simultaneously. At the same time, we have conducted good experiments on existing datasets. The experimental results show that the proposed Transformer-based network architecture is suitable for relative height estimation tasks and vastly outperforms other state-of-the-art DL (Deep Learning) methods.",deep learning,remote sensing,relative height estimation,semantic segmentation,end-to-end,"Xiang, Yuming",,,artificial intelligence,,,multi-task,,,,,,,,,,,,,,
Row_1108,"Chen, Long","Qu, Zhiyuan","Zhang, Yao",Edge-Enhanced GCIFFNet: A Multiclass Semantic Segmentation Network Based on Edge Enhancement and Multiscale Attention Mechanism,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"In recent years, remote sensing images (RSIs) have witnessed significant improvements in both quality and quantity. With the application of deep-learning techniques, these RSIs can be more effectively utilized to harnessed to aid in environment monitoring and urban planning. Semantic segmentation, as a common task in RSIs processing, confronts numerous challenges, including inaccurate classification, fuzzy boundaries, and other problems. This article proposes a novel semantic segmentation network known as the edge-enhanced global contextual information guided feature fusion network to address these challenges. This network consists of an edge-enhanced part and a backbone network part. First, in the encoding stage, the recurrent criss-cross attention block is employed, which incorporates spatial attention, mechanisms to capture global information. Second, in the decoding stage, a channel attention residual block module is proposed to facilitate the fusion of high-level and low-level features. Moreover, we enhance the network's ability to extract edge information during training by sharing parameters between the backbone and employing a specialized loss function. The network proposed in this article utilizes both channel attention and spatial attention at different stages, effectively utilizing edge information. Finally, we conduct experiments using the Yinchuan dataset and the LoveDA dataset. The experimental results show that the proposed network demonstrates excellent performance on both datasets.",Attention,convolutional neural network (CNN),edge segmentation,remote sensing images (RSIs),semantic segmentation,"Liu, Jingyang",,"Wang, Ruwen",,"Zhang, Dezheng",,,,,,,,,,,,,,,,
Row_1109,"Draeger, Nikolaus","Xu, Yonghao","Ghamisi, Pedram",Backdoor Attacks for Remote Sensing Data With Wavelet Transform,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,4,"Recent years have witnessed the great success of deep learning algorithms in the geoscience and remote sensing (RS) realm. Nevertheless, the security and robustness of deep learning models deserve special attention when addressing safety-critical RS tasks. In this article, we provide a systematic analysis of backdoor attacks for RS data, where both scene classification and semantic segmentation tasks are considered. While most of the existing backdoor attack algorithms rely on visible triggers such as squared patches with well-designed patterns, we propose a novel wavelet transform-based attack (WABA) method, which can achieve invisible attacks by injecting the trigger image into the poisoned image in the low-frequency domain. In this way, the high-frequency information in the trigger image can be filtered out in the attack, resulting in stealthy data poisoning. Despite its simplicity, the proposed method can significantly cheat the current state-of-the-art deep learning models with a high attack success rate. We further analyze how different trigger images and the hyperparameters in the wavelet transform would influence the performance of the proposed method. Extensive experiments on four benchmark RS datasets demonstrate the effectiveness of the proposed method for both scene classification and semantic segmentation tasks and thus highlight the importance of designing advanced backdoor defense algorithms to address this threat in RS scenarios. The code will be available online at https://github.com/ndraeger/waba.",~Artificial intelligence (AI),backdoor attack,deep learning,remote sensing (RS),scene classification,,,,semantic segmentation,,,wavelet transform,,,,,,,,,,,,,,
Row_1110,"Wen, Dawei","Zhu, Shihao","Tian, Yuan",Generating 10-Meter Resolution Land Use and Land Cover Products Using Historical Landsat Archive Based on Super Resolution Guided Semantic Segmentation Network,REMOTE SENSING,JUN 2024,0,"Generating high-resolution land cover maps using relatively lower-resolution remote sensing images is of great importance for subtle analysis. However, the domain gap between real lower-resolution and synthetic images has not been permanently resolved. Furthermore, super-resolution information is not fully exploited in semantic segmentation models. By solving the aforementioned issues, a deeply fused super resolution guided semantic segmentation network using 30 m Landsat images is proposed. A large-scale dataset comprising 10 m Sentinel-2, 30 m Landsat-8 images, and 10 m European Space Agency (ESA) Land Cover Product is introduced, facilitating model training and evaluation across diverse real-world scenarios. The proposed Deeply Fused Super Resolution Guided Semantic Segmentation Network (DFSRSSN) combines a Super Resolution Module (SRResNet) and a Semantic Segmentation Module (CRFFNet). SRResNet enhances spatial resolution, while CRFFNet leverages super-resolution information for finer-grained land cover classification. Experimental results demonstrate the superior performance of the proposed method in five different testing datasets, achieving 68.17-83.29% and 39.55-75.92% for overall accuracy and kappa, respectively. When compared to ResUnet with up-sampling block, increases of 2.16-34.27% and 8.32-43.97% were observed for overall accuracy and kappa, respectively. Moreover, we proposed a relative drop rate of accuracy metrics to evaluate the transferability. The model exhibits improved spatial transferability, demonstrating its effectiveness in generating accurate land cover maps for different cities. Multi-temporal analysis reveals the potential of the proposed method for studying land cover and land use changes over time. In addition, a comparison of the state-of-the-art full semantic segmentation models indicates that spatial details are fully exploited and presented in semantic segmentation results by the proposed method.",deep learning,super resolution,land cover,Sentinel,Landsat,"Guan, Xuehua",,"Lu, Yang",semantic segmentation,,,remote sensing,,,,,,,,,,,,,,
Row_1111,"Pan, Bin","Shi, Zhenwei","Xu, Xia",CoinNet: Copy Initialization Network for Multispectral Imagery Semantic Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,MAY 2019,48,"Remote sensing imagery semantic segmentation refers to assigning a label to every pixel. Recently, deep convolutional neural networks (CNNs)-based methods have presented an impressive performance in this task. Due to the lack of sufficient labeled remote sensing images, researchers usually utilized transfer learning (TL) strategies to fine tune networks which were pretrained in huge RGB-scene data sets. Unfortunately, this manner may not work if the target images are multispectral/hyperspectral. The basic assumption of TL is that the low-level features extracted by the former layers are similar in most data sets, hence users only require to train the parameters in the last layers that are specific to different tasks. However, if one should use a pretrained deep model in RGB data for multispectral /hyperspectral imagery semantic segmentation, the structure of the input layer has to be adjusted. In this case, the first convolutional layer has to be trained using the multispectral /hyperspectral data sets which are much smaller. Apparently, the feature representation ability of the first convolutional layer will decrease and it may further harm the following layers. In this letter, we propose a new deep learning model, COpy INitialization Network (CoinNet), for multispectral imagery semantic segmentation. The major advantage of CoinNet is that it can make full use of the initial parameters in the pretrained network's first convolutional layer. Comparison experiments on a challenging multispectral data set have demonstrated the effectiveness of the proposed improvement. The demo and a trained network will be published in our homepage.",CoinNet,deep convolutional network,semantic segmentation,transfer learning (TL),,"Shi, Tianyang",,"Zhang, Ning",,"Zhu, Xinzhong",,,,,,,,,,,,,,,,
Row_1112,"Yoo, S.","Ko, C.","Sohn, G.",YUTO SEMANTIC: A LARGE SCALE AERIAL LIDAR DATASET FOR SEMANTIC SEGMENTATION,,2023,0,"Creating virtual duplicates of the real world has garnered significant attention due to its applications in areas such as autonomous driving, urban planning, and urban mapping. One of the critical tasks in the computer vision community is semantic segmentation of outdoor collected point clouds. The development and research of robust semantic segmentation algorithms heavily rely on precise and comprehensive benchmark datasets. In this paper, we present the York University Teledyne Optech 3D Semantic Segmentation Dataset (YUTO Semantic), a multi-mission large-scale aerial LiDAR dataset specifically designed for 3D point cloud semantic segmentation. The dataset comprises approximately 738 million points, covering an area of 9.46 square kilometers, which results in a high point density of 100 points per square meter. Each point in the dataset is annotated with one of nine semantic classes. Additionally, we conducted performance tests of state-of-the-art algorithms to evaluate their effectiveness in semantic segmentation tasks. The YUTO Semantic dataset serves as a valuable resource for advancing research in 3D point cloud semantic segmentation and contributes to the development of more accurate and robust algorithms for real-world applications. The dataset is available at https://github.com/Yacovitch/YUTO_Semantic.",semantic segmentation,aerial imagery,laser scanning,evaluation,test,"Lee, H.","GEOSPATIAL WEEK 2023, VOL. 48-1",,,,,,,,,,,,,,,,,,,
Row_1113,"Zhu, Hongming","Tan, Rui","Han, Letong",DSSM: A Deep Neural Network with Spectrum Separable Module for Multi-Spectral Remote Sensing Image Segmentation,REMOTE SENSING,FEB 2022,5,"Over the past few years, deep learning algorithms have held immense promise for better multi-spectral (MS) optical remote sensing image (RSI) analysis. Most of the proposed models, based on convolutional neural network (CNN) and fully convolutional network (FCN), have been applied successfully on computer vision images (CVIs). However, there is still a lack of exploration of spectra correlation in MS RSIs. In this study, a deep neural network with a spectrum separable module (DSSM) is proposed for semantic segmentation, which enables the utilization of MS characteristics of RSIs. The experimental results obtained on Zurich and Potsdam datasets prove that the spectrum-separable module (SSM) extracts more informative spectral features, and the proposed approach improves the segmentation accuracy without increasing GPU consumption.",deep neural network,image segmentation,multi-spectral images,spectrum separable,,"Fan, Hongfei",,"Wang, Zeju",,"Du, Bowen","Liu, Sicong",,,,,,,"Liu, Qin",,,,,,,,
Row_1114,"He, Chu","Fang, Peizhang","Zhang, Zhi",An End-to-End Conditional Random Fields and Skip-Connected Generative Adversarial Segmentation Network for Remote Sensing Images,REMOTE SENSING,JUL 1 2019,16,"Semantic segmentation is an important process of scene recognition with deep learning frameworks achieving state of the art results, thus gaining much attention from the remote sensing community. In this paper, an end-to-end conditional random fields generative adversarial segmentation network is proposed. Three key factors of this algorithm are as follows. First, the network combines generative adversarial network and Bayesian framework to realize the estimation from the prior probability to the posterior probability. Second, the skip connected encoder-decoder network is combined with CRF layer to implement end-to-end network training. Finally, the adversarial loss and the cross-entropy loss guide the training of the segmentation network through back propagation. The experimental results show that our proposed method outperformed FCN in terms of mIoU for 0.0342 and 0.11 on two data sets, respectively.",generative adversarial network,conditional random fields,semantic segmentation,loss function,,"Xiong, Dehui",,"Liao, Mingsheng",,,,,,,,,,,,,,,,,,
Row_1115,"Sun, Qingwei","Chao, Jiangang","Lin, Wanhong",Learn to Few-Shot Segment Remote Sensing Images from Irrelevant Data,REMOTE SENSING,OCT 2023,4,"Few-shot semantic segmentation (FSS) is committed to segmenting new classes with only a few labels. Generally, FSS assumes that base classes and novel classes belong to the same domain, which limits FSS's application in a wide range of areas. In particular, since annotation is time-consuming, it is not cost-effective to process remote sensing images using FSS. To address this issue, we designed a feature transformation network (FTNet) for learning to few-shot segment remote sensing images from irrelevant data (FSS-RSI). The main idea is to train networks on irrelevant, already labeled data but inference on remote sensing images. In other words, the training and testing data neither belong to the same domain nor category. The FTNet contains two main modules: a feature transformation module (FTM) and a hierarchical transformer module (HTM). Among them, the FTM transforms features into a domain-agnostic high-level anchor, and the HTM hierarchically enhances matching between support and query features. Moreover, to promote the development of FSS-RSI, we established a new benchmark, which other researchers may use. Our experiments demonstrate that our model outperforms the cutting-edge few-shot semantic segmentation method by 25.39% and 21.31% in the one-shot and five-shot settings, respectively.",meta-learning,cross-domain segmentation,few-shot semantic segmentation,transformer,,"Xu, Zhenying",,"Chen, Wei",,"He, Ning",,,,,,,,,,,,,,,,
Row_1116,"Cao, Yinxia","Huang, Xin",,A coarse-to-fine weakly supervised learning method for green plastic cover segmentation using high-resolution remote sensing images,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,JUN 2022,47,"Green plastic cover (GPC) is a kind of green plastic fine mesh primarily used for covering construction sites and mitigating large amounts of dust during construction. Accurate GPC detection is vital for monitoring urban environment and understanding urban development. Convolutional neural network (CNN)-based segmentation methods are widely used for detecting object extents, while they rely on high-quality pixel-level labels with high acquisition cost. In this regard, weakly supervised learning can achieve pixel-level segmentation using only image-level labels, by first generating the class activation map (CAM) to obtain initial pixel-level labels and then applying the CNN-based segmentation methods to detect object extents. However, these initial labels are usually incomplete and noisy, caused by the local high response property of CAM. Moreover, the CNN-based segmentation methods often lead to blurry object boundaries due to the gradual down-sampling of feature maps, and meanwhile suffer from the class imbalance problem in real scenarios. Given these problems, we introduce weakly supervised learning into GPC detection to lower the label acquisition cost. Furthermore, to improve the completeness and correctness of initial labels and mitigate the blurry boundary problem, we propose a coarse-tofine weakly supervised segmentation method (called CFWS), consisting of three steps: 1) object-based label extraction; 2) noisy label correction; and 3) boundary-aware semantic segmentation. Moreover, to alleviate the class imbalance problem, we propose a classification-then-segmentation strategy and integrate it into the CFWS to detect GPC. We test the CFWS on two datasets from Google Earth and Gaofen-2 high-resolution images, respectively. The results show that the CFWS obtains more complete GPCs and effectively retains boundaries on both datasets compared to existing state-of-the-art methods. In real scenarios, the classification-thensegmentation strategy significantly reduces a large number of false alarms generated by direct segmentation. These findings confirm that the CFWS holds great potentials for large-scale GPC detection and urban environmental monitoring. The source code will be available at https://github.com/lauraset/Coarse-to-fine-weaklysupervised-GPC-segmentation.",Weakly supervised learning,Green plastic cover segmentation,Image-level label,High-resolution remote sensing,,,,,,,,,,,,,,,,,,,,,,
Row_1117,"Wang, Wei","Kang, Yuxi","Liu, Guanqun",SCU-Net: Semantic Segmentation Network for Learning Channel Information on Remote Sensing Images,COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE,APR 10 2022,6,"Extracting detailed information from remote sensing images is an important direction in semantic segmentation. Not only the amounts of parameters and calculations of the network model in the learning process but also the prediction effect after learning must be considered. This paper designs a new module, the upsampling convolution-deconvolution module (CDeConv). On the basis of CDeConv, a convolutional neural network (CNN) with a channel attention mechanism for semantic segmentation is proposed as a channel upsampling network (SCU-Net). SCU-Net has been verified by experiments. The mean intersection-over-union (MIOU) of the SCU-Net-102-A model reaches 55.84%, the pixel accuracy is 91.53%, and the frequency weighted intersection-over-union (FWIU) is 85.83%. Compared with some of the state-of-the-art methods, SCU-Net can learn more detailed information in the channel and has better generalization capabilities.",,,,,,"Wang, Xin",,,,,,,,,,,,,,,,,,,,
Row_1118,"Florea, Corneliu","Burghiu, Alexandru","Ivanovici, Mihai",LOGIT-BASED SUPERPIXEL SEMANTIC SEGMENTATION OF IMAGES FOR PRECISION AGRICULTURE,UNIVERSITY POLITEHNICA OF BUCHAREST SCIENTIFIC BULLETIN SERIES C-ELECTRICAL ENGINEERING AND COMPUTER SCIENCE,2024,0,"In this work we approach the problem of remote sensing image segmentation using a classical approach: the image is first segmented and, subsequently, each segment is labeled using a classifier. For segmentation, we rely on a superpixel framework and several methods are evaluated. For the classifier, again, several stateof-the-art algorithms are tested and performances are compared. The best performing method is obtained by a modified SEED superpixel algorithm with boosted trees for classification. The evaluation is carried out on the Agriculture -Vision database and the results are encouraging.",Superpixel,Logit,Boosted trees,Remote Sensing,Semantic segmentation,,,,Precision Agriculture,,,,,,,,,,,,,,,,,
Row_1119,"Hu, Yaosi","Chen, Zhenzhong","Lin, Weiyao",RGB-D SEMANTIC SEGMENTATION: A REVIEW,,2018,2,"Semantic segmentation aims at giving a class label for each pixel on the image according to its semantic meaning. This problem is one of the most challenging tasks in computer vision, and has received a lot of attention from the computer vision community. In particular, with the popularity of depth cameras, many researchers have used rich structural information of depth data to assist semantic segmentation. In this paper, we have conducted a concise review of RGB-D semantic segmentation methods, including traditional method and deep learning method. In addition, discussions on RGB-D semantic segmentation are also provided.",Semantic segmentation,RGB-D datasets,deep learning,,,,2018 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA & EXPO WORKSHOPS (ICMEW 2018),,,,,,,,,,,,,,,,,,,
Row_1120,"Sener, Abdullah","Ergen, Burhan",,LandslideSegNet: an effective deep learning network for landslide segmentation using remote sensing imagery,EARTH SCIENCE INFORMATICS,OCT 2024,2,"In recent years, remote sensing technologies have played a crucial role in the detection and management of natural disasters. In this context, deep learning models are of great importance for the early detection of natural disasters such as landslides. Landslide segmentation is a fundamental tool for the development of geographic information systems, natural disaster management and risk mitigation strategies. In this study, we propose a new semantic segmentation model called LandslideSegNet to improve early intervention capabilities for potential landslide scenarios. LandslideSegNet incorporates an encoder-decoder architecture that integrates local and contextual information, advanced encoder-decoder residual blocks and Efficient Hybrid Attentional Atrous Convolution. Thanks to this structure, the model is able to extract high-resolution feature maps from remote sensing imagery, accurately delineate the landslide areas and minimize the loss of contextual information. The developed LandslideSegNet model has shown significantly higher accuracy rates with fewer parameters compared to existing image segmentation models. The model was trained and tested using the Landslide4Sense dataset specially prepared for landslide detection. LandslideSegNet achieved an accuracy of 97.60% and 73.65% mean Intersection over Union of 73.65 on this dataset, demonstrating its efficiency. These results indicate the potential usability of the model in landslide detection and related disaster management applications.",Natural disasters,Landslide detection,Semantic segmentation,Lightweight convolutional neural networks,LandslideSegNet,,,,,,,,,,,,,,,,,,,,,
Row_1121,"Hao, Xuejie","Yin, Lizeyan","Li, Xiuhong",A Multi-Objective Semantic Segmentation Algorithm Based on Improved U-Net Networks,REMOTE SENSING,APR 2023,11,"The construction of transport facilities plays a pivotal role in enhancing people's living standards, stimulating economic growth, maintaining social stability and bolstering national security. During the construction of transport facilities, it is essential to identify the distinctive features of a construction area to anticipate the construction process and evaluate the potential risks associated with the project. This paper presents a multi-objective semantic segmentation algorithm based on an improved U-Net network, which can improve the recognition efficiency of various types of features in the construction zone of transportation facilities. The main contributions of this paper are as follows: A multi-class target sample dataset based on UAV remote sensing and construction areas is established. A new virtual data augmentation method based on semantic segmentation of transport facility construction areas is proposed. A semantic segmentation model for the construction regions based on data augmentation and transfer learning is developed and future research directions are given. The results of the study show that the validity of the virtual data augmentation approach has been verified; the semantic segmentation of the transport facility model can semantically segment a wide range of target features. The highest semantic segmentation accuracy of the feature type was 97.56%.",semantic segmentation,U-Net,data augmentation,virtual sample,construction of transport facilities,"Zhang, Le",,"Yang, Rongjin",,,,,,,,,,,,,,,,,,
Row_1122,"Wang, Yuanzhi","Zhao, Qingzhan","Wu, Yuzhen",SCA-Net: Multiscale Contextual Information Network for Building Extraction Based on High-Resolution Remote Sensing Images,REMOTE SENSING,SEP 2023,10,"Accurately extracting buildings is essential for urbanization rate statistics, urban planning, resource allocation, etc. The high-resolution remote sensing images contain rich building information, which provides an important data source for building extraction. However, the extreme abundance of building types with large differences in size, as well as the extreme complexity of the background environment, result in the accurate extraction of spatial details of multi-scale buildings, which remains a difficult problem worth studying. To this end, this study selects the representative Xinjiang Tumxuk urban area as the study area. A building extraction network (SCA-Net) with feature highlighting, multi-scale sensing, and multi-level feature fusion is proposed, which includes Selective kernel spatial Feature Extraction (SFE), Contextual Information Aggregation (CIA), and Attentional Feature Fusion (AFF) modules. First, Selective kernel spatial Feature Extraction modules are used for cascading composition, highlighting information representation of features, and improving the feature extraction capability. Adding a Contextual Information Aggregation module enables the acquisition of multi-scale contextual information. The Attentional Feature Fusion module bridges the semantic gap between high-level and low-level features to achieve effective fusion between cross-level features. The classical U-Net, Segnet, Deeplab v3+, and HRNet v2 semantic segmentation models are compared on the self-built Tmsk and WHU building datasets. The experimental results show that the algorithm proposed in this paper can effectively extract multi-scale buildings in complex backgrounds with IoUs of 85.98% and 89.90% on the two datasets, respectively. SCA-Net is a suitable method for building extraction from high-resolution remote sensing images with good usability and generalization.",high-resolution remote sensing imagery,building extraction,deep learning,semantic segmentation,,"Tian, Wenzhong",,"Zhang, Guoshun",,,,,,,,,,,,,,,,,,
Row_1123,"Majidizadeh, A.","Hasani, H.","Jafari, M.",SEMANTIC SEGMENTATION OF UAV IMAGES BASED ON U-NET IN URBAN AREA,,2023,1,"Semantic segmentation of aerial data has been one of the leading researches in the field of photogrammetry, remote sensing, and computer vision in recent years. Many applications, including airborne mapping of urban scenes, object positioning in aerial images, automatic extraction of buildings from remote sensing or high-resolution aerial images, etc., require accurate and efficient segmentation algorithms. According to the high potential of deep learning algorithms in the classification of complex scenes, this paper aims to train a deep learning model to evaluate the semantic segmentation accuracy of UAV-based images in urban areas. The proposed method implements a deep learning framework based on the U-Net encoder-decoder architecture, which extracts and classifies features through layers of convolution, max pooling, activation, and concatenation in an end-to-end process. The obtained results compare with two traditional machine learning models, Random Forest (RF) and Multi-Layer Perceptron (MLP). They rely on two steps that involve extracting features and classifying images. In this study, the experiments are performed on the UAVid2020 semantic segmentation dataset from the ISPRS database. Results show the effectiveness of the proposed deep learning framework, so that the U-Net architecture achieved the best results with 75.15% overall accuracy, compared to RF and MLP algorithms with 52.51% and 54.65% overall accuracy, respectively.",Semantic Segmentation,UAV,Deep Learning,Convolutional Neural Network,Encoder-Decoder Architecture,,"ISPRS GEOSPATIAL CONFERENCE 2022, JOINT 6TH SENSORS AND MODELS IN PHOTOGRAMMETRY AND REMOTE SENSING, SMPR/4TH GEOSPATIAL INFORMATION RESEARCH, GIRESEARCH CONFERENCES, VOL. 10-4",,UNet,,,,,,,,,,,,,,,,,
Row_1124,"Bassier, M.",,,ON THE SEMANTIC SEGMENTATION AND VALIDATION OF ELECTRICAL SUBSTATIONS,,2023,0,"Converting deep learning methods from benchmark testing to real applications is highly sought after both in academia and the industry. Key challenges that remain are the performance of the methods on new datasets, the preprocessing of the data and the integration of the results into application pipelines. Specifically for the implementation of semantic segmentation procedures, each of these challenges are still very much the subject of research. In this paper, we present a testcase to digitally twin and validate an electrical substation. Concretely, we discuss the data processing, training and the follow up integration of the results in the validation pipeline. In the experiments, we show that 86% initial F1-score can be achieved using the proper transfer learning on 14 classes and that this results in a 97% recall on the validation and 80% recall on the digitization of the substation. Overall, we show that the segmentation significantly contributes to these processes and that they are absolutely necessary for the automation of the digital twinning.",Semantic segmentation,Scan-vs-CAD,Modeling,Deep learning,point clouds,,"GEOSPATIAL WEEK 2023, VOL. 48-1",,Remote sensing,,,,,,,,,,,,,,,,,
Row_1125,"Zhou, Xixuan","Wang, Jinyu","Zheng, Fengjie",An Overview of Coastline Extraction from Remote Sensing Data,REMOTE SENSING,OCT 2023,9,"The coastal zone represents a unique interface between land and sea, and addressing the ecological crisis it faces is of global significance. One of the most fundamental and effective measures is to extract the coastline's location on a large scale, dynamically, and accurately. Remote sensing technology has been widely employed in coastline extraction due to its temporal, spatial, and sensor diversity advantages. Substantial progress has been made in coastline extraction with diversifying data types and information extraction methods. This paper focuses on discussing the research progress related to data sources and extraction methods for remote sensing-based coastline extraction. We summarize the suitability of data and some extraction algorithms for several specific coastline types, including rocky coastlines, sandy coastlines, muddy coastlines, biological coastlines, and artificial coastlines. We also discuss the significant challenges and prospects of coastline dataset construction, remotely sensed data selection, and the applicability of the extraction method. In particular, we propose the idea of extracting coastlines based on the coastline scene knowledge map (CSKG) semantic segmentation method. This review serves as a comprehensive reference for future development and research pertaining to coastal exploitation and management.",coastline extraction,remote sensing,deep learning,remote sensing knowledge map,,"Wang, Haoyu",,"Yang, Haitao",,,,,,,,,,,,,,,,,,
Row_1126,"Hong, T.","Ma, X.","Wang, X.",MAPMaN: Multi-Stage U-Shaped Adaptive Pattern Matching Network for Semantic Segmentation of Remote Sensing Images,COMPUTER GRAPHICS FORUM,OCT 2023,0,"Remote sensing images (RSIs) often possess obvious background noises, exhibit a multi-scale phenomenon, and are characterized by complex scenes with ground objects in diversely spatial distribution pattern, bringing challenges to the corresponding semantic segmentation. CNN-based methods can hardly address the diverse spatial distributions of ground objects, especially their compositional relationships, while Vision Transformers (ViTs) introduce background noises and have a quadratic time complexity due to dense global matrix multiplications. In this paper, we introduce Adaptive Pattern Matching (APM), a lightweight method for long-range adaptive weight aggregation. Our APM obtains a set of pixels belonging to the same spatial distribution pattern of each pixel, and calculates the adaptive weights according to their compositional relationships. In addition, we design a tiny U-shaped network using the APM as a module to address the large variance of scales of ground objects in RSIs. This network is embedded after each stage in a backbone network to establish a Multi-stage U-shaped Adaptive Pattern Matching Network (MAPMaN), for nested multi-scale modeling of ground objects towards semantic segmentation of RSIs. Experiments on three datasets demonstrate that our MAPMaN can outperform the state-of-the-art methods in common metrics. The code can be available at .",CCS Concepts,center dot Computing methodologies -> Neural networks,Image segmentation,,,"Che, R.",,"Hu, C.",,"Feng, T.","Zhang, W.",,,,,,,,,,,,,,,
Row_1127,"Lin, Xufeng","Cheng, Youwei","Chen, Gong",Semantic Segmentation of China's Coastal Wetlands Based on Sentinel-2 and Segformer,REMOTE SENSING,AUG 2023,9,"Concerning the ever-changing wetland environment, the efficient extraction of wetland information holds great significance for the research and management of wetland ecosystems. China's vast coastal wetlands possess rich and diverse geographical features. This study employs the SegFormer model and Sentinel-2 data to conduct a wetland classification study for coastal wetlands in Yancheng, Jiangsu, China. After preprocessing the Sentinel data, nine classification objects (construction land, Spartina alterniflora (S. alterniflora), Suaeda salsa (S. salsa), Phragmites australis (P. australis), farmland, river system, aquaculture and tidal falt) were identified based on the previous literature and remote sensing images. Moreover, mAcc, mIoU, aAcc, Precision, Recall and F-1 score were chosen as evaluation indicators. This study explores the potential and effectiveness of multiple methods, including data image processing, machine learning and deep learning. The results indicate that SegFormer is the best model for wetland classification, efficiently and accurately extracting small-scale features. With mIoU (0.81), mAcc (0.87), aAcc (0.94), mPrecision (0.901), mRecall (0.876) and mFscore (0.887) higher than other models. In the face of unbalanced wetland categories, combining CrossEntropyLoss and FocalLoss in the loss function can improve several indicators of difficult cases to be segmented, enhancing the classification accuracy and generalization ability of the model. Finally, the category scale pie chart of Yancheng Binhai wetlands was plotted. In conclusion, this study achieves an effective segmentation of Yancheng coastal wetlands based on the semantic segmentation method of deep learning, providing technical support and reference value for subsequent research on wetland values.",deep learning,semantic segmentation,SegFormer,coastal wetland,remote sensing images,"Chen, Wenjing",,"Chen, Rong",machine learning,"Gao, Demin","Zhang, Yinlong",,,,,,,"Wu, Yongbo",,,,,,,,
Row_1128,"Liu, Tao","Ye, Yun","Lei, Zhengling",A New Efficient Ship Detection Method Based on Remote Sensing Images by Device-Cloud Collaboration,JOURNAL OF MARINE SCIENCE AND ENGINEERING,AUG 2024,0,"Fast and accurate detection of ship objects in remote sensing images must overcome two critical problems: the complex content of remote sensing images and the large number of small objects reduce ship detection efficiency. In addition, most existing deep learning-based object detection models require vast amounts of computation for training and prediction, making them difficult to deploy on mobile devices. This paper focuses on an efficient and lightweight ship detection model. A new efficient ship detection model based on device-cloud collaboration is proposed, which achieves joint optimization by fusing the semantic segmentation module and the object detection module. We migrate model training, image storage, and semantic segmentation, which require a lot of computational power, to the cloud. For the front end, we design a mask-based detection module that ignores the computation of nonwater regions and reduces the generation and postprocessing time of candidate bounding boxes. In addition, the coordinate attention module and confluence algorithm are introduced to better adapt to the environment with dense small objects and substantial occlusion. Experimental results show that our device-cloud collaborative approach reduces the computational effort while improving the detection speed by 42.6% and also outperforms other methods in terms of detection accuracy and number of parameters.",ship detection,remote sensing image,device-cloud collaboration,semantic segmentation,,"Huo, Yuchi",,"Zhang, Xiaocai",,"Wang, Fang","Sha, Mei",,,,,,,"Wu, Huafeng",,,,,,,,
Row_1129,"Zheng, Daoyuan","Wang, Shaohua","Feng, Haixia",Weakly Supervised Building Extraction From High-Resolution Remote Sensing Images Based on Building-Aware Clustering and Activation Refinement Network,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"Weakly supervised building extraction methods, utilizing image-level labels, offer a cost-effective solution by significantly reducing the need for pixel-level annotation in high-resolution (HR) remote sensing (RS) images. These methods often focus on class activation map (CAM) optimization based on features extracted from individual images, missing out on the benefits of associating building features from multiple RS images (i.e., n images) to improve CAMs. This limitation leaves room for improvement in both CAM optimization and pseudo-mask generation. To address this, we propose the building-aware clustering and activation refinement network (BAC-AR-Net), a novel weakly supervised network to enhance weakly supervised building extraction performance. The building-aware clustering (BAC) module aggregates and clusters feature maps from multiple building samples to obtain common features of buildings. The common features are subsequently used to extract regions with similar building semantics, thereby enhancing the accuracy and completeness of building coverage in CAMs. Additionally, the activation refinement module is designed to generate pseudo-masks with clear boundaries and an effective separation of buildings and background. Experiments were conducted on the ISPRS Potsdam and Vaihingen datasets as well as a self-built building dataset to verify the effectiveness of our proposed method. The results show the proposed method outperforms both the weakly supervised semantic segmentation and weakly supervised building extraction methods that use image-level labels, achieving IoU accuracies of 0.8556, 0.8163, and 0.7797 on the respective datasets. This study introduces a novel weakly supervised learning framework to the RS application, with a particular focus on building extraction and semantic segmentation tasks.",Building extraction,high-resolution (HR) remote sensing (RS) images,image-level labels,weakly supervised semantic segmentation,Building extraction,"Wang, Shunli",,"Ai, Mingyao",high-resolution (HR) remote sensing (RS) images,"Zhao, Pengcheng","Li, Jiayuan",image-level labels,weakly supervised semantic segmentation,,,,,"Hu, Qingwu",,,,,,,,
Row_1130,"Wang, Jue","Zhong, Yanfei","Zhang, Liangpei",Semantic Change Detection Based on Supervised Contrastive Learning for High-Resolution Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Semantic change detection (SCD) for high-resolution remote sensing imagery involves simultaneously locating the changed regions and identifying the semantic change categories. Recently, a series of multitask Siamese networks have been proposed to model the SCD task by merging the binary change detection (BCD) results and the bitemporal land-cover classification (LCC) results. However, due to the large reflectance variability of the land cover in bitemporal high-resolution images, the land-cover feature clusters extracted by these methods are still considerably mixed, leading to the misidentification of change and their semantic changes types. In this article, to handle this problem, the SiamContrast method is proposed to learn temporally invariant discriminative land-cover features for SCD. As part of SiamContrast, a novel SCD contrastive loss (SCD-CL) is proposed to enhance the temporally invariant feature discrimination across bitemporal images. SCD-CL utilizes supervised contrastive learning and consists of two complementary components: mono-temporal contrastive loss (MCL) and cross-temporal contrastive loss (CCL). In particular, MCL contrasts the land-cover features within each temporal image, to enhance the mono-temporal feature discrimination. Meanwhile, CCL with a change-aware hard anchor sampling (CHAS) strategy contrasts the land-cover features across bitemporal images, to align the land cover features of the same category. To validate the effectiveness of SCD-CL, the SiamContrast method incorporates a difference feature pyramid (DFP) decoder, which leverages feature distance to model changes, allowing it to directly benefit from the discriminative features learned by SCD-CL. The comprehensive experimental results consistently confirm the effectiveness of the proposed SiamContrast in improving SCD performance, compared with the existing SCD methods. The code is available at https://github.com/wdczs/SiamContrast.",Change detection,fully convolutional network (FCN),multitask learning,multitask learning,semantic change detection (SCD),,,,semantic change detection (SCD),,,semantic segmentation,semantic segmentation,Siamese network,Siamese network,Siamese network,,,,,,,,,,
Row_1131,"Su, Yu-Chen","Liu, Tsung-Jung","Liu, Kuan-Hsien",Wavelet Frequency Channel Attention on Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,0,"In recent development of semantic segmentation, the deep convolutional encoder-decoder has become mainstream schemes for remote sensing images. In this paper, we proposed a U-Net like architecture for segmentation of remote sensing images using wavelet frequency channel attention (WFCA) blocks.",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_1132,"Tao, Wancheng","Xie, Zixuan","Zhang, Ying",Corn Residue Covered Area Mapping with a Deep Learning Method Using Chinese GF-1 B/D High Resolution Remote Sensing Images,REMOTE SENSING,AUG 2021,9,"Black soil is one of the most productive soils with high organic matter content. Crop residue covering is important for protecting black soil from alleviating soil erosion and increasing soil organic carbon. Mapping crop residue covered areas accurately using remote sensing images can monitor the protection of black soil in regional areas. Considering the inhomogeneity and randomness, resulting from human management difference, the high spatial resolution Chinese GF-1 B/D image and developed MSCU-net+C deep learning method are used to mapping corn residue covered area (CRCA) in this study. The developed MSCU-net+C is joined by a multiscale convolution group (MSCG), the global loss function, and Convolutional Block Attention Module (CBAM) based on U-net and the full connected conditional random field (FCCRF). The effectiveness of the proposed MSCU-net+C is validated by the ablation experiment and comparison experiment for mapping CRCA in Lishu County, Jilin Province, China. The accuracy assessment results show that the developed MSCU-net+C improve the CRCA classification accuracy from IOUAVG = 0.8604 and Kappa(AVG) = 0.8864 to IOUAVG = 0.9081 and Kappa(AVG) = 0.9258 compared with U-net. Our developed and other deep semantic segmentation networks (MU-net, GU-net, MSCU-net, SegNet, and Dlv3+) improve the classification accuracy of IOUAVG/Kappa(AVG) with 0.0091/0.0058, 0.0133/0.0091, 0.044/0.0345, 0.0104/0.0069, and 0.0107/0.0072 compared with U-net, respectively. The classification accuracies of IOUAVG/Kappa(AVG) of traditional machine learning methods, including support vector machine (SVM) and neural network (NN), are 0.576/0.5526 and 0.6417/0.6482, respectively. These results reveal that the developed MSCU-net+C can be used to map CRCA for monitoring black soil protection.",corn residue covered area,GF-1 B,D high resolution remote sensing images,deep semantic segmentation network,,"Li, Jiayu",,"Xuan, Fu",,"Huang, Jianxi","Li, Xuecao",,,,,,,"Su, Wei","Yin, Dongqin",,,,,,,
Row_1133,"Li, Lianfa",,,Deep Residual Autoencoder with Multiscaling for Semantic Segmentation of Land-Use Images,REMOTE SENSING,SEP 2019,27,"Semantic segmentation is a fundamental means of extracting information from remotely sensed images at the pixel level. Deep learning has enabled considerable improvements in efficiency and accuracy of semantic segmentation of general images. Typical models range from benchmarks such as fully convolutional networks, U-Net, Micro-Net, and dilated residual networks to the more recently developed DeepLab 3+. However, many of these models were originally developed for segmentation of general or medical images and videos, and are not directly relevant to remotely sensed images. The studies of deep learning for semantic segmentation of remotely sensed images are limited. This paper presents a novel flexible autoencoder-based architecture of deep learning that makes extensive use of residual learning and multiscaling for robust semantic segmentation of remotely sensed land-use images. In this architecture, a deep residual autoencoder is generalized to a fully convolutional network in which residual connections are implemented within and between all encoding and decoding layers. Compared with the concatenated shortcuts in U-Net, these residual connections reduce the number of trainable parameters and improve the learning efficiency by enabling extensive backpropagation of errors. In addition, resizing or atrous spatial pyramid pooling (ASPP) can be leveraged to capture multiscale information from the input images to enhance the robustness to scale variations. The residual learning and multiscaling strategies improve the trained model's generalizability, as demonstrated in the semantic segmentation of land-use types in two real-world datasets of remotely sensed images. Compared with U-Net, the proposed method improves the Jaccard index (JI) or the mean intersection over union (MIoU) by 4-11% in the training phase and by 3-9% in the validation and testing phases. With its flexible deep learning architecture, the proposed approach can be easily applied for and transferred to semantic segmentation of land-use variables and other surface variables of remotely sensed images.",residual learning,autoencoder,multiscale,atrous spatial pyramid pooling,semantic segmentation,,,,remotely sensed land-use images,,,,,,,,,,,,,,,,,
Row_1134,"Chen, Shiqi","Zhan, Ronghui","Zhang, Jun",Geospatial Object Detection in Remote Sensing Imagery Based on Multiscale Single-Shot Detector with Activated Semantics,REMOTE SENSING,JUN 2018,51,"Geospatial object detection from high spatial resolution (HSR) remote sensing imagery is a heated and challenging problem in the field of automatic image interpretation. Despite convolutional neural networks (CNNs) having facilitated the development in this domain, the computation efficiency under real-time application and the accurate positioning on relatively small objects in HSR images are two noticeable obstacles which have largely restricted the performance of detection methods. To tackle the above issues, we first introduce semantic segmentation-aware CNN features to activate the detection feature maps from the lowest level layer. In conjunction with this segmentation branch, another module which consists of several global activation blocks is proposed to enrich the semantic information of feature maps from higher level layers. Then, these two parts are integrated and deployed into the original single shot detection framework. Finally, we use the modified multi-scale feature maps with enriched semantics and multi-task training strategy to achieve end-to-end detection with high efficiency. Extensive experiments and comprehensive evaluations on a publicly available 10-class object detection dataset have demonstrated the superiority of the presented method.",high spatial resolution (HSR) remote sensing images,geospatial object detection,segmentation,semantic information,,,,,,,,,,,,,,,,,,,,,,
Row_1135,"Wang, Yongji","Tian, Zhihui","Qi, Qingwen",Double-Variance Measures: A Potential Approach to Parameter Optimization of Remote Sensing Image Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,5,"The unsupervised segmentation evaluation (USE) method has been commonly used for remote sensing segmentation parameter (SP) determinations to produce good segmentation results, due to its objectiveness and high efficiency. Existing studies have used different criteria to measure homogeneity and heterogeneity and have used certain combination strategies to form overall evaluations. However, different criteria have unique statistical characteristics. The differentiated statistical characteristics maintained in homogeneity and heterogeneity calculations may result in inherent instability in the USE results, leading to unsuitable SP selections. Moreover, few studies have focused on the simultaneous determination of a single optimal SP and multiple optimal SPs. In this article, double-variance (DV) measures were proposed for recognizing more suitable SPs. Then, two combination strategies, F-measure and local peak (LP), were applied to test the potential of using DV measures to determine a single SP and multiple SPs, respectively. The multiresolution segmentation algorithm and Gaofen-1 data were used to test the proposed method. The comparative results indicated that the DV is a more promising internal homogeneity and external heterogeneity metric for segmentation evaluation and optimal SP determination compared to conventional methods. The F-measure-based DV method could produce better overall goodness of segmentation for differently sized natural geo-objects, compared with the competing methods. The LP-based DV method could obtain multiple optimal scales that produced better segments for the identification of small, natural geo-objects to large, semantic geo-objects, compared to the competitive methods.",Image segmentation,Image analysis,Remote sensing,Satellites,Licenses,"Wang, Jun",,,Image resolution,,,Optimization,Double-variance (DV),geographic-object-based image analysis (GEOBIA),image segmentation,parameter optimization,unsupervised evaluation,,,,,,,,,
Row_1136,"Luo, Yiyun","Wang, Jinnian","Yang, Xiankun",Pixel Representation Augmented through Cross-Attention for High-Resolution Remote Sensing Imagery Segmentation,REMOTE SENSING,NOV 2022,3,"Natural imagery segmentation has been transferred to land cover classification in remote sensing imagery with excellent performance. However, two key issues have been overlooked in the transfer process: (1) some objects were easily overwhelmed by the complex backgrounds; (2) interclass information for indistinguishable classes was not fully utilized. The attention mechanism in the transformer is capable of modeling long-range dependencies on each sample for per-pixel context extraction. Notably, per-pixel context from the attention mechanism can aggregate category information. Therefore, we proposed a semantic segmentation method based on pixel representation augmentation. In our method, a simplified feature pyramid was designed to decode the hierarchical pixel features from the backbone, and then decode the category representations into learnable category object embedding queries by cross-attention in the transformer decoder. Finally, pixel representation is augmented by an additional cross-attention in the transformer encoder under the supervision of auxiliary segmentation heads. The results of extensive experiments on the aerial image dataset Potsdam and satellite image dataset Gaofen Image Dataset with 15 categories (GID-15) demonstrate that the cross-attention is effective, and our method achieved the mean intersection over union (mIoU) of 86.2% and 62.5% on the Potsdam test set and GID-15 validation set, respectively. Additionally, we achieved an inference speed of 76 frames per second (FPS) on the Potsdam test dataset, higher than all the state-of-the-art models we tested on the same device.",land cover classification,transformer,cross-attention,object embedding queries,,"Yu, Zhenyu",,"Tan, Zixuan",,,,,,,,,,,,,,,,,,
Row_1137,"Liu, Chenying","Albrecht, Conrad M.","Wang, Yi",AIO2: Online Correction of Object Labels for Deep Learning With Incomplete Annotation in Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"While the volume of remote sensing (RS) data is increasing daily, deep learning in Earth observation (EO) faces lack of accurate annotations for supervised optimization. Crowdsourcing projects such as openstreetmap (OSM) distribute the annotation load to their community. However, such annotation inevitably generates noise due to insufficient control of the label quality, lack of annotators, frequent changes of the Earth's surface as a result of natural disasters, and urban development, among many other factors. We present adaptively triggered online object-wise label correction (AIO2) to address annotation noise induced by incomplete label sets. AIO2 features an adaptive correction trigger (ACT) module that avoids label correction when the model training under- or overfits, and an online object-wise label correction (O2C) methodology that employs spatial information for automated label modification. AIO2 utilizes a mean teacher model to enhance training robustness with noisy labels to both stabilize the training accuracy curve for fitting in ACT and provide pseudo labels for correction in O2C. Moreover, O2C is implemented online without the need to store updated labels every training epoch. We validate our approach on two building footprint segmentation datasets with different spatial resolutions. Experimental results with varying degrees of building label noise demonstrate the robustness of AIO2. Source code will be available at https://github.com/zhu-xlab/AIO2.git.",Building detection,curriculum learning,deep learning,early learning,label correction,"Li, Qingyu",,"Zhu, Xiao Xiang",memorization effects,,,noisy labels,remote sensing (RS),semantic segmentation,,,,,,,,,,,,
Row_1138,"Lian, Yanchao","Feng, Tuo","Zhou, Jinliu",A DENSE POINTNET plus plus ARCHITECTURE FOR 3D POINT CLOUD SEMANTIC SEGMENTATION,,2019,11,"3D point cloud data has been widely used in remote sensing mapping because it is not affected by lighting, shadows and other factors. How to improve the performance of semantic segmentation of 3D point cloud data has attracted more and more attention. Previous works connected shallow features in encoders directly with deep features in decoders, which will lead to semantic gap. In this paper, we propose a Dense PointNet++ architecture, called DPNet, for semantic segmentation of 3D point cloud data. In order to weaken the semantic gap, multiple nested up-sampling layers and a series of cumulative, short and long skip link concatenation are introduced in the network to obtain more abundant point cloud features. Grid map and model fusion are used to further correct the results of network segmentation. The experimental results on US3D data set show that DPNet is superior to existing advanced architectures, especially for the categories with small samples. Moreover, DPNet with grid map and model fusion ranks the first place in 2019 IEEE GRSS Data fusion contest 3D point cloud classification challenge.",3D point cloud semantic segmentation,DPNet,short and long skip link concatenation,grid map,,,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),,,,,,,,,,,,,,,,,,,
Row_1139,"Zhu, Qiqi","Li, Zhen","Zhang, Yanan",Building Extraction from High Spatial Resolution Remote Sensing Images via Multiscale-Aware and Segmentation-Prior Conditional Random Fields,REMOTE SENSING,DEC 2020,44,"Building extraction is a binary classification task that separates the building area from the background in remote sensing images. The conditional random field (CRF) is directly modelled by the maximum posterior probability, which can make full use of the spatial neighbourhood information of both labelled and observed images. CRF is widely used in building footprint extraction. However, edge oversmoothing still exists when CRF is directly used to extract buildings from high spatial resolution (HSR) remote sensing images. Based on a computer vision multi-scale semantic segmentation network (D-LinkNet), a novel building extraction framework is proposed, named multiscale-aware and segmentation-prior conditional random fields (MSCRF). To solve the problem of losing building details in the downsampling process, D-LinkNet connecting the encoder and decoder is correspondingly used to generate the unary potential. By integrating multi-scale building features in the central module, D-LinkNet can integrate multiscale contextual information without loss of resolution. For the pairwise potential, the segmentation prior is fused to alleviate the influence of spectral diversity between the building and the background area. Moreover, the local class label cost term is introduced. The clear boundaries of the buildings are obtained by using the larger-scale context information. The experimental results demonstrate that the proposed MSCRF framework is superior to the state-of-the-art methods and performs well for building extraction of complex scenes.",HSR imagery,building extraction,conditional random fields,D-LinkNet,segmentation prior,"Guan, Qingfeng",,,,,,,,,,,,,,,,,,,,
Row_1140,"Sun, Zhongyu","Zhou, Wangping","Ding, Chen",Multi-Resolution Transformer Network for Building and Road Segmentation of Remote Sensing Image,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,MAR 2022,39,"Extracting buildings and roads from remote sensing images is very important in the area of land cover monitoring, which is of great help to urban planning. Currently, a deep learning method is used by the majority of building and road extraction algorithms. However, for existing semantic segmentation, it has a limitation on the receptive field of high-resolution remote sensing images, which means that it can not show the long-distance scene well during pixel classification, and the image features is compressed during down-sampling, meaning that the detailed information is lost. In order to address these issues, Hybrid Multi-resolution and Transformer semantic extraction Network (HMRT) is proposed in this paper, by which a global receptive field for each pixel can be provided, a small receptive field of convolutional neural networks (CNN) can be overcome, and the ability of scene understanding can be enhanced well. Firstly, we blend the features by branches of different resolutions to keep the high-resolution and multi-resolution during down-sampling and fully retain feature information. Secondly, we introduce the Transformer sequence feature extraction network and use encoding and decoding to realize that each pixel has the global receptive field. The recall, F1, OA and MIoU of HMPR obtain 85.32%, 84.88%, 85.99% and 74.19%, respectively, in the main experiment and reach 91.29%, 90.41%, 91.32% and 84.00%, respectively, in the generalization experiment, which prove that the method proposed is better than existing methods.",segmentation,high resolution,transformer,deep learning,,"Xia, Min",,,,,,,,,,,,,,,,,,,,
Row_1141,"Liu, Shengwei","Peng, Dailiang","Zhang, Bing",The Accuracy of Winter Wheat Identification at Different Growth Stages Using Remote Sensing,REMOTE SENSING,FEB 2022,19,"The aim of this study was to explore the differences in the accuracy of winter wheat identification using remote sensing data at different growth stages using the same methods. Part of northern Henan Province, China was taken as the study area, and the winter wheat growth cycle was divided into five periods (seeding-tillering, overwintering, reviving, jointing-heading, and flowering-maturing) based on monitoring data obtained from agrometeorological stations. With the help of the Google Earth Engine (GEE) platform, the separability between winter wheat and other land cover types was analyzed and compared using the Jeffries-Matusita (J-M) distance method. Spectral features, vegetation index, water index, building index, texture features, and terrain features were generated from Sentinel-2 remote sensing images at different growth periods, and then were used to establish a random forest classification and extraction model. A deep U-Net semantic segmentation model based on the red, green, blue, and near-infrared bands of Sentinel-2 imagery was also established. By combining models with field data, the identification of winter wheat was carried out and the difference between the accuracy of the identification in the five growth periods was analyzed. The experimental results show that, using the random forest classification method, the best separability between winter wheat and the other land cover types was achieved during the jointing-heading period: the overall identification accuracy for the winter wheat was then highest at 96.90% and the kappa coefficient was 0.96. Using the deep-learning classification method, it was also found that the semantic segmentation accuracy of winter wheat and the model performance were best during the jointing-heading period: a precision, recall, F1 score, accuracy, and IoU of 0.94, 0.93, 0.93, and 0.88, respectively, were achieved for this period. Based on municipal statistical data for winter wheat, the accuracy of the extraction of the winter wheat area using the two methods was 96.72% and 88.44%, respectively. Both methods show that the jointing-heading period is the best period for identifying winter wheat using remote sensing and that the identification made during this period is reliable. The results of this study provide a scientific basis for accurately obtaining the area planted with winter wheat and for further studies into winter wheat growth monitoring and yield estimation.",winter wheat identification,remote sensing,random forest,deep learning,semantic segmentation,"Chen, Zhengchao",,"Yu, Le",jointing-heading period,"Chen, Junjie","Pan, Yuhao",,,,,,,"Zheng, Shijun","Hu, Jinkang","Lou, Zihang","Chen, Yue",,,,,"Yang, Songlin"
Row_1142,"Ismael, Sarmad F.","Kayabol, Koray","Aptoula, Erchan",Unsupervised domain adaptation for the semantic segmentation of remote sensing images via a class-aware Fourier transform and a fine-grained discriminator,DIGITAL SIGNAL PROCESSING,AUG 2024,0,"The semantic segmentation of remote sensing images is vital for Earth observation purposes. However, its performance can decline significantly due to differences in dataset distributions between training (source) and deployment (target) settings. Unsupervised domain adaptation can be used to counter this problem by leveraging the knowledge acquired from the labelled source domain and by adapting it to the unlabelled target domain. Existing methods focus on either input -level or feature -level alignments, which can be sub -optimal for addressing large domain gaps. To this end, this paper introduces a new unsupervised domain adaptation method that employs concurrently two levels of alignment: first, at the input level, an adaptive Fourier -based image -toimage translation approach is utilised to generate target -styled source images with class -based low -amplitude changes. Then, at the feature level, an adaptive fine-grained domain discriminator is introduced that incorporates class information into two parallel discriminators, for source vs. target and target -styled source image vs. target settings. Experimental results indicate that the proposed method improves significantly cross -domain semantic segmentation performance with respect to the state-of-the-art.",Unsupervised domain adaptation,Semantic segmentation,Fourier-based image-to-image translation,Fine-grained domain discriminators,,,,,,,,,,,,,,,,,,,,,,
Row_1143,"Li, Junjie","Meng, Yizhuo","Dorjee, Donyu",Automatic Road Extraction From Remote Sensing Imagery Using Ensemble Learning and Postprocessing,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,18,"High-resolution satellite images contain valuable road semantic information, but the occlusion of vegetation and buildings and the sparse distribution and heterogeneous appearance of roads limit the accuracy of road extraction models. In this article, we propose a novel method for extracting roads using an ensemble learning model with a postprocessing stage. The network weights and biases of our proposed deep learning model are transmitted through the random combination of layers of different submodels during forward and backward propagation. In the gradient descent process, a superior loss function is designed to solve the problem of class imbalance caused by road sparseness, and more attention is given to hard classification samples to extract narrow and covered roads. In addition, we solve road disconnection issues in the results obtained with the neural network by extracting and analyzing the geometric structures and feature points of the roads. Experiments on two challenging datasets of remote sensing imagery show that the proposed method performs better than other models and can extract road information from complex scenes.",Convolutional neural network (CNN),ensemble learning,remote sensing,road extraction,semantic segmentation,"Wei, Xiaobing",,"Zhang, Zhiyuan",,"Zhang, Wen",,,,,,,,,,,,,,,,
Row_1144,"Xiong, Dehui","He, Chu","Liu, Xinlong",An End-To-End Bayesian Segmentation Network Based on a Generative Adversarial Network for Remote Sensing Images,REMOTE SENSING,JAN 2020,17,"Due to the development of deep convolutional neural networks (CNNs), great progress has been made in semantic segmentation recently. In this paper, we present an end-to-end Bayesian segmentation network based on generative adversarial networks (GANs) for remote sensing images. First, fully convolutional networks (FCNs) and GANs are utilized to realize the derivation of the prior probability and the likelihood to the posterior probability in Bayesian theory. Second, the cross-entropy loss in the FCN serves as an a priori to guide the training of the GAN, so as to avoid the problem of mode collapse during the training process. Third, the generator of the GAN is used as a teachable spatial filter to construct the spatial relationship between each label. Some experiments were performed on two remote sensing datasets, and the results demonstrate that the training of the proposed method is more stable than other GAN based models. The average accuracy and mean intersection (MIoU) of the two datasets were 0.0465 and 0.0821, and 0.0772 and 0.1708 higher than FCN, respectively.",image semantic segmentation,Bayesian,generative adversarial networks (GAN),fully convolutional networks (FCN),synthetic aperture radar (SAR),"Liao, Mingsheng",,,,,,,,,,,,,,,,,,,,
Row_1145,"Liu, Han","Du, Hang","Zeng, Dan",Cloud Detection Using Super Pixel Classification and Semantic Segmentation,JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY,MAY 2019,11,"Cloud detection plays a very significant role in remote sensing image processing. This paper introduces a cloud detection method based on super pixel level classification and semantic segmentation. Firstly, remote sensing images are segmented into super pixels. Segmented super pixels compose a super pixel level remote sensing image database. Though cloud detection is essentially a binary classification task, our database is labeled into four categories to improve the generalization ability: thick cloud, cirrus cloud, building, and other culture. Secondly, the super pixel level database is used to train our cloud detection models based on convolution neural network (CNN) and deep forest. Hierarchical fusion CNN is proposed considering super pixel level images contain less semantic information than normal images. Taking full advantage of low-level features like color and texture information, it is more applicable for super pixel level classification. Besides, a distance metric is proposed to refine ambiguous super pixels. Thirdly, an end-to-end cloud detection model based on semantic segmentation is introduced. This model has no restrictions on the input size, and takes less time. Experimental results show that compared with other cloud detection methods, our proposed method achieves better performance.",cloud detection,convolution neural network,deep forest,semantic segmentation,,"Tian, Qi",,,,,,,,,,,,,,,,,,,,
Row_1146,"Yan, Qiang","Cheng, Guojian",,Research on Deep Learning-based Semantic Segmentation Algorithm for UAV Images,,2023,0,"Most of the current image semantic segmentation algorithms on UAV vision segment remote sensing images, which cannot represent ground detail information, resulting in obstacles to real-time autonomous environment perception of UAVs in low altitude flight missions. To address this problem, this paper proposes a real-time image semantic segmentation method for low altitude UAVs. The network designs a novel hypernetwork structure that incorporates a context header weight generation module at the last layer of the encoder, the weights of each block in the decoder are generated before the end of encoding in the encoder to reduce the number of parameters and computation of the model to achieve real-time segmentation. In the decoder, a dynamic patch-wise convolutional algorithm is designed using the locally connection layer mechanism to take full account of the contextual semantic information when targeting large segmented objects that are in more than one piece, so that the decoder's weights change with the spatial location of the input feature map, and at the same time, the dynamic weights are used to target the segmentation of different objects, to maximise the network's adaptive nature. In order to verify the effectiveness of the method, this experiment uses the transfer learning technique to carry out pre-training on Cityscape data set, and uses the UAVid data set to validate the method of this paper, and the experimental results show that the mean intersection over union of this method is 66.3% for the images of the categories of buildings, roads, and static cars, and the prediction speed reaches 37.9 FPS, which significantly improves segmentation accuracy under the condition of guaranteeing the real-time performance.",Low Altitude,UAV,Semantic Segmentation,,,,"PROCEEDINGS OF 2023 7TH INTERNATIONAL CONFERENCE ON ELECTRONIC INFORMATION TECHNOLOGY AND COMPUTER ENGINEERING, EITCE 2023",,,,,,,,,,,,,,,,,,,
Row_1147,"Ulku, Irem","Akagunduz, Erdem","Ghamisi, Pedram",Deep Semantic Segmentation of Trees Using Multispectral Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,17,"Forests can be efficiently monitored by automatic semantic segmentation of trees using satellite and/or aerial images. Still, several challenges can make the problem difficult, including the varying spectral signature of different trees, lack of sufficient labelled data, and geometrical occlusions. In this article, we address the tree segmentation problem using multispectral imagery. While we carry out large-scale experiments on several deep learning architectures using various spectral input combinations, we also attempt to explore whether hand-crafted spectral vegetation indices can improve the performance of deep learning models in the segmentation of trees. Our experiments include benchmarking a variety of multispectral remote sensing image sets, deep semantic segmentation architectures, and various spectral bands as inputs, including a number of hand-crafted spectral vegetation indices. From our large-scale experiments, we draw several useful conclusions. One particularly important conclusion is that, with no additional computation burden, combining different categories of multispectral vegetation indices, such as NVDI, atmospherically resistant vegetation index, and soil-adjusted vegetation index, within a single three-channel input, and using the state-of-the-art semantic segmentation architectures, tree segmentation accuracy can be improved under certain conditions, compared to using high-resolution visible and/or near-infrared input.",Image segmentation,Vegetation,Vegetation mapping,Semantics,Random forests,,,,Satellites,,,Deep learning,Satellite imagery,semantic segmentation,tree segmentation,vegetation indices (VIs),,,,,,,,,,
Row_1148,"Li, Qingyun","Chen, Yushi","He, Xin","Co-Training Transformer for Remote Sensing Image Classification, Segmentation, and Detection",IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,8,"Several fundamental remote sensing (RS) image processing tasks, including classification, segmentation, and detection, have been set to serve for manifold applications. In the RS community, the individual tasks have been studied separately for many years. However, the specialized models were only capable of a single task. They lacked the adaptability for generalizing to the other tasks. Moreover, transformer exhibits a powerful generalization capacity because it has the property of dynamic feature weighting. Hence, there is a large potential of a uniform transformer to learn multiple tasks simultaneously, i.e., multitask learning (MTL). An MTL transformer can combine knowledge from different tasks by sharing a uniform network. In this study, a general-purpose transformer, which simultaneously processes the three tasks, is investigated for RS MTL. To build a transformer capable of the three tasks, an MTL framework named RSCoTr is proposed. The framework uses a shared encoder to extract multiscale features efficiently and three task-specific decoders to obtain different results. Moreover, a flexible training procedure named co-training is proposed. The MTL model is trained with multiple general datasets annotated for individual tasks. The co-training is as easy as training a specialized model for a single task. It can be developed into different learning strategies to meet various requirements. The proposed RSCoTr is trained jointly with various strategies on three challenging datasets of the three tasks. And the results demonstrate that the proposed MTL method achieves state-of-the-art performance in comparison with other competitive approaches. Code will be available at https://github.com/Li-Qingyun/RSCoTr.",Task analysis,Transformers,Feature extraction,Training,Scene classification,"Huang, Lingbo",,,Object detection,,,Semantic segmentation,Co-training,multitask learning (MTL),remote sensing (RS),transformer,,,,,,,,,,
Row_1149,"Giftlin, C. Jenifer Grace","Jenicka, S.","Juliet, S. Ebenezer",Building Footprint Semantic Segmentation using Bi-Channel Bi-Spatial (B2-CS) LinkNet,JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING,OCT 2022,2,"High-resolution satellite imagery provides the information about the planet's surface whose automated labelling helps in various practical applications. Segmentation of building footprint from remote sensing images is a challenging task. This paper proposes a novel architecture bi-channel bi-spatial (B-2-CS) LinkNet for implementing the semantic segmentation of building footprint. The proposed network uses two parallel semantic segmentation architectures (LinkNet) of RGB and HSV channels with dual spatial and channel attention schemes. By enhancing the features specifically in various channels and positions, result of the segmentation scheme improves. The proposed model outperforms other models with different characteristics like scales, spatial resolution, and object shapes while the analysis was done in two state-of-art datasets: INRIA aerial image labelling dataset and Massachusetts buildings dataset. Accuracy of the proposed work is 97.92% for both datasets which is better than many of the existing methods.",Bi-channel Bi-spatial LinkNet,Semantic segmentation,Building footprint segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_1150,Shen Yan-shan,Wang A-chuan,,Remote sensing image feature segmentation method based on deep learning,CHINESE JOURNAL OF LIQUID CRYSTALS AND DISPLAYS,APR 2021,2,"In view of the difficulty of traditional methods for simultaneous and effective multi-target segmentation, the existing ground object classification methods based on fully convolutional neural networks have low classification accuracy in complex scenes, this paper proposes an improved encoder-decoder based on the U-shaped network structure DL-Unet, which realizes the effective segmentation of remote sensing images. This network improves the traditional convolution method and introduces the expanded convolution, which increases the receptive field without increasing the network parameters. Aiming at the problem of imbalance in the clssification of featares in remote sensing images, weighted cross-entropy is used as the loss function of the model, which effectively overcomes the selection preference of the model function of the model effectively. The relative majority voting strategy is adopted for the prediction results to further improve the pixel accuracy (PA) of each feature category. The experimental results show that the mean pixel accuracy (MPA) and mean intersection over union (MIoU) of this model are improved by 5.94% and 9.45% respectively compared with the classic U-net, which verifies that the method in this paper is an effective remote sensing image classification method.",deep learning,dilation convolution,weighted cross entropy,ensemble learning,semantic segmentation,,,,image processing,,,,,,,,,,,,,,,,,
Row_1151,"Yao, Junyuan","Jin, Shuanggen",,Multi-Category Segmentation of Sentinel-2 Images Based on the Swin UNet Method,REMOTE SENSING,JUL 2022,17,"Medium-resolution remote sensing satellites have provided a large amount of long time series and full coverage data for Earth surface monitoring. However, the different objects may have similar spectral values and the same objects may have different spectral values, which makes it difficult to improve the classification accuracy. Semantic segmentation of remote sensing images is greatly facilitated via deep learning methods. For medium-resolution remote sensing images, the convolutional neural network-based model does not achieve good results due to its limited field of perception. The fast-emerging vision transformer method with self-attentively capturing global features well provides a new solution for medium-resolution remote sensing image segmentation. In this paper, a new multi-class segmentation method is proposed for medium-resolution remote sensing images based on the improved Swin UNet model as a pure transformer model and a new pre-processing, and the image enhancement method and spectral selection module are designed to achieve better accuracy. Finally, 10-categories segmentation is conducted with 10-m resolution Sentinel-2 MSI (Multi-Spectral Imager) images, which is compared with other traditional convolutional neural network-based models (DeepLabV3+ and U-Net with different backbone networks, including VGG, ResNet50, MobileNet, and Xception) with the same sample data, and results show higher Mean Intersection Over Union (MIOU) (72.06%) and better accuracy (89.77%) performance. The vision transformer method has great potential for medium-resolution remote sensing image segmentation tasks.",Swin UNet,Swin Transformer,remote sensing,semantic segmentation,Sentinel-2,,,,,,,,,,,,,,,,,,,,,
Row_1152,"Zhu, Xun","Xu, Mengqiu","Wu, Ming",Annotating Only at Definite Pixels: A Novel Weakly Supervised Semantic Segmentation Method for Sea Fog Recognition,REMOTE SENSING,2022,5,"Sea fog recognition is a challenging and significant semantic segmentation task in remote sensing images. The fully supervised learning method relies on the pixel-level label, which is labor-intensive and time-consuming. Moreover, it is impossible to accurately annotate all pixels of the sea fog region due to the limited ability of the human eye to distinguish between low clouds and sea fog. In this paper, we propose a novel approach of point-based annotation for weakly supervised semantic segmentation with the auxiliary information of International Comprehensive Ocean-Atmosphere Data Set (ICOADS) visibility data. It only needs several definite points for both foreground and background, which significantly reduces the annotation cost of manpower. We conduct extensive experiments on Himawari-8 satellite remote sensing images to demonstrate the effectiveness of our annotation method. The mean intersection over union (mIoU) and overall recognition accuracy of our annotation method reach 82.72% and 95.18%, respectively. Compared with the fully supervised learning method, the accuracy and the recognition rate of sea fog area are improved with a maximum increase of 7.69% and 9.69%, respectively.",sea fog recognition,weakly supervised learning,point annotation,semantic segmentation,remote sensing image,"Zhang, Chuang",,"Zhang, Bin",,,,,,,,,,,,,,,,,,
Row_1153,"Huang, Liang","Wu, Xuequn","Peng, Qiuzhi",Depth Semantic Segmentation of Tobacco Planting Areas from Unmanned Aerial Vehicle Remote Sensing Images in Plateau Mountains,JOURNAL OF SPECTROSCOPY,MAR 1 2021,22,"The tobacco in plateau mountains has the characteristics of fragmented planting, uneven growth, and mixed/interplanting of crops. It is difficult to extract effective features using an object-oriented image analysis method to accurately extract tobacco planting areas. To this end, the advantage of deep learning features self-learning is relied on in this paper. An accurate extraction method of tobacco planting areas based on a deep semantic segmentation model from the unmanned aerial vehicle (UAV) remote sensing images in plateau mountains is proposed in this paper. Firstly, the tobacco semantic segmentation dataset is established using Labelme. Four deep semantic segmentation models of DeeplabV3+, PSPNet, SegNet, and U-Net are used to train the sample data in the dataset. Among them, in order to reduce the model training time, the MobileNet series of lightweight networks are used to replace the original backbone networks of the four network models. Finally, the predictive images are semantically segmented by trained networks, and the mean Intersection over Union (mIoU) is used to evaluate the accuracy. The experimental results show that, using DeeplabV3+, PSPNet, SegNet, and U-Net to perform semantic segmentation on 71 scene prediction images, the mIoU obtained is 0.9436, 0.9118, 0.9392, and 0.9473, respectively, and the accuracy of semantic segmentation is high. The feasibility of the deep semantic segmentation method for extracting tobacco planting surface from UAV remote sensing images has been verified, and the research method can provide a reference for subsequent automatic extraction of tobacco planting areas.",,,,,,"Yu, Xueqin",,,,,,,,,,,,,,,,,,,,
Row_1154,"Bai, Haiwei","Cheng, Jian","Su, Yanzhou",Calibrated Focal Loss for Semantic Labeling of High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,9,"Currently, the most advanced high-resolution remote sensing image (HRRSI) semantic labeling methods rely on deep neural networks. However, HRRSIs naturally have a serious class imbalance problem, which is not yet well solved by the current method. The cross-entropy loss is often used to guide the training of semantic labeling neural networks for HRRSIs, but it is essentially dominated by the major classes in the image, resulting in poor predictions for the minority class. Based on the prediction results, focal loss (FL) effectively suppresses the negative impact of class imbalance in dense object detection by redistributing the loss of each sample. In this article, we thoroughly analyze the inadequacy of FL for semantic labeling, which inevitably introduces confusing-classified examples that are more difficult to classify while suppressing the loss of well-classified examples. Therefore, following the core idea of FL, we redefine the hard examples in semantic labeling of HRRSIs and propose the prediction confusion map to measure the classification difficulty. Based on this, we further propose the calibrated focal loss (CFL) for the semantic labeling of HRRSIs. Finally, we conduct complete experiments on the International Society for Photogrammetry and Remote Sensing Vaihingen and Potsdam datasets to analyze the semantic labeling performance, model uncertainty, and confidence calibration of different loss functions. Experimental results show that CFL can achieve outstanding results compared with other commonly used loss functions without increasing model parameters and training iterations, demonstrating the effectiveness of our method. In the end, combined with our previously proposed HCANet, we further verify the effectiveness of CFL on state-of-the-art network structures.",Semantics,Labeling,Training,Deep learning,Neural networks,"Liu, Siyu",,"Liu, Xin",Phase change materials,,,Calibration,Calibrated focal loss (CFL),class imbalance,focal loss (FL),high-resolution remote sensing images (HRRSIs),prediction confusion map (PCM),,,,,semantic labeling,,,,
Row_1155,"Cheng, Xi","Zhu, Qian","Song, Yujian",Precise City-Scale Urban Water Body Semantic Segmentation and Open-Source Sampleset Construction Based on Very High-Resolution Remote Sensing: A Case Study in Chengdu,REMOTE SENSING,OCT 2024,0,"Addressing the challenges related to urban water bodies is essential for advancing urban planning and development. Therefore, obtaining precise and timely information regarding urban water bodies is of paramount importance. To address issues such as incomplete extraction boundaries, mistaken feature identification, and omission of small water bodies, this study utilized very high-resolution (VHR) satellite images of the Chengdu urban area and its surroundings to create the Chengdu Urban Water Bodies Semantic Segmentation Dataset (CDUWD). Based on the shape characteristics of water bodies, these images were processed through annotation, cropping, and other operations. We introduced Ad-SegFormer, an enhanced model based on SegFormer, which integrates a densely connected atrous spatial pyramid pooling module (DenseASPP) and progressive feature pyramid network (AFPN) to better handle the multi-scale characteristics of urban water bodies. The experimental results demonstrate the effectiveness of combining the CDUWD dataset with the Ad-SegFormer model for large-scale urban water body extraction, achieving accuracy rates exceeding 96%. This study demonstrates the effectiveness of Ad-SegFormer in improving water body extraction and provides a valuable reference for extracting large-scale urban water body information using VHR images.",urban water body,very high-resolution remote sensing,CDUWD,Ad-SegFormer,semantic segmentation,"Yang, Jieyu",,"Wang, Tingting",Chengdu,"Zhao, Bin","Shen, Zhanfeng",,,,,,,,,,,,,,,
Row_1156,"Fan, Niming","Li, Deren","Pan, Jun",ANNet: Asymmetric Nested Network for Real-Time Cloud Detection in Remote Sensing,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"Cloud detection is one of the crucial tasks in the field of remote sensing, which is regarded as binary classification at the pixel level. Although a large number of recent deep learning-based methods have made great progress, most of their appealing performances come at the expense of a large amount of computation, which reduces the real-time performance accordingly. In order to bridge the gap between segmentation performance and inference speed, we propose a novel asymmetric nested network (ANNet) architecture termed ANNet, which is designed for real-time cloud detection with excellent performance. In the encoder branch of ANNet, we introduce an effective tiny U-shape block (TUB) to enrich detailed spatial contexts in each stage, which also allows ANNet to embed spatial recovery ability earlier, and a lightweight, simple feature fusing module (SFFM) is designed to refine the semantic features map at a lower level of TUB for better performance. Following the encoder, which is diverse from the most symmetric U-shape approaches, an asymmetric and lightweight decoder (ALD) with only convolution and bilinear up-sample operations is employed for spatial recovery. We also, moreover, demonstrated that using a constant channel size instead of a larger channel volume as the network goes deeper is an efficient and effective design for cloud detection tasks. Substantial experiments are performed on GF1_WHU and 95-Cloud datasets, which show that ANNet has achieved excellent performance with low computation cost compared to most of the existing state-of-the-art methods. On GF1_WHU dataset, ANNet-l achieves 93.79% on mIoU at 125 FPS, while ANNet-s with only 29.5 K parameters yields 90.74% mIoU at 251 FPS on the Nvidia RTX 2080Ti.",Feature extraction,Transformers,Remote sensing,Semantics,Real-time systems,"Huang, Shangren",,"Wang, Xinsheng",Decoding,,,Semantic segmentation,Convolution,Computer architecture,Computational modeling,Cloud detection,nested encoder (NE),,,,,real-time semantic segmentation,remote sensing,,,
Row_1157,"Ferreira, Joao P. K.","Pinto, Joao P. L.","Moura, Julia S.",A Novel Unsupervised Approach for Training Convolutional DSSNs on Remote Sensing,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,0,"This letter proposes an unsupervised approach for training deep semantic segmentation networks (DSSNs) with convolutional (CNN) layers: Weighted Dual SSN (WDSSN). Feature maps from the encoder-decoder block are grouped into subclusters before passing through the argmax layer. Pseudo labels for guiding the learning process are achieved with a superpixel refinement step. The network weight updates are computed through a batch-weighted cross-entropy (CE) loss proposed to tackle the problem of uneven class distributions usually present in remote sensing tasks. Experimental results show that our approach can learn high-level features from satellite imagery, and performs better than previously published unsupervised convolutional DSSN methods, obtaining accuracies of 63.3% in the Potsdam dataset, 76.3% in the Potsdam-3 dataset, and 39.0% in the Coco dataset. github.com/jpklock2/WDSSN.",Feature extraction,Pipelines,Remote sensing,Semantic segmentation,Training,"Castro, Cristiano L.",,,Semantics,,,Convolutional neural networks,Deep semantic segmentation network (DSSN) with convolutional layers (CNN),remote sensing,semantic segmentation,unsupervised learning,,,,,,,,,,
Row_1158,"Yan, Liang","Fan, Bin","Xiang, Shiming",CMT: Cross Mean Teacher Unsupervised Domain Adaptation for VHR Image Semantic Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,16,"Semantic segmentation of remote sensing images has achieved superior results with the supervised deep learning models. However, their performance to unseen data domains could be very bad due to the domain shift between different domains. Recently, a series of unsupervised domain adaptation (UDA) methods has been developed to solve the domain shift problem in semantic segmentation. Most of them use adversarial learning to achieve global cross-domain alignment and use a self-training (ST) strategy to generate pseudo-labels for classwise alignment. However, these methods ignore the pixels that are not assigned pseudo-labels. Those pixels are mostly at the boundaries, which are vital to the final segmentation results. To solve this problem, this letter proposes a cross mean teacher (CMT) UDA method. The whole framework consists of two parts. On the one hand, the global cross-domain distribution alignment is performed, and then, reliable pseudo-labels are assigned to the target data. On the other hand, a cross teacher-student network (CTSN) is developed to effectively use those pixels with and without pseudo-labels. This network contains two student networks (S and S) and two teacher networks (T and T) for cross-consistency constraints that supervises S (or S) by the prediction results of T (or T). The cross supervision by CTSN is helpful to prevent performance bottlenecks caused by the high coupling of teacher-student network in existing methods. Extensive experiments on three different remote sensing adaptation scenes verify the effectiveness and superiority of the proposed method.",Image segmentation,Semantics,Remote sensing,Training,Adaptation models,"Pan, Chunhong",,,Automation,,,Task analysis,Cross mean teacher (CMT),self-training (ST),semantic segmentation,unsupervised domain adaptation (UDA),very-high-resolution (VHR) image,,,,,,,,,
Row_1159,"Nivaggioli, Adrien","Randrianarivo, Hicham",,Weakly Supervised Semantic Segmentation of Satellite Images,,2019,14,"When one wants to train a neural network to perform semantic segmentation, creating pixel-level annotations for each of the images in the database is a tedious task. If he works with aerial or satellite images, which are usually very large, it is even worse. With that in mind, we investigate how to use image-level annotations in order to perform semantic segmentation. Image-level annotations are much less expensive to acquire than pixel-level annotations, but we lose a lot of information for the training of the model. From the annotations of the images, the model must find by itself how to classify the different regions of the image. In this work, we use the method proposed by Anh and Kwak [1] to produce pixel-level annotation from image level annotation.We compare the overall quality of our generated dataset with the original dataset.In addition, we propose an adaptation of the AffinityNet that allows us to directly perform a semantic segmentation.Our results show that the generated labels lead to the same performances for the training of several segmentation networks. Also, the quality of semantic segmentation performed directly by the AffinityNet and the Random Walk is close to the one of the best fully-supervised approaches.",Computer vision,Weak learning,Semantic segmentation,Land cover classification,,,2019 JOINT URBAN REMOTE SENSING EVENT (JURSE),,,,,,,,,,,,,,,,,,,
Row_1160,"Yang, Dongjie","Gao, Xianjun","Yang, Yuanwei",CSA-Net: Complex Scenarios Adaptive Network for Building Extraction for Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,2,"Building extraction is significant for the intelligent interpretation of high-resolution remote sensing images (HRSIs). However, in some complex scenarios where the features of the building and its adjacent ground objects are similar, the current segmentation model cannot distinguish them effectively. Therefore, we propose a complex scenarios adaptive network (CSA-Net) for building extraction. CSA-Net is comprised of the hierarchical-context feature extraction (HFE) module, the global-local feature interaction (GFI) module, and the multiscale-adaptive feature fusion (MFF) structure. The HFE obtains high-level semantic information at different levels and fuses it with low-level detailed information by skipping connections to enhance the reasoning and perception ability of building structure in complex scenes. Then, the GFI acquires global-local features of buildings and their surrounding environment via dense multiscale dilated convolution. The information can be shared through efficient interaction among features, and irrelevant backgrounds can be suppressed. Then, in the up-sampling process, the MFF alleviates the feature loss and enhances the robustness of the network by using feature fusion after layer-by-layer adaptive weight allocation. Experiments show that CSA-Net outperforms other comparable methods, with intersection over union values of 79.99%, 89.75%, and 73.59%, respectively, on the Google Arlinton, WHU, and Massachusetts building datasets. The visual comparison results demonstrate that our method can enhance the accuracy of building extraction in complex scenes. Meanwhile, the efficiency results indicate our approach strikes a balance between calculation parameters and time and achieves high levels of efficiency.",Feature extraction,Buildings,Data mining,Convolution,Semantic segmentation,"Jiang, Minghan",,"Guo, Kangliang",Remote sensing,"Liu, Bo","Li, Shaohua",Accuracy,Attention,building extraction,deep learning,global-local features,remote sensing images,"Yu, Shengyan",,,,,,,,
Row_1161,"Gu, Guowei","Wang, Zhongchen","Weng, Liguo",Attention Guide Axial Sharing Mixed Attention (AGASMA) Network for Cloud Segmentation and Cloud Shadow Segmentation,REMOTE SENSING,JUL 2024,0,"Segmenting clouds and their shadows is a critical challenge in remote sensing image processing. The shape, texture, lighting conditions, and background of clouds and their shadows impact the effectiveness of cloud detection. Currently, architectures that maintain high resolution throughout the entire information-extraction process are rapidly emerging. This parallel architecture, combining high and low resolutions, produces detailed high-resolution representations, enhancing segmentation prediction accuracy. This paper continues the parallel architecture of high and low resolution. When handling high- and low-resolution images, this paper employs a hybrid approach combining the Transformer and CNN models. This method facilitates interaction between the two models, enabling the extraction of both semantic and spatial details from the images. To address the challenge of inadequate fusion and significant information loss between high- and low-resolution images, this paper introduces a method based on ASMA (Axial Sharing Mixed Attention). This approach establishes pixel-level dependencies between high-resolution and low-resolution images, aiming to enhance the efficiency of image fusion. In addition, to enhance the effective focus on critical information in remote sensing images, the AGM (Attention Guide Module) is introduced, to integrate attention elements from original features into ASMA, to alleviate the problem of insufficient channel modeling of the self-attention mechanism. Our experimental results on the Cloud and Cloud Shadow dataset, the SPARCS dataset, and the CSWV dataset demonstrate the effectiveness of our method, surpassing the state-of-the-art techniques for cloud and cloud shadow segmentation.",high resolution,remote sensing image,attention,semantic segmentation,,"Lin, Haifeng",,"Zhao, Zikai",,"Zhao, Liling",,,,,,,,,,,,,,,,
Row_1162,"Li, Wenyuan","Chen, Keyan","Shi, Zhenwei",Geographical Supervision Correction for Remote Sensing Representation Learning,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,4,"Global land cover (GLC) products can be utilized to provide geographical supervision for remote sensing representation learning, which has significantly improved downstream tasks' performance and decreased the demand of manual annotations. However, the time differences between remote sensing images and GLC products may introduce deviations in geographical supervision. In this article, we propose a geographical supervision correction (GeCo) method for remote sensing representation learning. Deviated geographical supervision generated by GLC products can be corrected adaptively using the correction matrix during network pretraining and joint optimization process is designed to simultaneously update the correction matrix and network parameters. In addition, we identify prior knowledge on geographical supervision to guide representation learning and restrict the correction process. The prior knowledge named ""minor changes"" implies that the geographical supervision may not change significantly, whereas the prior knowledge named ""spatial aggregation"" implies that land covers are aggregated in their spatial distribution. According to the prior knowledge, corresponding regularization terms are proposed to prevent abrupt changes in the geographical supervision correction process and excessive smoothing of network outputs, thereby ensuring the adaptive correction process's correctness. Experimental results demonstrate that our proposed method outperforms random initialization, ImageNet pretraining, and other representation learning methods on a variety of downstream tasks. In particular, when compared to the method that learns representations directly from deviated geographical supervision, it is proven that our method can eliminate the influence of deviations and further improve the effect of representation learning.",Cloud/snow detection,object detection,remote sensing images,representation learning,scene classification,,,,semantic segmentation,,,,,,,,,,,,,,,,,
Row_1163,"Weng, Liguo","Xu, Yiming","Xia, Min",Water Areas Segmentation from Remote Sensing Images Using a Separable Residual SegNet Network,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,APR 2020,65,"Changes on lakes and rivers are of great significance for the study of global climate change. Accurate segmentation of lakes and rivers is critical to the study of their changes. However, traditional water area segmentation methods almost all share the following deficiencies: high computational requirements, poor generalization performance, and low extraction accuracy. In recent years, semantic segmentation algorithms based on deep learning have been emerging. Addressing problems associated to a very large number of parameters, low accuracy, and network degradation during training process, this paper proposes a separable residual SegNet (SR-SegNet) to perform the water area segmentation using remote sensing images. On the one hand, without compromising the ability of feature extraction, the problem of network degradation is alleviated by adding modified residual blocks into the encoder, the number of parameters is limited by introducing depthwise separable convolutions, and the ability of feature extraction is improved by using dilated convolutions to expand the receptive field. On the other hand, SR-SegNet removes the convolution layers with relatively more convolution kernels in the encoding stage, and uses the cascading method to fuse the low-level and high-level features of the image. As a result, the whole network can obtain more spatial information. Experimental results show that the proposed method exhibits significant improvements over several traditional methods, including FCN, DeconvNet, and SegNet.",semantic segmentation,water area segmentation,encoder-decoder,depthwise separable convolution,residual network,"Zhang, Yonghong",,"Liu, Jia",,"Xu, Yiqing",,,,,,,,,,,,,,,,
Row_1164,"Sui, Jialu","Ma, Xianping","Zhang, Xiaokang",Adaptive Semantic-Enhanced Denoising Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2025,0,"Remote sensing image super-resolution (SR) is a crucial task to restore high-resolution (HR) images from low-resolution (LR) observations. Recently, the denoising diffusion probabilistic model (DDPM) has shown promising performance in image reconstructions by overcoming problems inherent in generative models, such as oversmoothing and mode collapse. However, the high-frequency details generated by DDPM often suffer from misalignment with HR images due to the model's tendency to overlook long-range semantic contexts. This challenge is partly due to the prevalent use of a U-Net decoder in the conditional noise predictor, which favors local details and can introduce noise with considerable variance during prediction. To tackle these limitations, an adaptive semantic-enhanced DDPM (ASDDPM) is proposed to enhance the detail-preserving capability of the DDPM by integrating low-frequency semantic insights through a transformer. Specifically, a novel adaptive diffusion transformer decoder is developed to bridge the semantic gap between the encoder and decoder by regulating the noise prediction with the global contextual relationships and long-range dependencies in the diffusion process. In addition, a residual feature fusion strategy establishes information exchange between the two decoders at multiple levels. As a result, the predicted noise generated by our approach closely approximates that of the real noise distribution. Extensive experiments on two SR and two semantic segmentation datasets confirm the superior performance of the proposed ASDDPM in both SR and the subsequent downstream applications.",Decoding,Noise,Remote sensing,Transformers,Semantics,"Pun, Man-On",,"Wu, Hao",Image reconstruction,,,Feature extraction,Adaptation models,Training,Brain modeling,Denoising diffusion probabilistic model,remote sensing images,,,,,single image super-resolution,,,,
Row_1165,"Lei, Pengyu","Yi, Jizheng","Li, Sijia",Agricultural surface water extraction in environmental remote sensing: A novel semantic segmentation model emphasizing contextual information enhancement and foreground detail attention,NEUROCOMPUTING,FEB 7 2025,0,"The intelligent and efficient analysis of agricultural surface water is essential for agricultural irrigation, flood prevention, and resource utilization. However, accurate extraction from high-resolution optical remote sensing images is challenging due to agricultural heterogeneous terrains, dynamic water body scales, and diverse surrounding vegetation. To address these challenges, this paper introduces a novel architecture called the Complex Rural Water Extraction Network (CRWENet). The CRWENet is designed to enhance the understanding of contextual information and suppress random background noise, which comprises four key components including feature encoder, feature decoder, context information enhancement module, and foreground detail attention module. The context information enhancement module harnesses the long-range dependency-capturing capabilities of Transformer, merging multi-scale features to enhance global information perception. Meanwhile, the foreground detail attention module facilitates adaptive suppression of intricate background noise and accurate delineation of agricultural surface water boundaries by harmonizing interactions across channel, height, and width dimensions. Extensive experimentation on the LoveDA dataset and the GLH-water dataset validates the effectiveness of CRWENet. The results demonstrate that the proposed CRWENet outperforms the existing classical semantic segmentation networks and the mainstream water body extraction methods, achieving Intersection over Union (IoU) scores of 70.93 % and 80.55 % respectively.",Agricultural surface water extraction,Transformer,Attention mechanism,Remote sensing images,,"Li, Yasheng",,"Lin, Hui",,,,,,,,,,,,,,,,,,
Row_1166,"Xia, Liegang","Zhang, Xiongbo","Zhang, Junxia",Building Extraction from Very-High-Resolution Remote Sensing Images Using Semi-Supervised Semantic Edge Detection,REMOTE SENSING,JUN 2021,25,"The automated detection of buildings in remote sensing images enables understanding the distribution information of buildings, which is indispensable for many geographic and social applications, such as urban planning, change monitoring and population estimation. The performance of deep learning in images often depends on a large number of manually labeled samples, the production of which is time-consuming and expensive. Thus, this study focuses on reducing the number of labeled samples used and proposing a semi-supervised deep learning approach based on an edge detection network (SDLED), which is the first to introduce semi-supervised learning to the edge detection neural network for extracting building roof boundaries from high-resolution remote sensing images. This approach uses a small number of labeled samples and abundant unlabeled images for joint training. An expert-level semantic edge segmentation model is trained based on labeled samples, which guides unlabeled images to generate pseudo-labels automatically. The inaccurate label sets and manually labeled samples are used to update the semantic edge model together. Particularly, we modified the semantic segmentation network D-LinkNet to obtain high-quality pseudo-labels. Specifically, the main network architecture of D-LinkNet is retained while the multi-scale fusion is added in its second half to improve its performance on edge detection. The SDLED was tested on high-spatial-resolution remote sensing images taken from Google Earth. Results show that the SDLED performs better than the fully supervised method. Moreover, when the trained models were used to predict buildings in the neighboring counties, our approach was superior to the supervised way, with line IoU improvement of at least 6.47% and F1 score improvement of at least 7.49%.",semi-supervised,semantic edge detection,building extraction,deep learning,very-high-resolution image,"Yang, Haiping",,"Chen, Tingting",,,,,,,,,,,,,,,,,,
Row_1167,"Wang, Libo","Li, Rui","Wang, Dongzhi",Transformer Meets Convolution: A Bilateral Awareness Network for Semantic Segmentation of Very Fine Resolution Urban Scene Images,REMOTE SENSING,AUG 2021,129,"Semantic segmentation from very fine resolution (VFR) urban scene images plays a significant role in several application scenarios including autonomous driving, land cover classification, urban planning, etc. However, the tremendous details contained in the VFR image, especially the considerable variations in scale and appearance of objects, severely limit the potential of the existing deep learning approaches. Addressing such issues represents a promising research field in the remote sensing community, which paves the way for scene-level landscape pattern analysis and decision making. In this paper, we propose a Bilateral Awareness Network which contains a dependency path and a texture path to fully capture the long-range relationships and fine-grained details in VFR images. Specifically, the dependency path is conducted based on the ResT, a novel Transformer backbone with memory-efficient multi-head self-attention, while the texture path is built on the stacked convolution operation. In addition, using the linear attention mechanism, a feature aggregation module is designed to effectively fuse the dependency features and texture features. Extensive experiments conducted on the three large-scale urban scene image segmentation datasets, i.e., ISPRS Vaihingen dataset, ISPRS Potsdam dataset, and UAVid dataset, demonstrate the effectiveness of our BANet. Specifically, a 64.6% mIoU is achieved on the UAVid dataset.",urban scene segmentation,remote sensing,transformer,attention mechanism,,"Duan, Chenxi",,"Wang, Teng",,"Meng, Xiaoliang",,,,,,,,,,,,,,,,
Row_1168,"Yu, Yijiong","Wang, Tao","Ran, Kang",An intelligent remote sensing image quality inspection system,IET IMAGE PROCESSING,FEB 2024,0,"Due to the inevitable presence of quality problems, quality inspection of remote sensing images is indeed an indispensable step between the acquisition and the application of them. However, traditional manual inspection suffers from low efficiency. Hence, we propose a novel deep learning-based two-step intelligent system consisting of multiple advanced computer vision models, which first performs image classification by SwinV2 and then accordingly adopts the most appropriate method, such as semantic segmentation by Segformer, to localize the quality problems. Results demonstrate that the proposed method exhibits excellent performance and efficiency, surpassing traditional methods. Furthermore, an initial exploration of applying multimodal models to remote sensing image quality inspection is conducted.An Intelligent remote sensing image quality inspection system, which has a two-step structure and integrates multi models is proposed. Results demonstrate that it surpasses traditional methods in accuracy and efficiency. image",image segmentation,image classification,deep learning,image quality inspection,remote sensing,"Li, Chang",,"Wu, Hao",,,,,,,,,,,,,,,,,,
Row_1169,"Reyes, Mario Fuentes","Xie, Yuxing","Yuan, Xiangtian","A 2D/3D multimodal data simulation approach with applications on urban semantic segmentation, building extraction and change detection",ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,NOV 2023,10,"Advances in remote sensing image processing techniques have further increased the demand for annotated datasets. However, preparing annotated multi-temporal 2D/3D multimodal data is especially challenging, both for the increased costs of the annotation step and the lack of multimodal acquisitions available on the same area. We introduce the Simulated Multimodal Aerial Remote Sensing (SMARS) dataset, a synthetic dataset aimed at the tasks of urban semantic segmentation, change detection, and building extraction, along with a description of the pipeline to generate them and the parameters required to set our rendering. Samples in the form of orthorectified photos, digital surface models and ground truth for all the tasks are provided. Unlike existing datasets, orthorectified images and digital surface models are derived from synthetic images using photogrammetry, yielding more realistic simulations of the data. The increased size of SMARS, compared to available datasets of this kind, facilitates both traditional and deep learning algorithms. Reported experiments from state-of-the-art algorithms on SMARS scenes yield satisfactory results, in line with our expectations. Both benefits of the SMARS datasets and constraints imposed by its use are discussed. Specifically, building detection on the SMARS-real Potsdam cross-domain test demonstrates the quality and the advantages of proposed synthetic data generation workflow. SMARS is published as an ISPRS benchmark dataset and can be downloaded from https://www2.isprs.org/commissions/comm1/wg8/benchmark_smars/.",3D change detection,Building extraction,Urban semantic segmentation,Synthetic datasets,,"d'Angelo, Pablo",,"Kurz, Franz",,"Cerra, Daniele","Tian, Jiaojiao",,,,,,,,,,,,,,,
Row_1170,"Wang, Ziquan","Zhang, Yongsheng","Zhang, Zhenchao",SDAT-Former plus plus : A Foggy Scene Semantic Segmentation Method with Stronger Domain Adaption Teacher for Remote Sensing Images,REMOTE SENSING,DEC 2023,0,"Semantic segmentation based on optical images can provide comprehensive scene information for intelligent vehicle systems, thus aiding in scene perception and decision making. However, under adverse weather conditions (such as fog), the performance of methods can be compromised due to incomplete observations. Considering the success of domain adaptation in recent years, we believe it is reasonable to transfer knowledge from clear and existing annotated datasets to images with fog. Technically, we follow the main workflow of the previous SDAT-Former method, which incorporates fog and style-factor knowledge into the teacher segmentor to generate better pseudo-labels for guiding the student segmentor, but we identify and address some issues, achieving significant improvements. Firstly, we introduce a consistency loss for learning from multiple source data to better converge the performance of each component. Secondly, we apply positional encoding to the features of fog-invariant adversarial learning, strengthening the model's ability to handle the details of foggy entities. Furthermore, to address the complexity and noise in the original version, we integrate a simple but effective masked learning technique into a unified, end-to-end training process. Finally, we regularize the knowledge transfer in the original method through re-weighting. We tested our SDAT-Former++ on mainstream benchmarks for semantic segmentation in foggy scenes, demonstrating improvements of 3.3%, 4.8%, and 1.1% (as measured by the mIoU) on the ACDC, Foggy Zurich, and Foggy Driving datasets, respectively, compared to the original version.",semantic segmentation in foggy scenes,unsupervised domain adaptation,UDA,self-training,,"Jiang, Zhipeng",,"Yu, Ying",,"Li, Li","Zhang, Lei",,,,,,,,,,,,,,,
Row_1171,"Wang, Yongzhi","Huang, Qi","Zhao, Aiguo",Semantic Network-Based Impervious Surface Extraction Method for Rural-Urban Fringe From High Spatial Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,6,"Impervious surfaces, as a key indicator of urban spatial environmental factors, have great significance in exploring the distribution law and spatial pattern of rural-urban fringe areas. To handle the increasingly rich feature information and complicated urban spatial structure in high spatial resolution remote sensing images (HSRRSIs), a semantic network model-guided extraction method for HSRRSI impervious surfaces in rural-urban fringes is proposed. The proposed method mainly includes three parts: First, construction of a semantic network model of ground covers in the rural-urban fringe and dimensionality reduction of its features. Second, optimization of a multi-scale segmentation algorithm based on the estimation of scale parameter 2 method and the fitness function. Third, proposal of a feature reduction method based on the ReliefF feature selection algorithm for spectral, texture, and geometry features to reduce the data redundancy in HSRRSIs. Finally, with the Geoeye-1 image of the rural-urban fringe of Zhanggong District as the data source, CART, RF, and SVM classifiers are used to extract the impervious surfaces of two different areas (named as Q1 and Q2), Q1 comprises the edge of rural-urban fringe with densely distributed industrial plants, and Q2 comprises a rural-urban fringe with a pronounced transition from urban to rural areas. Results show that the highest impervious surface extraction accuracy of the SVM classifier based on the semantic network model is obtained when the segmentation scale is at 210 and 215. The producer accuracy and overall accuracy for Q1 and Q2 are (94.27%, 86.41%) and (94.46%, 89.47%), respectively.",Semantics,Feature extraction,Surface morphology,Image segmentation,Urban areas,"Lv, Hua",,"Zhuang, Shengbing",Spatial resolution,,,Indexes,Feature selection,impervious surface,multiscale segmentation,semantic network model,VHSRRSI,,,,,,,,,
Row_1172,"Cai, Yuxiang","Yang, Yingchun","Shang, Yongheng",IterDANet: Iterative Intra-Domain Adaptation for Semantic Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,8,"When segmenting the continuous proliferation of unlabeled remotely sensed images, unsupervised domain adaptation (UDA) has become one of the most critical techniques and achieved significant performance. But, in fact, there still exists a large performance gap between the existing UDA frameworks and supervised learning methods, for the majority of UDA frameworks do not consider the intra-domain gap in the target domain. In this article, to further minimize the complex intra-domain shift within the target domain in remote sensing, we propose a novel iterative intra-domain adaptation framework (IterDANet), which conducts inter-domain adaptation (InterDA), entropy-based ranking (ER), and iterative intra-domain adaptation (IntraDA). Specifically, first, to enhance the performance of InterDA built upon generative adversarial network (GAN)-based image-to-image (I2I) translation, we propose a new generator selection strategy (GSS) to assess and choose a well-trained generator for the inter-domain classifier. Then, to produce more accurate pseudolabels for IntraDA, we propose a new pseudolabel generation strategy (PLGS) to remove both high-entropy and low-confident pixels in predicted maps of inter-domain classifier. Finally, to better reduce the intra-domain gap, we propose to cluster all the target images into multiple subdomains using ER and iteratively align the cleanest subdomain with other noisy subdomains. The extensive experiments on the benchmark dataset, which includes cross-city aerial images, highlight the superiority and effectiveness of our IterDANet against the state-of-the-art UDA frameworks.",Adversarial learning,image-to-image (I2I) translation,inter-domain adaptation (InterDA),intra-domain adaptation (IntraDA),remote sensing images,"Chen, Zhenqian",,"Shen, Zhengwei",self-supervised learning (SSL),"Yin, Jianwei",,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,
Row_1173,"Pan, Xuran","Gao, Lianru","Marinoni, Andrea",Semantic Labeling of High Resolution Aerial Imagery and LiDAR Data with Fine Segmentation Network,REMOTE SENSING,MAY 2018,54,"In this paper, a novel convolutional neural network (CNN)-based architecture, named fine segmentation network (FSN), is proposed for semantic segmentation of high resolution aerial images and light detection and ranging (LiDAR) data. The proposed architecture follows the encoder-decoder paradigm and the multi-sensor fusion is accomplished in the feature-level using multi-layer perceptron (MLP). The encoder consists of two parts: the main encoder based on the convolutional layers of Vgg-16 network for color-infrared images and a lightweight branch for LiDAR data. In the decoder stage, to adaptively upscale the coarse outputs from encoder, the Sub-Pixel convolution layers replace the transposed convolutional layers or other common up-sampling layers. Based on this design, the features from different stages and sensors are integrated for a MLP-based high-level learning. In the training phase, transfer learning is employed to infer the features learned from generic dataset to remote sensing data. The proposed FSN is evaluated by using the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Vaihingen 2D Semantic Labeling datasets. Experimental results demonstrate that the proposed framework can bring considerable improvement to other related networks.",high resolution aerial imagery,LiDAR,spectral image,semantic segmentation,deep learning,"Zhang, Bing",,"Yang, Fan",convolutional neural network (CNN),"Gamba, Paolo",,,,,,,,,,,,,,,,
Row_1174,"Farhangfar, Saghar","Rezaeian, Mehdi",,Semantic Segmentation of Aerial Images using FCN-based Network,,2019,5,"In this paper, a Deep learning architecture for semantic segmentation of aerial images is proposed. One of the problems with convolutional neural network is the loss of spatial data after using convolution layers which can be retrieved using up-sampling layers. The proposed network uses skip connection with residual learning within an encoder-decoder architecture to reduce ambiguities in the up-sampling layers. We designed a FCN that uses Infra-Red-Green (IRRG), normalized Digital Surface Model (nDSM) and Normalized Difference Vegetation Index (NDVI) information from ISPRS Vaihingen dataset 2D semantic labeling and reached more than 87% as our best total accuracy.",Deep Learning,Semantic Segmentation,Vaihingen,Remote Sensing,photogrammetry,,2019 27TH IRANIAN CONFERENCE ON ELECTRICAL ENGINEERING (ICEE 2019),,,,,,,,,,,,,,,,,,,
Row_1175,"Chen, Bingyu","Xia, Min","Huang, Junqing",MFANet: A Multi-Level Feature Aggregation Network for Semantic Segmentation of Land Cover,REMOTE SENSING,FEB 2021,71,"Detailed information regarding land utilization/cover is a valuable resource in various fields. In recent years, remote sensing images, especially aerial images, have become higher in resolution and larger span in time and space, and the phenomenon that the objects in an identical category may yield a different spectrum would lead to the fact that relying on spectral features only is often insufficient to accurately segment the target objects. In convolutional neural networks, down-sampling operations are usually used to extract abstract semantic features, which leads to loss of details and fuzzy edges. To solve these problems, the paper proposes a Multi-level Feature Aggregation Network (MFANet), which is improved in two aspects: deep feature extraction and up-sampling feature fusion. Firstly, the proposed Channel Feature Compression module extracts the deep features and filters the redundant channel information from the backbone to optimize the learned context. Secondly, the proposed Multi-level Feature Aggregation Upsample module nestedly uses the idea that high-level features provide guidance information for low-level features, which is of great significance for positioning the restoration of high-resolution remote sensing images. Finally, the proposed Channel Ladder Refinement module is used to refine the restored high-resolution feature maps. Experimental results show that the proposed method achieves state-of-the-art performance 86.45% mean IOU on LandCover dataset.",land cover,high-resolution,remote sensing images,semantic segmentation,deep learning,,,,,,,,,,,,,,,,,,,,,
Row_1176,"Sung, Uk-Je","Eum, Jeong-Hee","Chung, Kyung-Jin",Evaluation of Tree Object Segmentation Performance for Individual Tree Recognition Using Remote Sensing Techniques Based on Urban Forest Green Structures,LAND,NOV 2024,0,"This study evaluated whether tree object segmentation using remote sensing techniques could be effectively conducted according to the green structures of urban forests. The remote sensing techniques used were handheld LiDAR and UAV-based photogrammetry. The data collected from both methods were merged to complement each other's limitations. The green structures of the study area were classified into three types based on the distance between canopy trees and the presence of shrubs. The ability to individually classify trees within each of the three types of green structures was then evaluated. The evaluation method was to assess the success rate by comparing the actual number of trees, which were visually counted in the field, with the number of tree objects classified in the study. To perform semantic segmentation of tree objects, a preprocessing step was conducted to extract only the data related to tree structures from the data collected through remote sensing techniques. The preprocessing steps included data merging, noise removal, separation of DTM and DSM, and separation of green areas and structures. The analysis results showed that tree object recognition was not efficient when the green structures were complex and mixed, and the recognition rate was highest when only canopy trees were present, and the canopies did not overlap. Therefore, when observing in high-density areas, the semantic segmentation algorithm's variables should be adjusted to narrow the object recognition range, and additional observations in winter are needed to compensate for areas obscured by leaves. By improving data collection methods and systematizing the analysis methods according to the green structures, the object recognition process can be enhanced.",LiDAR,photogrammetry,semantic segmentation,point cloud,,,,,,,,,,,,,,,,,,,,,,
Row_1177,"Hanyu, Taisei","Yamazaki, Kashu","Tran, Minh",AerialFormer: Multi-Resolution Transformer for Aerial Image Segmentation,REMOTE SENSING,AUG 2024,3,"When performing remote sensing image segmentation, practitioners often encounter various challenges, such as a strong imbalance in the foreground-background, the presence of tiny objects, high object density, intra-class heterogeneity, and inter-class homogeneity. To overcome these challenges, this paper introduces AerialFormer, a hybrid model that strategically combines the strengths of Transformers and Convolutional Neural Networks (CNNs). AerialFormer features a CNN Stem module integrated to preserve low-level and high-resolution features, enhancing the model's capability to process details of aerial imagery. The proposed AerialFormer is designed with a hierarchical structure, in which a Transformer encoder generates multi-scale features and a multi-dilated CNN (MDC) decoder aggregates the information from the multi-scale inputs. As a result, information is taken into account in both local and global contexts, so that powerful representations and high-resolution segmentation can be achieved. The proposed AerialFormer was benchmarked on three benchmark datasets, including iSAID, LoveDA, and Potsdam. Comprehensive experiments and extensive ablation studies show that the proposed AerialFormer remarkably outperforms state-of-the-art methods.",remote sensing,semantic segmentation,transformers,dilated convolution,,"McCann, Roy A.",,"Liao, Haitao",,"Rainwater, Chase","Adkins, Meredith",,,,,,,"Cothren, Jackson","Le, Ngan",,,,,,,
Row_1178,"Deng, Fei","Luo, Wen","Ni, Yudong",UMiT-Net: A U-Shaped Mix-Transformer Network for Extracting Precise Roads Using Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,7,"Automatic extraction of high-precision roads from remote sensing images is crucial for path planning and road monitoring. However, there is room to improve the accuracy and generalization of existing methods in segmentation due to the challenges posed by ground object occlusion and complex backgrounds. Most existing methods rely on convolutional neural networks (CNNs), but the limitations of convolution prevent direct semantic interaction at a distance. In contrast, mix-Transformer (MiT) obtains long-term modeling capability through the self-attention mechanism, and inspired by it, we propose a multiscale self-adaptive network [U-shaped MiT network (UMiT-Net)] based on the U-shaped structure. First, UMiT-Net extracts global features with the efficient MiT backbone. Second, the dilated attention module (DAM) is used in the bottleneck of the network to fuse semantic features further to ensure the connectivity of the road. Third, in the decoder, to improve the accuracy of road segmentation, we construct the multiscale self-adaptive module (MSAM), which summarizes rich scene understanding from dense contexts with strip windows conforming to road morphology, and embed an edge enhancement module (EEM) to correct road edges. Finally, we design patch expanding (PE), which solves the problem of heavy computation of upsampling due to high resolution. The experimental results show that our UMiT-Net is substantially ahead of other state-of-the-art methods and has a significant improvement in generalization ability.",Roads,Feature extraction,Remote sensing,Semantics,Decoding,"Wang, Xuben",,"Wang, Yan",Dams,"Zhang, Gulan",,Image edge detection,Mix-Transformer (MiT),remote sensing image,road extraction,semantic segmentation,U-shaped,,,,,,,,,
Row_1179,"Xu, Lu","Jing, Weipeng","Song, Houbing",High-Resolution Remote Sensing Image Change Detection Combined With Pixel-Level and Object-Level,IEEE ACCESS,2019,43,"High-resolution remote sensing images are abundant in texture information, and the detection method of the change of pixel-level mainly analyzes the spectral information of the image, which has certain limitations. In this paper, a high-resolution remote sensing image change detection method combining pixel and object levels is proposed to solve the problem that many pepper and salt phenomenon and false detection in the change detection of pixel-level and object-level change detection method are cumbersome for image segmentation process. We integrate the multi-dimensional features of high-resolution remote sensing images and use random forest classifiers to classify to obtain the pixel-level change detection results. Then, we use the improved U-net network to semantically segment the post-phase remote sensing image to obtain the image object segmentation result. Finally, the consequences of pixel-level change detection and image object segmentation result are fused to obtain the image changing area and the unchanging area. The experimental results demonstrate that the algorithm has a higher accuracy rate and detection precision.",Change detection,random forest,remote sensing,semantic segmentation,U-net,"Chen, Guangsheng",,,,,,,,,,,,,,,,,,,,
Row_1180,"Li, Lianfa","Zhu, Zhiping","Wang, Chengyi",Multiscale Entropy-Based Surface Complexity Analysis for Land Cover Image Semantic Segmentation,REMOTE SENSING,APR 2023,1,"Recognizing and classifying natural or artificial geo-objects under complex geo-scenes using remotely sensed data remains a significant challenge due to the heterogeneity in their spatial distribution and sampling bias. In this study, we propose a deep learning method of surface complexity analysis based on multiscale entropy. This method can be used to reduce sampling bias and preserve entropy-based invariance in learning for the semantic segmentation of land use and land cover (LULC) images. Our quantitative models effectively identified and extracted local surface complexity scores, demonstrating their broad applicability. We tested our method using the Gaofen-2 image dataset in mainland China and accurately estimated multiscale complexity. A downstream evaluation revealed that our approach achieved similar or better performance compared to several representative state-of-the-art deep learning methods. This highlights the innovative and significant contribution of our entropy-based complexity analysis and its applicability in improving LULC semantic segmentations through optimal stratified sampling and constrained optimization, which can also potentially be used to enhance semantic segmentation under complex geo-scenes using other machine learning methods.",remote sensing,land use,land cover,surface complexity,sampling bias,,,,stratification,,,samples,deep learning,constrained optimization,semantic segmentation,,,,,,,,,,,
Row_1181,"Cui, Binge","Zhang, Haoqing","Jing, Wei",SRSe-Net: Super-Resolution-Based Semantic Segmentation Network for Green Tide Extraction,REMOTE SENSING,FEB 2022,22,"Due to the phenomenon of mixed pixels in low-resolution remote sensing images, the green tide spectral features with low Enteromorpha coverage are not obvious. Super-resolution technology based on deep learning can supplement more detailed information for subsequent semantic segmentation tasks. In this paper, a novel green tide extraction method for MODIS images based on super-resolution and a deep semantic segmentation network was proposed. Inspired by the idea of transfer learning, a super-resolution model (i.e., WDSR) is first pre-trained with high spatial resolution GF1-WFV images, and then the representations learned in the GF1-WFV image domain are transferred to the MODIS image domain. The improvement of remote sensing image resolution enables us to better distinguish the green tide patches from the surrounding seawater. As a result, a deep semantic segmentation network (SRSe-Net) suitable for large-scale green tide information extraction is proposed. The SRSe-Net introduced the dense connection mechanism on the basis of U-Net and replaces the convolution operations with dense blocks, which effectively obtained the detailed green tide boundary information by strengthening the propagation and reusing features. In addition, the SRSe-Net reducs the pooling layer and adds a bridge module in the final stage of the encoder. The experimental results show that a SRSe-Net can obtain more accurate segmentation results with fewer network parameters.",harmful algal blooms,super-resolution,transfer learning,remote sensing images,,"Liu, Huifang",,"Cui, Jianming",,,,,,,,,,,,,,,,,,
Row_1182,"Xie, Yuxing","Tian, Jiaojiao","Zhu, Xiao Xiang",Linking Points With Labels in 3D: A Review of Point Cloud Semantic Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE,DEC 2020,255,"Ripe with possibilities offered by deep-learning techniques and useful in applications related to remote sensing, computer vision, and robotics, 3D point cloud semantic segmentation (PCSS) and point cloud segmentation (PCS) are attracting increasing interest. This article summarizes available data sets and relevant studies on recent developments in PCSS and PCS.",Three-dimensional displays,Laser radar,Sensors,Synthetic aperture radar,Semantics,,,,Cameras,,,Image segmentation,,,,,,,,,,,,,,
Row_1183,"Liu, Songlin","Chen, Linwei","Zhang, Li",A large-scale climate-aware satellite image dataset for domain adaptive land-cover semantic segmentation,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,NOV 2023,5,"A few well-annotated datasets for land-cover semantic segmentation have recently been introduced to advance the field of earth observation technologies. However, these datasets overlook the significant diversity among geographic areas with different climates, which can greatly impact and diversify land cover. Consequently, this leads to a domain gap in remote sensing images and severe performance degradation of the segmentation models. To enhance land-cover semantic segmentation with improved generalization ability, we conducted the first investigation into the impact of climate on this task. In this paper, we present a unique largescale Climate-Aware Satellite Images Dataset (CASID) specifically designed for domain adaptive land-cover semantic segmentation. It consists of 980 satellite images with a size of 5000 x 5000 pixels, collected from 30 different regions around Asia, covering over 24,500 square kilometers. These images are gathered from four distinct climate zones, namely temperate monsoon, subtropical monsoon, tropical monsoon, and tropical rainforest. It includes four sub-datasets/domains, each representing one of the aforementioned climate zones. This characteristic makes CASID the first climate-aware land-cover semantic segmentation dataset with multiple domains. Additionally, we provide a comprehensive analysis of the samples from the four climate zones, emphasizing differences in global image features, image texture, category distribution, spectral value, and object shape. These analyses offer valuable insights for subsequent research in this field. Moreover, we conduct extensive experiments to evaluate the latest semantic segmentation and unsupervised domain adaptation methods on the CASID dataset. These results serve as a robust baseline for future research endeavors. Our dataset will be made publicly available soon at the following link: https://github.com/Linwei-Chen/CASID.",Satellite image,Semantic segmentation,Unsupervised domain adaptation,,,"Hu, Jun",,"Fu, Ying",,,,,,,,,,,,,,,,,,
Row_1184,"Chen, Yantong","Li, Yuyang","Wang, Junsheng",An End-to-End Oil-Spill Monitoring Method for Multisensory Satellite Images Based on Deep Semantic Segmentation,SENSORS,FEB 2020,18,"In remote-sensing images, a detected oil-spill area is usually affected by spot noise and uneven intensity, which leads to poor segmentation of the oil-spill area. This paper introduced a deep semantic segmentation method that combined a deep-convolution neural network with the fully connected conditional random field to form an end-to-end connection. On the basis of Resnet, it first roughly segmented a multisource remote-sensing image as input by the deep convolutional neural network. Then, we used the Gaussian pairwise method and mean-field approximation. The conditional random field was established as the output of the recurrent neural network. The oil-spill area on the sea surface was monitored by the multisource remote-sensing image and was estimated by optical image. We experimentally compared the proposed method with other models on the dataset established by the multisensory satellite image. Results showed that the method improved classification accuracy and captured fine details of the oil-spill area. The mean intersection over the union was 82.1%, and the monitoring effect was obviously improved.",sea oil spill,convolutional neural network,semantic segmentation,conditional random field,remote-sensing image,,,,,,,,,,,,,,,,,,,,,
Row_1185,"Zhang, Mi","Hu, Xiangyun","Zhao, Like",Learning Dual Multi-Scale Manifold Ranking for Semantic Segmentation of High-Resolution Images,REMOTE SENSING,MAY 2017,30,"Semantic image segmentation has recently witnessed considerable progress by training deep convolutional neural networks (CNNs). The core issue of this technique is the limited capacity of CNNs to depict visual objects. Existing approaches tend to utilize approximate inference in a discrete domain or additional aides and do not have a global optimum guarantee. We propose the use of the multi-label manifold ranking (MR) method in solving the linear objective energy function in a continuous domain to delineate visual objects and solve these problems. We present a novel embedded single stream optimization method based on the MR model to avoid approximations without sacrificing expressive power. In addition, we propose a novel network, which we refer to as dual multi-scale manifold ranking (DMSMR) network, that combines the dilated, multi-scale strategies with the single stream MR optimization method in the deep learning architecture to further improve the performance. Experiments on high resolution images, including close-range and remote sensing datasets, demonstrate that the proposed approach can achieve competitive accuracy without additional aides in an end-to-end manner.",semantic segmentation,deep convolutional neural networks,manifold ranking,single stream optimization,high resolution image,"Lv, Ye",,"Luo, Min",,"Pang, Shiyan",,,,,,,,,,,,,,,,
Row_1186,"Chen, Zhenqian","Shang, Yongheng","Python, Andre",DB-BlendMask: Decomposed Attention and Balanced BlendMask for Instance Segmentation of High-Resolution Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,9,"Instance segmentation is an important method for high-resolution remote sensing images (HRRSIs) analysis. Traditional instance segmentation algorithms are not suitable to analyze complex HRRSIs that exhibit: 1) various shapes and sizes of targets; 2) a large number of small targets; and 3) data with long tail distribution. Here we introduce DB-BlendMask, an efficient and accurate instance segmentation method that can accommodate complex HRRSIs. It is composed of size balance coefficient (SBC), class balance module (CBM), and decomposed attention blender module (DA-Blender module). SBC consists of a fair weight allocation strategy for positive samples in object detection. CBM combines classification obtained in object detection stage to guide the semantic feature extraction. Complementary to a traditional convolutional neural network (CNN) architecture, DA-Blender module has the ability to considerably compress space complexity of attention and merge attention with semantic feature to generate the instance mask. We compare the performance of DB-BlendMask with a benchmark Mask R-CNN on two typical datasets, iSAID, and ISPRS Postdam. We obtain an average detection precision of 39.2% on iSAID and 63.6% on ISPRS Postdam, which corresponds to an improvement of 2.5% and 2.7%, respectively, compared to the benchmark in a real-time scenario.",Feature extraction,Semantics,Object detection,Image segmentation,Shape,"Cai, Yuxiang",,"Yin, Jianwei",Marine vehicles,,,Resource management,BlendMask,class balance module (CBM),decomposed attention blender module (DA-Blender module),high-resolution remote sensing images (HRRSIs),instance segmentation,,,,,size balance coefficient (SBC),,,,
Row_1187,"Gao, Kuiliang","Yu, Anzhu","You, Xiong",Cross-Domain Multi-Prototypes with Contradictory Structure Learning for Semi-Supervised Domain Adaptation Segmentation of Remote Sensing Images,REMOTE SENSING,JUL 2023,3,"Recently, unsupervised domain adaptation (UDA) segmentation of remote sensing images (RSIs) has attracted a lot of attention. However, the performance of such methods still lags far behind that of their supervised counterparts. To this end, this paper focuses on a more practical yet under-investigated problem, semi-supervised domain adaptation (SSDA) segmentation of RSIs, to effectively improve the segmentation results of targeted RSIs with a few labeled samples. First, differently from the existing single-prototype mode, a novel cross-domain multi-prototype constraint is proposed, to deal with large inter-domain discrepancies and intra-domain variations. Specifically, each class is represented as a set of prototypes, so that multiple sets of prototypes corresponding to different classes can better model complex inter-class differences, while different prototypes within the same class can better describe the rich intra-class relations. Meanwhile, the multi-prototypes are calculated and updated jointly using source and target samples, which can effectively promote the utilization and fusion of the feature information in different domains. Second, a contradictory structure learning mechanism is designed to further improve the domain alignment, with an enveloping form. Third, self-supervised learning is adopted, to increase the number of target samples involved in prototype updating and domain adaptation training. Extensive experiments verified the effectiveness of the proposed method for two aspects: (1) Compared with the existing SSDA methods, the proposed method could effectively improve the segmentation performance by at least 7.38%, 4.80%, and 2.33% on the Vaihingen, Potsdam, and Urban datasets, respectively; (2) with only five labeled target samples available, the proposed method could significantly narrow the gap with its supervised counterparts, which was reduced to at least 4.04%, 6.04%, and 2.41% for the three RSIs.",semi-supervised domain adaptation segmentation,remote sensing images,cross-domain multi-prototypes,contradictory structure learning,self-supervised learning,"Qiu, Chunping",,"Liu, Bing",,"Zhang, Fubing",,,,,,,,,,,,,,,,
Row_1188,"Lin, Haoning","Shi, Zhenwei","Zou, Zhengxia",Maritime Semantic Labeling of Optical Remote Sensing Images with Multi-Scale Fully Convolutional Network,REMOTE SENSING,MAY 2017,64,"In current remote sensing literature, the problems of sea-land segmentation and ship detection (including in-dock ships) are investigated separately despite the high correlation between them. This inhibits joint optimization and makes the implementation of the methods highly complicated. In this paper, we propose a novel fully convolutional network to accomplish the two tasks simultaneously, in a semantic labeling fashion, i.e., to label every pixel of the image into 3 classes, sea, land and ships. A multi-scale structure for the network is proposed to address the huge scale gap between different classes of targets, i.e., sea/land and ships. Conventional multi-scale structure utilizes shortcuts to connect low level, fine scale feature maps to high level ones to increase the network's ability to produce finer results. In contrast, our proposed multi-scale structure focuses on increasing the receptive field of the network while maintaining the ability towards fine scale details. The multi-scale convolution network accommodates the huge scale difference between sea-land and ships and provides comprehensive features, and is able to accomplish the tasks in an end-to-end manner that is easy for implementation and feasible for joint optimization. In the network, the input forks into fine-scale and coarse-scale paths, which share the same convolution layers to minimize network parameter increase, and then are joined together to produce the final result. The experiments show that the network tackles the semantic labeling problem with improved performance.",semantic labeling,convolution neural network,fully convolutional network,sea-land segmentation,ship detection,,,,,,,,,,,,,,,,,,,,,
Row_1189,"Zhang, Chengming","Chen, Yan","Yang, Xiaoxia",Improved Remote Sensing Image Classification Based on Multi-Scale Feature Fusion,REMOTE SENSING,JAN 2020,31,"When extracting land-use information from remote sensing imagery using image segmentation, obtaining fine edges for extracted objects is a key problem that is yet to be solved. In this study, we developed a new weight feature value convolutional neural network (WFCNN) to perform fine remote sensing image segmentation and extract improved land-use information from remote sensing imagery. The WFCNN includes one encoder and one classifier. The encoder obtains a set of spectral features and five levels of semantic features. It uses the linear fusion method to hierarchically fuse the semantic features, employs an adjustment layer to optimize every level of fused features to ensure the stability of the pixel features, and combines the fused semantic and spectral features to form a feature graph. The classifier then uses a Softmax model to perform pixel-by-pixel classification. The WFCNN was trained using a stochastic gradient descent algorithm; the former and two variants were subject to experimental testing based on Gaofen 6 images and aerial images that compared them with the commonly used SegNet, U-NET, and RefineNet models. The accuracy, precision, recall, and F1-Score of the WFCNN were higher than those of the other models, indicating certain advantages in pixel-by-pixel segmentation. The results clearly show that the WFCNN can improve the accuracy and automation level of large-scale land-use mapping and the extraction of other information using remote sensing imagery.",convolutional neural network,image segmentation,multi-scale feature fusion,semantic features,Gaofen 6,"Gao, Shuai",,"Li, Feng",aerial images,"Kong, Ailing","Zu, Dawei",land-use,Tai'an,,,,,"Sun, Li",,,,,,,,
Row_1190,"Talal, Mina","Panthakkan, Alavikunhu","Mukhtar, Husameldin",Detection of Water-Bodies Using Semantic Segmentation,,2018,0,"This paper proposes a semantic segmentation technique to automatically detect water-bodies from DubaiSat-2 images. The proposed method uses a deep convolutional neural network transfer-learning model. Several evaluation metrics such as accuracy, precision, and Jaccard coefficient are used to test our proposed algorithm. The overall accuracy for the prediction of water-bodies in DubaiSat-2 image dataset is 99.86%.",Deep learning,semantic segmentation,neural network,satellite images,remote sensing,"Mansoor, Waling",2018 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INFORMATION SECURITY (ICSPIS),"Almansoorit, Saeed",,"Al Alunad, Hussain",,,,,,,,,,,,,,,,
Row_1191,"Lu, Chen","Xia, Min","Qian, Ming",Dual-Branch Network for Cloud and Cloud Shadow Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,47,"Cloud and cloud shadow segmentation is one of the most important issues in remote sensing image processing. Most of the remote sensing images are very complicated. In this work, a dual-branch model composed of transformer and convolution network is proposed to extract semantic and spatial detail information of the image, respectively, to solve the problems of false detection and missed detection. To improve the model's feature extraction, a mutual guidance module (MGM) is introduced, so that the transformer branch and the convolution branch can guide each other for feature mining. Finally, in view of the problem of rough segmentation boundary, this work uses different features extracted by the transformer branch and the convolution branch for decoding and repairs the rough segmentation boundary in the decoding part to make the segmentation boundary clearer. Experimental results on the Landsat-8, Sentinel-2 data, the public dataset high-resolution cloud cover validation dataset created by researchers at Wuhan University (HRC_WHU), and the public dataset Spatial Procedures for Automated Removal of Cloud and Shadow (SPARCS) demonstrate the effectiveness of our method and its superiority to the existing state-of-the-art cloud and cloud shadow segmentation approaches.",Feature extraction,Transformers,Convolution,Clouds,Image segmentation,"Chen, Binyu",,,Decoding,,,Task analysis,Deep learning,dual branch,remote sensing image,segmentation,,,,,,,,,,
Row_1192,"Sun, Jiaxing","He, Wei","Zhang, Hongyan",G2LDIE: Global-to-Local Dynamic Information Enhancement Framework for Weakly Supervised Building Extraction From Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,2,"Image-level weakly supervised semantic segmentation (WSSS) methods have gained prominence in remote sensing image building extraction tasks, primarily due to their cost-effectiveness in manual annotation. However, owing to the intricate details present in remote sensing building images, the pseudolabels generated from existing image-level weakly supervised methods often encounter issues of incorrect activation and unclear boundaries. In this article, we propose a global-to-local dynamic information enhancement (G2LDIE) framework. This framework effectively extracts global information from remote sensing building images and supplements local details through a local information enhancement (LIE) module, generating more accurate pseudolabels. Additionally, we propose a dynamic label guide strategy (DLGS) to enhance model consistency in category representation across various scale images. To address the challenge of unclear building boundaries issue in pseudolabels, we introduce a segment anything model (SAM) postprocessing (SPP) method, which can better correct the boundaries of building images at different resolutions while reducing computational costs. Extensive and detailed experiments on three datasets confirm that our framework can generate refined pseudolabels and outperform other image-level weakly supervised methods in terms of accuracy and generalization performance in building extraction.",Building extraction,class activation map (CAM),remote sensing image,weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,,,,,,,,
Row_1193,"Yan, Geding","Jing, Haitao","Li, Hui",Enhancing Building Segmentation in Remote Sensing Images: Advanced Multi-Scale Boundary Refinement with MBR-HRNet,REMOTE SENSING,AUG 2023,7,"Deep learning algorithms offer an effective solution to the inefficiencies and poor results of traditional methods for building a footprint extraction from high-resolution remote sensing imagery. However, the heterogeneous shapes and sizes of buildings render local extraction vulnerable to the influence of intricate backgrounds or scenes, culminating in intra-class inconsistency and inaccurate segmentation outcomes. Moreover, the methods for extracting buildings from very high-resolution (VHR) images at present often lose spatial texture information during down-sampling, leading to problems, such as blurry image boundaries or object sticking. To solve these problems, we propose the multi-scale boundary-refined HRNet (MBR-HRNet) model, which preserves detailed boundary features for accurate building segmentation. The boundary refinement module (BRM) enhances the accuracy of small buildings and boundary extraction in the building segmentation network by integrating edge information learning into a separate branch. Additionally, the multi-scale context fusion module integrates feature information of different scales, enhancing the accuracy of the final predicted image. Experiments on WHU and Massachusetts building datasets have shown that MBR-HRNet outperforms other advanced semantic segmentation models, achieving the highest intersection over union results of 91.31% and 70.97%, respectively.",building footprint extraction,remote sensing imagery,boundary refinement,multi-scale context fusion,intra-class inconsistency,"Guo, Huanchao",,"He, Shi",,,,,,,,,,,,,,,,,,
Row_1194,"Zhang, Jingfeng","Zhou, Bin","Lu, Jin",Vegetation extraction from Landsat8 operational land imager remote sensing imagery based on Attention U-Net and vegetation spectral features,JOURNAL OF APPLIED REMOTE SENSING,JUL 1 2024,0,"The rapid, accurate, and intelligent extraction of vegetation areas is of great significance for conducting research on forest resource inventory, climate change, and the greenhouse effect. Currently, existing semantic segmentation models suffer from limitations such as insufficient extraction accuracy (ACC) and unbalanced positive and negative categories in datasets. Therefore, we propose the Attention U-Net model for vegetation extraction from Landsat8 operational land imager remote sensing images. By combining the convolutional block attention module, Visual Geometry Group 16 backbone network, and Dice loss, the model alleviates the phenomenon of omission and misclassification of the fragmented vegetation areas and the imbalance of positive and negative classes. In addition, to test the influence of remote sensing images with different band combinations on the ACC of vegetation extraction, we introduce near-infrared (NIR) and short-wave infrared (SWIR) spectral information to conduct band combination operations, thus forming three datasets, namely, the 432 dataset (R, G, B), 543 dataset (NIR, R, G), and 654 dataset (SWIR, NIR, R). In addition, to validate the effectiveness of the proposed model, it was compared with three classic semantic segmentation models, namely, PSP-Net, DeepLabv3+, and U-Net. Experimental results demonstrate that all models exhibit improved extraction performance on false color datasets compared with the true color dataset, particularly on the 654 dataset where vegetation extraction performance is optimal. Moreover, the proposed Attention U-Net achieves the highest overall ACC with mean intersection over union, mean pixel ACC, and ACC reaching 0.877, 0.940, and 0.946, respectively, providing substantial evidence for the effectiveness of the proposed model. Furthermore, the model demonstrates good generalizability and transferability when tested in other regions, indicating its potential for further application and promotion. (c) 2024 Society of Photo-Optical Instrumentation Engineers (SPIE)",vegetation extraction,remote sensing image,band combination,U-Net,semantic segmentation,"Wang, Ben",,"Ding, Zhipeng",,"He, Songyue",,,,,,,,,,,,,,,,
Row_1195,"Lei, Yanjing","Yu, Jiamin","Chan, Sixian",SNLRUX plus plus for Building Extraction From High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,14,"Building extraction plays an important role in high-resolution remote sensing image processing, which can be used as the basis for urban planning and demographic analysis. In recent years, many powerful general semantic segmentation models have emerged, but these models often perform poorly when transferred to remote sensing images because of the characteristics of remote sensing images. To this end, we propose a new deep learning network called Selective Nonlocal ResUNeXt++ (SNLRUX++) for building extraction. First, the cascaded multiscale feature fusion is proposed to transform the high-performance image classification network ResNeXt into the segmentation network ResUNeXt++. Second, selective nonlocal operation is designed to establish long-range dependencies while avoiding introducing excessive noise and computational effort. Finally, multiscale prediction is applied as deep supervision to accelerate training and convergence, and improves prediction performance of objects at different scales. The experimental results on two different remote sensing image datasets show the effectiveness and generalization ability of the proposed method.",Remote sensing,Feature extraction,Semantics,Image segmentation,Buildings,"Wu, Wei",,"Liu, Xiaoying",Task analysis,,,Deep learning,Building extraction,convolution neural network,deep learning,high-resolution image,remote sensing,,,,,,,,,
Row_1196,"Wang, Nan","Wu, Qingxi","Gui, Yuanyuan",Cross-Modal Segmentation Network for Winter Wheat Mapping in Complex Terrain Using Remote-Sensing Multi-Temporal Images and DEM Data,REMOTE SENSING,MAY 2024,0,"Winter wheat is a significant global food crop, and it is crucial to monitor its distribution for better agricultural management, land planning, and environmental sustainability. However, the distribution style of winter wheat planting fields is not consistent due to different terrain conditions. In mountainous areas, winter wheat planting units are smaller in size and fragmented in distribution compared to plain areas. Unfortunately, most crop-mapping research based on deep learning ignores the impact of topographic relief on crop distribution and struggles to handle hilly areas effectively. In this paper, we propose a cross-modal segmentation network for winter wheat mapping in complex terrain using remote-sensing multi-temporal images and DEM data. First, we propose a diverse receptive fusion (DRF) module, which applies a deformable receptive field to optical images during the feature fusion process, allowing it to match winter wheat plots of varying scales and a fixed receptive field to the DEM to extract evaluation features at a consistent scale. Second, we developed a distributed weight attention (DWA) module, which can enhance the feature intensity of winter wheat, thereby reducing the omission rate of planting areas, especially for the small-sized regions in hilly terrain. Furthermore, to demonstrate the performance of our model, we conducted extensive experiments and ablation studies on a large-scale dataset in Lanling county, Shandong province, China. Our results show that our proposed CM-Net is effective in mapping winter wheat in complex terrain.",deep learning,winter wheat,semantic segmentation,remote sensing image,attention block,"Hu, Qiao",,"Li, Wei",,,,,,,,,,,,,,,,,,
Row_1197,"Schenkel, Fabian","Middelmann, Wolfgang",,DOMAIN ADAPTATION FOR SEMANTIC SEGMENTATION OF AERIAL IMAGERY USING CYCLE-CONSISTENT ADVERSARIAL NETWORKS,,2020,3,"Semantic segmentation is an important computer vision task for the analysis of aerial imagery in many remote sensing applications. Due to the large availability of data it is possible to design efficient convolutional neural network based deep learning models for this purpose. But these methods usually show a weak performance when they are applied without any modifications to data from another domain with different characteristics relating to aspects concerning the sensor or environmental influences. To improve the performance of these methods domain adaptation approaches can be employed. In the following work, we want to present a method for unsupervised domain adaptation for semantic segmentation. We trained an encoder-decoder model on the source domain dataset as task application and adjusted the network to the target domain The adaptation process is based on a style transfer component, which is realized using a cycle-consistent adversarial network. Through a continuous adaptation of the task model we achieved a higher generalization of the network and increased the task method performance on the target domain.",Semantic Segmentation,Domain Adaptation,Unsupervised Learning,Cycle-Consistent Adversarial Networks,,,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,,,,,
Row_1198,"Abdollahi, Abolfazl","Pradhan, Biswajeet","Shukla, Nagesh",Multi-Object Segmentation in Complex Urban Scenes from High-Resolution Remote Sensing Data,REMOTE SENSING,SEP 2021,30,"Terrestrial features extraction, such as roads and buildings from aerial images using an automatic system, has many usages in an extensive range of fields, including disaster management, change detection, land cover assessment, and urban planning. This task is commonly tough because of complex scenes, such as urban scenes, where buildings and road objects are surrounded by shadows, vehicles, trees, etc., which appear in heterogeneous forms with lower inter-class and higher intra-class contrasts. Moreover, such extraction is time-consuming and expensive to perform by human specialists manually. Deep convolutional models have displayed considerable performance for feature segmentation from remote sensing data in the recent years. However, for the large and continuous area of obstructions, most of these techniques still cannot detect road and building well. Hence, this work's principal goal is to introduce two novel deep convolutional models based on UNet family for multi-object segmentation, such as roads and buildings from aerial imagery. We focused on buildings and road networks because these objects constitute a huge part of the urban areas. The presented models are called multi-level context gating UNet (MCG-UNet) and bi-directional ConvLSTM UNet model (BCL-UNet). The proposed methods have the same advantages as the UNet model, the mechanism of densely connected convolutions, bi-directional ConvLSTM, and squeeze and excitation module to produce the segmentation maps with a high resolution and maintain the boundary information even under complicated backgrounds. Additionally, we implemented a basic efficient loss function called boundary-aware loss (BAL) that allowed a network to concentrate on hard semantic segmentation regions, such as overlapping areas, small objects, sophisticated objects, and boundaries of objects, and produce high-quality segmentation maps. The presented networks were tested on the Massachusetts building and road datasets. The MCG-UNet improved the average F1 accuracy by 1.85%, and 1.19% and 6.67% and 5.11% compared with UNet and BCL-UNet for road and building extraction, respectively. Additionally, the presented MCG-UNet and BCL-UNet networks were compared with other state-of-the-art deep learning-based networks, and the results proved the superiority of the networks in multi-object segmentation tasks.",building extraction,boundary-aware loss,deep learning,remote sensing,road extraction,"Chakraborty, Subrata",,"Alamri, Abdullah",,,,,,,,,,,,,,,,,,
Row_1199,"Hordiiuk, Dariia","Oliinyk, Ievgenii","Hnatushenko, Volodymyr",Semantic Segmentation for Ships Detection from Satellite Imagery,,2019,21,"Ships detection, as well as other object detection, and localization tasks in satellite images are the central problems in the field where remote sensing and computer vision coalesce. They are commonly used in different areas like environment monitoring, fishery management, logistics, insurance and many others. This paper provides an approach based on the Convolutional Neural Networks (CNN) as the main algorithm/instrument for detecting ships in optical satellite images of different spatial resolution. For achieving the best performance, we divided the problem into stages, which gave a possibility to control the quality of intermediate outcomes. The proposed method contains two parts: 1) building a classifier based on XCeption, 2) using baseline Unet model with Resnet18 as encoder for exact segmentation which allow us to achieve accuracy of more than 84%.",remote sensing,ships detection,convolutional neural networks,image processing,semantic segmentation,"Maksymov, Kostiantyn",2019 IEEE 39TH INTERNATIONAL CONFERENCE ON ELECTRONICS AND NANOTECHNOLOGY (ELNANO),,,,,,,,,,,,,,,,,,,
Row_1200,"Li, Yansheng","Li, Xinwei","Zhang, Yongjun",Cost-efficient information extraction from massive remote sensing data: When weakly supervised deep learning meets remote sensing big data,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,JUN 2023,12,"With many platforms and sensors continuously observing the earth surface, the large amount of remote sensing data presents a big data challenge. While remote sensing data acquisition capability can fully meet the requirements of many application domains, there is still a need to further explore how to efficiently mine the useful information from remote sensing big data (RSBD). Many researchers in the remote sensing community have introduced deep learning in the process of RSBD, and deep learning-based methods have achieved better performance compared with traditional methods. However, there are still substantial obstacles to the application of deep learning in remote sensing. One of the major challenges is the generation of pixel-level labels with high quality for training samples, which is essential to deep learning models. Weakly supervised deep learning (WSDL) is a promising solution to address this problem as WSDL can utilize greedily labeled datasets that are easy to collect but not ideal to train the deep networks. In this review, we summarize the achievements of WSDL-driven cost-efficient information extraction from RSBD. We first analyze the opportunities and challenges of information extraction from RSBD. Based on the analysis of the theoretical foundations of WSDL in the computer vision (CV) domain, we conduct a survey on the WSDL-based information extraction methods under the data characteristic and task demand of RSBD in four different tasks: (i) scene classification, (ii) object detection, (iii) semantic segmentation and (iv) change detection. Finally, potential research directions are outlined to guide researchers to further exploit WSDL-based information extraction from RSBD.",Remote sensing big data mining,Weakly supervised deep learning,Cost-efficient information extraction,Future research directions,,"Peng, Daifeng",,"Bruzzone, Lorenzo",,,,,,,,,,,,,,,,,,
Row_1201,"He, Pei","Jiao, Licheng","Shang, Ronghua",MANet: Multi-Scale Aware-Relation Network for Semantic Segmentation in Aerial Scenes,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,28,"Semantic segmentation is an important yet unsolved problem in aerial scenes understanding. One of the major challenges is the intense variations of scenes and object scales. In this article, we propose a novel multi-scale aware-relation network (MANet) to tackle this problem in remote sensing. Inspired by the process of human perception of multi-scale (MS) information, we explore discriminative and diverse MS representations. For discriminative MS representations, we propose an inter-class and intra-class region refinement (IIRR) method to reduce feature redundancy caused by fusion. IIRR utilizes the refinement maps with intra-class and inter-class scale variation to guide MS fine-grained features. Then, we propose multi-scale collaborative learning (MCL) to enhance the diversity of MS feature representations. The MCL constrains the diversity of MS feature network parameters to obtain diverse information. Also, the segmentation results are rectified according to the dispersion of the multilevel network predictions. In this way, MANet can learn MS features by collaboratively exploiting the correlation among different scales. Extensive experiments on image and video datasets, which have large-scale variations, have demonstrated the effectiveness of our proposed MANet.",Semantics,Image segmentation,Remote sensing,Fuses,Collaborative work,"Wang, Shuang",,"Liu, Xu",Feature extraction,"Quan, Dou","Yang, Kun",Deep learning,Aware relation,collaborative learning,feature refinement,multi-scale (MS),remote sensing,"Zhao, Dong",,,,semantic segmentation,,,,
Row_1202,"Zhao, Tianyu","Xu, Jindong","Chen, Rui",Remote sensing image segmentation based on the fuzzy deep convolutional neural network,INTERNATIONAL JOURNAL OF REMOTE SENSING,JUN 16 2021,19,"Remote sensing image segmentation has large uncertainty related to the heterogeneity of similar objects and complex spectrum in satellite images, causing the traditional segmentation methods to be greatly limited. Existing semantic segmentation methods represented by deep learning have made breakthrough progress. However, traditional deep learning methods, such as deep convolution neural network, are a completely deterministic model, which cannot describe the uncertainty of remote sensing image well. To solve this problem, a new deep neural network combined with fuzzy logic units is proposed in this paper, called RSFCNN (Remote Sensing image segmentation with Fuzzy Convolutional Neural Network). The network integrates convolution units and fuzzy logic units. Convolution units are used to extract discriminant features with different proportions, thus providing comprehensive information for pixel-level remote sensing image segmentation. Fuzzy logic units are used to deal with various uncertainties and provide more reliable segmentation results, and each unit handles the feature maps at a particular image scale by Gaussian blur function. Experiments were carried out on two data sets, and the results showed that RSFCNN has higher segmentation accuracy and better performance than state-of-the-art algorithms.",,,,,,"Ma, Xiangyue",,,,,,,,,,,,,,,,,,,,
Row_1203,"Buttar, Preetpal Kaur","Sachan, Manoj Kumar",,Semantic segmentation of satellite images for crop type identification in smallholder farms,JOURNAL OF SUPERCOMPUTING,JAN 2024,1,"Accurate and reliable crop type identification from satellite images provides a foundation for crop yield predictions which paves the way to help ensure food security. Most of the work done in the field of crop type mapping using remote sensing is restricted to the developed countries having large field parcels, while a little effort has been directed towards doing so for developing countries, where this task becomes more challenging due to the small size of field parcels, irregular shapes of the fields, and an acute shortage of labelled datasets for training supervised machine learning models. In this research, we try to fill this gap in the literature by exploring the feasibility of performing the semantic segmentation of agricultural fields from satellite images by proposing an encoder-decoder-based semantic segmentation architecture, CropNet, with a ResNet network as the encoder backbone and the use of attention modules in the decoder to allow the model to focus on more important portions of the feature maps and the feature fusion to concatenate the feature maps from all the decoder nodes getting a more precise prediction by bringing the spatial location information from the previous layers. The architecture outperformed the state of the art by 0.51% and 1.3%, on overall accuracy and macro-F1 score, respectively, after being trained on the ""2019 Zindi's Farm Pin Crop Detection"" dataset of Sentinel-2 images. The model achieved a field-wise overall classification accuracy of 78.06% and macro-F1 score of 67.3% and a pixel-wise segmentation mean Intersection over Union (mIoU) of 62.22% which is an improvement of 2.56% over the state-of-the-art methods, thereby demonstrating that our model is computationally efficient for the job of semantic segmentation of crop types from the satellite images in the difficult scenario of smallholder farms.",Semantic segmentation,Remote sensing,Encoder-decoder architecture,Sentinel-2,Crop type mapping,,,,Smallholder farms,,,,,,,,,,,,,,,,,
Row_1204,"Baghbaderani, Razieh Kaviani","Qi, Hairong",,INCORPORATING SPECTRAL UNMIXING IN SATELLITE IMAGERY SEMANTIC SEGMENTATION,,2019,2,"Land-cover classification to distinguish physical covers of Earth's surface is one of the critical tasks in remote sensing. Although deep learning-based approaches have shown remarkable performance in semantic segmentation, they require a massive amount of training data. Thus, the generalization capability of these approaches is of great importance, especially in working with satellite images when the amount of available labeled data is quite limited. In this paper, we propose incorporating spectral unmixing methods to obtain powerful representations of spectral information for semantic segmentation of satellite images. We show that land-cover classification performance can be enhanced by this proper extraction of features as input to the deep learning-based model. The experimental results demonstrate promising potential improvements in terms of segmentation accuracy. In addition, qualitative assessments show a higher confidence level of the proposed framework in predicting a label for a given pixel.",Satellite image,semantic segmentation,deep learning,spectral unmixing,,,2019 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP),,,,,,,,,,,,,,,,,,,
Row_1205,"Wang, Junjue","Ma, Ailong","Chen, Zihang",EarthVQANet: Multi-task visual question answering for remote sensing image understanding,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,JUN 2024,2,"Monitoring and managing Earth's surface resources is critical to human settlements, encompassing essential tasks such as city planning, disaster assessment, etc. To accurately recognize the categories and locations of geographical objects and reason about their spatial or semantic relations , we propose a multi -task framework named EarthVQANet, which jointly addresses segmentation and visual question answering (VQA) tasks. EarthVQANet contains a hierarchical pyramid network for segmentation and semantic -guided attention for VQA, in which the segmentation network aims to generate pixel -level visual features and high-level object semantics, and semantic -guided attention performs effective interactions between visual features and language features for relational modeling. For accurate relational reasoning, we design an adaptive numerical loss that incorporates distance sensitivity for counting questions and mines hard -easy samples for classification questions, balancing the optimization. Experimental results on the EarthVQA dataset (city planning for Wuhan, Changzhou, and Nanjing in China), RSVQA dataset (basic statistics for general objects), and FloodNet dataset (disaster assessment for Texas in America attacked by Hurricane Harvey) show that EarthVQANet surpasses 11 general and remote sensing VQA methods. EarthVQANet simultaneously achieves segmentation and reasoning, providing a solid benchmark for various remote sensing applications. Data is available at http://rsidea.whu.edu.cn/EarthVQA.htm",Visual question answering,Semantic segmentation,Multi-modal fusion,Multi-task learning,Knowledge reasoning,"Zheng, Zhuo",,"Wan, Yuting",,"Zhang, Liangpei","Zhong, Yanfei",,,,,,,,,,,,,,,
Row_1206,"Luo, Chen","Feng, Shanshan","Wang, Huaibin",LGCNet: A Cloud Detection Method in Remote Sensing Images Using Local and Global Semantics,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Detecting and eliminating clouds is a crucial step in remote sensing image (RSI) preprocessing. The removal of clouds can significantly enhance the performance of subsequent remote sensing applications. Existing deep learning (DL)-based cloud detection methods extract semantic information to improve feature representation and, subsequently, detection performance. However, these methods do not fully utilize the potential of context semantic information. Besides, to capture semantics from large receptive fields, they employ convolution operators with large kernel sizes, which results in high computational costs. Thus, these computationally heavy models are not suitable for resource-limited devices, particularly satellites. To address this issue, we propose a cloud detection model, LGCNet. This model efficiently extracts both local and global contextual information, fully utilizing semantics while reducing resource usage. LGCNet is built on an encoder-decoder structure. Specifically, the encoder extracts local scale-aware semantics through proposed local semantic blocks (LSBs), which are then skip-connected to the decoder. This approach provides adaptive and diverse local contextual information. On the top of the encoder, the high-level global semantics are captured via the proposed global feature TransBlock (GFTB). A variety of extracted semantics ensure improved detection performance. We evaluate the proposed method using two public datasets: LandSat8 and Moderate-Resolution Imaging Spectroradiometer (MODIS). We conducted experiments on both a server and an edge computing device. Our extensive experiments revealed that LGCNet outperforms other lightweight cloud detection and semantic segmentation methods in terms of performance and computational load.",Cloud detection on remote sensing images (RSIs),lightweight network,semantic information extraction,transformer,Cloud detection on remote sensing images (RSIs),"Zhang, Baoquan",,"Yao, Pengjuan",lightweight network,"Luo, Chuyao","Ye, Yunming",semantic information extraction,transformer,,,,,"Xu, Yong","Li, Xutao","Fang, Hao",,,,,,
Row_1207,"Feng, Yingchao","Sun, Xian","Diao, Wenhui",Height aware understanding of remote sensing images based on cross-task interaction,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,JAN 2023,4,"3D scene perception that provides semantic interpretation for each point has a wide range of practical scenarios. However, existing methods are generally implemented based on 3D point cloud. In this work, we investigate the inverse projection problem from 2D plane image to 3D scene perception and propose a multi-task model to jointly predict height information and semantic categories. Taking into account the commonalities and differences between the two tasks, we have carefully designed the multi-task model to explicitly strengthen the establishment of cross-task correlations. Specifically, the calibration refinement attention (CRA) module is proposed before the classifier heads, incorporating the beneficial information while filtering the inconsistent characteristics among the two tasks. Besides, a spatial structure enhanced (SSE) module is introduced to integrate the spatial structure information into the output features of CRA module through skip-connection. After that, the neighboring pixel affinity (NPA) loss and the soft weighted ordinal (SWO) classification loss for the two tasks are introduced to optimize the direction of the task gradients. At the same time, to validate the effectiveness of the proposed method, a novel metric named height constrained semantic accuracy (HCSA) is proposed to consider the accuracy of semantic segmentation and height estimation jointly. Extensive experiments on Vaihingen, Potsdam, and DFC2019 demonstrate that the proposed method achieves generalization and competitive performance.",3D scene perception,Height estimation,Semantic segmentation,Multi-task learning,Remote sensing,"Li, Jihao",,"Niu, Ruigang",Deep neural network,"Gao, Xin","Fu, Kun",,,,,,,,,,,,,,,
Row_1208,"Ferreira de Carvalho, Osmar Luiz","de Albuquerque, Anesmar Olino","Luiz, Argelica Saiaka",A DATA-CENTRIC APPROACH FOR RAPID DATASET GENERATION USING ITERATIVE LEARNING AND SPARSE ANNOTATIONS,,2023,1,"This study investigates the application of iterative sparse annotations for semantic segmentation in remote-sensing imagery, focusing on minimizing the laborious and expensive data labeling process. By leveraging Geographic Information Systems (GIS), we implemented circular polygon shapefiles to label portions of each class, attributing a value of -1 outside these polygons. The model training used the simplified BSB Aerial Dataset with eight classes. The semantic segmentation model was U- Net architecture with the Efficient-net-B7 backbone and a modified cross-entropy loss function. Our results showed promising improvement, particularly in error-prone classes, with the iterative addition of more samples. This approach suggests a quicker method for dataset creation using sparse, iteratively enhanced annotations. Future work will aim to implement further iterative rounds to approximate the results of continuous labeling, thereby enhancing the efficiency of semantic segmentation in large-scale remote- sensing images.",Semantic segmentation,sparse annotation,iterative learning,remote sensing,,"Guimaraes Ferreira, Pedro Henrique",IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,"Mou, Lichao",,"Guerreiro e Silva, Daniel","de Carvalho Junior, Osmar Abilio",,,,,,,,,,,,,,,
Row_1209,"Xu, Zeyu","Su, Cheng","Zhang, Xiaocan",A semantic segmentation method with category boundary for Land Use and Land Cover (LULC) mapping of Very-High Resolution (VHR) remote sensing image,INTERNATIONAL JOURNAL OF REMOTE SENSING,APR 18 2021,18,"Convolutional Neural Network (CNN) is widely used for semantic segmentation and land-use and land-cover (LULC) mapping of very high-resolution (VHR) remote sensing images. The convolution operation is a powerful method for VHR classification, but the loss of high-frequency detail information caused during its operation decreases the classification accuracy, particularly in the boundary. Thus, it is necessary to supply additional boundary information to the CNN for alleviating this situation. In the classification task (and in LULC mapping), providing more effective information generates a better classification result. Current methods regard the boundary of images as the same category object and process it uniformly, which loses a notable amount of useful information because of the different properties, such as ambiguity and transition, between remote sensing images and their boundaries. Thus, a semantic segmentation method with category boundary for LULC mapping is proposed in this paper. First, a multi-task CNN called the category boundary detection network (CBDN) is designed to extract the boundary information of different category objects. Second, this category boundary and VHR images are used for initial semantic segmentation. Finally, the category boundary and the initial semantic segmentation result (ISSR) are fused to obtain the final LULC map by a two-step strategy, including the explicit fusion and the boundary attention loss function. To verify whether category boundary improved the classification accuracy, a set of comparative experiments were conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam datasets. The method in this paper was compared with a semantic segmentation method with no boundary information and a semantic segmentation method with global boundary. The results showed that the proposed method in this paper achieved good performances in the Vaihingen (overall accuracy (OA) = 0.924, Kappa coefficient (K) = 0.898, mean F1 score (mF1) = 0.896 and mean Intersection over Union (mIoU) = 0.817) and Potsdam datasets (OA = 0.890, K = 0.857, mF1 = 0.923, and mIoU = 0.860) based on the eroded labels.",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_1210,"Pastorino, Martina","Moser, Gabriele","Serpico, Sebastiano B.",Hierarchical Probabilistic Graphical Models and Deep Convolutional Neural Networks for Remote Sensing Image Classification,,2021,2,"The method presented in this paper for semantic segmentation of multiresolution remote sensing images involves convolutional neural networks (CNNs), in particular fully convolutional networks (FCNs), and hierarchical probabilistic graphical models (PGMs). These approaches are combined to overcome the limitations in classification accuracy of CNNs for small or non-exhaustive ground truth (GT) datasets. Hierarchical PGMs, e.g., hierarchical Markov random fields (MRFs), are structured output learning models that exploit information contained at different image scales. This perfectly matches the intrinsically multiscale behavior of the processes of a CNN (e.g., pooling layers). The framework consists of a hierarchical MRF on a quadtree and a planar Markov model on each layer, modeling the interactions among pixels and accounting for both the multiscale and the spatial-contextual information. The marginal posterior mode criterion is used for inference. The adopted FCN is the U-Net and the experimental validation is conducted on the ISPRS 2D Semantic Labeling Challenge Vaihingen dataset, with some modifications to approach the case of scarce GTs and to assess the classification accuracy of the proposed technique. The proposed framework attains a higher recall compared to the considered FCNs, progressively more relevant as the training set is further from the ideal case of exhaustive GTs.",Remote sensing,semantic segmentation,multiresolution,hierarchical PGM,CNN,"Zerubia, Josiane",29TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO 2021),,,,,,,,,,,,,,,,,,,
Row_1211,"Liu, Ming","Ren, Dong","Sun, Hang",Multibranch Unsupervised Domain Adaptation Network for Cross Multidomain Orchard Area Segmentation,REMOTE SENSING,OCT 2022,1,"Although unsupervised domain adaptation (UDA) has been extensively studied in remote sensing image segmentation tasks, most UDA models are designed based on single-target domain settings. Large-scale remote sensing images often have multiple target domains in practical applications, and the simple extension of single-target UDA models to multiple target domains is unstable and costly. Multi-target unsupervised domain adaptation (MTUDA) is a more practical scenario that has great potential for solving the problem of crossing multiple domains in remote sensing images. However, existing MTUDA models neglect to learn and control the private features of the target domain, leading to missing information and negative migration. To solve these problems, this paper proposes a multibranch unsupervised domain adaptation network (MBUDA) for orchard area segmentation. The multibranch framework aligns multiple domain features, while preventing private features from interfering with training. We introduce multiple ancillary classifiers to help the model learn more robust latent target domain data representations. Additionally, we propose an adaptation enhanced learning strategy to reduce the distribution gaps further and enhance the adaptation effect. To evaluate the proposed method, this paper utilizes two settings with different numbers of target domains. On average, the proposed method achieves a high IoU gain of 7.47% over the baseline (single-target UDA), reducing costs and ensuring segmentation model performance in multiple target domains.",semantic segmentation,multi-target unsupervised domain adaptation,remote sensing,orchard area,,"Yang, Simon X.",,,,,,,,,,,,,,,,,,,,
Row_1212,"Nakajima, Masahiro","Watanabe, Toshinori","Koga, Hisashi",COMPRESSION-BASED SEMANTIC-SENSITIVE IMAGE SEGMENTATION: PRDC-SSIS,,2012,2,"This paper proposes PRDC-SSIS, a new compressibility-feature based semantic-sensitive image segmentation method using PRDC. One of the drawbacks of traditional signal (pixel-color) based image segmentation is the poor capability to capture the semantical information contained in the images. Because the semantic information tends to be carried by a set of neighboring pixels, rather than an individual pixel, we divide the image into patches and classify the patches based on their semantical contents. The crucial problem is classifying the patches into groups of similar patches according to their contents, and so we exploit the compressibility feature vector space of PRDC to accomplish this. An application of this method to an EO-image confirmed the proposed scheme can be carried out without any of the human-tailored target object models required by almost all traditional methods.",Image segmentation,semantic-sensitivity,compressibility feature,,,,2012 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),,,,,,,,,,,,,,,,,,,
Row_1213,"Hong, Liang",,,Classification of high resolution remote sensing image based on Geo-ontology and Conditional Random Fields,,2013,0,"The availability of high spatial resolution remote sensing data provides new opportunities for urban land-cover classification. More geometric details can be observed in the high resolution remote sensing image, Also Ground objects in the high resolution remote sensing image have displayed rich texture, structure, shape and hierarchical semantic characters. More landscape elements are represented by a small group of pixels. Recently years, the an object-based remote sensing analysis methodology is widely accepted and applied in high resolution remote sensing image processing. The classification method based on Geo-ontology and conditional random fields is presented in this paper. The proposed method is made up of four blocks: (1) the hierarchical ground objects semantic framework is constructed based on geo-ontology; (2) segmentation by mean-shift algorithm, which image objects are generated. And the mean-shift method is to get boundary preserved and spectrally homogeneous over-segmentation regions;(3) the relations between the hierarchical ground objects semantic and over-segmentation regions are defined based on conditional random fields framework;(4) the hierarchical classification results are obtained based on geo-ontology and conditional random fields. Finally, high-resolution remote sensed image data -GeoEye, is used to testify the performance of the presented method. And the experimental results have shown the superiority of this method to the eCognition method both on the effectively and accuracy, which implies it is suitable for the classification of high resolution remote sensing image.",Classification,high resolution remote sensing image,Geo-ontology,Conditional Random Fields (CRF),,,"MIPPR 2013: REMOTE SENSING IMAGE PROCESSING, GEOGRAPHIC INFORMATION SYSTEMS, AND OTHER APPLICATIONS",,,,,,,,,,,,,,,,,,,
Row_1214,"Zhang, Heqing","Wang, Zhenxin","Song, Jun-feng",Transformer for the Building Segmentation of Urban Remote Sensing,PHOTOGRAMMETRIC ENGINEERING AND REMOTE SENSING,SEP 2022,3,"The automatic extraction of urban buildings based on remote sens-ing images is important for urban dynamic monitoring, planning, and management. The deep learning has significantly helped improve the accuracy of building extraction. Most remote sensing image segmentation methods are based on convolution neural networks, which comprise encoding and decoding structures. However, the convolution operation cannot learn the remote spatial correlation. Herein we propose the Shift Window Attention of building SWAB-net based on the transformer model to solve the semantic segmenta-tion of building objects. Moreover, the shift window strategy was adopted to determine buildings using urban satellite images with 4 m resolution to extract the features of sequence images efficiently and accurately. We evaluated the proposed network on SpaceNet 7, and the results of comprehensive analysis showed that the net-work is conducive for efficient remote sensing image research.",,,,,,"Li, Xueyan",,,,,,,,,,,,,,,,,,,,
Row_1215,"Wang, Jiahao","Liu, Fang","Jiao, Licheng",SSCFNet: A Spatial-Spectral Cross Fusion Network for Remote Sensing Change Detection,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,10,"Convolutional neural networks (CNNs) are data-driven methods that automatically extract the rich information embedded in remote sensing images. However, most current deep learning-based remote sensing image change detection methods prioritize high-level semantic features, while not enough attention is given to low-level semantic features, resulting in the loss of edges and details of the change region. To address this problem, this article constructs a spatial-spectral cross fusion network (SSCFNet), divided into the following three modules: 1) a feature extractor network module; 2) a combined enhancement module; 3) a semantic cross-fusion module. A new combined enhancement strategy is proposed to construct several semantic feature blocks in the combined enhancement module. Different convolution operations are applied to the newly constructed semantic feature blocks in the semantic cross fusion module, and the obtained semantic features at various levels are cross-fused. Experiments show that the proposed SSCFNet outperforms the other six state-of-the-art methods on four publicly available remote sensing image change detection datasets.",Feature extraction,Remote sensing,Semantics,Task analysis,Monitoring,"Wang, Hao",,"Yang, Hua",Semantic segmentation,"Liu, Xu","Li, Lingling",Data mining,Change detection,combined enhancement,convolutional neural network (CNN),cross-fusion,remote sensing image,"Chen, Puhua",,,,,,,,
Row_1216,"Liang, Chenbin","Cheng, Bo","Xiao, Baihua",Semi-/Weakly-Supervised Semantic Segmentation Method and Its Application for Coastal Aquaculture Areas Based on Multi-Source Remote Sensing Images-Taking the Fujian Coastal Area (Mainly Sanduo) as an Example,REMOTE SENSING,MAR 2021,20,"Coastal aquaculture areas are some of the main areas to obtain marine fishery resources and are vulnerable to storm-tide disasters. Obtaining the information of coastal aquaculture areas quickly and accurately is important for the scientific management and planning of aquaculture resources. Recently, deep neural networks have been widely used in remote sensing to deal with many problems, such as scene classification and object detection, and there are many data sources with different spatial resolutions and different uses with the development of remote sensing technology. Thus, using deep learning networks to extract coastal aquaculture areas often encounters the following problems: (1) the difficulty in labeling; (2) the poor robustness of the model; (3) the spatial resolution of the image to be processed is inconsistent with that of the existing samples. In order to fix these problems, this paper proposes a novel semi-/weakly-supervised method, the semi-/weakly-supervised semantic segmentation network (Semi-SSN), and adopts 3 data sources: GaoFen-2 image, GaoFen-1(PMS)image, and GanFen-1(WFV)image with a 0.8 m, 2 m, and 16 m spatial resolution, respectively, and through experiments, we analyze the extraction effect of the model comprehensively. After comparing with other the-state-of-art methods and verifying on an open remote sensing dataset, we take the Fujian coastal area (mainly Sanduo) as the experimental area and employ our method to detect the effect of storm-tide disasters on coastal aquaculture areas, monitor the production, and make the distribution map of coastal aquaculture areas.",coastal aquaculture areas,semantic segmentation,semi-,weakly-supervised learning,GAN,"He, Chenlinqiu",,"Liu, Xunan",conditional adversarial learning,"Jia, Ning","Chen, Jinfen",,,,,,,,,,,,,,,
Row_1217,"Gao, Lin","Liu, Yu","Chen, Xi",CUS3D: A New Comprehensive Urban-Scale Semantic-Segmentation 3D Benchmark Dataset,REMOTE SENSING,MAR 2024,0,"With the continuous advancement of the construction of smart cities, the availability of large-scale and semantically enriched datasets is essential for enhancing the machine's ability to understand urban scenes. Mesh data have a distinct advantage over point cloud data for large-scale scenes, as they can provide inherent geometric topology information and consume less memory space. However, existing publicly available large-scale scene mesh datasets are limited in scale and semantic richness and do not cover a wide range of urban semantic information. The development of 3D semantic segmentation algorithms depends on the availability of datasets. Moreover, existing large-scale 3D datasets lack various types of official annotation data, which hinders the widespread applicability of benchmark applications and may cause label errors during data conversion. To address these issues, we present a comprehensive urban-scale semantic segmentation benchmark dataset. It is suitable for various research pursuits on semantic segmentation methodologies. This dataset contains finely annotated point cloud and mesh data types for 3D, as well as high-resolution original 2D images with detailed 2D semantic annotations. It is constructed from a 3D reconstruction of 10,840 UVA aerial images and spans a vast area of approximately 2.85 square kilometers that covers both urban and rural scenes. The dataset is composed of 152,298,756 3D points and 289,404,088 triangles. Each 3D point, triangular mesh, and the original 2D image in the dataset are carefully labeled with one of the ten semantic categories. Six typical 3D semantic segmentation methods were compared on the CUS3D dataset, with KPConv demonstrating the highest overall performance. The mIoU is 59.72%, OA is 89.42%, and mAcc is 97.88%. Furthermore, the experimental results on the impact of color information on semantic segmentation suggest that incorporating both coordinate and color features can enhance the performance of semantic segmentation. The current limitations of the CUS3D dataset, particularly in class imbalance, will be the primary target for future dataset enhancements.",urban scene understanding,comprehensive urban-scale dataset,semantic segmentation,benchmark dataset,,"Liu, Yuxiang",,"Yan, Shen",,"Zhang, Maojun",,,,,,,,,,,,,,,,
Row_1218,"Igonin, D. M.","Kolganov, P. A.","Tiumentsev, Yu, V",Choosing Hyperparameter Values of the Convolution Neural Network When Solving the Problem of Semantic Segmentation of Images Obtained by Remote Sensing of the Earth's Surface,OPTICAL MEMORY AND NEURAL NETWORKS,OCT 2020,0,"Among the tasks solved by artificial neural networks are the tasks of analyzing objects on the images of the underlying Earth's surface, obtained by the on-board equipment of unmanned aerial vehicle (UAV). For the solution of such problems, the convolutional neural networks (CNN), operating semantic segmentation of the received image, are widely used. In this case, the designer of such networks has to solve the difficult task of selecting hyperparameter values for them. These values' choice is one of the most critical tasks that have to be solved when forming a CNN. Existing attempts to solve this problem are usually based on one of two approaches. The first one involves a set of experiments with different values of hyperparameters of the CNN with learning each of the network variants. These experiments are performed until a CNN with acceptable characteristics is obtained. This approach is simple to implement but does not guarantee a CNN with high performance. The second approach treats the selection of hyperparameter values in the network as an optimization problem. If this problem is successfully solved, it is possible to obtain a CNN with sufficiently high characteristics. However, this task has a significant complexity and also requires a large consumption of computing resources. Images in the form of multidimensional arrays are used as source data to analyze objects on the underlying surface. It means that CNN will contain a significant number of parameters. Accordingly, it will take considerable time to find a suitable CNN by searching for possible hyperparameter values. This paper proposes an alternative approach to the problem of selecting the hyperparameter values of CNN based on the analysis of the processes running in the network. The effectiveness of this approach is demonstrated by solving the problem of semantic segmentation of the underlying surface obtained by remote sensing of the Earth's surface.",artificial neural network,hyperparameters,convolutional neural network,intranetwork process analysis,UAV flight control,,,,target environment,,,Earth's surface,remote sensing,semantic image segmentation,,,,,,,,,,,,
Row_1219,"Zhang, Zebing","Wang, Leiguang","Chen, Yuncheng",Crop Identification of UAV Images Based on an Unsupervised Semantic Segmentation Method,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,0,"Crop identification is a fundamental task in remote sensing image interpretation. The rapid development of unmanned aerial vehicle (UAV) has revolutionized the acquisition of super-high-resolution images. Compared with remote sensing ones, the fact that UAV images are easier to flexibly acquire and contain more information brings opportunities for refined semantic segmentation. Recently deep learning methods have gained substantial popularity in the field of semantic segmentation. However, the practical application of deep learning methods is often hindered by heavy labeling tasks and computational resources. The object-based Markov random field (OMRF) offers a both time and labor cost-effective unsupervised approach. Nevertheless, the high-spatial heterogeneity exhibited by crops in UAV images brings great difficulties to the application of this method. To address these challenges, this letter introduces RA-OMRF model, an unsupervised approach that improves the OMRF model through our newly proposed pre-processing step named Region Aid (RA). RA is used to increase the amount of data for categories with fewer samples to alleviate the problem of high-spatial heterogeneity of ground object categories, thereby improving the performance of the OMRF model. Compared to five deep learning models, our method achieves matching or even higher segmentation accuracy [for instance, an overall accuracy (OA) of 97.43% on the UAV Dataset of Wheats in Shuidao Township (DWST)] in crop identification tasks of UAV images, while reducing time and labor costs.",Autonomous aerial vehicles,Crops,Task analysis,Semantic segmentation,Remote sensing,"Zheng, Chen",,,Deep learning,,,Sensors,Region Aid (RA),semantic segmentation,unmanned aerial vehicle (UAV) image,unsupervised object-based Markov random field (OMRF) model,,,,,,,,,,
Row_1220,"Shaar, Fadi","Yilmaz, Arif","Topcu, Ahmet Ercan",Remote Sensing Image Segmentation for Aircraft Recognition Using U-Net as Deep Learning Architecture,APPLIED SCIENCES-BASEL,MAR 2024,2,"Recognizing aircraft automatically by using satellite images has different applications in both the civil and military sectors. However, due to the complexity and variety of the foreground and background of the analyzed images, it remains challenging to obtain a suitable representation of aircraft for identification. Many studies and solutions have been presented in the literature, but only a few studies have suggested handling the issue using semantic image segmentation techniques due to the lack of publicly labeled datasets. With the advancement of CNNs, researchers have presented some CNN architectures, such as U-Net, which has the ability to obtain very good performance using a small training dataset. The U-Net architecture has received much attention for segmenting 2D and 3D biomedical images and has been recognized to be highly successful for pixel-wise satellite image classification. In this paper, we propose a binary image segmentation model to recognize aircraft by exploiting and adopting the U-Net architecture for remote sensing satellite images. The proposed model does not require a significant amount of labeled data and alleviates the need for manual aircraft feature extraction. The public dense labeling remote sensing dataset is used to perform the experiments and measure the robustness and performance of the proposed model. The mean IoU and pixel accuracy are adopted as metrics to assess the obtained results. The results in the testing dataset indicate that the proposed model can achieve a 95.08% mean IoU and a pixel accuracy of 98.24%.",remote,image,segmentation,aircraft,deep learning,"Alzoubi, Yehia Ibrahim",,,ensemble,,,U-Net,,,,,,,,,,,,,,
Row_1221,"Liang, Chenbin","Li, Weibin","Dong, Yunyun",Single Domain Generalization Method for Remote Sensing Image Segmentation via Category Consistency on Domain Randomization,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,2,"Single domain generalization (SDG) is a more realistic setting than domain generalization (DG) and domain adaptation (DA). It aims to train a domain-agnostic model in the presence of a single source domain to perform well on arbitrary unseen target domains. To facilitate the practical deployment of remote sensing image segmentation in the real world, we propose a novel SDG method, termed category consistency on domain randomization (CCDR). To expand the coverage of a single source domain, CCDR implements a simple yet effective data generation module to perform domain randomization of texture and style information, since texture divergences from different geographical environments or phenological periods, and style divergences from different illumination or weather conditions, are the main causes of the domain shift in remote sensing. In addition to emphasizing inter and intraclass relationships within each domain via multidomain supervised learning, CCDR further draws inspiration from the triple loss to enhance the semantic correlation across the source domain and the generated auxiliary domain, which makes the segmentation model more sensitive to class discriminative information and better adapted to unseen target domains. In comparison to other state-of-the-art SDG methods, CCDR can synthesize the more effective auxiliary domain, and perform more reliable classification on the unseen target domains without the sophisticated training pipeline and cumbersome data generation process. The experimental results on two public remote sensing datasets demonstrate that CCDR has remarkable advantages in remote sensing image segmentation tasks. And the code is publicly available at: https://github.com/LCB1970/CCDR.",Domain expansion,single domain generalization (SDG),style randomization,texture randomization,triplet loss,"Fu, Wenlin",,,,,,,,,,,,,,,,,,,,
Row_1222,"Bai, Ruifeng","Jiang, Shan","Sun, Haijiang",Deep Neural Network-Based Semantic Segmentation of Microvascular Decompression Images,SENSORS,FEB 2021,15,"Image semantic segmentation has been applied more and more widely in the fields of satellite remote sensing, medical treatment, intelligent transportation, and virtual reality. However, in the medical field, the study of cerebral vessel and cranial nerve segmentation based on true-color medical images is in urgent need and has good research and development prospects. We have extended the current state-of-the-art semantic-segmentation network DeepLabv3+ and used it as the basic framework. First, the feature distillation block (FDB) was introduced into the encoder structure to refine the extracted features. In addition, the atrous spatial pyramid pooling (ASPP) module was added to the decoder structure to enhance the retention of feature and boundary information. The proposed model was trained by fine tuning and optimizing the relevant parameters. Experimental results show that the encoder structure has better performance in feature refinement processing, improving target boundary segmentation precision, and retaining more feature information. Our method has a segmentation accuracy of 75.73%, which is 3% better than DeepLabv3+.",microvascular decompression image,semantic segmentation,DeepLabv3+,encoder structure,decoder structure,"Yang, Yifan",,"Li, Guiju",,,,,,,,,,,,,,,,,,
Row_1223,"Yuan, Panli","Zhao, Qingzhan","Zheng, Yuchen",Capturing Small Objects and Edges Information for Cross-Sensor and Cross-Region Land Cover Semantic Segmentation in Arid Areas,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,6,"In the oasis area adjacent to the desert, there is more complex land cover information with rich details, multiscales of interest objects, and blur edge information, which poses some challenges to the semantic segmentation task in remote sensing images (RSIs). In traditional semantic segmentation methods, detailed spatial information is more likely lost in feature extraction stage and the global context information is more effectively integrated into segmentation results. To overcome these land cover semantic segmentation model, FPN_PSA_DLV3+ network, is proposed in an encoder-decoder manner capturing more fine edge and small objects information in RSIs. In the encoder stage, the improved atrous spatial pyramid pooling module extracts the multiscale features, especially small-scale feature details; feature pyramid network (FPN) module realizes better integration of detailed information and semantic information; and the spatial context information at both global and local levels is enhanced by introducing polarized self-attention (PSA) module. For the decoder stage, the FPN_PSA_DLV3+ network further adds a feature fusion branch to concatenate more low-level features. We select Landsat5/7/8 satellite RSIs from the areas of north and south of Xinjiang. Then, three self-annotated time-series datasets with more small objects and fine edges information are constructed by data augmentation. The experimental results show that the proposed method improves the segmentation performance of small targets and edges, and the classification performance increases from 81.55% to 83.10% F1 score and from 72.65% to 74.82% mean intersection over union only using red-green-blue bands. Meanwhile, the FPN_PSA_DLV3+ network shows great generalization in cross region and cross sensor.",Convolutional neural networks (CNNs),fine edge,remote sensing image (RSI),semantic segmentation,small objects,"Wang, Xuewen",,"Hu, Bin",,,,,,,,,,,,,,,,,,
Row_1224,"Li, Ning","Yu, Xiaopeng","Yu, Miao",CMPF-UNet: a ConvNeXt multi-scale pyramid fusion U-shaped network for multi-category segmentation of remote sensing images,GEOCARTO INTERNATIONAL,JAN 1 2024,2,"Most U-shaped convolutional neural network (CNN) methods have the problems of insufficient feature extraction and fail to fully utilize global/multi-scale context information, which makes it difficult to distinguish similar objects and shadow occluded objects in remote sensing images. This article proposes a ConvNeXt multi-scale pyramid fusion U-shaped network (CMPF-UNet). In this work, we first propose a novel backbone network based on ConvNeXt to enhance image feature extraction, and use ConvNeXt bottleneck blocks to reconstruct the decoder. Furthermore, a scale aware pyramid fusion (SAPF) module and Residual Atrous Spatial Pyramid Pooling (RASPP) module are proposed to dynamically fuse the rich multi-scale context information in advanced features. Finally, multiple Global Pyramid Guidance (GPG) modules are embedded in the network, aiming to provide different levels of global context information for the decoder by reconstructing skip-connections. Experiments on the Vaihingen and Potsdam datasets indicate that the proposed CMPF-UNet segmentation achieves more accurate results.",ConvNeXt,remote sensing,semantic segmentation,multi-scale fusion,CMPF-UNet,,,,,,,,,,,,,,,,,,,,,
Row_1225,"Usmani, Munazza","Bovolo, Francesca","Napolitano, Maurizio",Remote Sensing and Deep Learning to Understand Noisy OpenStreetMap,REMOTE SENSING,SEP 2023,1,"The OpenStreetMap (OSM) project is an open-source, community-based, user-generated street map/data service. It is the most popular project within the state of the art for crowdsourcing. Although geometrical features and tags of annotations in OSM are usually precise (particularly in metropolitan areas), there are instances where volunteer mapping is inaccurate. Despite the appeal of using OSM semantic information with remote sensing images, to train deep learning models, the crowdsourced data quality is inconsistent. High-resolution remote sensing image segmentation is a mature application in many fields, such as urban planning, updated mapping, city sensing, and others. Typically, supervised methods trained with annotated data may learn to anticipate the object location, but misclassification may occur due to noise in training data. This article combines Very High Resolution (VHR) remote sensing data with computer vision methods to deal with noisy OSM. This work deals with OSM misalignment ambiguity (positional inaccuracy) concerning satellite imagery and uses a Convolutional Neural Network (CNN) approach to detect missing buildings in OSM. We propose a translating method to align the OSM vector data with the satellite data. This strategy increases the correlation between the imagery and the building vector data to reduce the noise in OSM data. A series of experiments demonstrate that our approach plays a significant role in (1) resolving the misalignment issue, (2) instance-semantic segmentation of buildings with missing building information in OSM (never labeled or constructed in between image acquisitions), and (3) change detection mapping. The good results of precision (0.96) and recall (0.96) demonstrate the viability of high-resolution satellite imagery and OSM for building detection/change detection using a deep learning approach.",remote sensing,crowdsourcing,data reliability,deep learning,change detection,,,,,,,,,,,,,,,,,,,,,
Row_1226,"Zhang, Caiguang","Xiong, Boli","Li, Xiao",Learning Higher Quality Rotation Invariance Features for Multioriented Object Detection in Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,9,"Multioriented object detection, an important yet challenging task because of the bird's-eye-view perspective, complex background, and densely packed objects, is in the spotlight of detection in remote sensing images. Although existing methods have recently experienced substantial progress based on oriented head, they learn little about essential rotation invariance of the objects. In this article, a novel framework is proposed that can learn high-quality rotation invariance features of the multioriented objects by three measures. Given a remote sensing image, the multiscale semantic segmentation feature fusion module first merges the global semantic segmentation features predicted by the semantic segmentation branch and the multiscale features extracted by the backbone with FPN in order to distinguish complex background. Then, the discriminative features are used by rotation mainstream, whose structure is similar to cascade R-CNN and can extract higher quality rotation invariance features and predict more accurate location information by adaptively adjusting the distribution of the samples through progressive intersection over union thresholds. And in order to improve the performance of mainstream to predict more accurate oriented bounding box, the horizontal tributaries that can fully leverage the reciprocal relationship between the oriented detection and horizontal detection were added to the latter two stages. Extensive experiments on three public datasets for remote sensing images, i.e., Gaofen Airplane, HRSC2016, and DOTA demonstrate that without bells and whistles, the proposed method achieves superior performances compared with the existing state-of-the-art methods for multioriented detection. Moreover, our overall system achieves 59.264% mAP of airplane Detection in 2020 Gaofen challenge, ranking third in the final.",Feature extraction,Object detection,Remote sensing,Proposals,Detectors,"Zhang, Jinqian",,"Kuang, Gangyao",Semantics,,,Image segmentation,Cascade,multioriented object detection,remote sensing images,rotation invariance,,,,,,,,,,
Row_1227,"Liu, Lanfa","Tong, Zichen","Cai, Zhanchuan",HierU-Net: A Hierarchical Semantic Segmentation Method for Land Cover Mapping,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,2,"Land cover mapping is crucial for natural resource assessment, urban planning, and sustainable development. Land cover nomenclature often includes two or three hierarchical levels with tree-like hierarchical structures. This study aims to explore these hierarchical relationships and the potential of hierarchical semantic segmentation for land cover mapping. We propose a hierarchical semantic segmentation architecture by taking advantage of dual U-shaped network, named as HierU-Net. The coarse-level result is ingested to the fine-level segmentation functioned as soft constraints. The propagation of error will not be certain. Moreover, we employ a multitask loss function weighted by homoscedastic uncertainty to optimize the training. To evaluate the performance of the proposed method, we create a hierarchical semantic segmentation dataset (HierToulouse), which contains 11 528 samples, including images and land cover labels at two hierarchical levels. The experiments demonstrate that the proposed approach is capable of achieving accurate land cover segmentation at both coarse and fine levels, with segmentation results surpassing those obtained using the flat method.",Land surface,Semantic segmentation,Task analysis,Remote sensing,Semantics,"Wu, Hao",,"Zhang, Rongchun",Training,"Le Bris, Arnaud","Olteanu-Raimond, Ana-Maria",Feature extraction,Hierarchical semantic segmentation,land cover mapping,U-Net,very high-resolution (VHR) imagery,,,,,,,,,,
Row_1228,"Maxwell, Aaron E.","Bester, Michelle S.","Ramezan, Christopher A.",Enhancing Reproducibility and Replicability in Remote Sensing Deep Learning Research and Practice,REMOTE SENSING,NOV 2022,3,"Many issues can reduce the reproducibility and replicability of deep learning (DL) research and application in remote sensing, including the complexity and customizability of architectures, variable model training and assessment processes and practice, inability to fully control random components of the modeling workflow, data leakage, computational demands, and the inherent nature of the process, which is complex, difficult to perform systematically, and challenging to fully document. This communication discusses key issues associated with convolutional neural network (CNN)-based DL in remote sensing for undertaking semantic segmentation, object detection, and instance segmentation tasks and offers suggestions for best practices for enhancing reproducibility and replicability and the subsequent utility of research results, proposed workflows, and generated data. We also highlight lingering issues and challenges facing researchers as they attempt to improve the reproducibility and replicability of their experiments.",deep learning,replicability,reproducibility,semantic segmentation,object detection,,,,,,,,,,,,,,,,,,,,,
Row_1229,"Liu, Mengxi","Zhang, Pengyuan","Shi, Qian",An Adversarial Domain Adaptation Framework With KL-Constraint for Remote Sensing Land Cover Classification,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,7,"Land cover classification plays a crucial role in land resource monitoring and planning. Recently, deep learning-based methods are becoming the dominating method for precise land cover mapping. However, the large-scale application of them is deeply hindered by the domain shift between different images, which is easily caused by illumination, climate, regional divergence, and so on. With the aim to cope with the problem of domain shift, many domain adaptation (DA) methods have been provided and great achievements have been made, especially the newborn adversarial DA, which usually contains a generator and a discriminator. Among these methods, the pixel-level methods are of high memory consumption, whereas feature-level methods are found hard to decode the structured information for semantic segmentation tasks due to the lack of low-dimensional information. Therefore, we propose an adversarial domain adaptation framework with Kullback-Leibler constraint (KL-ADDA) for remote sensing land cover classification. A state-of-the-art (SOTA) semantic segmentation network is utilized as the generator, which directly outputs the segmentation results to the discriminator to retain more low-level information. Besides, a Kullback-Leibler (KL)-divergence is calculated to improve the discriminative ability of the discriminator and thus enhance the generator's performance. Experiments on the international society for photogrammetry and remote sensing (ISPRS) data set and two simulated target data sets have shown the effectiveness of KL-ADDA for DA.",Generators,Remote sensing,Feature extraction,Training,Semantics,"Liu, Mengwei",,,Image segmentation,,,Planning,Domain adaptation (DA),generative adversarial network (GAN),Kullback-Leibler (KL) divergence,land cover classification,semantic segmentation,,,,,,,,,
Row_1230,"Wang, Qixiong","Luo, Xiaoyan","Feng, Jiaqi",Multiscale Prototype Contrast Network for High-Resolution Aerial Imagery Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,5,"Semantic segmentation of high-resolution aerial images is a challenging task on account of complex scene variations and large-scale differences. However, these two issues are inadequately addressed in general semantic segmentation methods. In this article, we propose a multiscale prototype contrast network (MPCNet) to improve the adaptive capability for different scenes and scales. Specifically, a novel multiscale prototype transformer decoder (MPTD) is designed to extract dynamic scene-specific prototypes as pixel classifiers by fusing information from feature maps and learnable class tokens. To exploit cross-scene context information and accommodate the large-scale difference in the aerial image, we build a multiscale prototype memory queue to store these multiscale prototypes during training. Upon the multiscale prototype memory queue, a novel multiscale prototype contrastive loss is proposed to increase object feature discriminability across multiple scales, which brings better consistency of intermediate features and boosts the convergence of the network. Extensive experimental results on three publicly available datasets demonstrate the effectiveness and efficiency of our MPCNet over other state-of-the-art methods. The code is available at https://github.com/qixiong-wang/mmsegmentation-mpcnet.",Prototypes,Semantic segmentation,Transformers,Feature extraction,Decoding,"Zhang, Guangyun",,"Jia, Xiuping",Training,"Yin, Jihao",,Remote sensing,Aerial imagery,deep learning,prototype,semantic segmentation,transformer,,,,,,,,,
Row_1231,"Meng, Yuke","Yuan, Zhanliang","Yang, Jian",Cross-Domain Land Cover Classification of Remote Sensing Images Based on Full-Level Domain Adaptation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,4,"The use of remote sensing images for land cover classification is an important and challenging pixel-level classification task. However, the different distribution of the same land cover categories across different datasets, the accuracy of the classification is significantly reduced when a classification model trained on one dataset is used directly on another dataset. To address the issue, numerous unsupervised domain adaptation (UDA) methods have been proposed. However, the existing UDA methods focus mainly on natural images and are not suited to remote sensing images with large variations in spectral information and texture features. Therefore, we develop a new full-level domain adaptation network (FLDA-NET) applicable for cross-domain land cover classification of remote sensing images. It aligns the source domain and target domain through a two-stage process encompassing image-level, feature-level, and output-level. In stage I, we align at image-level by converting the source domain image to the target domain image style. In stage II, at the feature-level we align the entropy of the two domain features. At the output-level we do not use simple global alignment, but category-level alignment. Furthermore, a self-training strategy based on superpixel segmentation and softmax probability is proposed to further enhance the model's performance on the target domain. Extensive experiments on our proposed FLDA-NET are performed on the Potsdam and Vaihingen datasets and compared with other advanced UDA methods. The outcome demonstrates that this approach greatly improves the ability of cross-domain land cover classification in remote sensing images.",Domain adaptation,generative adversarial network (GAN),land cover,remote sensing (RS),semantic segmentation,"Liu, Peizhuo",,"Yan, Jian",,"Zhu, Hongbo","Ma, Zhigao",,,,,,,"Jiang, Zhenzhao","Zhang, Zhouwei","Mi, Xiaofei",,,,,,
Row_1232,"Zhang, Zhiqi","Lu, Wen","Cao, Jinshan",MKANet: An Efficient Network with Sobel Boundary Loss for Land-Cover Classification of Satellite Remote Sensing Imagery,REMOTE SENSING,SEP 2022,16,"Land cover classification is a multiclass segmentation task to classify each pixel into a certain natural or human-made category of the earth's surface, such as water, soil, natural vegetation, crops, and human infrastructure. Limited by hardware computational resources and memory capacity, most existing studies preprocessed original remote sensing images by downsampling or cropping them into small patches less than 512 x 512 pixels before sending them to a deep neural network. However, downsampling incurs a spatial detail loss, renders small segments hard to discriminate, and reverses the spatial resolution progress obtained by decades of efforts. Cropping images into small patches causes a loss of long-range context information, and restoring the predicted results to their original size brings extra latency. In response to the above weaknesses, we present an efficient lightweight semantic segmentation network termed MKANet. Aimed at the characteristics of top view high-resolution remote sensing imagery, MKANet utilizes sharing kernels to simultaneously and equally handle ground segments of inconsistent scales, and also employs a parallel and shallow architecture to boost inference speed and friendly support image patches more than 10x larger. To enhance boundary and small segment discrimination, we also propose a method that captures category impurity areas, exploits boundary information, and exerts an extra penalty on boundaries and small segment misjudgments. Both visual interpretations and quantitative metrics of extensive experiments demonstrate that MKANet obtains a state-of-the-art accuracy on two land-cover classification datasets and infers 2x faster than other competitive lightweight networks. All these merits highlight the potential of MKANet in practical applications.",semantic segmentation,convolutional neural network,land-cover classification,,,"Xie, Guangqi",,,,,,,,,,,,,,,,,,,,
Row_1233,"Yao, Hongtai","Zhang, Min","Wang, Bingxue",A Top-down Application of Multi-resolution Markov Random Fields with Bilateral Information in Semantic Segmentation of Remote Sensing Images,,2018,3,"This paper presents a new multi-resolution Markov Random Field (MRF) method for semantic segmentation of remote sensing images. The main contribution of this paper is to propose a new method of information interaction between the scales so that macroscopic information and microscopic information can be captured on each scale. First, we established a multi-scale structure. Second, in the modeling process of label field in each scale, we not only consider the spatial information between the pixels of the layer. But also take the spatial interaction between this layer and its upper and lower layers into account. Finally, using the most classic the maximum a posterior (MAP) criteria, start from the top level and solve it layer by layer. Experiments were performed on texture image, synthetic geographic image and remote sensing image. These experiments show that the proposed method provides a better performance than other Markov-based methods. (The accuracy increases by about 2%).",bilateral information,multi-resolution,"top-down,random field",Segmentation,,,2018 26TH INTERNATIONAL CONFERENCE ON GEOINFORMATICS (GEOINFORMATICS 2018),,,,,,,,,,,,,,,,,,,
Row_1234,"Sech, Gregory","Soleni, Paolo","Verschoof-van der Vaart, Wouter B.",TRANFER LEARNING OF SEMANTIC SEGMENTATION METHODS FOR IDENTIFYING BURIED ARCHAEOLOGICAL STRUCTURES ON LIDAR DATA,,2023,0,"When applying deep learning to remote sensing data in archaeological research, a notable obstacle is the limited availability of suitable datasets for training models. The application of transfer learning is frequently employed to mitigate this drawback. However, there is still a need to explore its effectiveness when applied across different archaeological datasets. This paper compares the performance of various transfer learning configurations using two semantic segmentation deep neural networks on two LiDAR datasets. The experimental results indicate that transfer learning-based approaches in archaeology can lead to performance improvements, although a systematic enhancement has not yet been observed. We provide specific insights about the validity of such techniques that can serve as a baseline for future works.",Transfer learning,LIDAR,Semantic segmentation,Deep learning,Cultural heritage,"Kokalj, Ziga",IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,"Traviglia, Arianna",,"Fiorucci, Marco",,,,,,,,,,,,,,,,
Row_1235,"Li, Ge","Li, Lingling","Zhu, Hao",Adaptive Multiscale Deep Fusion Residual Network for Remote Sensing Image Classification,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,NOV 2019,64,"With the development of remote sensing imaging technology, remote sensing images with high-resolution and complex structure can be acquired easily. The classification of remote sensing images is always a hot and challenging problem. In order to improve the performance of remote sensing image classification, we propose an adaptive multiscale deep fusion residual network (AMDF-ResNet). The AMDF-ResNet consists of a backbone network and a fusion network. The backbone network including several residual blocks generates multiscale hierarchy features, which contain semantic information from low to high levels. In the fusion network, the adaptive feature fusion module proposed can emphasize useful information and suppress useless information by learning the weights, which represent the importance of the features. The AMDF-ResNet can make full use of the multiscale hierarchy features and the extracted feature is discriminative. In addition, we propose a samples selection method named important samples selection strategy (ISSS). Based on superpixels segmentation result, gradient information and spatial distribution are used as two references to determine the selection numbers and select samples. Compared with the random selection strategy, training samples selected by ISSS are more representative and diverse. The experimental results on four data sets demonstrate that the AMDF-ResNet and ISSS are effective.",Feature extraction,Training,Semantics,Image segmentation,Adaptive systems,"Liu, Xu",,"Jiao, Licheng",Hyperspectral sensors,,,Deep learning (DL),feature extraction,image classification,multispectral (MS) images,remote sensing,,,,,,,,,,
Row_1236,"Du, Shouhang","Du, Shihong","Liu, Bo",Mapping large-scale and fine-grained urban functional zones from VHR images using a multi-scale semantic segmentation network and object based approach,REMOTE SENSING OF ENVIRONMENT,AUG 2021,82,"Urban functional zones (UFZs) are essential for characterizing both urban spatial configurations and socioeconomic properties and monitoring urbanization process, thus UFZs are fundamental to urban planning, management and renewal. Although many efforts in remote sensing field have been made to map UFZs, largescale and fine-grained UFZ maps required by a broad range of urban applications are still unavailable. Existing methods generally rely on pre-determined mapping units, such as image tiles and road blocks, which significantly limit the mapping quality and the automation degree of mapping UFZs. Given that UFZs are composed of diverse geographic objects, this study proposes a novel object-based UFZ mapping method using very-high-resolution (VHR) remote sensing images. First, a multi-scale semantic segmentation network that achieves pixel-wised predictions is proposed to predict urban-functions for geographic objects by capturing multi-scale contextual information. Afterwards, a conditional random field (CRF) framework is designed to regroup objects into UFZs to produce the final UFZ map, wherein road vectors are incorporated to restrict the procedure. The presented object-as-analysis-unit scheme conquers the drawbacks of mapping-unit pre-determination and the semantic segmentation model provides accurate function information for objects, thus they can be applied for producing large-scale and fine-grained UFZ maps. In the experiment, the proposed method is evaluated by producing UFZ maps for Beijing and Shanghai, China, and competitive results with overall accuracy of 91.6% and 89.1% are achieved, respectively. Finally, the generated UFZ maps are utilized to analyze the urbanfunction structures of the two cities. The proposed method can be regarded as a significant development that appears to be promising and practical for mapping UFZ maps for real-world urban applications.",Urban functional zones,Land use,OBIA,Image semantic segmentation,CRF,"Zhang, Xiuyuan",,,VHR images,,,,,,,,,,,,,,,,,
Row_1237,"Sun, Wei","Sheng, Wenyi","Zhou, Rong",Deep edge enhancement-based semantic segmentation network for farmland segmentation with satellite imagery,COMPUTERS AND ELECTRONICS IN AGRICULTURE,NOV 2022,7,"The participation of insured smallholders and involvement of related requirements have been emerging and increasing with the popularization and promotion of agricultural insurance. Compared with traditional field surveys, remote sensing and deep learning technologies, with their rapid development in recent years, have provided an automatic and effective means for underwriting control in the agricultural insurance industry. Moreover, ""smallholder-wise underwriting "" is intensively expected by the local government and insurance companies to ensure precise claim settlements. In this process, farmland segmentation is a crucial and fundamental step for smallholder farmland claim settlements. However, segmentation of smallholder farmland parcels from satellite images remains a challenge, as the ridges and roads between farmland parcels are usually narrow and display confusing characteristics with their neighboring farmlands. This challenge causes undersegmentation on the farmland boundaries, leading to inaccurate and incomplete farmland parcel recognition. In this study, we aim to solve this problem by proposing a novel Deep Edge Enhancement Semantic Segmentation Network to refine parcels' boundary segmentation and improve the closure level of the farmland segmentation. We designed a framework for farmland edge enhancement through pseudo road label generation and model fusion. The proposed network aggregates feature extraction for ridge and road segmentation to improve its performance on farmland parcel recognition. Furthermore, we found that the commonly used image segmentation evaluation metric, such as pixel-wise-based mean Intersection over Union, cannot objectively reflect the effectiveness of the smallholder farmland segmentation. Therefore, we proposed two block-wise farmland evaluation metrics that are consistent with the practical evaluation rules and requirements of farmland segmentation at the parcel level. We implemented experiments in the study area of Jiaxiang County in northern China using the GaoFen-1 4-band multispectral images (red, green, blue, and near-infrared) at a spatial resolution of 2 m. Experimental results demonstrated the effectiveness of our method, which outperformed the baseline DeepLabv3+ network.",Agricultural insurance,Farmland segmentation,Remote sensing,Satellite imagery,Deep semantic learning,"Zhu, Yuxia",,"Chen, Ailian",Deep edge enhancement learning,"Zhao, Sijian","Zhang, Qiao",Block -wise evaluation metric,,,,,,,,,,,,,,
Row_1238,"Lv, Ning","Ma, Hongxiang","Chen, Chen",Remote Sensing Data Augmentation Through Adversarial Training,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,28,"The lack of remote sensing images and poor quality limit the performance improvement of follow-up research such as remote sensing interpretation. In this article, a generative adversarial network (GAN) is proposed for data augmentation of remote sensing images abstracted from Jiangxi and Anhui Provinces in China, i.e., deeply supervised GAN (D-sGAN). D-sGAN can generate high-quality images that are rich in changes, greatly shorten the generation time, and provide data support for applications such as semantic interpretation of remote sensing images. First, to modulate the layer activations, a downsampling scheme is designed based on the segmentation map. Then, the architecture of the generator is Unet++ with the proposed downsampling module. Next, the generator of this net is deeply supervised by the discriminator using deep convolutional neural network. This article further proved that the proposed downsampling module and the dense connection characteristics of UNet++ are significantly beneficial to the retention of semantic information of remote sensing images. Numerical results demonstrated that the images generated by D-sGAN could be used to improve accuracy of the segmentation network, with the faster generation speed compared to the CoGAN, SimGAN, and CycleGAN models. Furthermore, the remote sensing data generated by the model helped the interpretation network to increase the accuracy by 9%, meeting actual generation requirements.",Remote sensing,Generative adversarial networks,Generators,Semantics,Training,"Pei, Qingqi",,"Zhou, Yang",Task analysis,"Xiao, Fenglin","Li, Ji",Image synthesis,Data augmentation,deep supervision,downsampling,GAN,,,,,,,,,,
Row_1239,"Han, Bingnan","Yin, Jihao","Luo, Xiaoyan",Multibranch Spatial-Channel Attention for Semantic Labeling of Very High-Resolution Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,DEC 2021,4,"Very high-resolution (VHR) remote sensing images can provide fine but sometimes trivial ground object details; thus, the semantic labeling of VHR images is a challenging task. To improve the VHR labeling performance, spatial multiscale information and channel attention have been employed recently. However, the exploitation of global object features is still limited, which leads to the loss of capturing within-class variation from location to location. In this letter, we present a multibranch spatial-channel attention (MSCA) model to efficiently extract global dependency and combine it with multiscale and channel attention methods. In the spatial multiscale attention block, a multibranch feature fusion model is established to exploit the global relationship captured by self-attention and the multiscale correlation learned from dilated convolutions. To alleviate the computational cost of pixel-by-pixel self-attention operation, a spatial pyramid compressing method is also designed. In the channel attention block, average and max global pooling strategies are applied, respectively, in two channel attention branches to generalize global information from different perspectives. Those two blocks are then adaptively united by learnable weighting parameters. Experiments on two VHR image data sets demonstrate that the proposed network can yield better performance in comparison with state-of-the-art labeling methods tested.",Labeling,Semantics,Feature extraction,Computational modeling,Computational efficiency,"Jia, Xiuping",,,Remote sensing,,,Spatial resolution,Attention mechanism,convolutional neural networks (CNNs),remote sensing (RS),semantic labeling,very high-resolution (VHR) images,,,,,,,,,
Row_1240,"Wang, Wei","Cheng, Yong","Ren, Zhoupeng",A Novel Hybrid Method for Urban Green Space Segmentation from High-Resolution Remote Sensing Images,REMOTE SENSING,DEC 2023,1,"The comprehensive use of high-resolution remote sensing (HRS) images and deep learning (DL) methods can be used to further accurate urban green space (UGS) mapping. However, in the process of UGS segmentation, most of the current DL methods focus on the improvement of the model structure and ignore the spectral information of HRS images. In this paper, a multiscale attention feature aggregation network (MAFANet) incorporating feature engineering was proposed to achieve segmentation of UGS from HRS images (GaoFen-2, GF-2). By constructing a new decoder block, a bilateral feature extraction module, and a multiscale pooling attention module, MAFANet enhanced the edge feature extraction of UGS and improved segmentation accuracy. By incorporating feature engineering, including false color image and the Normalized Difference Vegetation Index (NDVI), MAFANet further distinguished UGS boundaries. The UGS labeled datasets, i.e., UGS-1 and UGS-2, were built using GF-2. Meanwhile, comparison experiments with other DL methods are conducted on UGS-1 and UGS-2 to test the robustness of the MAFANet network. We found the mean Intersection over Union (MIOU) of the MAFANet network on the UGS-1 and UGS-2 datasets was 72.15% and 74.64%, respectively; outperforming other existing DL methods. In addition, by incorporating false color image in UGS-1, the MIOU of MAFANet was improved from 72.15% to 74.64%; by incorporating vegetation index (NDVI) in UGS-1, the MIOU of MAFANet was improved from 72.15% to 74.09%; and by incorporating false color image and the vegetation index (NDVI) in UGS-1, the MIOU of MAFANet was improved from 72.15% to 74.73%. Our experimental results demonstrated that the proposed MAFANet incorporating feature engineering (false color image and NDVI) outperforms the state-of-the-art (SOTA) methods in UGS segmentation, and the false color image feature is better than the vegetation index (NDVI) for enhancing green space information representation. This study provided a practical solution for UGS segmentation and promoted UGS mapping.",urban green space,deep learning,high-resolution remote sensing images,multiscale pooling attention,feature engineering,"He, Jiaxin",,"Zhao, Yingfen",,"Wang, Jun","Zhang, Wenjie",,,,,,,,,,,,,,,
Row_1241,"Li, Bo","Xu, Jin","Chu, Lilin",Oil Film Semantic Segmentation Method in X-Band Marine Radar Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,1,"Effective oil-spill monitoring is critical for timely response to minimize the impact on the environment. In response to the difficulty in extracting suspected oil films from marine radar images, a semantic segmentation method was proposed. In the preprocessing of training samples in similar scene, a slicing solution was used to compensate for the small sample of original data. The U-Net semantic segmentation network was used to classify oil film into two categories: real and suspected. Existing mainstream marine radar oil-spill identification methods were compared. Experimental results demonstrate that the proposed method achieves more reliability in oil-spill semantic segmentation. It can provide real-time information for oil-spill emergency response and disaster assessment.",Marine radar,oil spill,U-Net,,,"Yang, Yuqiang",,"Huang, Xili",,"Liu, Peng",,,,,,,,,,,,,,,,
Row_1242,"Han, Xiaoxiang","Liu, Yiman","Liu, Gang",LOANet: a lightweight network using object attention for extracting buildings and roads from UAV aerial remote sensing images,PEERJ COMPUTER SCIENCE,JUL 11 2023,8,"Semantic segmentation for extracting buildings and roads from uncrewed aerial vehicle (UAV) remote sensing images by deep learning becomes a more efficient and convenient method than traditional manual segmentation in surveying and mapping fields. In order to make the model lightweight and improve the model accuracy, a lightweight network using object attention (LOANet) for buildings and roads from UAV aerial remote sensing images is proposed. The proposed network adopts an encoder-decoder architecture in which a lightweight densely connected network (LDCNet) is developed as the encoder. In the decoder part, the dual multi-scale context modules which consist of the atrous spatial pyramid pooling module (ASPP) and the object attention module (OAM) are designed to capture more context information from feature maps of UAV remote sensing images. Between ASPP and OAM, a feature pyramid network (FPN) module is used to fuse multi-scale features extracted from ASPP. A private dataset of remote sensing images taken by UAV which contains 2431 training sets, 945 validation sets, and 475 test sets is constructed. The proposed basic model performs well on this dataset, with only 1.4M parameters and 5.48G floating point operations (FLOPs), achieving excellent mean Intersection-over-Union (mIoU). Further experiments on the publicly available LoveDA and CITY-OSM datasets have been conducted to further validate the effectiveness of the proposed basic and large model, and outstanding mIoU results have been achieved. All codes are available on https://github.com/GtLinyer/LOANet.",Remote sensing image,Semantic segmentation,Context features,Lightweight network,Object attention,"Lin, Yuanjie",,"Liu, Qiaohong",,,,,,,,,,,,,,,,,,
Row_1243,"Voulgaris, Georgios","Philippides, Andrew","Quadrianto, Novi",WATER PHYSICS AWARE SEMANTIC SEGMENTATION THROUGH TEXTURE-BIASED U-NET ARCHITECTURES,,2023,0,"Reliably identifying water bodies is an important step in automating the identification of potable water. This work therefore investigates water scene segmentation, with a focus on water's physical properties which give it features that distinguish it from other elements. We propose a physics-aware water segmentation method, in which we adapt both a U-Net and MACUNet model so that they are biased towards texture information by using a Gabor convolutional layer as the first layer, combined with a mixture of average and maximum pooling layers in the encoder. To train the networks, a dataset of annotated water images was created, comprising water bodies from diverse light, atmospheric, geographic, and environmental conditions. We show that the physics-aware, texture-biased models result in effective water segmentation. We then test the texture-biased models using 3 standard aerial scene segmentation benchmarks and show that in all cases they outperform the standard U-Net or MACUNet models. We suggest this is because the new models are sensitive to small variations in texture, meaning they can extract information from scenes affected by light, canopy or shadows.",Semantic Segmentation,U-Net,Gabor,Aerial Images,Remote Sensing,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,Water,,,,,,,,,,,,,,,,,
Row_1244,"Angelis, G. F.","Domi, A.","Zamichos, A.",On The Exploration of Vision Transformers in Remote Sensing Building Extraction,,2022,3,"Extracting building information using artificial sources from satellite and remote sensing data has become a valuable tool for a variety of applications such as, damage detection, infrastructure construction, land use management, and building energy consumption estimation. Recently, deep learning methods have made much progress in extracting building footprints from remote sensing (RS) imagery but many challenges persist. Convolutional Neural Networks (CNN) have been the fundamental way to extract segments of buildings, but they are not able to capture accurately the global connectivity of representations. To overcome this boundary, researchers proposed Vision Transformers which achieved state of the art accuracy in computer vision tasks [1]. Especially in building extraction from RS imagery several architectures that based on Transformers have been proposed lately. However the experimental scenarios make them difficult to compare and extract meaningful conclusions. Considering this, the current manuscript presents an analytical comparison between diverse Transformer-based semantic segmentation architectures, aiming to observe their predictive performance and computational efficiency in three building footprint extraction RS imagery datasets. Moreover, this work introduces four new architectures which are extensively compared with literature baselines.",Remote Sensing,Transformers,building,extraction,segmentation,"Tsourma, M.",2022 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM),"Drosou, A.",,"Tzovaras, D.",,,,,,,,,,,,,,,,
Row_1245,"Cao, Zhiying","Diao, Wenhui","Sun, Xian","C3Net: Cross-Modal Feature Recalibrated, Cross-Scale Semantic Aggregated and Compact Network for Semantic Segmentation of Multi-Modal High-Resolution Aerial Images",REMOTE SENSING,FEB 2021,16,"Semantic segmentation of multi-modal remote sensing images is an important branch of remote sensing image interpretation. Multi-modal data has been proven to provide rich complementary information to deal with complex scenes. In recent years, semantic segmentation based on deep learning methods has made remarkable achievements. It is common to simply concatenate multi-modal data or use parallel branches to extract multi-modal features separately. However, most existing works ignore the effects of noise and redundant features from different modalities, which may not lead to satisfactory results. On the one hand, existing networks do not learn the complementary information of different modalities and suppress the mutual interference between different modalities, which may lead to a decrease in segmentation accuracy. On the other hand, the introduction of multi-modal data greatly increases the running time of the pixel-level dense prediction. In this work, we propose an efficient C3Net that strikes a balance between speed and accuracy. More specifically, C3Net contains several backbones for extracting features of different modalities. Then, a plug-and-play module is designed to effectively recalibrate and aggregate multi-modal features. In order to reduce the number of model parameters while remaining the model performance, we redesign the semantic contextual extraction module based on the lightweight convolutional groups. Besides, a multi-level knowledge distillation strategy is proposed to improve the performance of the compact model. Experiments on ISPRS Vaihingen dataset demonstrate the superior performance of C3Net with 15x fewer FLOPs than the state-of-the-art baseline network while providing comparable overall accuracy.",semantic segmentation,multi-modal learning,deep neural network design,,,"Lyu, Xiaode",,"Yan, Menglong",,"Fu, Kun",,,,,,,,,,,,,,,,
Row_1246,"Kong, Yingying","Zhang, Bowen","Yan, Biyuan",Affiliated Fusion Conditional Random Field for Urban UAV Image Semantic Segmentation,SENSORS,FEB 2020,7,"Unmanned aerial vehicles (UAV) have had significant progress in the last decade, which is applied to many relevant fields because of the progress of aerial image processing and the convenience to explore areas that men cannot reach. Still, as the basis of further applications such as object tracking and terrain classification, semantic image segmentation is one of the most difficult challenges in the field of computer vision. In this paper, we propose a method for urban UAV images semantic segmentation, which utilizes the geographical information of the region of interest in the form of a digital surface model (DSM). We introduce an Affiliated Fusion Conditional Random Field (AF-CRF), which combines the information of visual pictures and DSM, and a multi-scale strategy with attention to improve the segmenting results. The experiments show that the proposed structure performs better than state-of-the-art networks in multiple metrics.",semantic image segmentation,CRF,DSM,UAV,remote sensing,"Liu, Yanjuan",,"Leung, Henry",,"Peng, Xiangyang",,,,,,,,,,,,,,,,
Row_1247,"Wang, Haoyu","Li, Xiaofeng",,"Expanding Horizons: U-Net Enhancements for Semantic Segmentation, Forecasting, and Super-Resolution in Ocean Remote Sensing",JOURNAL OF REMOTE SENSING,AUG 5 2024,2,"Originally designed for medical segmentation, the U-Net model excels in ocean remote sensing for segmentation, forecasting, and image enhancement. We propose enhancements like attention mechanisms, knowledge-data integration, and diffusion models to improve small target detection, ocean phenomena forecasting, and image super-resolution, expanding U-Net's application and support in oceanographic research.",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_1248,"Zhang, Jie","Shao, Mingwen","Qiao, Yuanjian",Enhancing Efficient Global Understanding Network With CSWin Transformer for Urban Scene Images Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,2,"The global context is crucial to the semantic segmentation task of remote sensing (RS) urban scene imagery since objects have large size variations, high similarity, and mutual occlusion. However, the existing methods for extracting global context information have limitations when directly applied to very high-resolution RS images, mainly in high complexity of computation and memory consumption. To alleviate this limitation, we propose a novel Efficient Global Understanding semantic segmentation Network (EGUNet) to extract global context information efficiently for applicability to RS images. Specifically, EGUNet is a hybrid U-shaped architecture of convolutional neural networks (CNNs) and Transformer in which the encoder uses the CSWin Transformer to capture global semantic information, and the decoder uses the CNNs structure to recover local detail information. Thus, the proposed EGUNet has a powerful global extraction capability and local position information recovery capability. In addition, three effective modules are proposed to improve the segmentation accuracy to make EGUNet more applicable for urban scene image segmentation tasks. First, a feature adaptive fusion module is introduced in the decoder to improve the fusion of the deep semantics and the location detail features. Second, an adaptive atrous-spatial pyramid pooling is designed at the skip connections to enhance the multiscale understanding of high-level semantic context. Finally, we introduce a lightweight enhanced segmentation head to utilize the information from each decoder stage for segmentation. Extensive experimental results on ISPRS Vaihingen and Potsdam datasets demonstrate the exceptional segmentation accuracy of EGUNet, outperforming the state-of-the-art methods.",CSWin Transformer,global information extraction,remote sensing (RS) urban scene imagery,semantic segmentation,,"Cao, Xiangyong",,,,,,,,,,,,,,,,,,,,
Row_1249,"Song, Hunsoo","Yang, Lexie","Jung, Jinha",Self-Filtered Learning for Semantic Segmentation of Buildings in Remote Sensing Imagery With Noisy Labels,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,4,"Not all building labels for training improve the performance of the deep learning model. Some labels can be falsely labeled or too ambiguous to represent their ground truths, resulting in poor performance of the model. For example, building labels in OpenStreetMap (OSM) and Microsoft Building Footprints (MBF) are publicly available training sources that have great potential to train deep models, but directly using those labels for training can limit the model's performance as their labels are incomplete and inaccurate, called noisy labels. This article presents self-filtered learning (SFL) that helps a deep model learn well with noisy labels for building extraction in remote sensing images. SFL iteratively filters out noisy labels during the training process based on loss of samples. Through a multiround manner, SFL makes a deep model learn progressively more on refined samples from which the noisy labels have been removed. Extensive experiments with the simulated noisy map as well as real-world noisy maps, OSM and MBF, showed that SFL can improve the deep model's performance in diverse error types and different noise levels.",Building extraction,deep learning,label noise,semantic segmentation,weakly supervised learning (WSL),,,,,,,,,,,,,,,,,,,,,
Row_1250,"Chen, Ziyi","Wang, Cheng","Li, Jonathan",Adaboost-like End-to-End multiple lightweight U-nets for road extraction from optical remote sensing images,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,AUG 2021,43,"Road extraction from optical remote sensing images has many important application scenarios, such as navigation, automatic driving and road network planning, etc. Current deep learning based models have achieved great successes in road extraction. Most deep learning models improve abilities rely on using deeper layers, resulting to the obese of the trained model. Besides, the training of a deep model is also difficult, and may be easy to fall into over fitting. Thus, this paper studies to improve the performance through combining multiple lightweight models. However, in fact multiple isolated lightweight models may perform worse than a deeper and larger model. The reason is that those models are trained isolated. To solve the above problem, we propose an Adaboost-like End-To-End Multiple Lightweight U-Nets model (AEML U-Nets) for road extraction. Our model consists of multiple lightweight U-Net parts. Each output of prior U-Net is as the input of next U-Net. We design our model as multiple-objective optimization problem to jointly train all the U-Nets. The approach is tested on two open datasets (LRSNY and Massachusetts) and Shaoshan dataset. Experimental results prove that our model has better performance compared with other state-of-the-art semantic segmentation methods.",Road extraction,Remote sensing image,Semantic segmentation,,,"Fan, Wentao",,"Du, Jixiang",,"Zhong, Bineng",,,,,,,,,,,,,,,,
