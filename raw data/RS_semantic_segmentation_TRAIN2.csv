,author1,author2,author3,title,journal/book,publish time,citation,abstract,keyword1,keyword2,keyword3,keyword4,author4,author5,author6,author7,keyword5,keyword6,keyword7,keyword8,keyword9,keyword10,keyword11,keyword12,author8,conference,author9,author10,author11,keyword13,keyword14,keyword15,keyword16,keyword17,keyword18,keyword19,keyword20,keyword21,keyword22,keyword23,author12
Row_401,"Liu, Tao","Cheng, Shuli","Yuan, Jian",Category-Based Interactive Attention and Perception Fusion Network for Semantic Segmentation of Remote Sensing Images,REMOTE SENSING,OCT 2024,0,"With the development of CNNs and the application of transformers, the segmentation performance of high-resolution remote sensing image semantic segmentation models has been significantly improved. However, the issue of category imbalance in remote sensing images often leads to the model's segmentation ability being biased towards categories with more samples, resulting in suboptimal performance for categories with fewer samples. To make the network's learning and representation capabilities more balanced across different classes, in this paper we propose a category-based interactive attention and perception fusion network (CIAPNet), where the network divides the feature space by category to ensure the fairness of learning and representation for each category. Specifically, the category grouping attention (CGA) module utilizes self-attention to reconstruct the features of each category in a grouped manner, and optimize the foreground-background relationship and its feature representation for each category through the interactive foreground-background relationship optimization (IFBRO) module therein. Additionally, we introduce a detail-aware fusion (DAF) module, which uses shallow detail features to complete the semantic information of deep features. Finally, a multi-scale representation (MSR) module is deployed for each class in the CGA and DAF modules to enhance the description capability of different scale information for each category. Our proposed CIAPNet achieves mIoUs of 54.44%, 85.71%, and 87.88% on the LoveDA urban-rural dataset, and the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam urban datasets, respectively. Compared with current popular methods, our network not only achieves excellent performance but also demonstrates outstanding class balance.",remote images,semantic segmentation,detail-aware fusion,category grouping attention,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_402,"Zhao, Boyu","Zhang, Mengmeng","Wang, Jianbu",Multiple Attention Network for Spartina alterniflora Segmentation Using Multitemporal Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,11,"The semantic segmentation of multitemporal remote sensing images to construct wetland land surface coverage is the basis for the perception and dynamic modeling of geographic scenes. However, the segmentation of Spartina alterniflora in remote sensing images on wetlands faces the problems such as a low level of cooperative interpretation in multitemporal images and high fragmentation in the distribution of S. alterniflora. To solve the issues, a multiple attention network (MARNet) based on transfer learning is proposed. The method is designed with a plug-and-play attention module to enhance the learning of vegetation features and improve the network's ability to focus on small areas of S. alterniflora. At the same time, MARNet designs the transfer learning architecture from both interdomain alignment and intradomain adaptation perspectives, aligning the statistical distribution by using the maximum mean difference (MMD) between the source and target domains (TDs), and entropy minimization within the domain of the TD to enhance the high confidence prediction of this domain. In addition, since the samples have a serious imbalance problem, redundant cutting and splicing steps are employed for the prediction results to prevent the poor edge prediction of some image blocks. Experimental results on three cross-year RSI datasets demonstrate that the proposed MARNet performs significantly better than other networks and is able to extract S. alterniflora in wetlands more accurately.",Remote sensing,Feature extraction,Wetlands,Kernel,"Song, Xiukai","Gui, Yuanyuan","Zhang, Yuxiang","Li, Wei",Semantic segmentation,Transfer learning,Semantics,Attention block,deep learning,multitemporal remote sensing images,Spartina alterniflora segmentation,transfer learning,,,,,,,,,,,,,,,,,
Row_403,"Nuradili, Pakezhamu","Zhou, Ji","Zhou, Xiangbing",UAV Remote-Sensing Image Semantic Segmentation Strategy Based on Thermal Infrared and Multispectral Image Features,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,SEP 2023,4,"The availability of high-resolution imagery resources for semantic segmentation research has expanded significantly due to the rapid development of remote-sensing technology utilizing unmanned aerial vehicles (UAVs). These images provide researchers with a more accurate view of the region of interest and allow for more detailed analysis and interpretation of the images. However, semantic segmentation based on UAV remote-sensing imagery still faces new challenges in deriving ground objects. In contrast to the commonly used multispectral (MS) imagery, thermal infrared (TIR) imagery can record the emission of ground objects, making the temperature characteristics of TIR imagery and the color characteristics of MS imagery complementary. These two approaches can be used synergistically to provide more comprehensive image information. On this basis, we propose a strategy for semantic segmentation of UAV images by utilizing both TIR and MS image features. The approach combines principal component analysis (PCA) transformation with a deep learning semantic segmentation network, namely, Deeplv3. The effectiveness of the proposed strategy is evaluated by comparing it with both traditional supervised classification algorithms and deep learning algorithms. According to the results, the proposed strategy exhibits greater robustness, achieving a mean pixel accuracy (MPA) of 92.8% and a mean intersection over union (MIOU) of 73.5%. These results outperform several classical deep learning semantic segmentation algorithms that were also evaluated. The proposed strategy would be beneficial to promote the development of semantic segmentation technology for UAV remote-sensing images.",Semantic segmentation,Remote sensing,Feature extraction,Deep learning,"Ma, Jin","Wang, Ziwei","Meng, Lingxuan","Tang, Wenbin",Classification algorithms,Autonomous aerial vehicles,Semantics,image semantic segmentation,multispectral (MS) image,principal component analysis (PCA) image fusion,thermal infrared (TIR) image,unmanned aerial vehicle (UAV) remote sensing,"Meng, Yizhen",,,,,,,,,,,,,,,,
Row_404,"Liu, Jiang","Cheng, Shuli","Du, Anyu",Multi-View Feature Fusion and Rich Information Refinement Network for Semantic Segmentation of Remote Sensing Images,REMOTE SENSING,SEP 2024,0,"Semantic segmentation is currently a hot topic in remote sensing image processing. There are extensive applications in land planning and surveying. Many current studies combine Convolutional Neural Networks (CNNs), which extract local information, with Transformers, which capture global information, to obtain richer information. However, the fused feature information is not sufficiently enriched and it often lacks detailed refinement. To address this issue, we propose a novel method called the Multi-View Feature Fusion and Rich Information Refinement Network (MFRNet). Our model is equipped with the Multi-View Feature Fusion Block (MAFF) to merge various types of information, including local, non-local, channel, and positional information. Within MAFF, we introduce two innovative methods. The Sliding Heterogeneous Multi-Head Attention (SHMA) extracts local, non-local, and positional information using a sliding window, while the Multi-Scale Hierarchical Compressed Channel Attention (MSCA) leverages bar-shaped pooling kernels and stepwise compression to obtain reliable channel information. Additionally, we introduce the Efficient Feature Refinement Module (EFRM), which enhances segmentation accuracy by interacting the results of the Long-Range Information Perception Branch and the Local Semantic Information Perception Branch. We evaluate our model on the ISPRS Vaihingen and Potsdam datasets. We conducted extensive comparison experiments with state-of-the-art models and verified that MFRNet outperforms other models.",semantic segmentation,feature refinement,remote sensing,multi-view feature fusion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_405,"Sun, Shihao","Yang, Lei","Liu, Wenjie",Feature Fusion through Multitask CNN for Large-scale Remote Sensing Image Segmentation,,2018,1,"In recent years, Fully Convolutional Networks (FCN) has been widely used in various semantic segmentation tasks, including multi-modal remote sensing imagery. How to fuse multi-modal data to improve the segmentation performance has always been a research hotspot. In this paper, a novel end-to-end fully convolutional neural network is proposed for semantic segmentation of natural color, infrared imagery and Digital Surface Models (DSM). It is based on a modified DeepUNet and perform the segmentation in a multi-task way. The channels are clustered into groups and processed on different task pipelines. After a series of segmentation and fusion, their shared features and private features are successfully merged together. Experiment results show that the feature fusion network is efficient. And our approach achieves good performance in ISPRS Semantic Labeling Contest (2D).",remote sensing,feature fusion,DeepUNet,ISPRS,"Li, Ruirui",,,,semantic segmentation,pixel-wise classification,,,,,,,,2018 10TH IAPR WORKSHOP ON PATTERN RECOGNITION IN REMOTE SENSING (PRRS),,,,,,,,,,,,,,,
Row_406,"Zheng, Jianwei","Shao, Anhao","Yan, Yidong",Remote Sensing Semantic Segmentation via Boundary Supervision-Aided Multiscale Channelwise Cross Attention Network,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,16,"High spatial resolution (HSR) remote sensing (RS) images inevitably pose the challenge of multiscale transformation, as small objects, such as cars and helicopters (HCs), may occupy only a few pixel points. This incurs a significant hurdle for global context modeling, particularly in backbone networks with large downsampling coefficients. Simple summation or concatenation techniques, such as skip connections, fail to address semantic gaps and even impose negative impacts on multiscale feature fusion. Meanwhile, due to the complexity of foreground objects, the boundary details of HSR RS images are easy to lose in sampling operations. To overcome these challenges, we propose a multiscale channelwise cross attention network (MCCANet) assisted by boundary supervision (BS). Technically, MCCA captures the channel attention (CA) with various scales, which allows dynamic and adaptive feature fusion in a contextual scale-aware manner and focuses on both large and small objects distributed throughout the inputs. Besides, a channel and context strainer (CCS) module is proposed and embedded in MCCA, filtering channels and contexts for the mitigation of intraclass differences. In addition, we apply a BS module to recover boundary contour, avoiding the blurring effect during the construction of contextual information. The refined boundary allows for the effective recognition of surrounding pixels, ensuring a better segmentation performance. Extensive experiments on the instance segmentation in aerial images dataset (iSAID), International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam, and land-cover domain adaptive (LoveDA) datasets demonstrate that our proposed MCCANet achieves a good balance of high accuracy and efficiency. Code will be available at: https://github.com/ZhengJianwei2/MCCANet.",Attention module,boundary supervision (BS),convolutional neural network (CNN),remote sensing (RS),"Wu, Jie","Zhang, Meiyu",,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_407,"Hou, Yandong","Wu, Zhengbo","Ren, Xinghua",BFFNet: a bidirectional feature fusion network for semantic segmentation of remote sensing objects,INTERNATIONAL JOURNAL OF INTELLIGENT COMPUTING AND CYBERNETICS,FEB 29 2024,8,"PurposeHigh-resolution remote sensing images possess a wealth of semantic information. However, these images often contain objects of different sizes and distributions, which make the semantic segmentation task challenging. In this paper, a bidirectional feature fusion network (BFFNet) is designed to address this challenge, which aims at increasing the accurate recognition of surface objects in order to effectively classify special features.Design/methodology/approachThere are two main crucial elements in BFFNet. Firstly, the mean-weighted module (MWM) is used to obtain the key features in the main network. Secondly, the proposed polarization enhanced branch network performs feature extraction simultaneously with the main network to obtain different feature information. The authors then fuse these two features in both directions while applying a cross-entropy loss function to monitor the network training process. Finally, BFFNet is validated on two publicly available datasets, Potsdam and Vaihingen.FindingsIn this paper, a quantitative analysis method is used to illustrate that the proposed network achieves superior performance of 2-6%, respectively, compared to other mainstream segmentation networks from experimental results on two datasets. Complete ablation experiments are also conducted to demonstrate the effectiveness of the elements in the network. In summary, BFFNet has proven to be effective in achieving accurate identification of small objects and in reducing the effect of shadows on the segmentation process.Originality/valueThe originality of the paper is the proposal of a BFFNet based on multi-scale and multi-attention strategies to improve the ability to accurately segment high-resolution and complex remote sensing images, especially for small objects and shadow-obscured objects.",Remote sensing images,Semantic segmentation,Deep learning,Branch network,"Liu, Kaiwen","Chen, Zhengquan",,,Multi-scale fusion,,,,,,,,,,,,,,,,,,,,,,,,
Row_408,"Guo, Zhiling","Wu, Guangming","Song, Xiaoya",Super-Resolution Integrated Building Semantic Segmentation for Multi-Source Remote Sensing Imagery,IEEE ACCESS,2019,39,"Multi-source remote sensing imagery has become widely accessible owing to the development of data acquisition systems. In this paper, we address the challenging task of the semantic segmentation of buildings via multi-source remote sensing imagery with different spatial resolutions. Unlike previous works that mainly focused on optimizing the segmentation model, which did not enable the severe problems caused by the unaligned resolution between the training and testing data to be fundamentally solved, we propose to integrate SR techniques with the existing framework to enhance the segmentation performance. The feasibility of the proposed method was evaluated by utilizing representative multi-source study materials: high-resolution (HR) aerial and low-resolution (LR) panchromatic satellite imagery as the training and testing data, respectively. Instead of directly conducting building segmentation from the LR imagery by using the model trained using the HR imagery, the deep learning-based super-resolution (SR) model was first adopted to super-resolved LR imagery into SR space, which could mitigate the influence of the difference in resolution between the training and testing data. The experimental results obtained from the test area in Tokyo, Japan, demonstrate that the proposed SR-integrated method significantly outperforms that without SR, improving the Jaccard index and kappa by approximately 19.01% and 19.10%, respectively. The results confirmed that the proposed method is a viable tool for building semantic segmentation, especially when the resolution is unaligned.",Building segmentation,deep learning,remote sensing,super-resolution,"Yuan, Wei","Chen, Qi","Zhang, Haoran","Shi, Xiaodan",,,,,,,,,"Xu, Mingzhou",,"Xu, Yongwei","Shibasaki, Ryosuke","Shao, Xiaowei",,,,,,,,,,,,
Row_409,"Wang, Hengyou","Li, Xiao","Huo, Lianzhi",Global and edge enhanced transformer for semantic segmentation of remote sensing,APPLIED INTELLIGENCE,APR 2024,1,"Global context information and edge information are the keys to remote sensing (RS) image semantic segmentation. However, the existing methods have limited ability to obtain global and edge information, and category edge blurring and efficiency problems in small-scale object recognition in remote sensing image semantic segmentation tasks. In this work, we propose a global and edge enhanced Transformer (GE-Swin) for the semantic segmentation of remote sensing images. To improve the sensitivity to edge information, we design dual decoders based on the parallel model. One is the main decoder, which extracts multi-level semantic information from multi-scale features. The other is an auxiliary decoder related to low-layer features with low resolution. Thus, the auxiliary decoder has better sensitivity to edge information. Then, the feature fusion module (FFM) is designed between the encoder and decoder to fuse the multilevel features, enhancing the model's ability to obtain global features. Finally, to verify the performance of the proposed approach, we perform extensive experiments with the ISPRS and LoveDA datasets. The experimental results illustrate that the proposed model achieves superior performance compared to state-of-the-art methods.",Semantic segmentation,Feature fusion module,Auxiliary decoder,Transformer,"Hu, Changmiao",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_410,"Zhang, Jing","Li, Bin","Li, Jun",A Novel Semantic Segmentation Method for Remote Sensing Images Through Adaptive Scale-Based Convolution Neural Network,IEEE ACCESS,2024,0,"In semantic segmentation tasks for remote sensing images, effective feature extraction acts as the most important foundation. As a result, this paper proposes a novel semantic segmentation method for remote sensing images through adaptive scale-based convolution neural network. Firstly, it uses an encoder to extract features from remote sensing images, and utilizes attention mechanisms to control the information flow in the neural network. This is expected to reduce the impact between different scales. Then, a semantic segmentation for updating scale weights is established, and a scale-adaptive convolutional neural network is constructed. The upsampling unit is improved to increase the resolution to original image level, in order to better identify smaller targets. For large-sized problems, pyramid-like processing is used to segment images from multiple scales, and results are finally merged. Besides, we also make some experiments on ISPRS Potsdam dataset, UC Merced dataset, and DeepGlobe dataset, in order to make performance evaluation. The research shows that the maximum pixel accuracy of the proposed remote sensing semantic segmentation method is increased to 86.18%, the average value of the semantic segmentation task is up to 63.72, and the fastest running speed is up to 9.16FPS. In other words, the method proposed in this study has more accurate processing results and better stability.",Remote sensing,Semantic segmentation,Feature extraction,Kernel,,,,,Convolutional neural networks,Accuracy,Adaptive scale,convolutional neural network,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,
Row_411,"Ju, Haoran","Bi, Fukun","Bian, Mingming",Multiscale feature fusion network for automatic port segmentation from remote sensing images,JOURNAL OF APPLIED REMOTE SENSING,OCT 2022,1,"In recent years, remote sensing image observation technology has developed rapidly. Extracting coastlines from remote sensing images has become an indispensable means of port area measurement. Port segmentation from remote sensing images is an important method of coastline extraction and measurement. The remote sensing images used are panchromatic remote sensing images. Due to complex remote sensing image scenes and the large difference in feature information at different scales, traditional segmentation methods cannot perform effective extraction, and it is difficult to accurately segment the coastline in remote sensing images. We propose a multiscale feature fusion network for automatic port segmentation from remote sensing images. First, to reduce the redundant parameters and complex operation problems in traditional convolutional neural networks, we propose using MobileNetv2 as the base network for feature extraction to achieve a lightweight model. Then aiming at the feature differences of remote sensing images at different scales, we present atrous convolution as a convolution method for the entire network and combine a multiscale feature fusion method to extract the features of remote sensing images and improve the feature extraction ability. To reduce the problem of ships calling at the port being easily mistaken for port area and causing false segmentation, we propose a method of eliminating the ship area to reduce the interference. Finally, the comprehensive evaluation of a large number of Google port remote sensing data shows that compared with existing methods, the proposed method has the characteristics of being lightweight and having high precision. (c) 2022 Society of Photo-Optical Instrumentation Engineers (SPIE)",remote sensing image segmentation,portsegmentation,multiscale features,semantic,"Shi, Yinni",,,,segmentation,weighted loss,,,,,,,,,,,,,,,,,,,,,,,
Row_412,"Cai, Yuxiang","Yang, Yingchun","Zheng, Qiyi",BiFDANet: Unsupervised Bidirectional Domain Adaptation for Semantic Segmentation of Remote Sensing Images,REMOTE SENSING,JAN 2022,19,"When segmenting massive amounts of remote sensing images collected from different satellites or geographic locations (cities), the pre-trained deep learning models cannot always output satisfactory predictions. To deal with this issue, domain adaptation has been widely utilized to enhance the generalization abilities of the segmentation models. Most of the existing domain adaptation methods, which based on image-to-image translation, firstly transfer the source images to the pseudo-target images, adapt the classifier from the source domain to the target domain. However, these unidirectional methods suffer from the following two limitations: (1) they do not consider the inverse procedure and they cannot fully take advantage of the information from the other domain, which is also beneficial, as confirmed by our experiments; (2) these methods may fail in the cases where transferring the source images to the pseudo-target images is difficult. In this paper, in order to solve these problems, we propose a novel framework BiFDANet for unsupervised bidirectional domain adaptation in the semantic segmentation of remote sensing images. It optimizes the segmentation models in two opposite directions. In the source-to-target direction, BiFDANet learns to transfer the source images to the pseudo-target images and adapts the classifier to the target domain. In the opposite direction, BiFDANet transfers the target images to the pseudo-source images and optimizes the source classifier. At test stage, we make the best of the source classifier and the target classifier, which complement each other with a simple linear combination method, further improving the performance of our BiFDANet. Furthermore, we propose a new bidirectional semantic consistency loss for our BiFDANet to maintain the semantic consistency during the bidirectional image-to-image translation process. The experiments on two datasets including satellite images and aerial images demonstrate the superiority of our method against existing unidirectional methods.",unsupervised domain adaptation,bidirectional domain adaptation,convolutional neural networks (CNNs),image-to-image translation,"Shen, Zhengwei","Shang, Yongheng","Yin, Jianwei","Shi, Zhongtian",generative adversarial networks (GANs),remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,
Row_413,"Zhang, Tianyang","Zhang, Xiangrong","Zhu, Peng",Semantic Attention and Scale Complementary Network for Instance Segmentation in Remote Sensing Images,IEEE TRANSACTIONS ON CYBERNETICS,OCT 2022,31,"In this article, we focus on the challenging multicategory instance segmentation problem in remote sensing images (RSIs), which aims at predicting the categories of all instances and localizing them with pixel-level masks. Although many landmark frameworks have demonstrated promising performance in instance segmentation, the complexity in the background and scale variability instances still remain challenging, for instance, segmentation of RSIs. To address the above problems, we propose an end-to-end multicategory instance segmentation model, namely, the semantic attention (SEA) and scale complementary network, which mainly consists of a SEA module and a scale complementary mask branch (SCMB). The SEA module contains a simple fully convolutional semantic segmentation branch with extra supervision to strengthen the activation of interest instances on the feature map and reduce the background noise's interference. To handle the undersegmentation of geospatial instances with large varying scales, we design the SCMB that extends the original single mask branch to trident mask branches and introduces complementary mask supervision at different scales to sufficiently leverage the multiscale information. We conduct comprehensive experiments to evaluate the effectiveness of our proposed method on the iSAID dataset and the NWPU Instance Segmentation dataset and achieve promising performance.",Semantics,Image segmentation,Remote sensing,Feature extraction,"Tang, Xu","Li, Chen","Jiao, Licheng","Zhou, Huiyu",Proposals,Marine vehicles,Task analysis,Instance segmentation,remote sensing images (RSIs),scale complementarity,semantic attention (SEA),,,,,,,,,,,,,,,,,,
Row_414,"Huynh-The, Thien","Truong, Son Ngoc","Nguyen, Gia-Vuong",HBSeNet: A Hybrid Bilateral Network for Accurate Semantic Segmentation of Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"Semantic segmentation of aerial and satellite images plays a crucial role in a wide range of applications and services, catering to the increasing needs of environmental resource management, urban planning, and traffic safety. Many efficient semantic segmentation methods have been proposed by exploiting deep learning techniques with convolution neural networks (CNNs) architectures and self-attention mechanisms to achieve superior accuracy if compared with conventional machine learning-based approaches. In this article, we introduce a hybrid bilateral segmentation network (HBSeNet), a novel semantic segmentation architecture. Inspired by the success of dual-path network models that replace conventional single-branch encoder-decoder architectures, we construct a model with the core idea of combining the context path and the spatial path to optimize both the accuracy and the complexity of deep learning model in the field of remote sensing image segmentation. Moreover, HBSeNet innovates with auxiliary modules designed to enhance its performance, such as sequential atrous convolution, information synthesis module, and bridge for efficient multiscale feature extraction, fusion, and integration. In simulations, our model achieves a global accuracy of 92.04%, a mean intersection-over-union of 83.57%, and a mean boundary-F1-score of 90.23% when evaluated on the ISPRS Potsdam dataset, surpassing the state-of-the-art segmentation models, such as DeepLabV3+, SwinCNN, and ST-UNet.",Semantic segmentation,Computer architecture,Accuracy,Feature extraction,,,,,Convolution,Remote sensing,Context modeling,Artificial intelligence,bilateral network,image segmentation,object recognition,remote sensing,,,,,,,,,,,,,,,,,
Row_415,"Gu, Xingjian","Li, Sizhe","Ren, Shougang",Adaptive enhanced swin transformer with U-net for remote sensing image segmentation*,COMPUTERS & ELECTRICAL ENGINEERING,SEP 2022,21,"Semantic segmentation of remote sensing images often faces complex situations, such as variable scale objects, large intra-class differences, and imbalanced distribution among classes. Convolutional Neural Network (CNN) based models have been widely used in remote sensing image segmentation tasks for its powerful feature extraction capability. Due to intrinsic locality of CNN architectures, it is difficult to understand the long-range dependencies among image patches. Recently, the transformer leverages long-range dependencies and performs well in computer vision tasks. To take advantages of both CNN and Transformer, a novel Adaptive Enhanced Swin Transformer with U-Net (AESwin-UNet) is proposed for remote sensing segmentation. AESwinUNet uses a hybrid Transformer-based U-type Encoder-Decoder architecture with skip connections to extract local and global semantic features. Specifically, the Enhanced Swin Transformer (E-Swin Transformer) contains Enhanced Multi-head Self-Attention and Deformable Adaptive Patch Merging layer in encoder. A symmetric cascaded decoder is designed for up-sampling to obtain higher resolution feature maps. Experiments on two public benchmark datasets, WHDLD and LoveDA, demonstrate that the proposed AESwin-UNet performs well in semantic segmentation.",Remote sensing,Semantic segmentation,Unet,Transformer,"Zheng, Hengbiao","Fan, Chengcheng","Xu, Huanliang",,CNN,,,,,,,,,,,,,,,,,,,,,,,,
Row_416,"Zhong, Hai-Feng","Sun, Qing","Sun, Hong-Mei",NT-Net: A Semantic Segmentation Network for Extracting Lake Water Bodies From Optical Remote Sensing Images Based on Transformer,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,34,"The automatic extraction of lake water is one of the research hotspots in the field of remote sensing image processing. Due to the small interclass variance between lakes and other ground objects, and the complex texture characteristics of lake boundaries, existing methods often have problems such as over-segmentation and inaccurate boundary segmentation when segmenting lake water bodies. To alleviate these problems, this article designs an end-to-end semantic segmentation network [noise-canceling transformer network (NT-Net)] for the automatic extraction of lake water bodies from remote sensing images. Aiming at the problem of over-segmentation caused by nonlake objects, an interference attenuation module is designed in the network. This module can model the key features that are distinguishable and suitable for segmenting lake water by analyzing the difference in feature representation between lakes and other ground objects, thus suppressing the feature representation of nonlake objects. To more accurately segment the lake boundary, a multilevel transformer module is designed. This module can capture the context association of boundary information and enhance the feature representation of boundary information by using the self-attention mechanism. The comparative experimental results show that, compared with the current mainstream semantic segmentation networks, the method in this article has advantages in extracting lake water bodies comprehensively and coherently.",Lakes,Image segmentation,Transformers,Feature extraction,"Jia, Rui-Sheng",,,,Remote sensing,Semantics,Data mining,Convolutional neural network (CNN),lake segmentation,optical remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,
Row_417,"Wang, Zhimin","Wang, Jiasheng","Yang, Kun",Semantic segmentation of high-resolution remote sensing images based on a class feature attention mechanism fused with Deeplabv3+,COMPUTERS & GEOSCIENCES,JAN 2022,75,"Aiming at solving the problems of inaccurate segmentation of edge targets, inconsistent segmentation of different types of targets, and slow prediction efficiency on semantic segmentation of high-resolution remote sensing images by classical semantic segmentation network, this study proposed a class feature attention mechanism fused with an improved Deeplabv3+ network called CFAMNet for semantic segmentation of common features in remote sensing images. First, the correlation between classes is enhanced using the class feature attention module to extract and process different categories of semantic information better. Second, the multi-parallel atrous spatial pyramid pooling structure is used to enhance the correlation between spaces, to extract the context information of different scales of an image better. Finally, the encoder-decoder structure is used to refine the segmentation results. The segmentation effect of the proposed network is verified by experiments on the public data set GaoFen image dataset (GID). The experimental results show that the CFAMNet can achieve the mean intersection over union (MIOU) and overall accuracy (OA) of 77.22% and 85.01%, respectively, on the GID, thus surpassing the current mainstream semantic segmentation networks.",Remote sensing,Deep learning,Convolution neural network,Semantic segmentation,"Wang, Limeng","Su, Fanjie","Chen, Xinya",,Attention mechanism,Deeplabv3+,,,,,,,,,,,,,,,,,,,,,,,
Row_418,"Chen, Guanzhou","Zhang, Xiaodong","Wang, Qing",Symmetrical Dense-Shortcut Deep Fully Convolutional Networks for Semantic Segmentation of Very-High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,MAY 2018,122,"Semantic segmentation has emerged as a mainstream method in very-high-resolution remote sensing land-use/land-cover applications. In this paper, we first review the state-of-the-art semantic segmentation models in both computer vision and remote sensing fields. Subsequently, we introduce two semantic segmentation frameworks: SNFCN and SDFCN, both of which contain deep fully convolutional networks with shortcut blocks. We adopt an overlay strategy as the postprocessing method. Based on our frameworks, we conducted experiments on two online ISPRS datasets: Vaihingen and Potsdam. The results indicate that our frameworks achieve higher overall accuracy than the classic FCN-8s and Seg-Net models. In addition, our postprocessing method can increase the overall accuracy by about 1%-2% and help to eliminate ""salt and pepper"" phenomena and block effects.",Convolutional neural networks (CNN),deep learning (DL),fully convolutional networks (FCN),remote sensing,"Dai, Fan","Gong, Yuanfu","Zhu, Kun",,SDFCN,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_419,"Zhang, Bin","Zhang, Yongjun","Li, Yansheng",Semi-supervised Deep Learning via Transformation Consistency Regularization for Remote Sensing Image Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,13,"Deep convolutional neural networks have gotten a lot of press in the last several years, especially in domains like computer vision and remote sensing (RS). However, achieving superior performance with deep networks highly depends on a massive number of accurately labeled training samples. In real-world applications, gathering a large number of labeled samples is time consuming and labor intensive, especially for pixel-level data annotation. This dearth of labels in land-cover classification is especially pressing in the RS domain because high-precision high-quality labeled samples are extremely difficult to acquire, but unlabeled data are readily available. In this study, we offer a new semisupervised deep semantic labeling framework for the semantic segmentation of high-resolution RS images to take advantage of the limited amount of labeled examples and numerous unlabeled samples. Our model uses transformation consistency regularization to encourage consistent network predictions under different random transformations or perturbations. We try three different transforms to compute the consistency loss and analyze their performance. Then, we present a deep semisupervised semantic labeling technique by using a hybrid transformation consistency regularization. A weighted sum of losses, which contains a supervised term computed on labeled samples and an unsupervised regularization term computed on unlabeled data, may be used to update the network parameters in our technique. Our comprehensive experiments on two RS datasets confirmed that the suggested approach utilized latent information from unlabeled samples to obtain more precise predictions and outperformed existing semisupervised algorithms in terms of performance. Our experiments further demonstrated that our semisupervised semantic labeling strategy has the potential to partially tackle the problem of limited labeled samples for high-resolution RS image land-cover segmentation.",Consistency regularization,convolutional neural network (CNN),semantic segmentation,semisupervised learning (SSL),"Wan, Yi","Guo, Haoyu","Zheng, Zhi","Yang, Kun",unlabeled data,remote sensing (RS) imagery,,,,,,,,,,,,,,,,,,,,,,,
Row_420,"Lu, Chen","Zhang, Xian","Du, Kaile",CTCFNet: CNN-Transformer Complementary and Fusion Network for High-Resolution Remote Sensing Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Semantic segmentation of high-resolution remote sensing images poses challenges such as scale variability, diverse objects, and obstruction by surface elements. These factors often lead existing methods to suffer from issues like missed and false detections, as well as coarse segmentation boundaries. To tackle these challenges, this article proposes a CNN-transformer complementary and fusion network, termed as CTCFNet. It aims to enhance segmentation accuracy and robustness by extracting and integrating the complementary global and local information from high-resolution remote sensing images. The CTCFNet operates through two primary stages: feature extraction and fusion. In the feature extraction stage, a feature extractor employs convolutional neural network (CNN) and pyramid vision transformer (PVT) blocks to extract both local and global features. A boundary loss is also proposed to improve the segmentation performance for object textures and boundaries. In the feature fusion stage, a feature aggregation module (FAM) is first designed to effectively fuse local and global features at the same scale, facilitating the feature extractor to obtain more comprehensive representations. On this basis, a bi-directional decoder (BiDecoder) reconstructs multiscale features through both top-down and bottom-up directions, resulting in more precise segmentation outputs. Experiments on several high-resolution remote sensing image datasets demonstrate that the proposed method outperforms the state-of-the-art methods in terms of segmentation accuracy and generalization. The code is available at https://github.com/ChenLu0000/CTCFNet.",Image segmentation,Feature extraction,Remote sensing,Transformers,"Xu, Han","Liu, Guangcan",,,Decoding,Semantic segmentation,Bidirectional control,Complementary information,convolutional neural network (CNN) transformer,feature fusion,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_421,"Yu, Lei","Jin, Qizhao","Wang, Wei",Intra- and Inter-Image Causal Intervention for Robust Semantic Segmentation in Remote-Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,0,"Semantic segmentation in remote-sensing (RS) images is a critical component for advanced RS scene understanding. Traditional methods learn to segment using direct supervision of paired data, yet fall short by not fully addressing the noncausal biases behind RS images. These include intra-image biases, such as varying background and object appearance, and inter-image biases, such as inconsistent geographical environments and class distributions, which can mislead models to infer by relying on these biases. Given the high cost and effort needed for pixelwise annotation, the limited volume of RS images exacerbates this problem. In this letter, we propose a novel intra- and inter-image causal intervention (I-3-CI) learning paradigm, designed specifically to mitigate the influence of both biases on RS semantic segmentation. Specifically, to reduce the intra-image biases, our I-3-CI utilizes intra-image contrastive learning to promote closer alignment of features from the same category and distancing those of disparate categories in the same image. Consequently, the compact feature space is more robust to the varying appearance. As for eliminating the inter-image level biases, our I-3-CI further explores the inter-image contrastive learning of all pixels within the entire dataset. To overcome the difficulty of learning from all pixels of the dataset simultaneously, our I-3-CI introduces a set of proxy prototype features to keep track of the global centroids for the features of different categories. We substantiate the I-3-CI paradigm's efficacy through rigorous testing on the LoveDA, Vaihingen, and Potsdam datasets with an average improvement of 0.55% on the mIoU metric, proving the value of causal interventions in achieving robust and accurate semantic segmentation.",Causal intervention,contrastive learning,noncausal biases,remote-sensing (RS) image,,,,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_422,"Jiang, Zhongze","Chen, Zhong","Ji, Kaixiang",Semantic segmentation network combined with edge detection for building extraction in remote sensing images,,2020,3,"Extracting buildings from remote sensing images is a significant task with many applications such as map drawing, city planning, population estimation, etc. However, traditional methods that rely on artificially designed features struggle to perform well due to the diverse appearance and complicated background. In this paper, we design an end-to-end convolutional neural network that combines semantic segmentation and edge detection for building extraction. In addition, we propose a residual unit combined with spatial pyramid pooling (SPP-RU) to yield representations of different size receptive fields by multi-branch network. We conduct experiments on WHU building dataset, and the experimental results demonstrate the effectiveness of our method in terms of quantitative and qualitative performance compared with state-of-the-art methods.",Building extraction,Edge detection,Semantic Segmentation,Convolutional neural network,"Yang, Jian",,,,Remote sensing,,,,,,,,,MIPPR 2019: PATTERN RECOGNITION AND COMPUTER VISION,,,,,,,,,,,,,,,
Row_423,"Zhang, Qi","Geng, Guohua","Zhou, Pengbo",Link Aggregation for Skip Connection-Mamba: Remote Sensing Image Segmentation Network Based on Link Aggregation Mamba,REMOTE SENSING,OCT 2024,0,"The semantic segmentation of satellite and UAV remote sensing imagery is pivotal for address exploration, change detection, quantitative analysis and urban planning. Recent advancements have seen an influx of segmentation networks utilizing convolutional neural networks and transformers. However, the intricate geographical features and varied land cover boundary interferences in remote sensing imagery still challenge conventional segmentation networks' spatial representation and long-range dependency capabilities. This paper introduces a novel U-Net-like network for UAV image segmentation. We developed a link aggregation Mamba at the critical skip connection stage of UNetFormer. This approach maps and aggregates multi-scale features from different stages into a unified linear dimension through four Mamba branches containing state-space models (SSMs), ultimately decoupling and fusing these features to restore the contextual relationships in the mask. Moreover, the Mix-Mamba module is incorporated, leveraging a parallel self-attention mechanism with SSMs to merge the advantages of a global receptive field and reduce modeling complexity. This module facilitates nonlinear modeling across different channels and spaces through multipath activation, catering to international and local long-range dependencies. Evaluations on public remote sensing datasets like LovaDA, UAVid and Vaihingen underscore the state-of-the-art performance of our approach.",semantic segmentation,remote sensing,Mamba,state-space model,"Liu, Qinglin","Wang, Yong","Li, Kang",,link aggregation,,,,,,,,,,,,,,,,,,,,,,,,
Row_424,"Lin, Baihong","Zou, Zhengxia","Shi, Zhenwei",RSBEV: Multiview Collaborative Segmentation of 3-D Remote Sensing Scenes With Bird's-Eye-View Representation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Perception of 3-D remote sensing scenes plays a crucial role in accurately recognizing and locating ground objects, as it enables a deeper understanding of complex environments by capturing scene geometry, object relationships, and occlusion patterns. Inspired by the powerful multisensor fusion capabilities in autonomous driving, we explore a new task in this article: given a set of multiview images of a 3-D remote sensing scene, we aim to obtain bird's-eye-view (BEV) scene information under the common view area in the world coordinate system. In this work, we focus on the task of semantic segmentation to demonstrate the feasibility of our approach and introduce a BEV modeling technique tailored for remote sensing scenes, which facilitates the projection of 3-D scene details from multiple perspective views onto a BEV. We then utilize a dual-encoder structure based on the vision transformer (VIT) architecture to extract relevant spatial information using self-attention mechanisms. Within the decoder, we employ a feature pyramid network (FPN) to integrate BEV patch encoding with spatial feature residuals, enabling fine-grained segmentation results at the original input resolution. Furthermore, we curated the LEVIR-MDS multidrone segmentation dataset, comprising scenes from ten community-level areas across three continents, totaling 243k images and their corresponding annotated BEV semantic maps, amounting to approximately 500 GB. This dataset serves as a robust benchmark to assess the effectiveness and generalization capability of our proposed method. To our knowledge, this is the first semantic segmentation dataset designed specifically for collaborative multidrone applications. We further show that our method achieves a 12% improvement in mean IoU (mIoU), reaching 69.73%, compared to a pure convolutional network model.",Bird's-eye-view (BEV) representation,multiview collaborative segmentation,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_425,"Zhou, Xuanyu","Zhou, Lifan","Gong, Shengrong",Swin Transformer Embedding Dual-Stream for Semantic Segmentation of Remote Sensing Imagery,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,11,"The acquisition of global context and boundary information is crucial for the semantic segmentation of remote sensing (RS) images. In contrast to convolutional neural networks (CNNs), transformers exhibit superior performance in global modeling and shape feature encoding, which provides a novel avenue for obtaining global context and boundary information. However, current methods fail to effectively leverage these distinctive advantages of transformers. To address this issue, we propose a novel single encoder and dual decoders architecture called STDSNet, which embeds the Swin transformer into the dual-stream network for semantic segmentation of RS imagery. The proposed STDSNet employs the Swin transformer as the network backbone in the encoder to address the limitations of CNNs in global modeling and encoding shape features. The dual decoder comprises two parallel streams, namely the global stream (GS) and the shape stream (SS). The GS utilizes the global context fusion module (GCFM) to address the loss of global context during upsampling. It further integrates GCFMs with skip connections and a multiscale fusion strategy to mitigate large-scale regional object classification errors resulting from similar features or shadow occlusion in RS images. The SS introduces the gate convolution module (GCM) to filter out irrelevant features, allowing it to focus on processing boundary information, which improves the semantic segmentation performance of small targets and their boundaries in RS images. Extensive experiments demonstrate that STDSNet outperforms other state-of-the-art methods on the ISPRS Vaihingen and Potsdam benchmarks.",Transformers,Shape,Semantic segmentation,Feature extraction,"Zhong, Shan","Yan, Wei","Huang, Yizhou",,Streams,Semantics,Decoding,Dual-stream,remote sensing (RS),semantic segmentation,Swin transformer,,,,,,,,,,,,,,,,,,
Row_426,"Wang, Hao","Guo, Mingning","Li, Shaoxian",Global-Local Coupled Style Transfer for Semantic Segmentation of Bitemporal Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Due to the different acquisition conditions, large variations in the feature distributions of two temporal domains generally exist, known as temporal domain shift. The temporal domain shift is primarily influenced by coupled dual-factor: global style variations (such as illumination and weather conditions) and local style variations (such as the inherent phenological properties of land cover classes). In this article, we first formulate the temporal domain shift problem as an issue of dual-factor coupled interference on feature distributions in remote sensing (RS) community. To address this issue, we propose a semantic-guided style transfer (SGST) framework seamlessly integrating global feature alignment with local feature semantic matching. We use an adaptive segmentation model to provide pseudosegmentation maps and feed them into the style transfer model as semantic guidance. Under semantic guidance, a semantic-constrained style normalization (SCSN) module is designed to achieve style transfer at both global and local levels. Furthermore, a dual learning approach is introduced to make the style transfer model and the adaptive segmentation model promote each other. As a result, the style transfer model generates high-quality style-transferred images, and the adaptive segmentation model progressively predicts more accurate pseudosegmentation maps. Extensive experiments demonstrate the superiority of our proposed framework over state-of-the-art methods in terms of both perceptual quality and quantitative performance.",Adaptation models,Semantics,Land surface,Semantic segmentation,"Li, Haifeng","Tao, Chao",,,Remote sensing,Predictive models,Visualization,Dual learning,global-local coupled style transfer,semantic guidance,temporal domain shift,,,,,,,,,,,,,,,,,,
Row_427,"Li, Yuanjun","Zhu, Zhiyu","Li, Yuanjiang",CTMU-Net: An Improved U-Net for Semantic Segmentation of Remote-Sensing Images Based on the Combined Attention Mechanism,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,4,"With the development of remote-sensing technology, it is important to use semantic segmentation methods to obtain detailed information in remote-sensing images. However, the objects in the images reveal significant intraclass differences and slight interclass differences, thus affecting the acquisition of terrain information. To tackle the problems, this article proposes an improved U-Net for the semantic segmentation of remote-sensing images. First, the local importance-based pooling is introduced to alleviate the loss of feature details in the coding part. Second, a combined attention module with a double-branch structure is designed, which models the local relationship and the global relationship at the same time to obtain more typical features. Finally, in order to make full use of the feature information extracted from the coding part, the combined attention module and the channel attention module are added to different positions in U-Net. In order to validate the proposed method, we conduct experiments on the WHDLD dataset and compare the experimental results with other semantic segmentation methods. On the WHDLD dataset, the MPA, MIOU, and FWIOU of the proposed method reach 76$\%$, 64.11$\%$, and 75.64$\%$, respectively, revealing its priority. To demonstrate the generalization of the proposed method, generalization experiments are conducted via the LandCover.ai dataset and the Massachusetts-building dataset. The simulation results testify that the proposed method provides an excellent generalization ability.",Semantic segmentation,Remote sensing,Semantics,Feature extraction,"Zhang, Jinglin","Li, Xi","Shang, Shuyao","Zhu, Dewen",Deep learning,Transformers,Electronic mail,Attention mechanism,deep learning,remote-sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_428,"Yuan, Genji","Li, Jianbo","Lv, Zhiqiang",DDCAttNet: Road Segmentation Network for Remote Sensing Images,"WIRELESS ALGORITHMS, SYSTEMS, AND APPLICATIONS, WASA 2021, PT II",2021,1,"Semantic segmentation of remote sensing images based on deep convolutional neural networks has proven its effectiveness. However, due to the complexity of remote sensing images, deep convolutional neural networks have difficulties in segmenting objects with weak appearance coherences even though they can represent local features of object effectively. The road networks segmentation of remote sensing images faces two major problems: high inter-individual similarity and ubiquitous occlusion. In order to address these issues, this paper develops a novel method to extract roads from complex remote sensing images. We designed a Dual Dense Connected Attention network (DDCAttNet) that establishes long-range dependencies between road features. The architecture of the network is designed to incorporate both spatial attention and channel attention information into semantic segmentation for accurate road segmentation. Experimental results on the benchmark dataset demonstrate the superiority of our proposed approach both in quantitative and qualitative evaluation.",Remote sensing,Road segmentation,Attention mechanism,,"Li, Yinong","Xu, Zhihao",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_429,"Ma, Yuefeng","Wang, Yingli","Liu, Xingya",SWINT-RESNet: An Improved Remote Sensing Image Segmentation Model Based on Transformer,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,2,"Deep neural networks have been widely used in remote sensing image segmentation. Nowadays, artificial intelligence methods are increasingly applied to remote sensing feature classification. Although convolutional neural networks (CNNs) are widely used for image segmentation tasks, their global feature extraction with increasing image samples is insufficient. Furthermore, transformer is now being focused on computer vision. However, although transformer can capture the global information of remote sensing images, it cannot adequately model the detailed information of image changes. To comprehensively compensate for the defects of CNNs and the transformer network in feature extraction, this study proposes a semantic segmentation network with multifeature fusion (SWINT-RESNet). This network combines the transformer-extracted global and local features and those of CNNs to improve the accuracy of remote sensing image segmentation. The experiments show that the segmentation performance of SWINT-RESNet is superior for both small and medium sample remote sensing image datasets.",Feature extraction,Transformers,Remote sensing,Accuracy,"Wang, Haiying",,,,Semantics,Convolutional neural networks,Semantic segmentation,Deep learning,remote sensing image segmentation,RESNet,semantic segmentation,transformer,,,,,,,,,,,,,,,,,
Row_430,"Liu, Yutong","Gao, Kun","Wang, Hong",A Transformer-based multi-modal fusion network for semantic segmentation of high-resolution remote sensing imagery,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,SEP 2024,0,"Semantic segmentation of high-resolution multispectral remote sensing image has been intensely studied. However, the shadow occlusions, or the similar color and textures, between the categories influence the segmentation accuracy. Concomitantly, the size of targets in the remote sensing images is diverse and the network cannot balance their segmentation. This paper introduces a network, Transformer-based Multi-modal Fusion Network (TMFNet), which fuses the multi-modal features and incorporates height features from the digital surface model (DSM) to supplement the extra different features between each category. Particularly, we introduce two parallel encoders to extract the features from different modalities, a Multi-Modal fusion model based on the Transformer (MMformer) to complete the multi-modal fusion, and a Border Region Attention based multi-level Fusion Module (BRAFM) to integrate the cross-level features and enhance the small target segmentation by utilizing the details around the border. The experiment results on the ISPRS Vaihingen and Potsdam benchmark datasets indicate that the proposed TMFNet outperforms the SOTA methods on the segmentation performance.",High-resolution remote sensing,Semantic segmentation,Transformer,Multi-modal fusion,"Yang, Zhijia","Wang, Pengyu","Ji, Shijing","Huang, Yanjun",,,,,,,,,"Zhu, Zhenyu",,"Zhao, Xiaobin",,,,,,,,,,,,,,
Row_431,"Zeng, Qiaolin","Zhou, Jingxiang","Tao, Jinhua",Multiscale Global Context Network for Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,5,"Semantic segmentation of high-resolution remote sensing images (HRSIs) is a challenging task because objects in HRSIs usually have great scale variance and appearance variance. Although deep convolutional neural networks (DCNNs) have been widely applied in the semantic segmentation of HRSIs, they have inherent limitations in capturing global context. Attention mechanisms and transformer can effectively model long-range dependencies, but they often result in high computational costs when being applied to process HRSIs. In this article, an encoder-decoder network (MSGCNet) is proposed to fully and efficiently model multiscale context and long-range dependencies of HRSIs. Specifically, the multiscale interaction (MSI) module employs an efficient cross-attention to facilitate interaction among multiscale features of the encoder, which bridges the semantic gap between high- and low-level features and introduces more scale information to the network. In order to efficiently model long-range dependencies in both spatial and channel dimensions, the transformer-based decoder block (TBDB) implements window-based efficient multihead self-attention (W-EMSA) and enables interactions cross windows. Furthermore, to further integrate the global context generated by TBDB, the scale-aware fusion (SAF) module is proposed to deeply supervise the decoder, which iteratively fuses hierarchical features through spatial attention. As demonstrated by both quantitative and qualitative experimental results on two publicly available datasets, the proposed MSGCNet exhibits superior performance compared to currently popular methods. The code will be available at http://github.com/JingxiangZhou/MSGCNet.",Fuses,Semantic segmentation,Computational modeling,Semantics,"Chen, Liangfu","Niu, Xuerui","Zhang, Yumeng",,Transformers,Decoding,Sensors,Attention mechanism,remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,
Row_432,"Wang, Xinyao","Wang, Haitao","Jing, Yuqian",A Bio-Inspired Visual Perception Transformer for Cross-Domain Semantic Segmentation of High-Resolution Remote Sensing Images,REMOTE SENSING,MAY 2024,1,"Pixel-level classification of very-high-resolution images is a crucial yet challenging task in remote sensing. While transformers have demonstrated effectiveness in capturing dependencies, their tendency to partition images into patches may restrict their applicability to highly detailed remote sensing images. To extract latent contextual semantic information from high-resolution remote sensing images, we proposed a gaze-saccade transformer (GSV-Trans) with visual perceptual attention. GSV-Trans incorporates a visual perceptual attention (VPA) mechanism that dynamically allocates computational resources based on the semantic complexity of the image. The VPA mechanism includes both gaze attention and eye movement attention, enabling the model to focus on the most critical parts of the image and acquire competitive semantic information. Additionally, to capture contextual semantic information across different levels in the image, we designed an inter-layer short-term visual memory module with bidirectional affinity propagation to guide attention allocation. Furthermore, we introduced a dual-branch pseudo-label module (DBPL) that imposes pixel-level and category-level semantic constraints on both gaze and saccade branches. DBPL encourages the model to extract domain-invariant features and align semantic information across different domains in the feature space. Extensive experiments on multiple pixel-level classification benchmarks confirm the effectiveness and superiority of our method over the state of the art.",transformer,semantic segmentation,pseudo-label,high-resolution remote-sensing images,"Yang, Xianming","Chu, Jianbo",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_433,"Lenczner, Gaston","Chan-Hon-Tong, Adrien","Le Saux, Bertrand",DIAL: Deep Interactive and Active Learning for Semantic Segmentation in Remote Sensing,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,17,"In this article, we propose to build up a collaboration between a deep neural network and a human in the loop to swiftly obtain accurate segmentation maps of remote sensing images. In a nutshell, the agent iteratively interacts with the network to correct its initially flawed predictions. Concretely, these interactions are annotations representing the semantic labels. Our methodological contribution is twofold. First, we propose two interactive learning schemes to integrate user inputs into deep neural networks. The first one concatenates the annotations with the other network's inputs. The second one uses the annotations as a sparse ground truth to retrain the network. Second, we propose an active learning (AL) strategy to guide the user toward the most relevant areas to annotate. To this purpose, we compare different state-of-the-art acquisition functions to evaluate the neural network uncertainty such as ConfidNet, entropy, or ODIN. Through experiments on three remote sensing datasets, we show the effectiveness of the proposed methods. Notably, we show that AL based on uncertainty estimation enables to quickly lead the user toward mistakes and that it is thus relevant to guide the user interventions. Code will be open-source and released in this repository.(1)",Neural networks,Annotations,Image segmentation,Uncertainty,"Luminari, Nicola","Le Besnerais, Guy",,,Semantics,Remote sensing,Deep learning,Active learning (AL),deep learning,earth observation,interactive segmentation,semantic segmentation,,,,,,,,,,,,,,,,,
Row_434,"Wang, Zhaoxin","Zheng, Chengyu","Wang, Chenglong",Statistical texture involved multi-granularity attention network for remote sensing semantic segmentation,MULTIMEDIA TOOLS AND APPLICATIONS,MAR 2024,0,"In recent years,semantic segmentation technology plays an important role in land resource management tasks. However, many classic semantic segmentation models often fail to obtain satisfactory results for remote sensing images with large difference of object scale span. The statistical texture involved multi-granularity attention network has been proposed to improve this situation. Statistical texture involved multi-granularity attention network has an encoder-decoder structure which is similar to DeeplabV3+ 2018. On this basis,texture excitation module and multi-granularity attention module are proposed to improve the accuracy of semantic segmentation. Texture excitation module is designed to extract statistical texture feature that are hidden in the image so that they can be combined with spatial structure feature for better prediction. Specifically, this module calculates the pixel co-occurrence matrix in the horizontal, vertical, diagonal, and anti diagonal directions. Multi-granularity attention module provides a more comprehensive understanding of the image by simultaneously focusing on the image at coarse,medium and fine granularity. Specifically, this module designs coarse granularity, medium granularity, and fine granularity spatial attention mechanisms and coarse granularity, medium granularity, and fine granularity channel attention mechanisms based on the core idea of self attention mechanism. Statistical texture involved multi-granularity attention network is compared with several of the most advanced deep learning methods on the Vaihingen data set and Potsdam data set.Compared with the best performing method, our method has increased by 0.73%, 1.88% and 0.013 in pixel accuracy,mean intersection over union and F1 score respectively on Vaihingen data set; our method improves the pixel accuracy from 87.82% to 88.50%, the mean intersection over union from 74.53% to 75.82%, and the F1 score from 0.8488 to 0.8573 on Potsdam data set.",Statistical textures,Multi-granularity,Remote sensing images,Semantic segmentation,"Wang, Jingyu","Yu, Shusong","Nie, Jie",,Attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,
Row_435,"Wang, Guoying","Chen, Jiahao","Mo, Lufeng",Lightweight land cover classification via semantic segmentation of remote sensing imagery and analysis of influencing factors,FRONTIERS IN ENVIRONMENTAL SCIENCE,FEB 15 2024,2,"Land cover classification is of great value and can be widely used in many fields. Earlier land cover classification methods used traditional image segmentation techniques, which cannot fully and comprehensively extract the ground information in remote sensing images. Therefore, it is necessary to integrate the advanced techniques of deep learning into the study of semantic segmentation of remote sensing images. However, most of current high-resolution image segmentation networks have disadvantages such as large parameters and high network training cost. In view of the problems above, a lightweight land cover classification model via semantic segmentation, DeepGDLE, is proposed in this paper. The model DeepGDLE is designed on the basis of DeeplabV3+ network and utilizes the GhostNet network instead of the backbone feature extraction network in the encoder. Using Depthwise Separable Convolution (DSC) instead of dilation convolution. This reduces the number of parameters and increases the computational speed of the model. By optimizing the dilation rate of parallel convolution in the ASPP module, the ""grid effect"" is avoided. ECANet lightweight channel attention mechanism is added after the feature extraction module and the pyramid pooling module to focus on the important weights of the model. Finally, the loss function Focal Loss is utilized to solve the problem of category imbalance in the dataset. As a result, the model DeepGDLE effectively reduces the parameters of the network model and the network training cost. And extensive experiments compared with several existing semantic segmentation algorithms such as DeeplabV3+, UNet, SegNet, etc. show that DeepGDLE improves the quality and efficiency of image segmentation. Therefore, compared to other networks, the DeepGDLE network model can be more effectively applied to land cover classification. In addition, in order to investigate the effects of different factors on the semantic segmentation performance of remote sensing images and to verify the robustness of the DeepGDLE model, a new remote sensing image dataset, FRSID, is constructed in this paper. This dataset takes into account more influences than the public dataset. The experimental results show that on the WHDLD dataset, the experimental metrics mIoU, mPA, and mRecall of the proposed model, DeepGDLE, are 62.29%, 72.85%, and 72.46%, respectively. On the FRSID dataset, the metrics mIoU, mPA, and mRecall are 65.89%, 74.43%, and 74.08%, respectively. For the future scope of research in this field, it may focus on the fusion of multi-source remote sensing data and the intelligent interpretation of remote sensing images.",semantic Segmentation,remote sensing image,land cover classification,attention mechanism,"Wu, Peng","Yi, Xiaomei",,,classification of surface objects,,,,,,,,,,,,,,,,,,,,,,,,
Row_436,"He, You","Zhang, Hanchao","Ning, Xiaogang",Spatial-Temporal Semantic Perception Network for Remote Sensing Image Semantic Change Detection,REMOTE SENSING,AUG 2023,11,"Semantic change detection (SCD) is a challenging task in remote sensing, which aims to locate and identify changes between the bi-temporal images, providing detailed ""from-to"" change information. This information is valuable for various remote sensing applications. Recent studies have shown that multi-task networks, with dual segmentation branches and single change branch, are effective in SCD tasks. However, these networks primarily focus on extracting contextual information and ignore spatial details, resulting in the missed or false detection of small targets and inaccurate boundaries. To address the limitations of the aforementioned methods, this paper proposed a spatial-temporal semantic perception network (STSP-Net) for SCD. It effectively utilizes spatial detail information through the detail-aware path (DAP) and generates spatial-temporal semantic-perception features through combining deep contextual features. Meanwhile, the network enhances the representation of semantic features in spatial and temporal dimensions by leveraging a spatial attention fusion module (SAFM) and a temporal refinement detection module (TRDM). This augmentation results in improved sensitivity to details and adaptive performance balancing between semantic segmentation (SS) and change detection (CD). In addition, by incorporating the invariant consistency loss function (ICLoss), the proposed method constrains the consistency of land cover (LC) categories in invariant regions, thereby improving the accuracy and robustness of SCD. The comparative experimental results on three SCD datasets demonstrate the superiority of the proposed method in SCD. It outperforms other methods in various evaluation metrics, achieving a significant improvement. The Sek improvements of 2.84%, 1.63%, and 0.78% have been observed, respectively.",semantic change detection,change detection,semantic segmentation,spatial detail,"Zhang, Ruiqian","Chang, Dong","Hao, Minghui",,semantic perception,spatial-temporal semantic,,,,,,,,,,,,,,,,,,,,,,,
Row_437,"Xue, Guangkuo","Liu, Yikun","Huang, Yuwen",A3Seg: An Annealing Augmented and Aligned Semantic Segmentation Network for High Spatial Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,1,"Semantic segmentation of high spatial resolution (HSR) remote sensing images (RSIs) plays an important role in many applications. However, HSR RSIs have significantly larger spatial sizes than typical natural images, which results in fewer valuable samples when training models. In addition, fusing multiscale features is the key step in obtaining features with strong semantic and high spatial information. However, current feature fusion methods are too straightforward to address misalignment issues. To handle these two problems, we propose an annealing augmented and aligned segmentation network, named A(3)Seg. Specifically, we propose an annealing online hard example mining (AOHEM) strategy to automatically select more valuable samples during the training stage. Based on AOHEM, a contextual augmentation block is proposed to extract sufficient contextual information using three different attention mechanisms in consideration of three different feature properties. Finally, we propose a novel feature alignment block to fuse features at different levels by alignment with the guidance of a salient feature. Experimental results on three different HSR RSIs datasets demonstrate that the proposed method outperforms the state-of-the-art general semantic segmentation methods with a better tradeoff between accuracy and complexity.",Aligned feature fusion,remote sensing imagery,semantic segmentation,valuable sample mining,"Li, Mingsong","Yang, Gongping",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_438,"Wu, Honglin","Zhang, Min","Huang, Peng",CMLFormer: CNN and Multiscale Local-Context Transformer Network for Remote Sensing Images Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,7,"The characteristics of remote sensing images, such as complex ground objects, rich feature details, large intraclass variance and small interclass variance, usually require deep learning semantic segmentation methods to have strong feature learning representation ability. Due to the limitation of convolutional operation, convolutional neural networks (CNNs) are good at capturing local details, but perform poorly at modeling long-range dependencies. Transformers rely on multihead self-attention mechanisms to extract global contextual information, but it usually leads to high complexity. Therefore, this article proposes CNN and multiscale local-context transformer network (CMLFormer), a novel encoder-decoder structured network for remote sensing image semantic segmentation. Specifically, for the features extracted by the lightweight ResNet18 encoder, we design a transformer decoder based on multiscale local-context transform block (MLTB) to enhance the ability of feature learning. By using a self-attention mechanism with nonoverlapping windows and with the help of multiscale horizontal and vertical interactive stripe convolution, MLTB is able to capture both local feature information and global feature information at different scales with low complexity. In addition, the feature enhanced module is introduced into the decoder to further facilitate the learning of global and local information. Experimental results show that our proposed CMLFormer exhibits excellent performance on the Vaihingen and Potsdam datasets.",Transformers,Semantic segmentation,Remote sensing,Feature extraction,"Tang, Wenlong",,,,Computational modeling,Convolutional neural networks,Adaptation models,Convolutional neural network (CNN),multiscale,remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,
Row_439,"Geng, Jie","Song, Shuai","Jiang, Wen",Dual-Path Feature Aware Network for Remote Sensing Image Semantic Segmentation,IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY,MAY 2024,0,"Semantic segmentation is a significant task for remote sensing interpretation, which takes advantage of contextual semantic information to classify each pixel into a specific category. Most current methods apply convolutional neural networks (CNN) to learn feature representation from remote sensing images, which may ignore the global dependencies due to the limitation of convolutional kernels. Inspired by the global feature learning ability of Transformer, we propose a novel deep model called dual-path feature aware network (DPFANet), which combines the structure of CNN and Transformer for semantic segmentation of remote sensing images. DPFANet aims to learn effective modeling ability from local to global features of images. Simultaneously, an adaptive feature fusion network is developed to fuse features from dual-path networks. Moreover, an edge optimization block is applied to constrain the edge features, whose purpose is to obtain more representative features for segmentation. Experimental results on three public remote sensing datasets verify that our proposed network yields better segmentation performance compared to other related methods.",Semantics,Task analysis,feature fusion,transformer,,,,,remote sensing image,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,
Row_440,Priyanka,"Sravya, N.","Lal, Shyam",DIResUNet: Architecture for multiclass semantic segmentation of high resolution remote sensing imagery data,APPLIED INTELLIGENCE,OCT 2022,21,"Scene understanding is an important task in information extraction from high-resolution aerial images, an operation which is often involved in remote sensing applications. Recently, semantic segmentation using deep learning has become an important method to achieve state-of-the-art performance in pixel-level classification of objects. This latter is still a challenging task due to large pixel variance within classes possibly coupled with small pixel variance between classes. This paper proposes an artificial-intelligence (AI)-based approach to this problem, by designing the DIResUNet deep learning model. The model is built by integrating the inception module, a modified residual block, and a dense global spatial pyramid pooling (DGSPP) module, in combination with the well-known U-Net scheme. The modified residual blocks and the inception module extract multi-level features, whereas DGSPP extracts contextual intelligence. In this way, both local and global information about the scene are extracted in parallel using dedicated processing structures, resulting in a more effective overall approach. The performance of the proposed DIResUNet model is evaluated on the Landcover and WHDLD high resolution remote sensing (HRRS) datasets. We compared DIResUNet performance with recent benchmark models such as U-Net, UNet++, Attention UNet, FPN, UNet+SPP, and DGRNet to prove the effectiveness of our proposed model. Results show that the proposed DIResUNet model outperforms benchmark models on two HRRS datasets.",Deep learning,Semantic segmentation,Spatial pyramid pooling,Remote sensing,"Nalini, J.","Reddy, Chintala Sudhakar","Dell'Acqua, Fabio",,Residual block and inception module,,,,,,,,,,,,,,,,,,,,,,,,
Row_441,"Zhang, Bin","Wan, Yi","Zhang, Yongjun",JSH-Net: joint semantic segmentation and height estimation using deep convolutional networks from single high-resolution remote sensing imagery,INTERNATIONAL JOURNAL OF REMOTE SENSING,SEP 2 2022,7,"Semantic segmentation for high-resolution remote sensing imagery is a pivotal component of land use and land cover categorization, and height estimation is essential for rebuilding the 3D information of an image. Because of the higher intra-class variation and smaller inter-class dissimilarity, these two challenging tasks are generally treated separately. This paper proposes a fully convolutional network that can tackle these problems simultaneously by estimating the land-cover categories and height values of pixels from a single aerial image. To handle these tasks, we develop a multi-task learning architecture (JSH-Net) that employs a shared feature representation and exploits their potential consistency across tasks, resulting in robust features and better prediction accuracy. Specifically, we propose a novel skip connection module that aggregates the contexts from the encoder part to the decoder part, bridging the semantic gap between them. In addition, we propose a progressive refinement strategy to recover detailed information about the objects. Moreover, we also proposed a height estimation branch on the head of the model to utilize shared features. The experiments we conducted on ISPRS 2D Labelling dataset verified that our network provided precise results of semantic segmentation and height estimation from two output branches and outperformed other state-of-the-art approaches.",remote sensing,semantic segmentation,height estimation,deep learning,"Li, Yansheng",,,,multi-task learning,convolutional neural network,,,,,,,,,,,,,,,,,,,,,,,
Row_442,"Chong, Yanwen","Chen, Xiaoshu","Pan, Shaoming",Context Union Edge Network for Semantic Segmentation of Small-Scale Objects in Very High Resolution Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,17,"Semantic segmentation of small-scale objects in very high resolution (VHR) remote sensing images plays an important role in some special tasks, such as change detection and mapping of land cover. However, due to small size, small-scale objects are more likely to be completely obscured by shadows than large-scale objects, which make it difficult for the traditional convolutional neural network (CNN) to distinguish small-scale objects from shadows. Furthermore, even if small-scale objects are distinguished, their boundaries are still difficult to refine. To solve the above problems, a novel context union edge network (CEN) for small-scale objects semantic segmentation is proposed by comprehensively considering both the contextual and edge information. In CEN, a plug-and-play context-based feature enhancement module (CFEM) is designed to enhance the ability of CNNs to distinguish small-scale objects. Then, an information exchange mechanism (IEM) is proposed based on the dual-stream (semantic and edge stream) network to refine the boundaries of small-scale objects. Finally, some experiments based on the ISPRS Vaihingen data set are conducted in terms of both overall accuracy (OA) and F1-score. The proposed CEN achieves 89.9% of F1-score for small-scale objects (cars) and 90.9% of OA, harvesting new state-of-the-art results.",Semantics,Feature extraction,Image edge detection,Streaming media,,,,,Remote sensing,Image segmentation,Information exchange,Context,edge guidance,information exchange mechanism (IEM),semantic segmentation,small-scale objects,,,,,,,,,,,,,,,,,
Row_443,"Shi, Shuting","Li, Baohong","Zhang, Laifu",Causality-Guided Stepwise Intervention and Reweighting for Remote Sensing Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Semantic segmentation is one of the most significant tasks in remote sensing (RS) image interpretation, which focuses on learning global and local information to infer the semantic label of each pixel. Previous studies devise encoder-decoder structured deep learning (DL) models to extract global and local features from RS images with the help of pretraining knowledge to predict semantic labels. However, due to the common heterogeneity between the data for pretraining and the data to be semantically segmented, these models fail to learn general features appropriate to RS datasets. In this article, we propose a novel formulation of the above problem from a causal perspective, where the learned features from pretrained models result from causality and spurious correlations, and only the former carries general information that remains invariant regardless of the exact task and dataset. Based on the above formulation, we propose stepwise intervention and reweighting (SIR). It can reduce the confounding bias introduced by the pretraining knowledge and improve the model's ability to learn general features, making semantic segmentation of RS images benefit more from pretraining. Besides, we conduct a detailed theoretical analysis of our methods and conduct extensive experiments on two widely used public RS datasets. Experimental results demonstrate that applying SIR to encoder-decoder semantic segmentation models achieves performance improvements, proving the effectiveness and application values of the proposed method.",Task analysis,Semantics,Transformers,Feature extraction,"Kuang, Kun","Wu, Sensen","Feng, Tian","Yan, Yiming",Unified modeling language,Data models,Causal inference,deep learning (DL),remote sensing (RS),semantic segmentation,transfer learning,transfer learning,"Du, Zhenhong",,,,,,,,,,,,,,,,
Row_444,"Zhang, Xiuwei","Yang, Yizhe","Ran, Lingyan",Remote Sensing Image Semantic Change Detection Boosted by Semi-Supervised Contrastive Learning of Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Semantic change detection (SCD) is a challenging task in remote sensing image (RSI) interpretation, which adopts multitemporal images to detect, locate, and analyze pixel-level land-cover ""from-to"" changes. In SCD, the severe class imbalance problem and the occurrence of confusing categories are very typical, making it challenging to accurately distinguish the easily confused categories with limited semantic context information. However, previous works did not address these issues in depth. This article proposes a novel SCD method named semi-supervised contrastive learning (SSCLNet), in which a simple and effective SCD network is designed as a strong baseline, and a semi-supervised contrastive learning module of semantic segmentation (SS) is presented to enhance the distinguishability of categories. Our baseline extracts semantic context through high-resolution network (HRNet), gets change information simply through an absolute difference, and then directly performs SCD based on the fusion of semantic context and change information. To utilize the semantic context information of the unlabeled non-changed regions, we employ a self-training (ST) method for semi-supervised SS. To learn distinguishable feature representations for easily confused categories, we present contrastive learning with an adaptive sampling strategy for SS. It selects challenging negative samples for each category from the other categories that exhibit similar features or attributes. The sampling space includes both the labeled changed samples and the non-changed samples predicted by ST. The comprehensive experiments on the SECOND and the Landsat-SCD dataset demonstrate that the proposed SSCLNet achieves the state-of-the-art (SOTA) performance, with a significant improvement of 2.07% and 4.15% in the score value, respectively.",Semantics,Remote sensing,Semantic segmentation,Task analysis,"Chen, Liang","Wang, Kangwei","Yu, Lei","Wang, Peng",Self-supervised learning,Earth,Training,Contrastive learning,self-training (ST),semantic change detection (SCD),,,"Zhang, Yanning",,,,,,,,,,,,,,,,
Row_445,"Song, Wanying","Nie, Fangxin","Wang, Chi",Unsupervised Multi-Scale Hybrid Feature Extraction Network for Semantic Segmentation of High-Resolution Remote Sensing Images,REMOTE SENSING,OCT 2024,0,"Generating pixel-level annotations for semantic segmentation tasks of high-resolution remote sensing images is both time-consuming and labor-intensive, which has led to increased interest in unsupervised methods. Therefore, in this paper, we propose an unsupervised multi-scale hybrid feature extraction network based on the CNN-Transformer architecture, referred to as MSHFE-Net. The MSHFE-Net consists of three main modules: a Multi-Scale Pixel-Guided CNN Encoder, a Multi-Scale Aggregation Transformer Encoder, and a Parallel Attention Fusion Module. The Multi-Scale Pixel-Guided CNN Encoder is designed for multi-scale, fine-grained feature extraction in unsupervised tasks, efficiently recovering local spatial information in images. Meanwhile, the Multi-Scale Aggregation Transformer Encoder introduces a multi-scale aggregation module, which further enhances the unsupervised acquisition of multi-scale contextual information, obtaining global features with stronger feature representation. The Parallel Attention Fusion Module employs an attention mechanism to fuse global and local features in both channel and spatial dimensions in parallel, enriching the semantic relations extracted during unsupervised training and improving the performance of unsupervised semantic segmentation. K-means clustering is then performed on the fused features to achieve high-precision unsupervised semantic segmentation. Experiments with MSHFE-Net on the Potsdam and Vaihingen datasets demonstrate its effectiveness in significantly improving the accuracy of unsupervised semantic segmentation.",high-resolution remote sensing,unsupervised,semantic segmentation,global context information,"Jiang, Yinyin","Wu, Yan",,,fine-grained features,feature fusion,,,,,,,,,,,,,,,,,,,,,,,
Row_446,"Qi, Xiyu","Mao, Yongqiang","Zhang, Yidan",PICS: Paradigms Integration and Contrastive Selection for Semisupervised Remote Sensing Images Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,7,"Remote sensing images semantic segmentation is a fundamental yet challenging task, which has long relied heavily on sufficient pixelwise annotations. Semisupervised learning is proposed to address the problem of high dependence on labeled data by exploiting more learnable samples generated from the large amounts of accessible unlabeled data. However, affected by the complexity and diversity of remote sensing images, various misclassifications often occur and lead to errors accumulation during model training. Errors accumulation will destroy the consistency of model training and lead to degradation of final segmentation performance. In this article, in order to further alleviate the damage caused by the errors to the consistency of model training and improve final segmentation accuracy, we propose a novel semisupervised segmentation framework, paradigms integration and contrastive selection (PICS). First, multiple proven semisupervised paradigms are integrated to generate pseudolabeled samples with less noise. Second, a loss-based contrastive selection method is explored to distinguish generated samples that contain different degrees of inevitable misclassification, thereby further maintaining the approximation of the generated samples and the ground truth in the sample space. By generating and selecting high-quality pseudolabeled samples for selective self-training, we can better guarantee consistency during model training and obtain better segmentation results. Extensive experiments over the ISPRS Vaihingen, Potsdam, and the challenging iSAID benchmarks demonstrate that our method yields significant accuracy boosting on the segmentation results and achieves on-par performance with the state of the arts.",Remote sensing,Training,Semantic segmentation,Semisupervised learning,"Deng, Yawen","Wei, Haoran","Wang, Lei",,Data models,Predictive models,Visualization,Paradigms integration,remote sensing,selective self-training,semantic segmentation,semisupervised learning (SSL),,,,,,,,,,,,,,,,,
Row_447,"Ma, Xiaowen","Che, Rui","Wang, Xinyu",DOCNet: Dual-Domain Optimized Class-Aware Network for Remote Sensing Image Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,4,"The spatial attention mechanism has been frequently employed for the semantic segmentation of remote sensing images, given its renowned capability to model long-range dependencies. As remote sensing images often exhibit intricate backgrounds, significant intraclass variability, and a foreground-background imbalance, spatial attention mechanism-based methods somehow tend to introduce an extensive amount of background context through intensive affinity operations, causing unsatisfactory segmentation outcomes. While several class-aware methods attempt to attenuate the interference of background context by generating class representations as representative features, they still encounter challenges related to independent correlation calculation and single-confidence scale class representations. We introduce a dual-domain optimized class-aware network designed to address these challenges. In the semantic domain, we use category confidence as a scaling criterion to derive class representations at multiple confidence scales, effectively modeling pixel-class relationships. In the spatial domain, we leverage pixel-class relationships and their consensus to enhance relevant correlations while suppressing erroneous ones. Experimental results on three datasets demonstrate that the proposed method surpasses previous state-of-the-art ones for remote sensing image segmentation. Code is available at https://github.com/xwmaxwma/rssegmentation.",Class-aware,pixel-class relation,remote sensing,semantic segmentation,"Ma, Mengting","Wu, Sensen","Feng, Tian","Zhang, Wei",,,,,,,,,,,,,,,,,,,,,,,,,
Row_448,"Ni, Huan","Liu, Qingshan","Guan, Haiyan",Category-Level Assignment for Cross-Domain Semantic Segmentation in Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,12,"Deep learning-based semantic segmentation has made great progress in understanding very-high-resolution (VHR) remote sensing images (RSIs). However, large-scale applications are still limited. The main reason is that diverse imaging modes and geographical differences make it difficult to transfer a model trained in the source domain to the target domain. To solve this problem, unsupervised domain adaptation (UDA) for VHR RSIs has received some attention, but the accuracy of cross-domain semantic segmentation still needs to be improved. Currently, one reasonable proposal for improving accuracy is to take a close look at the category-level information. In this article, we reveal an integer programming mechanism for modeling the category-level relationship between the source and target domains. The mechanism is based on the solution of the assignment problem, and thus, the proposed method is called category-level assignment for UDA (ClA-UDA). In ClA-UDA, a category-level assignment problem with additional constraints is defined for UDA tasks, and the solution is provided. Based on the solution, an assignment-based image-to-image transferring algorithm (AIT) is first proposed to transfer the source-domain images based on the style of the target-domain images. AIT minimizes a weighted discrepancy and provides an analytical solution for the transfer. Two assignment-based alignment losses are then introduced to align the source and target domains based on the category-level relationship in a concise way. To validate the performance of ClA-UDA, three VHR RSI datasets are employed, and six UDA tasks are designed. Extensive experiments are conducted, and the results demonstrate the superiority of ClA-UDA compared with the existing methods.",Training,Semantic segmentation,Remote sensing,Task analysis,"Tang, Hong","Chanussot, Jocelyn",,,Adaptation models,Semantics,Adversarial machine learning,Assignment problem,category-level relationship,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,
Row_449,"Li, Zhenghong","Chen, Hao","Wu, Jiangjiang",SegMind: Semisupervised Remote Sensing Image Semantic Segmentation With Masked Image Modeling and Contrastive Learning Method,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,6,"Remote sensing (RS) image semantic segmentation has attracted much attention due to its wide applications. However, deep learning-based RS image semantic segmentation methods usually require substantial manual pixelwise annotations, which are expensive and hard to obtain in practice. Although the existing semisupervised RS semantic segmentation methods effectively reduce dependence on labeled data, they generally focus on information consistency between labeled and unlabeled images, but ignore the potential context information between different areas of the RS image. In fact, the objects contained in an RS image usually have some long-range dependence between each other, since trees are usually on both sides of a road, and the middle of two rows of houses is commonly a road. Therefore, we believe that the potential dependencies between different areas of the RS image should be beneficial to reduce the label dependence of RS semantic segmentation. Based on this point, we propose a novel semisupervised RS image semantic segmentation network named SegMind, which is based on mean-teacher (MT) architecture and adopts masked image modeling (MIM) to enhance information interactions of different areas. Moreover, contrastive learning (CL) and entropy loss are introduced to SegMind framework to further improve the linear separability and prediction confidence of the proposed model. Experiments on three datasets have demonstrated the superiority of the proposed method over the state-of-the-art methods. The code is available at https://github.com/lzh-ggs-ddu/SegMind.",Contrastive learning (CL),masked image mod-eling (MIM),remote sensing (RS) image semantic segmentation,semisupervised learning,"Li, Jun","Jing, Ning",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_450,"da Silva, Caio C., V","Nogueira, Keiller","Oliveira, Hugo N.",TOWARDS OPEN-SET SEMANTIC SEGMENTATION OF AERIAL IMAGES,,2020,14,"Classical and more recently deep computer vision methods are optimized for visible spectrum images, commonly encoded in grayscale or ROB colorspaces acquired from smartphones or cameras. A more uncommon source of images exploited in the remote sensing field are satellite and aerial images. However the development of pattern recognition approaches for these data is relatively recent, mainly due to the limited availability of this type of images, as until recently they were used exclusively for military purposes. Access to aerial imagery, including spectral information, has been increasing mainly due to the low cost of drones, cheapening of imaging satellite launch costs, and novel public datasets. Usually remote sensing applications employ computer vision techniques strictly modeled for classification tasks in closed set scenarios. However, real-world tasks rarely fit into closed set contexts, frequently presenting previouSly unknown classes, characterizing them as open set scenarios. Focusing on this problem, this is the first paper to study and develop semantic segmentation techniques for open set scenarios applied to remote sensing images. The main contributions of this paper are: I) a discussion of related works in open set semantic segmentation, showing evidence that these techniques can be adapted for open set remote sensing tasks; 2) the development and evaluation of a novel approach for open set semantic segmentation. Our method yielded competitive results when compared to closed set methods for the same dataset.",Open Set,Deep Learning,Semantic Segmentation,Remote Sensing,"dos Santos, Jefersson A.",,,,,,,,,,,,,2020 IEEE LATIN AMERICAN GRSS & ISPRS REMOTE SENSING CONFERENCE (LAGIRS),,,,,,,,,,,,,,,
Row_451,"Zheng, Wei","Feng, Jiangfan","Gu, Zhujun",A Stage-Adaptive Selective Network with Position Awareness for Semantic Segmentation of LULC Remote Sensing Images,REMOTE SENSING,MAY 29 2023,3,"Deep learning has proven to be highly successful at semantic segmentation of remote sensing images (RSIs); however, it remains challenging due to the significant intraclass variation and interclass similarity, which limit the accuracy and continuity of feature recognition in land use and land cover (LULC) applications. Here, we develop a stage-adaptive selective network that can significantly improve the accuracy and continuity of multiscale ground objects. Our proposed framework can learn to implement multiscale details based on a specific attention method (SaSPE) and transformer that work collectively. In addition, we enhance the feature extraction capability of the backbone network at both local and global scales by improving the window attention mechanism of the Swin Transfer. We experimentally demonstrate the success of this framework through quantitative and qualitative results. This study demonstrates the strong potential of the prior knowledge of deep learning-based models for semantic segmentation of RSIs.",remote sensing,semantic segmentation,position awareness,attention network,"Zeng, Maimai",,,,land use and land cover (LULC),,,,,,,,,,,,,,,,,,,,,,,,
Row_452,"Xiao, Tao","Liu, Yikun","Huang, Yuwen",Enhancing Multiscale Representations With Transformer for Remote Sensing Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,49,"Semantic segmentation is an extremely challenging task in high-resolution remote sensing (HRRS) images as objects have complex spatial layouts and enormous variations in appearance. Convolutional neural networks (CNNs) have an excellent ability to extract local features and have been widely applied as a feature extractor for various vision tasks. However, due to the inherent inductive bias of convolution operation, CNNs inevitably have limitations in modeling long-range dependencies. Transformer can capture global representations well but unfortunately ignores the details of local features and has high computational and spatial complexity in processing high-resolution feature maps. In this article, we propose a novel hybrid architecture for HRRS image segmentation, termed Enhancing Multiscale Representations with Transformer (EMRT), to exploit the advantages of convolution operations and Transformer to enhance multiscale representation learning. We incorporate the deformable self-attention mechanism in the Transformer to automatically adjust the receptive field and design an encoder-decoder architecture accordingly to achieve efficient context modeling. Specifically, CNN is constructed to extract feature representations. In the encoder, local features and global representations at different resolutions are extracted by the CNN and Transformer, respectively, and fused in an interactive manner. Moreover, a separate spatial branch is designed to extract multiscale contextual information as queries, and global dependencies between features at different scales are efficiently established by the decoder. Extensive experiments on three public remote sensing datasets demonstrate the superiority of EMRT and indicate that the overall performance of our method outperforms state-of-the-art methods. Code is available at https://github.com/peach-xiao/EMRT.",Transformers,Feature extraction,Remote sensing,Task analysis,"Li, Mingsong","Yang, Gongping",,,Convolutional neural networks,Semantic segmentation,Semantics,Convolutional neural networks (CNNs),multiscale representation learning,remote sensing,semantic segmentation,Transformer,,,,,,,,,,,,,,,,,
Row_453,"Li, Yansheng","Ouyang, Song","Zhang, Yongjun",Combining deep learning and ontology reasoning for remote sensing image semantic segmentation,KNOWLEDGE-BASED SYSTEMS,MAY 11 2022,44,"Because of its wide potential applications, remote sensing (RS) image semantic segmentation has attracted increasing research interest in recent years. Until now, deep semantic segmentation network (DSSN) has achieved a certain degree of success on semantic segmentation of RS imagery and can obviously outperform the traditional methods based on hand-crafted features. As a classic data-driven technique, DSSN can be trained by an end-to-end mechanism and is competent for employing lowlevel and mid-level cues (i.e., the discriminative image structure) to understand RS images. However, its interpretability and reliability are poor due to the nature weakness of the data-driven deep learning methods. By contrast, human beings have an excellent inference capacity and can reliably interpret RS imagery with the basic RS domain knowledge. Ontological reasoning is an ideal way to imitate and employ the domain knowledge of human beings. However, it is still rarely explored and adopted in the RS domain. As a solution of the aforementioned critical limitation of DSSN, this study proposes a collaboratively boosting framework (CBF) to combine the data-driven deep learning module and knowledge-guided ontology reasoning module in an iterative manner. The deep learning module adopts the DSSN architecture and takes the integration of the original image and inferred channels as the input of the DSSN. In addition, the ontology reasoning module is composed of intra-and extra-taxonomy reasoning. More specifically, the intra-taxonomy reasoning directly corrects misclassifications of the deep learning module based on the domain knowledge, which is the key to improve the classification performance. The extra-taxonomy reasoning aims to generate the inferred channels beyond the current taxonomy to improve the discriminative performance of DSSN in the original RS image space. On the one hand, benefiting from the referred channels from the ontology reasoning module, the deep learning module using the integration of the original image and referred channels can achieve better classification performance than only using the original image. On the other hand, better classification results from the deep learning module further improve the performance of the ontology reasoning module. As a whole, the deep learning and ontology reasoning modules are mutually boosted in the iterations. Extensive experiments on two publicly open RS datasets such as UCM and ISPRS Potsdam show that our proposed CBF can outperform the competitive baselines with a large margin. (c) 2022 Elsevier B.V. All rights reserved.",Collaboratively boosting framework (CBF),Deep learning,Ontology reasoning,Deep semantic segmentation network (DSSN),,,,,Remote sensing (RS) imagery,,,,,,,,,,,,,,,,,,,,,,,,
Row_454,"Wang, Xinhua","Yuan, Botao","Li, Zhuang",A Fractal Curve-Inspired Framework for Enhanced Semantic Segmentation of Remote Sensing Images,SENSORS,NOV 2024,1,"The classification and recognition of features play a vital role in production and daily life; however, the current semantic segmentation of remote sensing images is hampered by background interference and other factors, leading to issues such as fuzzy boundary segmentation. To address these challenges, we propose a novel module for encoding and reconstructing multi-dimensional feature layers. Our approach first utilizes a bilinear interpolation method to downsample the multi-dimensional feature layer in the coding stage of the U-shaped framework. Subsequently, we incorporate a fractal curve module into the encoder, which aggregates points on feature maps from different layers, effectively grouping points from diverse regions. Finally, we introduce an aggregation layer that combines the upsampling method from the UNet series, employing the multi-scale censoring of multi-dimensional feature map outputs from various layers to efficiently capture both spatial and feature information. The experimental results across diverse scenarios demonstrate that our model achieves excellent performance in aggregating point information from feature maps, significantly enhancing semantic segmentation tasks.",remote sensing images,bilinear interpolation,fractal curve,gather layers,"Wang, Heqi",,,,encoder-decoder,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_455,"Zheng, Chen","Hu, Chen","Chen, Yuncheng",A Self-Learning-Update CNN Model for Semantic Segmentation of Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,11,"Convolutional neural network (CNN) has been widely used in semantic segmentation for remote sensing images, and it has achieved great success. Due to the diversity of the spatial distribution of terrestrial objects in remote sensing images, it is difficult to effectively learn general geographical laws and apply them to a specific image. To introduce geographical knowledge into the CNN model more effectively, a self-learning-update CNN model (SLU-CNN) is proposed in this letter. It learns the representation of specific spatial dependence among different objects according to the CNN result, and then incorporates it with the CNN result to make semantic inference available. The proposed method mainly involves two modules. First, geographical objects generated from the CNN result are used as inference units. Second, the spatial dependence between inference units is learned to build a specific adaptive geographical relationship. And then, it is embedded as an adaptive penalty term into an object-based Markov random field model (OMRF) to achieve the collaboration between the CNN result and the semantic inference. Our method provides a general data-knowledge dual-driven framework for the deep neural network. Experiments of the Gaofen Image Dataset (GID) and Sentinel-2 datasets validate the effectiveness of the proposed method by comparing it with different state-of-the-art CNN methods.",Mathematical models,Convolutional neural networks,Semantics,Remote sensing,"Li, Jingying",,,,Semantic segmentation,Knowledge engineering,Training,Convolutional neural network (CNN),geographical knowledge,Markov random field,semantic inference,semantic segmentation,,,,,,,,,,,,,,,,,
Row_456,"Cao, Yong","Shi, Yiwen","Liu, Yiwei",Dual Stream Fusion Network for Multi-spectral High Resolution Remote Sensing Image Segmentation,"PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2021, PT II",2021,1,"Semantic segmentation is in-demand in High Resolution Remote Sensing (HRRS) image processing. Unlike natural images, HRRS images usually provide channels such as Near Infrared (NIR) in addition to RGB channels. However, in order to make use of the pre-trained model, the current semantic segmentation methods in remote sensing field usually only use the RGB channel and discard the information of other channels. In this paper, to make full use of the HRRS image information, a dual-stream fusion network is proposed to fuse the information of different channel combinations through a Feature Pyramid Network (FPN), then a Stage Pyramid Pooling (SPP) module is used to integrate the features of different scales and produce the final segmentation results. Experiments on the RSCUP competition dataset show that the proposed approach can effectively improve the segmentation performance.",Semantic segmentation,Remote sensing,Stream fusion,,"Huo, Chunlei","Xiang, Shiming","Pan, Chunhong",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_457,"Bi, Hanbo","Feng, Yingchao","Mao, Yongqiang",AgMTR: Agent Mining Transformer for Few-Shot Segmentation in Remote Sensing,INTERNATIONAL JOURNAL OF COMPUTER VISION,OCT 2024,0,"Few-shot Segmentation aims to segment the interested objects in the query image with just a handful of labeled samples (i.e., support images). Previous schemes would leverage the similarity between support-query pixel pairs to construct the pixel-level semantic correlation. However, in remote sensing scenarios with extreme intra-class variations and cluttered backgrounds, such pixel-level correlations may produce tremendous mismatches, resulting in semantic ambiguity between the query foreground (FG) and background (BG) pixels. To tackle this problem, we propose a novel Agent Mining Transformer, which adaptively mines a set of local-aware agents to construct agent-level semantic correlation. Compared with pixel-level semantics, the given agents are equipped with local-contextual information and possess a broader receptive field. At this point, different query pixels can selectively aggregate the fine-grained local semantics of different agents, thereby enhancing the semantic clarity between query FG and BG pixels. Concretely, the Agent Learning Encoder is first proposed to erect the optimal transport plan that arranges different agents to aggregate support semantics under different local regions. Then, for further optimizing the agents, the Agent Aggregation Decoder and the Semantic Alignment Decoder are constructed to break through the limited support set for mining valuable class-specific semantics from unlabeled data sources and the query image itself, respectively. Extensive experiments on the remote sensing benchmark iSAID indicate that the proposed method achieves state-of-the-art performance. Surprisingly, our method remains quite competitive when extended to more common natural scenarios, i.e., PASCAL-5i\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$5<^>i$$\end{document} and COCO-20i\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$20<^>{i}$$\end{document}.",Few-shot learning,Few-shot segmentation,Remote sensing,Semantic segmentation,"Pei, Jianning","Diao, Wenhui","Wang, Hongqi","Sun, Xian",,,,,,,,,,,,,,,,,,,,,,,,,
Row_458,"Bokhovkin, Alexey","Burnaev, Evgeny",,Boundary Loss for Remote Sensing Imagery Semantic Segmentation,"ADVANCES IN NEURAL NETWORKS - ISNN 2019, PT II",2019,84,"In response to the growing importance of geospatial data, its analysis including semantic segmentation becomes an increasingly popular task in computer vision today. Convolutional neural networks are powerful visual models that yield hierarchies of features and practitioners widely use them to process remote sensing data. When performing remote sensing image segmentation, multiple instances of one class with precisely defined boundaries are often the case, and it is crucial to extract those boundaries accurately. The accuracy of segments boundaries delineation influences the quality of the whole segmented areas explicitly. However, widely-used segmentation loss functions such as BCE, IoU loss or Dice loss do not penalize misalignment of boundaries sufficiently. In this paper, we propose a novel loss function, namely a differentiable surrogate of a metric accounting accuracy of boundary detection. We can use the loss function with any neural network for binary segmentation. We performed validation of our loss function with various modifications of UNet on a synthetic dataset, as well as using real-world data (ISPRS Potsdam, INRIA AIL). Trained with the proposed loss function, models outperform baseline methods in terms of IoU score.",Semantic segmentation,Deep learning,Aerial imagery,CNN,,,,,Loss function,Building detection,Computer vision,,,,,,,,,,,,,,,,,,,,,,
Row_459,"Zhang, Zhaoyang","Ren, Zhen","Tao, Chao",GraSS: Contrastive Learning With Gradient-Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,22,"Self-supervised contrastive learning (SSCL) has achieved significant milestones in remote sensing image (RSI) understanding. Its essence lies in designing an unsupervised instance discrimination pretext task to extract image features from a large number of unlabeled images that are beneficial for downstream tasks. However, existing instance discrimination-based SSCL suffers from two limitations when applied to the RSI semantic segmentation task: 1) positive sample confounding issue (SCI), SSCL treats different augmentations of the same RSI as positive samples, but the richness, complexity, and imbalance of RSI ground objects lead to the model actually pulling a variety of different ground objects closer while pulling positive samples closer, which confuse the feature of different ground objects, and 2) feature adaptation bias, SSCL treats RSI patches containing various ground objects as individual instances for discrimination and obtains instance-level features, which are not fully adapted to pixel-level or object-level semantic segmentation tasks. To address the above limitations, we consider constructing samples containing single ground objects to alleviate positive SCI and make the model obtain object-level features from the contrastive between single ground objects. Meanwhile, we observed that the discrimination information can be mapped to specific regions in RSI through the gradient of unsupervised contrastive loss, and these specific regions tend to contain single ground objects. Based on this, we propose contrastive learning with gradient-guided sampling strategy (GraSS) for RSI semantic segmentation. GraSS consists of two stages: 1) the instance discrimination warm-up stage to provide initial discrimination information to the contrastive loss gradients and 2) the gradient-guided sampling contrastive training stage to adaptively construct samples containing more singular ground objects using the discrimination information. Experimental results on three open datasets demonstrate that GraSS effectively enhances the performance of SSCL in high-resolution RSI semantic segmentation. Compared with eight baseline methods from six different types of SSCL, GraSS achieves an average improvement of 1.57% and a maximum improvement of 3.58% in terms of mean intersection over the union (mIoU). In addition, we discovered that the unsupervised contrastive loss gradients contain rich feature information, which inspires us to use gradient information more extensively during model training to attain additional model capacity. The source code is available at https://github.com/GeoX-Lab/GraSS.",Contrastive loss,gradient-guided,remote sensing image (RSI),self-supervised learning,"Zhang, Yunsheng","Peng, Chengli","Li, Haifeng",,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_460,"Che, Zhihao","Shen, Li","Huo, Lianzhi",MAFF-HRNet: Multi-Attention Feature Fusion HRNet for Building Segmentation in Remote Sensing Images,REMOTE SENSING,MAR 2023,13,"Built-up areas and buildings are two main targets in remote sensing research; consequently, automatic extraction of built-up areas and buildings has attracted extensive attention. This task is usually difficult because of boundary blur, object occlusion, and intra-class inconsistency. In this paper, we propose the multi-attention feature fusion HRNet, MAFF-HRNet, which can retain more detailed features to achieve accurate semantic segmentation. The design of a pyramidal feature attention (PFA) hierarchy enhances the multilevel semantic representation of the model. In addition, we develop a mixed convolutional attention (MCA) block, which increases the capture range of receptive fields and overcomes the problem of intra-class inconsistency. To alleviate interference due to occlusion, a multiscale attention feature aggregation (MAFA) block is also proposed to enhance the restoration of the final prediction map. Our approach was systematically tested on the WHU (Wuhan University) Building Dataset and the Massachusetts Buildings Dataset. Compared with other advanced semantic segmentation models, our model achieved the best IoU results of 91.69% and 68.32%, respectively. To further evaluate the application significance of the proposed model, we migrated a pretrained model based on the World-Cover Dataset training to the Gaofen 16 m dataset for testing. Quantitative and qualitative experiments show that our model can accurately segment buildings and built-up areas from remote sensing images.",remote sensing,building extraction,built-up extraction,semantic segmentation,"Hu, Changmiao","Wang, Yanping","Lu, Yao","Bi, Fukun",,,,,,,,,,,,,,,,,,,,,,,,,
Row_461,"Luo, Wen","Deng, Fei","Jiang, Peifan",FSegNet: A Semantic Segmentation Network for High-Resolution Remote Sensing Images That Balances Efficiency and Performance,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,0,"In recent years, convolutional neural networks (CNNs) and vision transformers (ViTs) have become the mainstream segmentation methods for high-resolution remote sensing images (HRSIs). CNNs can quickly acquire the correlation between local neighboring pixels through convolutional operations, but it is difficult to establish global contextual relationships, resulting in limited segmentation accuracy. ViTs are able to establish reliable global semantic dependencies through the mechanism of self-attention, but the quadratic computational complexity of self-attention makes the ViTs present high accuracy but low efficiency. Therefore, in this letter, to balance the efficiency and accuracy of HRSIs segmentation, we combine the respective advantages of CNNs and ViTs to propose the FSegNet network. Specifically, we introduce FasterViT and utilize its efficient hierarchical attention (HAT) to mitigate the surge in self-attention computation due to the high resolution of HRSIs. On this basis, we construct a lightweight decoder based on intensive computation, which achieves fast generation of segmentation results by reshaping and mapping multilevel features. Experiments on the ISPRS Potsdam and Vaihingen datasets show that the proposed FSegNet best balances performance and efficiency. The code is available at https://github.com/Rowan-L/FSegNet.",Feature extraction,Image segmentation,Decoding,Semantics,"Dong, Xiujun","Zhang, Gulan",,,Convolution,Spatial resolution,Remote sensing,FasterViT,high-resolution remote sensing images (HRSIs),semantic segmentation,,,,,,,,,,,,,,,,,,,
Row_462,"Li, Jiaju","Wang, Hefeng","Zhang, Anbing",Semantic Segmentation of Hyperspectral Remote Sensing Images Based on PSE-UNet Model,SENSORS,DEC 2022,13,"With the development of deep learning, the use of convolutional neural networks (CNN) to improve the land cover classification accuracy of hyperspectral remote sensing images (HSRSI) has become a research hotspot. In HSRSI semantics segmentation, the traditional dataset partition method may cause information leakage, which poses challenges for a fair comparison between models. The performance of the model based on ""convolutional-pooling-fully connected"" structure is limited by small sample sizes and high dimensions of HSRSI. Moreover, most current studies did not involve how to choose the number of principal components with the application of the principal component analysis (PCA) to reduce dimensionality. To overcome the above challenges, firstly, the non-overlapping sliding window strategy combined with the judgment mechanism is introduced, used to split the hyperspectral dataset. Then, a PSE-UNet model for HSRSI semantic segmentation is designed by combining PCA, the attention mechanism, and UNet, and the factors affecting the performance of PSE-UNet are analyzed. Finally, the cumulative variance contribution rate (CVCR) is introduced as a dimensionality reduction metric of PCA to study the Hughes phenomenon. The experimental results with the Salinas dataset show that the PSE-UNet is superior to other semantic segmentation algorithms and the results can provide a reference for HSRSI semantic segmentation.",hyperspectral remote sensing images,dataset partition method,convolutional neural networks,PSE-UNet,"Liu, Yuliang",,,,Hughes phenomenon,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_463,"Su, Yanzhou","Wu, Yongjian","Wang, Min",SEMANTIC SEGMENTATION OF HIGH RESOLUTION REMOTE SENSING IMAGE BASED ON BATCH-ATTENTION MECHANISM,,2019,15,"Deep convolution neural network has been widely used in recent works for semantic segmentation of High Resolution Remote Sensing(HRRS) images. Because of the limitation of GPU memory, HRRS images are usually split into several sub-images for training convolutional neural networks. For each sub-image, the segmentation model may not have enough information to predict the segmentation map very well. In order to alleviate this problem, we propose to apply a batch-attention module to capture the discriminative information from similar objects, which come from other sub-images in a mini-batch. We also utilize global attention upsample module as the decoder to provide global context and fuse high and low level information better. We evaluate our model on the Potsdam dataset and achieve 88.30% pixAcc and 73.78% mIoU.",HRRS images,Semantic Segmentation,batch-attention,Global Attention Upsample,"Wang, Feng","Cheng, Jian",,,,,,,,,,,,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),,,,,,,,,,,,,,,
Row_464,"Zhou, Yuan","Zhu, Jiahang","Huo, Leigang",MULTI-TASK LEARNING FOR SEMANTIC CHANGE DETECTION ON VHR REMOTE SENSING IMAGES,,2022,2,"Remote Sensing Images Change Detection (RSICD) aims to locate the changed regions between bitemporal very-high-resolution (VHR) sensing images. However, existing deep learning-based RSICD methods are from the requirements by practical application, mainly due to the low feature discrimination and limited accuracy. We propose a novel multi-task and multi-temporal encoder-decoder changed detection network (MMNet) for VHR images, which accomplished both semantic segmentation and change detection at the same time. The encoder extracts multi-level contextual information, which contains two semantic segmentation branches (SSB) and a change detection branch (CDB). In this way, change representation constrains semantic representation during training, which introduces a novel loss function to ensure the semantic consistency within the unchanged regions. Furthermore, to utilize multi-level feature representation for enhancing the separability of features, a multi-scale feature fusion module (MFFM) is presented.",Change detection,Multi-task learning,Semantic segmentation,Deep learning,"Huo, Chunlei",,,,,,,,,,,,,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),,,,,,,,,,,,,,,
Row_465,"Li, Xin","Li, Tao","Chen, Ziqi",Attentively Learning Edge Distributions for Semantic Segmentation of Remote Sensing Imagery,REMOTE SENSING,JAN 2022,18,"Semantic segmentation has been a fundamental task in interpreting remote sensing imagery (RSI) for various downstream applications. Due to the high intra-class variants and inter-class similarities, inflexibly transferring natural image-specific networks to RSI is inadvisable. To enhance the distinguishability of learnt representations, attention modules were developed and applied to RSI, resulting in satisfactory improvements. However, these designs capture contextual information by equally handling all the pixels regardless of whether they around edges. Therefore, blurry boundaries are generated, rising high uncertainties in classifying vast adjacent pixels. Hereby, we propose an edge distribution attention module (EDA) to highlight the edge distributions of leant feature maps in a self-attentive fashion. In this module, we first formulate and model column-wise and row-wise edge attention maps based on covariance matrix analysis. Furthermore, a hybrid attention module (HAM) that emphasizes the edge distributions and position-wise dependencies is devised combing with non-local block. Consequently, a conceptually end-to-end neural network, termed as EDENet, is proposed to integrate HAM hierarchically for the detailed strengthening of multi-level representations. EDENet implicitly learns representative and discriminative features, providing available and reasonable cues for dense prediction. The experimental results evaluated on ISPRS Vaihingen, Potsdam and DeepGlobe datasets show the efficacy and superiority to the state-of-the-art methods on overall accuracy (OA) and mean intersection over union (mIoU). In addition, the ablation study further validates the effects of EDA.",semantic segmentation,remote sensing imagery,covariance matrix analysis,edge distributions,"Zhang, Kaiwen","Xia, Runliang",,,end-to-end neural network,,,,,,,,,,,,,,,,,,,,,,,,
Row_466,"Zheng, Jianhua","Fu, Yusha","Chen, Xiaohan",EGCM-UNet: Edge Guided Hybrid CNN-Mamba UNet for farmland remote sensing image semantic segmentation,GEOCARTO INTERNATIONAL,DEC 31 2025,0,"Segmenting farmland images is challenging due to their high color similarity to the background and irregular shapes, resulting in over/undersegmentation. To tackle these challenges, we propose the Edge Guided Hybrid CNN-Mamba UNet (EGCM-UNet) and design the oriented residual convolutional edge branch (ORCEB) to mine prior edge information. Additionally, the model designs a MaUNet module, which introduces the Visual State Space (VSS) block fused with Mamba to manage long-distance dependencies of image features, and uses the Edge-Guided Semantic Aggregation Module (EGSAM) for precise segmentation by fusing edge features with the VSS block's output. Lastly, comparative experiments were conducted using selected baseline models on the AgriculturalField-Seg dataset. The results show that EGCM-UNet outperformed U-Net with a Mean Intersection over Union (mIoU) of 0.394 vs. 0.379 on the test set. This indicates the proposed model delivers good performance in the semantic segmentation task of farmland remote sensing images.",Remote sensing images of farmland,semantic segmentation,feature fusion,Mamba,"Zhao, Ruolin","Lu, Junde","Zhao, Huanghui","Chen, Qian",,,,,,,,,,,,,,,,,,,,,,,,,
Row_467,"Zhao, Qi","Lyu, Shuchang","Zhao, Hongbo",Self-training guided disentangled adaptation for cross-domain remote sensing image semantic segmentation,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,MAR 2024,6,"Remote sensing (RS) image semantic segmentation using deep convolutional neural networks (DCNNs) has shown great success in various applications. However, the high dependence on annotated data makes challenging for DCNNs to adapt to different RS scenes. To address this challenge, we propose a cross domain RS image semantic segmentation task that considers ground sampling distance, remote sensing sensor variation, and different geographical landscapes as the main factors causing domain shifts between source and target images. To mitigate the negative impact of domain shift, we propose a self-training guided disentangled adaptation network (ST-DASegNet) that consists of source and target student backbones extract source-style and target-style features. To align cross-domain single-style features, we adopt feature-level adversarial learning. We also propose a domain disentangled module (DDM) to extract universal and distinct features from single-domain cross-style features. Finally, we fuse these features and generate predictions using source and target student decoders. Moreover, we employ an exponential moving average (EMA) based cross-domain separated self-training mechanism to ease the instability and disadvantageous effect during adversarial optimization. Our experiments on several prominent RS datasets (Potsdam, Vaihingen, and LoveDA) demonstrate that ST-DASegNet outperforms previous methods and achieves new state-of-theart results. Visualization and analysis also confirm the interpretability of ST-DASegNet. The code is publicly available at https://github.com/cv516Buaa/ST-DASegNet.",Remote sensing image semantic segmentation,Unsupervised domain adaptation,Self-training,Domain disentangling,"Liu, Binghao","Chen, Lijiang","Cheng, Guangliang",,Adversarial learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_468,"Zheng, Chen","Wang, Leiguang",,Semantic Segmentation of Remote Sensing Imagery Using Object-Based Markov Random Field Model With Regional Penalties,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,MAY 2015,46,"This paper proposes a novel object-based Markov random field model (OMRF) for semantic segmentation of remote sensing images. First, the method employs the region size and edge information to build a weighted region adjacency graph (WRAG) for capturing the complicated interactions among objects. Thereafter, aimed at modeling object interactions in the OMRF, the size and edge information are further introduced into the Gibbs joint distribution of the random field as regional penalties. Finally, the semantic segmentation is achieved through a principled probabilistic inference of the OMRF with regional penalties. The proposed method is compared with other MRF-based methods and some state-of-the-art methods. Experiments are conducted on a series of synthetic and real-world images. Segmentation results demonstrate that our method provides better performance (an accuracy improvement about 3%). Moreover, we further discuss the application of the proposed method for classification.",Object-based Markov random field (OMRF),regional penalties,remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_469,"Ning, Xiaogang","He, You","Zhang, Hanchao",Semantic Information Collaboration Network for Semantic Change Detection in Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,1,"Semantic change detection (SCD) extends the traditional change detection (CD) task to simultaneously identify the change areas and their corresponding land cover categories in bi-temporal images. This ""from-to"" change information holds significant value in numerous practical applications and is increasingly garnering attention in the remote sensing domain. However, prevalent challenges such as loss of detail and class imbalance significantly hinder the efficacy of SCD applications. To address this challenge, we propose a novel semantic information collaboration network (SIC-Net). This network incorporates a detail capture path and a spatial-temporal semantic coordination module aiming to effectively execute SCD by fusing detailed information with contextual features and harnessing synergies between binary CD and semantic segmentation. Additionally, we introduce a pseudo-label growth algorithm to mitigate the substantial loss of sample category information. The experimental results on two widely used SCD datasets demonstrate that SIC-Net outperforms other methods across various evaluation metrics, achieving SeK of 23.96% and 61.29%, respectively. These findings not only validate the effectiveness of the SIC-Net but also provide new ideas and directions for future research in the field of remote sensing, particularly in SCD.",Semantics,Task analysis,Feature extraction,Remote sensing,"Zhang, Ruiqian","Chang, Dong","Hao, Minghui",,Semantic segmentation,Land surface,Accuracy,Change detection,pseudo-label growth algorithm (PLGA),semantic change detection (SCD),semantic coordination,semantic segmentation,,,,,,spatial detail,,,,,,,,,,,
Row_470,"Zhu, Xiaotong","Peng, Taile","Guo, Jia",CNN-transformer dual branch collaborative model for semantic segmentation of high-resolution remote sensing images,PHOTOGRAMMETRIC RECORD,NOV 2024,0,"High-resolution remote sensing images play an important role in geological surveys, disaster detection, and other fields. However, highly imbalanced ground target classes and easily confused small ground targets pose significant challenges to the semantic segmentation task. We propose IC-TransUNet, a new dual branch model based on an encoder-decoder structure that fully exploits the advantages of convolutional neural networks and transformers and considers both detailed information and semantic information capture. Specifically, a lightweight CSwin transformer and InceptionNeXt are used as the dual branch backbone of the model. To further improve the model performance, first, we designed the InceptionNeXt-CSwin Transformer Fusion Module (ICFM) and Edge Enhancement Module (EEM) to guide the dual branch backbone to obtain features. Second, a detachable Spatial-channel Attention Fusion Module (SCAFM) is designed to be flexibly inserted into multiple positions of the model. Finally, we designed a decoder with significant performance based on a global local transformer block, SCAFM, and a multilayer perceptron segmentation head. IC-TransUNet achieved highly competitive performance in experiments on the Vaihingen and Potsdam datasets from the International Society for Photogrammetry and Remote Sensing.",CNN,dual encoder,high-resolution remote sensing images,semantic segmentation transformer,"Wang, Hao","Cao, Taotao",,,transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_471,"Gao, Hao","Cao, Lin","Yu, Dingfeng",Semantic Segmentation of Marine Remote Sensing Based on a Cross Direction Attention Mechanism,IEEE ACCESS,2020,10,"With the development of remote sensing technology, the semantic segmentation and recognition of various things in the ocean have become more and more frequent. Due to the wide variety of marine things and the large differences in morphology, it has brought greater difficulties to the recognition of marine remote sensing images. In order to obtain better segmentation results of ocean remote sensing images, this paper proposes an cross attention mechanism(Horizontal and Vertical) of exponential operation combined with multi-scale convolution algorithm. Among them, the cross attention mechanism and expanded distribution weight coefficient mentioned in this paper are first proposed. First, Input the marine remote sensing image features into an cross attention mechanism algorithm of exponential operation to obtain feature weight coefficients and joint weight coefficients in multiple directions; Then, the features with weight coefficients are input into the multi-access convolutional layer and the multi-scale dilated convolutional layer respectively for deep feature mining; Then the above steps are repeated twice, and finally the semantic segmentation of marine remote sensing images is achieved by fusing multiple deep-level features afterwards. Experiments were conducted on three public marine remote sensing data sets, and the results proved the effectiveness of our proposed cross attention mechanism of extended operation algorithm. The F values of the MAMC model on Beach, Island and Sea ice data sets have reached 99.4%, 91.25%, 87.08% respectively. Compared with other models, the effect is significantly improved, and proved the powerful performance of the algorithm in the semantic segmentation of marine remote sensing images.",Remote sensing,Feature extraction,Convolution,Image segmentation,"Xiong, Xuejun","Cao, Maoyong",,,Semantics,Data mining,Oceans,Cross direction attention mechanism,marine remote sensing,multi-access convolutional,deep learning,convolution and dilated convolution,,,,,,,,,,,,,,,,,
Row_472,"Lan, Yubin","Huang, Kanghua","Yang, Chang",Real-Time Identification of Rice Weeds by UAV Low-Altitude Remote Sensing Based on Improved Semantic Segmentation Model,REMOTE SENSING,NOV 2021,29,"Real-time analysis of UAV low-altitude remote sensing images at airborne terminals facilitates the timely monitoring of weeds in the farmland. Aiming at the real-time identification of rice weeds by UAV low-altitude remote sensing, two improved identification models, MobileNetV2-UNet and FFB-BiSeNetV2, were proposed based on the semantic segmentation models U-Net and BiSeNetV2, respectively. The MobileNetV2-UNet model focuses on reducing the amount of calculation of the original model parameters, and the FFB-BiSeNetV2 model focuses on improving the segmentation accuracy of the original model. In this study, we first tested and compared the segmentation accuracy and operating efficiency of the models before and after the improvement on the computer platform, and then transplanted the improved models to the embedded hardware platform Jetson AGX Xavier, and used TensorRT to optimize the model structure to improve the inference speed. Finally, the real-time segmentation effect of the two improved models on rice weeds was further verified through the collected low-altitude remote sensing video data. The results show that on the computer platform, the MobileNetV2-UNet model reduced the amount of network parameters, model size, and floating point calculations by 89.12%, 86.16%, and 92.6%, and the inference speed also increased by 2.77 times, when compared with the U-Net model. The FFB-BiSeNetV2 model improved the segmentation accuracy compared with the BiSeNetV2 model and achieved the highest pixel accuracy and mean Intersection over Union ratio of 93.09% and 80.28%. On the embedded hardware platform, the optimized MobileNetV2-UNet model and FFB-BiSeNetV2 model inferred 45.05 FPS and 40.16 FPS for a single image under the weight accuracy of FP16, respectively, both meeting the performance requirements of real-time identification. The two methods proposed in this study realize the real-time identification of rice weeds under low-altitude remote sensing by UAV, which provide a reference for the subsequent integrated operation of plant protection drones in real-time rice weed identification and precision spraying.",low-altitude remote sensing,semantic segmentation model,real-time,rice weeds,"Lei, Luocheng","Ye, Jiahang","Zhang, Jianling","Zeng, Wen",target spraying,,,,,,,,"Zhang, Yali",,"Deng, Jizhong",,,,,,,,,,,,,,
Row_473,"Chen, Jie","Wang, Hao","Guo, Ya",Strengthen the Feature Distinguishability of Geo-Object Details in the Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,11,"Semantic segmentation is one of the hot topics in the field of remote sensing image intelligent analysis. Deep convolutional neural network (DCNN) has become a mainstream technology in semantic segmentation due to its powerful semantic feature representation. The emergence of high-resolution remote sensing imagery has provided massive detail information, but difficulties and challenges remain in the ""feature representation of fine geo objects"" and ""feature distinction of easily confusing geo objects."" To this end, this article focuses on the distinguishing features of geo-object details and proposes a novel DCNN-based semantic segmentation. First, the cascaded relation attention module is adopted to determine the relationship among different channels or positions. Then, information connection and error correction are used to capture and fuse the features of geo-object details. The output feature representations are provided by the multiscale feature module. Besides, the proposed model uses the boundary affinity loss to gain accurate and clear geo-object boundary. The experimental results on the Potsdam and Vaihingen datasets demonstrate that the proposed model can achieve excellent segmentation performance on overall accuracy and mean intersection over union. Furthermore, the results of ablation and visualization analyses also verify the feasibility and effectiveness of the proposed method.",Feature extraction,Semantics,Image segmentation,Visualization,"Sun, Geng","Zhang, Yi","Deng, Min",,Remote sensing,Decoding,Data mining,Attention mechanism,geo-object details,high-resolution remote sensing imagery,multiscale feature representation,semantic segmentation,,,,,,,,,,,,,,,,,
Row_474,"Wang, Luhan","Xiao, Pengfeng","Zhang, Xueliang",A Fine-Grained Unsupervised Domain Adaptation Framework for Semantic Segmentation of Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,10,"Unsupervised domain adaptation (UDA) aims at adapting a model from the source domain to the target domain by tackling the issue of domain shift. Cross-domain segmentation of remote sensing images (RSIs) remains a big challenge due to the unique properties of RSIs. On the one hand, the divergence of data distribution in different local regions leads to negative transfer by directly applying the global alignment method in RSIs. On the other hand, the underlying category-level structure in the target domain is often ignored, which confuses the decision of semantic boundaries on the dispersed category features caused by large intraclass variance and small interclass variance in RSIs. In this study, we propose a novel fine-grained adaptation framework combining two stages of global-local alignment and category-level alignment to solve the above-mentioned problems. In the first stage of global-local adaptation, an attention map is derived from an intermediate discriminator and focuses on hard-to-align regions to mitigate negative transfer due to global adversarial learning. In the second stage of category-level adaptation, the category feature compact module is utilized to address the issue of dispersed features in the target domain attained by the cross-domain network, which will facilitate the fine-grained alignment of categories. Experiments under various scenarios, including geographic location variation and spectral band composition variation, demonstrate that the local adaptation and category-level adaptation of RSIs are complementary in the cross-domain segmentation, and the integrated framework helps achieve outstanding performance for UDA semantic segmentation of RSIs.",Adversarial machine learning,Semantic segmentation,Semantics,Remote sensing,"Chen, Xinyang",,,,Adaptation models,Training,Prototypes,Adversarial learning,category-level alignment,global-local alignment,remote sensing images (RSIs),semantic segmentation,,,,,,unsupervised domain adaptation (UDA),,,,,,,,,,,
Row_475,"Mo, Youda","Li, Huihui","Xiao, Xiangling",Swin-Conv-Dspp and Global Local Transformer for Remote Sensing Image Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,6,"compared with the traditional method based on hand-crafted features, deep neural network has achieved a certain degree of success on remote sensing (RS) image semantic segmentation. However, there are still serious holes, rough edge segmentation, and false detection or even missed detection due to the light and its shadow in the segmentation. Aiming at the above problems, this article proposes a RS semantic segmentation model SCG-TransNet that is a hybrid model of Swin transformer and Deeplabv3+, which includes Swin-Conv-Dspp (SCD) and global local transformer block (GLTB). First, the SCD module which can efficiently extract feature information from objects at different scales is used to mitigate the hole phenomenon, reducing the loss of detailed information. Second, we construct a GLTB with spatial pyramid pooling shuffle module to extract critical detail information from the limited visible pixels of the occluded objects, which alleviates the problem of difficult object recognition due to occlusion effectively. Finally, the experimental results show that our SCG-TransNet achieves a mean intersection over union of 70.29% on the Vaihingen datasets, which is 3% higher than the baseline model. It also achieved good results on POSDAM datasets. These demonstrate the effectiveness, robustness, and superiority of our proposed method compared with existing state-of-the-art methods.",Global local transformer block (GLTB),remote sensing (RS) image,semantic segmentation,Swin transformer,"Zhao, Huimin","Liu, Xiaoyong","Zhan, Jin",,Swin-Conv-Dspp (SCD),,,,,,,,,,,,,,,,,,,,,,,,
Row_476,"Chen, Man","Xu, Kun","Chen, Enping",Semantic Attention and Structured Model for Weakly Supervised Instance Segmentation in Optical and SAR Remote Sensing Imagery,REMOTE SENSING,NOV 2023,1,"Instance segmentation in remote sensing (RS) imagery aims to predict the locations of instances and represent them with pixel-level masks. Thanks to the more accurate pixel-level information for each instance, instance segmentation has enormous potential applications in resource planning, urban surveillance, and military reconnaissance. However, current RS imagery instance segmentation methods mostly follow the fully supervised paradigm, relying on expensive pixel-level labels. Moreover, remote sensing imagery suffers from cluttered backgrounds and significant variations in target scales, making segmentation challenging. To accommodate these limitations, we propose a semantic attention enhancement and structured model-guided multi-scale weakly supervised instance segmentation network (SASM-Net). Building upon the modeling of spatial relationships for weakly supervised instance segmentation, we further design the multi-scale feature extraction module (MSFE module), semantic attention enhancement module (SAE module), and structured model guidance module (SMG module) for SASM-Net to enable a balance between label production costs and visual processing. The MSFE module adopts a hierarchical approach similar to the residual structure to establish equivalent feature scales and to adapt to the significant scale variations of instances in RS imagery. The SAE module is a dual-stream structure with semantic information prediction and attention enhancement streams. It can enhance the network's activation of instances in the images and reduce cluttered backgrounds' interference. The SMG module can assist the SAE module in the training process to construct supervision with edge information, which can implicitly lead the model to a representation with structured inductive bias, reducing the impact of the low sensitivity of the model to edge information caused by the lack of fine-grained pixel-level labeling. Experimental results indicate that the proposed SASM-Net is adaptable to optical and synthetic aperture radar (SAR) RS imagery instance segmentation tasks. It accurately predicts instance masks without relying on pixel-level labels, surpassing the segmentation accuracy of all weakly supervised methods. It also shows competitiveness when compared to hybrid and fully supervised paradigms. This research provides a low-cost, high-quality solution for the instance segmentation task in optical and SAR RS imagery.",weakly supervised instance segmentation,remote sensing imagery,semantic attention,structured model,"Zhang, Yao","Xie, Yifei","Hu, Yahao","Pan, Zhisong",multi-scale feature extraction,,,,,,,,,,,,,,,,,,,,,,,,
Row_477,"Zhou, Wujie","Fan, Xiaomin","Yan, Weiqing",Graph Attention Guidance Network With Knowledge Distillation for Semantic Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,13,"Deep learning has become a popular method for studying the semantic segmentation of high-resolution remote sensing images (HRRSIs). Existing methods have adopted convolutional neural networks (CNNs) to achieve better segmentation accuracy of HRRSIs, and the success of these models often depends on the model complexity and parameter quantity. However, the deployment of these models on equipment with limited resources is a significant challenge. To solve this problem, a lightweight student network framework-a graph attention guidance network (GAGNet) with knowledge distillation (KD), called GAGNet-S*-is proposed in this study, which distills knowledge from pretrained large teacher network (GAGNet-T) and builds reliable weak labels to optimize untrained student network (GAGNet-S). Inspired by the graph convolution network, this study designs a graph convolution module called the attention-graph decoder (AGD), which combines attention mechanisms with graph convolution to optimize image features and improve segmentation accuracy in the semantic segmentation task of HRRSIs. In addition, a dense cross-decoder (DCD) was designed for multiscale dense fusion, which utilizes rich semantic information in the high-level features to guide and refine the low-level features from the bottom up. Extensive experiments showed that GAGNet-S* (GAGNet-S with KD) achieved excellent segmentation performance on two widely used datasets: Potsdam and Vaihingen. The code and models are available at https://github.com/F8AoMn/GAGNet-KD.",Index Terms-Dense cross-decoder (DCD),graph convolution,high-resolution remote sensing images (HRRSIs),knowledge distillation (KD),"Shan, Shengdao","Jiang, Qiuping","Hwang, Jenq-Neng",,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_478,"Li, Zhiyang","Pan, Xuran","Xu, Kexing",Multi-modal remote sensing image segmentation based on attention-driven dual-branch encoding framework,JOURNAL OF APPLIED REMOTE SENSING,APR 1 2024,0,"The high resolution remote sensing images are characterized by rich surface details and diverse features, and the single-modality high-resolution images suffer from limited expressive ability in the earth object segmentation application scenarios. We propose a multi-modal remote sensing image segmentation method based on attention-driven dual-branch encoding framework. The method involves parallel encoding of multi-modal remote sensing data to thoroughly extract features from each modality. Furthermore, multistage multi-modal features are fused by attention-driven feature fusion modules to generate high-quality multi-modal feature representation. Extensive experiments are carried out on the International Society for Photogrammetry and Remote Sensing Vaihingen and Potsdam 2D semantic labeling datasets. The datasets include both RGB/IRRG images and digital surface model (DSM) images. Experimental results demonstrate that: (1) the elevation information of DSM images can bring obvious benefits to the earth objects with significant heights, and introducing DSM images properly can improve the segmentation accuracy compared to using only RGB/IRRG images; (2) the attention-driven feature fusion module outperforms traditional feature fusion methods in capturing cross-modal complementary features, leading to outstanding segmentation accuracy for each earth object.",multi-modal remote sensing images,semantic segmentation,convolutional neural networks,attention-driven feature fusion,"Yang, Xinqi",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_479,"Li, Zhen","Zhang, Zhenxin","Chen, Dong",HCRB-MSAN: Horizontally Connected Residual Blocks-Based Multiscale Attention Network for Semantic Segmentation of Buildings in HSR Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,11,"Accurate and efficient semantic segmentation of buildings in high spatial resolution (HSR) remote sensing images is the basis for applications such as fine urban management, high-precision mapping, land resource utilization investigation, and human settlement suitability evaluation. The current building extraction methods based on deep learning can obtain high-level abstract features of images. However, due to the limitation of convolution kernel size and the vanishing gradient, the extraction of some buildings is inaccurate, and some small-volume buildings are missing as the network deepens. In this regard, we design a horizontally connected residual blocks-based multiscale attention network to achieve high-quality extraction of buildings in HSR remote sensing image. In this network, we subdivide each residual block by channel grouping and feature horizontal connection to consider the difference and saliency of feature information between channels, and then combine the output features with multiscale attention module to consider the contextual semantic relationship of different regions and integrate multilevel local and global information of buildings. A stepwise up-sampling mechanism is designed in the decoding process to finally achieve precise semantic segmentation of buildings. We conduct experiments on two public datasets and compare the proposed method with state-of-the-art semantic segmentation methods. The experiments show that our method could achieve better building extraction results in HSR remote sensing image, which proves the effectiveness of our proposed method.",Feature extraction,Buildings,Remote sensing,Image segmentation,"Zhang, Liqiang","Zhu, Lin","Wang, Qiang","Chen, Siyun",Semantics,Data mining,Deep learning,Building semantic segmentation,deep learning,horizontally connected residual block,high spatial resolution (HSR) remote sensing image,multiscale attention,"Peng, Xueli",,,,,,,,,,,,,,,,
Row_480,"Chen, Li","Dou, Xin","Peng, Jian",EFCNet: Ensemble Full Convolutional Network for Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,15,"Convolutional neural networks (CNNs) have achieved remarkable results in semantic segmentation of high-resolution remote sensing images (HRRSIs). However, the scales and textures of HRRSIs are diverse, which makes it difficult for a fixed-layer CNN to obtain rich features. In this regard, we propose an end-to-end ensemble fully convolutional network (EFCNet), which mainly includes two modules: the adaptive fusion module (AFM) and the separable convolutional module (SCM). The AFM can fuse features of different scales based on ensemble learning, whereas the SCM can reduce the complexity of the model under multifeature fusion. In the experiment, we use UNet and PSPNet to verify the framework on the ISPRS Vaihingen and Potsdam datasets. The experimental results show that the EFCNet can effectively improve the final segmentation performance and reduce the complexity of the ensemble model.",Convolution,Feature extraction,Kernel,Training,"Li, Wenbo","Sun, Bingyu","Li, Haifeng",,Remote sensing,Image segmentation,Spatial resolution,Convolutional neural network (CNN),ensemble learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_481,"Fang, Bo","Chen, Gang","Chen, Jifa",CCT: Conditional Co-Training for Truly Unsupervised Remote Sensing Image Segmentation in Coastal Areas,REMOTE SENSING,SEP 2021,6,"As the fastest growing trend in big data analysis, deep learning technology has proven to be both an unprecedented breakthrough and a powerful tool in many fields, particularly for image segmentation tasks. Nevertheless, most achievements depend on high-quality pre-labeled training samples, which are labor-intensive and time-consuming. Furthermore, different from conventional natural images, coastal remote sensing ones generally carry far more complicated and considerable land cover information, making it difficult to produce pre-labeled references for supervised image segmentation. In our research, motivated by this observation, we take an in-depth investigation on the utilization of neural networks for unsupervised learning and propose a novel method, namely conditional co-training (CCT), specifically for truly unsupervised remote sensing image segmentation in coastal areas. In our idea, a multi-model framework consisting of two parallel data streams, which are superpixel-based over-segmentation and pixel-level semantic segmentation, is proposed to simultaneously perform the pixel-level classification. The former processes the input image into multiple over-segments, providing self-constrained guidance for model training. Meanwhile, with this guidance, the latter continuously processes the input image into multi-channel response maps until the model converges. Incentivized by multiple conditional constraints, our framework learns to extract high-level semantic knowledge and produce full-resolution segmentation maps without pre-labeled ground truths. Compared to the black-box solutions in conventional supervised learning manners, this method is of stronger explainability and transparency for its specific architecture and mechanism. The experimental results on two representative real-world coastal remote sensing datasets of image segmentation and the comparison with other state-of-the-art truly unsupervised methods validate the plausible performance and excellent efficiency of our proposed CCT.",remote sensing,image segmentation,coastal areas,deep learning,"Ouyang, Guichong","Kou, Rong","Wang, Lizhe",,co-training,,,,,,,,,,,,,,,,,,,,,,,,
Row_482,"Cui, Wei","Yao, Meng","Hao, Yuanjie",Knowledge and Geo-Object Based Graph Convolutional Network for Remote Sensing Semantic Segmentation,SENSORS,JUN 2021,14,"Pixel-based semantic segmentation models fail to effectively express geographic objects and their topological relationships. Therefore, in semantic segmentation of remote sensing images, these models fail to avoid salt-and-pepper effects and cannot achieve high accuracy either. To solve these problems, object-based models such as graph neural networks (GNNs) are considered. However, traditional GNNs directly use similarity or spatial correlations between nodes to aggregate nodes' information, which rely too much on the contextual information of the sample. The contextual information of the sample is often distorted, which results in a reduction in the node classification accuracy. To solve this problem, a knowledge and geo-object-based graph convolutional network (KGGCN) is proposed. The KGGCN uses superpixel blocks as nodes of the graph network and combines prior knowledge with spatial correlations during information aggregation. By incorporating the prior knowledge obtained from all samples of the study area, the receptive field of the node is extended from its sample context to the study area. Thus, the distortion of the sample context is overcome effectively. Experiments demonstrate that our model is improved by 3.7% compared with the baseline model named Cluster GCN and 4.1% compared with U-Net.",remote sensing images,semantic segmentation,geo-object prior knowledge,graph neural network,"Wang, Ziwei","He, Xin","Wu, Weijie","Li, Jie",,,,,,,,,"Zhao, Huilin",,"Xia, Cong","Wang, Jin",,,,,,,,,,,,,
Row_483,"Ma, Xianping","Zhang, Xiaokang","Pun, Man-On",A Crossmodal Multiscale Fusion Network for Semantic Segmentation of Remote Sensing Data,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,42,"Driven by the rapid development of Earth observation sensors, semantic segmentation using multimodal fusion of remote sensing data has drawn substantial research attention in recent years. However, existing multimodal fusion methods based on convolutional neural networks cannot capture long-range dependencies across multiscale feature maps of remote sensing data in different modalities. To circumvent this problem, this work proposes a crossmodal multiscale fusion network (CMFNet) by exploiting the transformer architecture. In contrast to the conventional early, late, or hybrid fusion networks, the proposed CMFNet fuses information of different modalities at multiple scales using the cross-attention mechanism. More specifically, the CMFNet utilizes a novel cross-modal attention architecture to fuse multiscale convolutional feature maps of optical remote sensing images and digital surface model data through a crossmodal multiscale transformer (CMTrans) and a multiscale context augmented transformer (MCATrans). The CMTrans can effectively model long-range dependencies across multiscale feature maps derived from multimodal data, while the MCATrans can learn discriminative integrated representations for semantic segmentation. Extensive experiments on two large-scale fine-resolution remote sensing datasets, namely ISPRS Vaihingen and Potsdam, confirm the excellent performance of the proposed CMFNet as compared to other multimodal fusion methods.",Transformers,Remote sensing,Semantics,Image segmentation,,,,,Feature extraction,Fuses,Decoding,Combined squeeze-and-excitation (CSE),cross attention,crossmodal multiscale fusion,transformer,,,,,,,,,,,,,,,,,,
Row_484,"You, Chao","Jiao, Licheng","Liu, Xu",Boundary-Aware Multiscale Learning Perception for Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,3,"For remote sensing image segmentation, the boundaries of objects are difficult to distinguish, which is ignored by most methods. Therefore, it is challenging how to excavate and recover the boundaries of objects accurately. In this article, we propose a boundary-aware multiscale network (BMNet) to solve this problem. The key components of BMNet include the scale attention module (SAM) and boundary guidance module (BGM). Specifically, the SAM is proposed to guide the refinement of multiscale features in a context-aware way. It enhances the discriminability of multiscale features by establishing contextual dependencies, which enables the refinement of the prediction of objects. Then, the BGM is proposed to enable networks to distinguish the boundary of objects. It uses manifold information of features to generate BG maps and forces the network to focus more on the boundary of objects. The effectiveness of the proposed BMNet is demonstrated on two public remote sensing datasets: ISPRS 2-D semantic labeling Potsdam dataset and Vaihingen dataset, where BMNet achieves better segmentation than prevalent methods. Finally, the experimental results indicate that BMNet can produce sharper boundaries of objects to reconstruct more detailed segmentation results.",Boundary attention guidance,learning perception,multiscale,remote sensing,"Li, Lingling","Liu, Fang","Ma, Wenping","Yang, Shuyuan",semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_485,"Gao, Yupeng","Luo, Xiaoling","Gao, Xiaojing",Semantic segmentation of remote sensing images based on multiscale features and global information modeling,EXPERT SYSTEMS WITH APPLICATIONS,SEP 1 2024,6,"The main difficulties in semantic segmentation of remote sensing images include the effect of shadows, the blurring of feature differences between categories, and loss of small-scale category features during processing. To deal with these challenges, we propose a semantic segmentation network for RSI based on multi -scale features and global information modeling. A skillfully designed two -branch fusion attention based on an adaptive fusion converter is added to the multilevel cascaded HRNet structure to better combine multiscale features and global modeling information. Prior to this, "" Coordinate attention "" was combined with "" Spatial attention "" designed in this paper to form a Feed -forward Attention Layer (FAL) to encode finer feature information cues. Meanwhile, a more refined multilayer decoder is designed to obtain better image category recovery. Various experiments were conducted on four different scenarios and types of datasets including WHDLD, Potsdam, Vaihingen, and LoveDa, and in terms of the significant evaluation metric MIOU, with dividing the training, validation, and test sets in the ratio of 6:2:2, the performance of our model on the above four datasets was obtained with 61.79%, 70.79%, 81.15%, and 52.55%.In addition, a comparison with other excellent works is made and the results show that our designed model has better performance.",Multiscale features,Global modeling information,Remote sensing image,Semantic segmentation,"Yan, Weihong","Pan, Xin","Fu, Xueliang",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_486,"Li, Yansheng","Chen, Wei","Huang, Xin",MFVNet: a deep adaptive fusion network with multiple field-of-views for remote sensing image semantic segmentation,SCIENCE CHINA-INFORMATION SCIENCES,APR 2023,44,"In recent years, the remote sensing image (RSI) semantic segmentation attracts increasing research interest due to its wide application. RSIs are difficult to be processed holistically on current GPU cards on account of their large field-of-views (FOVs). However, the prevailing practices such as downsampling and cropping will inevitably decrease the quality of semantic segmentation. To address this conflict, this paper proposes a new deep adaptive fusion network with multiple FOVs (MFVNet), which is specially designed for RSI semantic segmentation. Different from existing methods, MFVNet takes into consideration the differences among multiple FOVs. By pyramid sampling the RSI, we first obtain images on different scales with multiple FOVs. Images on the high scale with a large FOV can capture larger spatial contexts and complete object contours, while images on the low scale with a small FOV can keep the higher spatial resolution and more detailed information. Then scale-specific models are chosen to make the best predictions for all scales. Next, the output feature maps and score maps are aligned through the scale alignment module to overcome spatial misregistration among scales. Finally, the aligned score maps are fused with the help of adaptive weight maps generated by the adaptive fusion module, producing the fused prediction. The performance of MFVNet surpasses the previous state-of-the-art semantic segmentation models on three typical RSI datasets, demonstrating the effectiveness of the proposed MFVNet.",semantic segmentation,remote sensing image (RSI),field-of-view (FOV),adaptive fusion,"Gao, Zhi","Li, Siwei","He, Tao","Zhang, Yongjun",convolutional neural network,,,,,,,,,,,,,,,,,,,,,,,,
Row_487,"Wang, Wei","Zhang, Yingfeng","Wang, Xin",A Boundary Guided Cross Fusion Approach for Remote Sensing Image Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,1,"Remote sensing images have a variety of application prospects because of their rich information. Due to recent advances in deep learning methods, solid improvements have been made in the semantic segmentation of high-resolution remote sensing images. However, achieving precise segmentation of small and crowded objects remains a challenge. To tackle this challenging task, a Boundary Guided Cross Fusion module (BGCFM) is proposed. The Bidirectional Boundary Gate module (BBGM) is designed to provide reliable boundary information for BGCFM. Based on these two models, a remote sensing images real-time semantic segmentation network, boundary guided cross fusion network (BGCFNet), is designed. The effectiveness of the boundary-guided fusion method and the performance of BGCFNet were verified on the GID-5 dataset without pretraining. The application of the boundary-guided fusion method on the SOTA dual-branch real-time semantic segmentation network improves segmentation accuracy. BGCFNet achieves the best performance with a Mean Intersection over Union (mIoU) of 88.82%. Its inference speed is about 1.5 times that of other networks in the experiment, achieving an excellent balance between accuracy and speed.",Boundary information,multiscale feature fusion method,real-time semantic segmentation,remote sensing image,"Li, Ji",,,,small object,,,,,,,,,,,,,,,,,,,,,,,,
Row_488,"Liu, Jingyi","Wu, Jiawei","Xie, Hongfei",Semantic Segmentation of Urban Remote Sensing Images Based on Deep Learning,APPLIED SCIENCES-BASEL,SEP 2024,0,"In the realm of urban planning and environmental evaluation, the delineation and categorization of land types are pivotal. This study introduces a convolutional neural network-based image semantic segmentation approach to delineate parcel data in remote sensing imagery. The initial phase involved a comparative analysis of various CNN architectures. ResNet and VGG serve as the foundational networks for training, followed by a comparative assessment of the experimental outcomes. Subsequently, the VGG+U-Net model, which demonstrated superior efficacy, was chosen as the primary network. Enhancements to this model were made by integrating attention mechanisms. Specifically, three distinct attention mechanisms-spatial, SE, and channel-were incorporated into the VGG+U-Net framework, and various loss functions were evaluated and selected. The impact of these attention mechanisms, in conjunction with different loss functions, was scrutinized. This study proposes a novel network model, designated VGG+U-Net+Channel, that leverages the VGG architecture as the backbone network in conjunction with the U-Net structure and augments it with the channel attention mechanism to refine the model's performance. This refinement resulted in a 1.14% enhancement in the network's overall precision and marked improvements in MPA and MioU. A comparative analysis of the detection capabilities between the enhanced and original models was conducted, including a pixel count for each category to ascertain the extent of various semantic information. The experimental validation confirms the viability and efficacy of the proposed methodology.",deep learning,semantic segmentation,remote sensing image,convolutional neural network,"Xiao, Dong","Ran, Mengying",,,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,
Row_489,"Li, Xiao","Lei, Lin","Kuang, Gangyao",Multilevel Adaptive-Scale Context Aggregating Network for Semantic Segmentation in High-Resolution Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,12,"High-resolution remote sensing ((HRS)-S-2) images contain complex land objects of difference sizes, and it is important for semantic segmentation of the (HRS)-S-2 images to extract multiscale information. In this letter, we introduce a novel multilevel adaptive-scale context aggregating network (MACANet) for semantic segmentation of the (HRS)-S-2 images, which mainly consists of two parts--adaptive-scale context extraction block (AS-CEB) and sequential aggregation block (SAB). In particular, the AS-CEB introduces an inflexible strategy to obtain the features with appropriate scale information based on different asymmetric convolutions and the gated mechanism. Meanwhile, the SAB progressively aggregates multilevel adaptive-scale features, which are used to relieve the semantic gap between different-level features and generate precise score maps. Experimental results on representative (HRS)-S-2 datasets show the advantages of our method. The code is available at https://github.com/RSIP-NUDT/MACANet.",Semantics,Image segmentation,Feature extraction,Data mining,,,,,Remote sensing,Logic gates,Kernel,Fully convolutional network (FCN),S) images,multiscale information,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_490,"Lv, Liang","Zhang, Lefei",,Advancing Data-Efficient Exploitation for Semi-Supervised Remote Sensing Images Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,6,"To reduce the dependence of remote sensing (RS) image semantic segmentation models on extensive pixel-level annotated images, this article aims to address the issue of insufficient exploitation of RS images' potential within existing semi-supervised learning methods, introducing a novel semi-supervised RS image semantic segmentation method. Specifically, for unlabeled samples, the multiperturbation dynamic consistency (MDC) is proposed to align multiple predictions from diverse data augmentations; MDC leverages a dynamic decay threshold (DDT) instead of fixed thresholds to learn more reliable information, enriching the perturbation space and assisting the segmentation model in acquiring more discriminative feature representations. Furthermore, considering the rich contextual information in RS images, the class prototype memory (CPM) derived from labeled samples is maintained during the training stage, which is leveraged to guide the refinement of predictions from segmentation model at the inference stage. Extensive experiments are conducted on six RS image semantic segmentation datasets, including DFC22, iSAID, MER, MSL, GID-15, and Vaihingen. The experimental results demonstrate the superiority of the proposed method. The code is available at https://github.com/lvliang6879/MCSS.",Semantic segmentation,Training,Data augmentation,Predictive models,,,,,Deep learning,Data models,Computational modeling,Class prototype memory (CPM),dynamic decay threshold (DDT),multiperturbation dynamic consistency (MDC),remote sensing (RS) images,semantic segmentation,,,,,,semi-supervised learning,,,,,,,,,,,
Row_491,"Wei, Guangyi","Xu, Jindong","Yan, Weiqing",Dual-Domain Fusion Network Based on Wavelet Frequency Decomposition and Fuzzy Spatial Constraint for Remote Sensing Image Segmentation,REMOTE SENSING,OCT 2024,0,"Semantic segmentation is crucial for a wide range of downstream applications in remote sensing, aiming to classify pixels in remote sensing images (RSIs) at the semantic level. The dramatic variations in grayscale and the stacking of categories within RSIs lead to unstable inter-class variance and exacerbate the uncertainty around category boundaries. However, existing methods typically emphasize spatial information while overlooking frequency insights, making it difficult to achieve desirable results. To address these challenges, we propose a novel dual-domain fusion network that integrates both spatial and frequency features. For grayscale variations, a multi-level wavelet frequency decomposition module (MWFD) is introduced to extract and integrate multi-level frequency features to enhance the distinctiveness between spatially similar categories. To mitigate the uncertainty of boundaries, a type-2 fuzzy spatial constraint module (T2FSC) is proposed to achieve flexible higher-order fuzzy modeling to adaptively constrain the boundary features in the spatial by constructing upper and lower membership functions. Furthermore, a dual-domain feature fusion (DFF) module bridges the semantic gap between the frequency and spatial features, effectively realizes semantic alignment and feature fusion between the dual domains, which further improves the accuracy of segmentation results. We conduct comprehensive experiments and extensive ablation studies on three well-known datasets: Vaihingen, Potsdam, and GID. In these three datasets, our method achieved 74.56%, 73.60%, and 81.01% mIoU, respectively. Quantitative and qualitative results demonstrate that the proposed method significantly outperforms state-of-the-art methods, achieving an excellent balance between segmentation accuracy and computational overhead.",remote sensing,semantic segmentation,wavelet transform,type-2 fuzzy,"Chong, Qianpeng","Xing, Haihua","Ni, Mengying",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_492,"Zhao, Weiheng","Cao, Jiannong","Dong, Xueyan",U-shaped contourlet network for high-spatial-resolution remote sensing images segmentation,JOURNAL OF APPLIED REMOTE SENSING,JUL 1 2023,3,"Accurate semantic segmentation of images has long been a research priority in remote sensing. However, the presence of geometrically complex and spatially diverse objects increases the difficulty in simultaneously obtaining coherent and accurate labeling result. To solve this challenge, our study combined multiscale geometric feature extraction with convolutional neural network and proposed a new U-shaped contourlet network (USCNet) for segmentation from high-spatial-resolution remote sensing images (HSRRSIs). This network is designed to learn and characterize the geometric features present in HSRRSIs. The USCNet first transforms the original dataset into a pyramidal structure containing multiscale and multidirectional geometric information and then fuses the spatial and geometric features to extract high-level semantic information. This network has two advantages: (1) coarse-to-fine spatial features are learned efficiently using a hierarchical learning structure and (2) the multiscale learning scheme captures geometric information in different directions. The results of extensive experiments conducted on two remote sensing datasets (the International Society for Photogrammetry and Remote Sensing Vaihingen and Potsdam challenge datasets) show that the proposed approach outperforms several state-of-the-art semantic segmentation methods.",semantic segmentation,contourlet transform,convolutional neural network,multiscale,,,,,high-spatial-resolution remote sensing images,,,,,,,,,,,,,,,,,,,,,,,,
Row_493,"Zhang, Kai","Han, Yu","Chen, Jian",Semantic Segmentation for Remote Sensing based on RGB Images and Lidar Data using Model-Agnostic Meta-Learning and Partical Swarm Optimization,,2020,11,"Urban remote sensing has the problems that land cover categories are usually highly unbalanced and annotated samples are scarce, which brings great limitations to monitoring the change of urban coverage and periodically evaluating urban ecological conditions. Semantic segmentation is one of the main applications in urban remote sensing image analysis. Because the ground objects in remote sensing images have the characteristics of disordered distribution and irregular morphology. The classical semantic segmentation model based on deep learning U-Net cannot achieve high semantic segmentation accuracy for urban ground objects. This paper proposes to optimize the neural network structure and introduce lidar data to solve the above problems. In this paper, the Model-Agnostic Meta-Learning and fully convolutional neural networks are fused which be trained and tested by remote sensing images. It makes the training process into inner loop and outer loop. And Partical Swarm Optimization (PSO) is used to optimize the parameter updating process of neural network to further improve the test accuracy. This paper fuses Lidar data and remote sensing images, and comprehensively use the position and elevation information of Lidar data and the spectrum and texture information of remote sensing images to classify the ground features. The datasets used in this paper are RGB remote sensing images and Digital Elevation Model (DEM) images. The test accuracy of U-Net network optimized by MAML can be improved by 6%-7% under the same network parameters and training data sets. With the introduction of Lidar data, the accuracy of the test is increased by 3-5%. The experimental results show that the precision before and after PSO optimization is improved by 6%-9%, which verifies the idea in the paper. Copyright (C) 2020 The Authors.",Urban remote sensing,semantic segmentation,Model-Agnostic Meta-Learning (MAML),Partical Swarm Optimization (PSO),"Zhang, Zichao","Wang, Shubo",,,Lidar data,RGB remote sensing images,Digital Elevation Model (DEM),,,,,,,IFAC PAPERSONLINE,,,,,,,,,,,,,,,
Row_494,"Weng, Mengqian","Hu, Zhibo","Xie, Xiaopeng",Semantic Segmentation of Remote Sensing Images Based on Dual Attention and Multi-scale Feature Fusion,,2021,0,"We propose a remote sensing image semantic segmentation model based on dual attention and multi-scale feature fusion to solve the problems of objects scale differences and missing small objects. This model uses ResNet50 in the coding part to extract features. First of all, the output features of each stage of ResNet50 are introduced into the pyramid pooling module, making full use of the multi-scale context information of the image to cope with the change of the object scales. Secondly, the dual attention is introduced in the final output features of ResNet50 to establish the semantic relationship between the spatial and channel dimensions, which enhances the ability of feature representation and improve the condition that small targets are difficult to segment. Finally, starting from the output features of the attention module, the features of all levels are gradually integrated to complete decoding to refine the target segmentation edge. The designed comparative experiments results show the effectiveness of the proposed method.",Deep learning,remote sensing image,semantic segmentation,pyramid pooling,"Li, Yunhong","Hu, Lei",,,dual attention,,,,,,,,,TWELFTH INTERNATIONAL CONFERENCE ON GRAPHICS AND IMAGE PROCESSING (ICGIP 2020),,,,,,,,,,,,,,,
Row_495,"Wang, Wenna","Ran, Lingyan","Yin, Hanlin",Hierarchical Shared Architecture Search for Real-Time Semantic Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,2,"Real-time semantic segmentation of remote-sensing images demands a trade-off between speed and accuracy, which makes it challenging. Apart from manually designed networks, researchers seek to adopt neural architecture search (NAS) to discover a real-time semantic segmentation model with optimal performance automatically. Most existing NAS methods stack up no more than two types of searched cells, omitting the characteristics of resolution variation. This article proposes the hierarchical shared architecture search (HAS) method to automatically build a real-time semantic segmentation model for remote sensing images. Our model contains a lightweight backbone and a multiscale feature fusion module. The lightweight backbone is carefully designed with low computational cost. The multiscale feature fusion module is searched using the NAS method, where only the blocks from the same layer share identical cells. Extensive experiments reveal that our searched real-time semantic segmentation model of remote sensing images achieves the state-of-the-art trade-off between accuracy and speed. Specifically, on the LoveDA, Potsdam, and Vaihingen datasets, the searched network achieves 54.5% mIoU, 87.8% mIoU, and 84.1% mIoU, respectively, with an inference speed of 132.7 FPS. Besides, our searched network achieves 72.6% mIoU at 164.0 FPS on the CityScapes dataset and 72.3% mIoU at 186.4 FPS on the CamVid dataset.",Feature aggregation module,hierarchical shared search strategy,neural architecture search (NAS),real-time semantic segmentation,"Sun, Mingjun","Zhang, Xiuwei","Zhang, Yanning",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_496,"Yang, Yang","Zheng, Shunyi","Wang, Xiqi",AMMUNet: Multiscale Attention Map Merging for Remote Sensing Image Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2025,0,"The advancement of deep learning has driven notable progress in remote sensing semantic segmentation. Multihead self-attention (MSA) mechanisms have been widely adopted in semantic segmentation tasks. Network architectures exemplified by Vision Transformers have implemented window-based operations in the spatial domain to reduce computational costs. However, this approach comes at the expense of a weakened capacity to capture long-range dependencies, potentially limiting their efficacy in remote sensing image processing. In this letter, we propose AMMUNet, a UNet-based framework that employs multiscale attention map (AM) merging, comprising two key innovations: the attention map merging mechanism (AMMM) module and the granular multihead self-attention (GMSA). AMMM effectively combines multiscale AMs into a unified representation using a fixed mask template, enabling the modeling of a global attention mechanism. By integrating precomputed AMs in preceding layers, AMMM reduces computational costs while preserving global correlations. The proposed GMSA efficiently acquires global information while substantially mitigating computational costs in contrast to the global MSA mechanism. This is accomplished through the strategic alignment of granularity and the reduction of relative position bias parameters, thereby optimizing computational efficiency. Experimental evaluations highlight the superior performance of our approach, achieving remarkable mean intersection over union (mIoU) scores of 75.48% on the challenging Vaihingen dataset and an exceptional 77.90% on the Potsdam dataset, demonstrating the superiority of our method in precise remote sensing semantic segmentation. Codes are available at https://github.com/interpretty/AMMUNet.",Remote sensing,Transformers,Merging,Decoding,"Ao, Wei","Liu, Zhao",,,Computational modeling,Computational efficiency,Semantic segmentation,Feature extraction,Costs,Correlation,Attention map (AM) merging,global attention mechanism,,,,,,remote sensing,semantic segmentation,,,,,,,,,,
Row_497,"Zhou, Yongxiu","Wang, Honghui","Yao, Guangle",A Novel Remote Sensing Landslide Semantic Segmentation Method: Using CycleGAN-Based Change Detection Algorithms,,2024,0,"The study of landslide segmentation using remote sensing images is now focused on change detection and deep learning semantic segmentation algorithm. Deep learning-based semantic segmentation algorithms often require a considerable amount of pixel-level labeled training data. In the study of landslide segmentation research, too high cost of labeling is a barrier to develop deep learning methods. Although the change detection method does not require training data, it is necessary to gather the pre-event and post-event remote sensing images. When change detection is applied to landslide segmentation, it is a big challenge to obtain pre-event remote sensing images. To address this issue, a cycleGAN-based change detection technique for remote sensing landslide semantic segmentation has been developed. First, we used landslide images and non-landslide images to train the cycleGAN model. Then, we applied the cycleGAN approach to produce a non-landslide image from the landslide remote sensing image. Finally, using change detection method, landslide regions are segregated. In addition, we evaluated the suggested technique on the Bijie remote sensing landslide dataset, and got 0.845 accuracy, 0.404 recall, and 0.184 mIoU, demonstrating the method's viability.",Weakly supervised learning,Landslide,Remote sensing,CycleGAN,"Liu, Mingzhe","Xu, Qiang",,,,,,,,,,,,"ENGINEERING GEOLOGY FOR A HABITABLE EARTH, VOL 4, IAEG XIV CONGRESS 2023",,,,,,,,,,,,,,,
Row_498,"Qi, Zipeng","Zou, Zhengxia","Chen, Hao",Remote-Sensing Image Segmentation Based on Implicit 3-D Scene Representation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,1,"Remote-sensing image segmentation, as a challenging but fundamental task, has drawn increasing attention in the remote-sensing field. Recent advances in deep learning have greatly boosted research on this task. However, the existing deep-learning-based segmentation methods heavily rely on a large amount of pixelwise labeled training data, and the labeling process is time-consuming and labor-intensive. In this letter, we focus on the scenario that leverages the 3-D structure of multiview images and a limited number of annotations to generate accurate novel view segmentation. Under this scenario, we propose a novel method for remote-sensing image segmentation based on implicit 3-D scene representation, which generates arbitrary-view segmentation output from limited segmentation annotations. The proposed method employs a two-stage training strategy. In the first stage, we optimize the implicit neural representations of a 3-D scene and encode their multiview images into a neural radiance field. In the second stage, we transform the scene color attribute into semantic labels and propose a ray-convolution network to aggregate local 3-D consistency cues across different locations. We also design a color-radiance network to help our method generalize to unseen views. Experiments on both synthetic and real-world data suggest that our method significantly outperforms deep convolutional neural networks (CNNs)-based methods and other view synthesis-based methods. We also show that the proposed method can be applied as a novel data augmentation approach that benefits CNN-based segmentation methods.",Image segmentation,implicit neural representations,neural radiance field,remote sensing,"Shi, Zhenwei",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_499,"Liang, Guozhen","Xie, Fengxi","Chien, Ying-Ren",Class-Aware Self- and Cross-Attention Network for Few-Shot Semantic Segmentation of Remote Sensing Images,MATHEMATICS,SEP 2024,0,"Few-Shot Semantic Segmentation (FSS) has drawn massive attention recently due to its remarkable ability to segment novel-class objects given only a handful of support samples. However, current FSS methods mainly focus on natural images and pay little attention to more practical and challenging scenarios, e.g., remote sensing image segmentation. In the field of remote sensing image analysis, the characteristics of remote sensing images, like complex backgrounds and tiny foreground objects, make novel-class segmentation challenging. To cope with these obstacles, we propose a Class-Aware Self- and Cross-Attention Network (CSCANet) for FSS in remote sensing imagery, consisting of a lightweight self-attention module and a supervised prior-guided cross-attention module. Concretely, the self-attention module abstracts robust unseen-class information from support features, while the cross-attention module generates a superior quality query attention map for directing the network to focus on novel objects. Experiments demonstrate that our CSCANet achieves outstanding performance on the standard remote sensing FSS benchmark iSAID-5i, surpassing the existing state-of-the-art FSS models across all combinations of backbone networks and K-shot settings.",few-shot learning,few-shot semantic segmentation,remote sensing,class-aware self- and cross-attention,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_500,"Chantharaj, Sirinthra","Pornratthanapong, Kissada","Chitsinpchayakun, Pitchayut",Semantic Segmentation on Medium-Resolution Satellite Images using Deep Convolutional Networks with Remote Sensing Derived Indices,,2018,6,"Semantic Segmentation is a fundamental task in computer vision and remote sensing imagery. Many applications, such as urban planning, change detection, and environmental monitoring, require the accurate segmentation; hence, most segmentation tasks are performed by humans. Currently, with the growth of Deep Convolutional Neural Network (DCNN), there are many works aiming to find the best network architecture fitting for this task. However, all of the studies are based on very-high resolution satellite images, and surprisingly; none of them are implemented on medium resolution satellite images. Moreover, no research has applied geoinformatics knowledge. Therefore, we purpose to compare the semantic segmentation models, which are FCN, SegNet, and GSN using medium resolution images from Landsat-8 satellite. In addition, we propose a modified SegNet model that can be used with remote sensing derived indices. The results show that the model that achieves the highest accuracy RGB bands of medium resolution aerial imagery is SegNet. The overall accuracy of the model increases when includes Near Infrared (NIR) and Short-Wave Infrared (SWIR) band. The results showed that our proposed method (our modified SegNet model, named RGB-IR-IDX-MSN method) outperforms all of the baselines in terms of mean F1 scores.",semantic segmentation,deep convolutional neural network,remote sensing,medium-resolution satellite image,"Panboonyuen, Teerapong","Vateekul, Peerapon","Lawavirojwong, Siam","Srestasathiern, Panu",landsat-8,,,,,,,,"Jitkajornwanich, Kulsawasd",2018 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER SCIENCE AND SOFTWARE ENGINEERING (JCSSE),,,,,,,,,,,,,,,
Row_501,"Ding, Lei","Tang, Hao","Bruzzone, Lorenzo",LANet: Local Attention Embedding to Improve the Semantic Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,JAN 2021,255,"The trade-off between feature representation power and spatial localization accuracy is crucial for the dense classification/semantic segmentation of remote sensing images (RSIs). High-level features extracted from the late layers of a neural network are rich in semantic information, yet have blurred spatial details; low-level features extracted from the early layers of a network contain more pixel-level information but are isolated and noisy. It is therefore difficult to bridge the gap between high- and low-level features due to their difference in terms of physical information content and spatial distribution. In this article, we contribute to solve this problem by enhancing the feature representation in two ways. On the one hand, a patch attention module (PAM) is proposed to enhance the embedding of context information based on a patchwise calculation of local attention. On the other hand, an attention embedding module (AEM) is proposed to enrich the semantic information of low-level features by embedding local focus from high-level features. Both proposed modules are lightweight and can be applied to process the extracted features of convolutional neural networks (CNNs). Experiments show that, by integrating the proposed modules into a baseline fully convolutional network (FCN), the resulting local attention network (LANet) greatly improves the performance over the baseline and outperforms other attention-based methods on two RSI data sets.",Semantics,Image segmentation,Feature extraction,Decoding,,,,,Remote sensing,Correlation,Convolutional neural networks,Convolutional neural network (CNN),deep learning,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_502,"Jiang, Xufeng","Zhou, Nan","Li, Xiang",Few-Shot Segmentation of Remote Sensing Images Using Deep Metric Learning,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,20,"Current convolutional neural network (CNN)-based methods for remote sensing image segmentation require a large number of densely annotated images for model training and have limited generalization abilities for unseen object categories. In this letter, we propose a novel few-shot learning-based method for the semantic segmentation of remote sensing images. Our method can perform semantic labeling for unseen object categories with only a few annotated samples. More specifically, our model starts by using a deep CNN to extract high-level semantic features. The prototype representation of each class is then generated by using a masked average pooling on the feature embeddings of the support images with ground truth masks. Finally, our model performs semantic labeling over the query images by matching the feature embedding of each pixel to its nearest prototypes in the embedding space. Our model is optimized with a nonparametric metric learning-based loss function to maximize the intra-class similarity of learned prototypes while minimizing the inter-class similarity. Experiments on International Society for Photogrammetry and Remote Sensing (ISPRS) 2-D semantic labeling dataset demonstrate satisfying in-domain and cross-domain transferring abilities of our model.",Image segmentation,Semantics,Prototypes,Feature extraction,,,,,Training,Remote sensing,Labeling,Few-shot learning,prototype representation,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_503,"Li, Xin","Xu, Feng","Gao, Hongmin",A Frequency Domain Feature-Guided Network for Semantic Segmentation of Remote Sensing Images,IEEE SIGNAL PROCESSING LETTERS,2024,6,"Semantic segmentation of Remote Sensing Images (RSIs) entails assigning semantic labels to each pixel accurately. RSIs are rich in spatial and spectral data, revealing diverse material and object characteristics. Yet, current RSI-focused computer vision models struggle with significant intra-class variation and inter-class resemblance due to limited spectral data usage. We propose the Frequency Domain Feature-Guided Network (FFGNet) for RSI semantic segmentation, influenced by digital signal processing theories. FFGNet initially generates frequency domain features via patch partitioning and 2D discrete cosine transformation. Our Frequency Enhancement Attention module (FEA) then distinguishes and intensifies frequency components to retain detailed information. These enhanced features are integrated with the Spatial-Spectral Attention (SSA) for enriched spectral signals. In the inference phase, these features are upsampled and combined with decoded features, emphasizing spectral details. Additionally, our novel loss function combines frequency and cross-entropy losses. Experiments on LoveDA and ISPRS Potsdam datasets demonstrate FFGNet's effectiveness, surpassing other mainstream models. An ablation study further validates our dual-guidance design.",Frequency-domain analysis,Discrete cosine transforms,Spatial databases,Semantics,"Liu, Fan","Lyu, Xin",,,Remote sensing,Training,Logic gates,Semantic segmentation,remote sensing images,attention mechanism,spatial-spectral attention,,,,,,,,,,,,,,,,,,
Row_504,"Zhang, Lili","Lu, Wanxuan","Zhang, Jinming",A Semisupervised Convolution Neural Network for Partial Unlabeled Remote-Sensing Image Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,7,"Semantic segmentation methods for remote-sensing images based on the deep learning framework have achieved significant performance improvements. However, most of the existing work is based on fully supervised methods, which rely on a large number of manually annotated pixel-level labels. However, for remote-sensing images, labeling the ground-truth takes time and effort. To solve the problem in existing methods of overly relying on manual labeling, in this study, we propose a semisupervised convolution neural network based on contrastive loss for partial unlabeled remote-sensing image segmentation. In the design of the contrastive loss function, to capture the semantic relationship of pixels and improve the separability between different categories, we propose pixel-level and region-level contrastive loss. The pixel-level contrastive loss is designed to learn the correlation between different images, while region-level contrastive loss is designed to improve the quality of generated pseudo-labels. In addition, we designed a propagated self-training method that further guarantees the quality of the pseudo-labels and improves the richness of the labeled data. Experiments on POTSDAM and Vaihingen datasets demonstrate that the proposed method achieves the highest Mean Intersection over Union (mIOU) and significantly outperforms previous methods.",Image segmentation,Remote sensing,Training,Semantics,"Wang, Hongqi",,,,Propagation losses,Data models,Head,Deep learning,remote-sensing image,semantic segmentation,semisupervised,,,,,,,,,,,,,,,,,,
Row_505,"Wang, Wenxiu","Fu, Yutian","Dong, Feng",Semantic segmentation of remote sensing ship image via a convolutional neural networks model,IET IMAGE PROCESSING,MAY 2019,27,"Semantic segmentation of remote sensing ship targets is one of the most challenging works in image processing, especially for small and multi-scale ship target detection. To solve these problems, an efficient method based on convolutional neural networks (CNN) to detect ship targets is proposed. This method introduces the attention model to the network to enhance the characteristics of small targets and combines atrous convolution with traditional CNN to increase the receptive field. To preserve the information lost by pooling, the proposed method uses the passthrough layer method to retain more features and concatenate the high- and low-resolution features. To verify the effectiveness of the method proposed in this study, the performance was evaluated by using precision, recall, F1-Score, mean intersection-over-union (IoU), and pixel accuracy measurements. These performances are all higher than the traditional semantic segmentation network SegNet. Mean IoU increases to 0.783 and pixel accuracy increases to 0.935. This method can conclusively identify ship targets in remote sensing images and has a certain reference value for remote sensing target detection.",remote sensing,neural nets,feature extraction,image classification,"Li, Feng",,,,object detection,ships,image fusion,image segmentation,geophysical image processing,remote sensing ship image,convolutional neural networks model,remote sensing ship targets,,,,,,image processing,multiscale ship target detection,attention model,combines atrous convolution,traditional CNN,passthrough layer method,high- resolution features,low-resolution features,remote sensing target detection,SegNet,traditional semantic segmentation network,
Row_506,"Yuan, Min","Ren, Dingbang","Feng, Qisheng",MCAFNet: A Multiscale Channel Attention Fusion Network for Semantic Segmentation of Remote Sensing Images,REMOTE SENSING,JAN 2023,24,"Semantic segmentation for urban remote sensing images is one of the most-crucial tasks in the field of remote sensing. Remote sensing images contain rich information on ground objects, such as shape, location, and boundary and can be found in high-resolution remote sensing images. It is exceedingly challenging to identify remote sensing images because of the large intraclass variance and low interclass variance caused by these objects. In this article, we propose a multiscale hierarchical channel attention fusion network model based on a transformer and CNN, which we name the multiscale channel attention fusion network (MCAFNet). MCAFNet uses ResNet-50 and Vit-B/16 to learn the global-local context, and this strengthens the semantic feature representation. Specifically, a global-local transformer block (GLTB) is deployed in the encoder stage. This design handles image details at low resolution and extracts global image features better than previous methods. In the decoder module, a channel attention optimization module and a fusion module are added to better integrate high- and low-dimensional feature maps, which enhances the network's ability to obtain small-scale semantic information. The proposed method is conducted on the ISPRS Vaihingen and Potsdam datasets. Both quantitative and qualitative evaluations show the competitive performance of MCAFNet in comparison to the performance of the mainstream methods. In addition, we performed extensive ablation experiments on the Vaihingen dataset in order to test the effectiveness of multiple network components.",semantic segmentation,transformer,channel attention module,hybrid structure,"Wang, Zhaobin","Dong, Yongkang","Lu, Fuxiang","Wu, Xiaolin",,,,,,,,,,,,,,,,,,,,,,,,,
Row_507,"Yang, Jingyu","Zhao, Liang","Dang, Jianwu",A Semantic Segmentation Method for High-resolution Remote Sensing Images Based on Encoder-Decoder,"COMPUTER VISION - ECCV 2018, PT VII",2022,2,"Image segmentation is a key technology in remote sensing image interpretation, and it is widely used in many fields. Aiming at the common problems of low segmentation accuracy and blurred target boundary in the semantic segmentation of high-resolution remote sensing images, a semantic segmentation method of high-resolution remote sensing images based on encoder-decoder structure is proposed, in which an attention mechanism is introduced to highlight important features,and an optimized Pyramid pooling module is used to extract multi-scale features from different layers. Finally, a multi-level and multi-scale feature fusion strategy is adopted to achieve fine-grained segmentation of high-resolution remote sensing images. The method is also compared and tested on the ISPRS Vaihingen dataset to verify the effectiveness.",semantic segmentation,attention mechanism,multi-scale feature fusion,label smoothing,"Wang, Yangping","Yue, Biao","Gu, Zongliang",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_508,"Qi, Xingqun","Li, Kaiqi","Liu, Pengkun",Deep Attention and Multi-Scale Networks for Accurate Remote Sensing Image Segmentation,IEEE ACCESS,2020,36,"Remote sensing image segmentation is a challenging task in remote sensing image analysis. Remote sensing image segmentation has great significance in urban planning, crop planting, and other fields that need plentiful information about the land. Technically, this task suffers from the ultra-high resolution, large shooting angle, and feature complexity of the remote sensing images. To address these issues, we propose a deep learning-based network called ATD-LinkNet with several customized modules. Specifically, we propose a replaceable module named AT block using multi-scale convolution and attention mechanism as the building block in ATD-LinkNet. AT block fuses different scale features and effectively utilizes the abundant spatial and semantic information in remote sensing images. To refine the nonlinear boundaries of internal objects in remote sensing images, we adopt the dense upsampling convolution in the decoder part of ATD-LinkNet. Experimentally, we enforce sufficient comparative experiments on two public remote sensing datasets (Potsdam and DeepGlobe Road Extraction). The results show our ATD-LinkNet achieves better performance against most state-of-the-art networks. We obtain 89.0% for pixel-level accuracy in the Potsdam dataset and 62.68% for mean Intersection over Union in the DeepGlobe Road Extraction dataset.",Remote sensing,Image segmentation,Feature extraction,Semantics,"Zhou, Xiaoguang","Sun, Muyi",,,Image resolution,Convolution,Roads,Remote sensing image,convolutional neural network,semantic segmentation,attention,multi-scale,,,,,,dense upsampling convolution,,,,,,,,,,,
Row_509,"Wang, Libo","Li, Rui","Zhang, Ce",UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,AUG 2022,402,"Semantic segmentation of remotely sensed urban scene images is required in a wide range of practical applications, such as land cover mapping, urban change detection, environmental protection, and economic assessment. Driven by rapid developments in deep learning technologies, the convolutional neural network (CNN) has dominated semantic segmentation for many years. CNN adopts hierarchical feature representation, demonstrating strong capabilities for information extraction. However, the local property of the convolution layer limits the network from capturing the global context. Recently, as a hot topic in the domain of computer vision, Transformer has demonstrated its great potential in global information modelling, boosting many vision-related tasks such as image classification, object detection, and particularly semantic segmentation. In this paper, we propose a Transformer-based decoder and construct an UNet-like Transformer (UNetFormer) for real-time urban scene segmentation. For efficient segmentation, the UNetFormer selects the lightweight ResNet18 as the encoder and develops an efficient global-local attention mechanism to model both global and local information in the decoder. Extensive experiments reveal that our method not only runs faster but also produces higher accuracy compared with state-of-the-art lightweight models. Specifically, the proposed UNetFormer achieved 67.8% and 52.4% mIoU on the UAVid and LoveDA datasets, respectively, while the inference speed can achieve up to 322.4 FPS with a 512 x 512 input on a single NVIDIA GTX 3090 GPU. In further exploration, the proposed Transformer-based decoder combined with a Swin Transformer encoder also achieves the state-of-the-art result (91.3% F1 and 84.1% mIoU) on the Vaihingen dataset. The source code will be freely available at https://github. com/WangLibo1995/GeoSeg.",Semantic Segmentation,Remote Sensing,Vision Transformer,Fully Transformer Network,"Fang, Shenghui","Duan, Chenxi","Meng, Xiaoliang","Atkinson, Peter M.",Global-local Context,Urban Scene,,,,,,,,,,,,,,,,,,,,,,,
Row_510,"Adam, Jibril Muhammad","Liu, Weiquan","Yu, Zang",Deep learning-based semantic segmentation of urban-scale 3D meshes in remote sensing: A survey,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,JUL 2023,14,"Semantic segmentation in 3D meshes is the classification of its constituent element(s) into specific classes or categories. Using the powerful feature extraction abilities of deep neural networks (DNNs), significant results have been obtained in the semantic segmentation of various remotely sensed data formats. With the increased utilization of DNNs to segment remotely sensed data, there have been commensurate in-depth reviews and surveys summarizing the various learning-based techniques and methodologies that entail these methods. However, most of these surveys focused on methods that involve popular data formats like LiDAR point clouds, synthetic aperture radar (SAR) images, and hyperspectral images (HSI) while 3D meshes hardly received any attention. In this paper, to our best knowledge, we present the first comprehensive and contemporary survey of recent advances in utilizing deep learning techniques for the semantic segmentation of urban-scale 3D meshes. We first describe the different approaches employed by mesh-based learning methods to generalize and implement learning techniques on the mesh surface, and then describe how the element-wise classification tasks are achieved through these methods. We also provide an in-depth discussion and comparative analysis of the surveyed methods followed by a summary of the benchmark large-scale mesh datasets accompanied with the evaluation metrics for assessing the segmentation performance of the methods. Finally, we summarize some of the contemporary problems of the field and provide future research directions that may help researchers in the community.",Semantic segmentation,Deep learning,3D meshes,Urban-scale,"Afzal, Muhammad Kamran","Bello, Saifullahi Aminu","Muhammad, Abdullahi Uwaisu","Wang, Cheng",Survey,Remote sensing,,,,,,,"Li, Jonathan",,,,,,,,,,,,,,,,
Row_511,"Hou, Yunlong","Zhu, Lei","Chen, Qin",Remote Sensing Image Segmentation Based on U-shaped Network with Atrous Spatial Pyramid,,2020,0,"Semantic segmentation in remote sensing images has always been an important research direction of computer vision, and is widely used in land and resources related fields such as land mapping, hydrology and environmental testing, urban and rural construction. This paper focuses on remote sensing image segmentation problem, which is still challenging for current semantic segmentation networks. To address this problem, this paper takes the DeepLabv3+([1]) network as the baseline structure, and attaches a multi-level feature fusion module to well utilize the low-level information of the network. The proposed network performs better on multiple evaluations such as Precision, Recall, and Mean Absolute Error (MAE) compared to several state-of-the-art methods, and is able to extract details of the segmentation more precisely.",Semantic Segmentation,Remote Sensing Image,DeepLabv3+,FAM,,,,,MAE,,,,,,,,,PROCEEDINGS OF THE 39TH CHINESE CONTROL CONFERENCE,,,,,,,,,,,,,,,
Row_512,"Bello, Inuwa M.","Zhang, Ke","Wang, Jingyu",Lightweight multiscale framework for segmentation of high-resolution remote sensing imagery,JOURNAL OF APPLIED REMOTE SENSING,AUG 5 2021,6,"Deep convolutional neural network semantic segmentation has played a significant role in remote sensing applications due to its capability of end-to-end training and automatic high-level features extraction. Highly accurate predictions have been recorded from different network architectures that are designed using the convolutional layers. However, the accuracy is mainly achieved at a very high cost of computation, which renders the network infeasible for real-time application on resource-constrained devices. Therefore, there is a need to establish a trade-off between accuracy and model efficiency. Our paper proposes a lightweight, highly accurate, memory-efficient segmentation network capable of deployment on resource-constrained devices. Our proposed network of 1.09 million trainable parameters attains an appreciable accuracy of 89.41% and 88.78% on the Vaihingen and Potsdam respective dataset of the ISPRS 2D Semantic Labeling Challenge. The result from the speed and the memory efficiency experiment shows that our proposed network is suitable for real-time remote sensing applications. (C) 2021 Society of Photo-Optical Instrumentation Engineers (SPIE)",multiscale multispectral image,neural networks,remote sensing,image segmentation,"Li, Haoyu",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_513,"Jin, Huazhong","Bao, Zhixi","Chang, Xueli",Semantic segmentation of remote sensing images based on dilated convolution and spatial-channel attention mechanism,JOURNAL OF APPLIED REMOTE SENSING,JAN 1 2023,3,"The rich context information and multiscale ground information in remote sensing images are crucial to improving the semantic segmentation accuracy. Therefore, we propose a remote sensing image semantic segmentation method that integrates multilevel spatial channel attention and multi-scale dilated convolution, effectively addressing the issue of poor segmentation performance of small target objects in remote sensing images. This method builds a multilevel characteristic fusion structure, combining deep-level semantic characteristics with the details of the shallow levels to generate multiscale feature diagrams. Then, we introduce the dilated convolution of the series combination in each layer of the atrous spatial pyramid pooling structure to reduce the loss of small target information. Finally, using convolutional conditional random field to describe the context information on the space and edges to improve the model's ability to extract details. We prove the effectiveness of the model on the three public datasets. From the quantitative point of view, we mainly evaluate the four indicators of the model's F1 score, overall accuracy (OA), Intersection over Union (IoU), and Mean Intersection over Union (MIoU). On GID dataset, F1 score, OA, and MIoU reach 87.27, 87.80, and 77.70, respectively, superior to most mainstream semantic segmentation networks.",semantic segmentation,dilated convolution,spatial-channel attention,convolutional conditional random field,"Zhang, Tingtao","Chen, Can",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_514,"Zeng, Qiaolin","Zhou, Jingxiang","Niu, Xuerui",Cross-Scale Feature Propagation Network for Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,8,"Over the past few years, various strategies have been proposed to improve the multiscale information capture capability of networks, such as encoder-decoder framework, convolution layers with different kernel sizes in parallel, and multiple branches framework. However, many methods only rely on one of the strategies, which limits their performance when processing remote sensing images (RSIs) with large-scale variance. To address this issue and enable the fast and effective extraction of multiscale semantic information, this manuscript introduces a novel cross-scale feature propagation network (CFPNet). Specifically, the multiscale convolution (MSC) module aims to capture fine-grained multiscale context with different receptive fields, and the attention upsample (AUS) module embeds the semantic information of high-level features into low-level features while maintaining spatial details. Besides, the feature semantic enhancement (FSE) module is proposed to aggregate the multilayer features of the decoder to enhance the final feature representation. The experimental results on the Beijing Land-Use (BLU) and GID datasets demonstrate the effectiveness and efficiency of our CFPNet.",Attention mechanism,deep learning,remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_515,"Ma, Xianping","Zhang, Xiaokang","Ding, Xingchen",Decomposition-Based Unsupervised Domain Adaptation for Remote Sensing Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"Unsupervised domain adaptation (UDA) techniques are vital for semantic segmentation in geosciences, effectively utilizing remote sensing imagery across diverse domains. However, most existing UDA methods, which focus on domain alignment at the high-level feature space, struggle to simultaneously retain local spatial details and global contextual semantics. To overcome these challenges, a novel decomposition scheme is proposed to guide domain-invariant representation learning. Specifically, multiscale high/low-frequency decomposition (HLFD) modules are proposed to decompose feature maps into high- and low-frequency components across different subspaces. This decomposition is integrated into a fully global-local generative adversarial network (GLGAN) that incorporates global-local transformer blocks (GLTBs) to enhance the alignment of decomposed features. By integrating the HLFD scheme and the GLGAN, a novel decomposition-based UDA framework called De-GLGAN is developed to improve the cross-domain transferability and generalization capability of semantic segmentation models. Extensive experiments on two UDA benchmarks, namely ISPRS Potsdam and Vaihingen, and LoveDA Rural and Urban, demonstrate the effectiveness and superiority of the proposed approach over existing state-of-the-art UDA methods. The source code for this work is accessible at https://github.com/sstary/SSRS.",Semantic segmentation,Remote sensing,Training,Semantics,"Pun, Man-On","Ma, Siwei",,,Feature extraction,Context modeling,Transformers,Generators,Generative adversarial networks,Adaptation models,Global-local information,high/low-frequency decomposition (HLFD),,,,,,remote sensing,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,
Row_516,"Zou, Zhengxia","Shi, Tianyang","Li, Wenyuan",Do Game Data Generalize Well for Remote Sensing Image Segmentation?,REMOTE SENSING,JAN 2020,15,"Despite the recent progress in deep learning and remote sensing image interpretation, the adaption of a deep learning model between different sources of remote sensing data still remains a challenge. This paper investigates an interesting question: do synthetic data generalize well for remote sensing image applications? To answer this question, we take the building segmentation as an example by training a deep learning model on the city map of a well-known video game ""Grand Theft Auto V"" and then adapting the model to real-world remote sensing images. We propose a generative adversarial training based segmentation framework to improve the adaptability of the segmentation model. Our model consists of a CycleGAN model and a ResNet based segmentation network, where the former one is a well-known image-to-image translation framework which learns a mapping of the image from the game domain to the remote sensing domain; and the latter one learns to predict pixel-wise building masks based on the transformed data. All models in our method can be trained in an end-to-end fashion. The segmentation model can be trained without using any additional ground truth reference of the real-world images. Experimental results on a public building segmentation dataset suggest the effectiveness of our adaptation method. Our method shows superiority over other state-of-the-art semantic segmentation methods, for example, Deeplab-v3 and UNet. Another advantage of our method is that by introducing semantic information to the image-to-image translation framework, the image style conversion can be further improved.",remote sensing,deep learning,video game,domain adaptation,"Zhang, Zhou","Shi, Zhenwei",,,building segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_517,"Ye, Ziran","Lin, Yue","Dong, Baiyu",An Object-Aware Network Embedding Deep Superpixel for Semantic Segmentation of Remote Sensing Images,REMOTE SENSING,OCT 2024,0,"Semantic segmentation forms the foundation for understanding very high resolution (VHR) remote sensing images, with extensive demand and practical application value. The convolutional neural networks (CNNs), known for their prowess in hierarchical feature representation, have dominated the field of semantic image segmentation. Recently, hierarchical vision transformers such as Swin have also shown excellent performance for semantic segmentation tasks. However, the hierarchical structure enlarges the receptive field to accumulate features and inevitably leads to the blurring of object boundaries. We introduce a novel object-aware network, Embedding deep SuperPixel, for VHR image semantic segmentation called ESPNet, which integrates advanced ConvNeXt and the learnable superpixel algorithm. Specifically, the developed task-oriented superpixel generation module can refine the results of the semantic segmentation branch by preserving object boundaries. This study reveals the capability of utilizing deep convolutional neural networks to accomplish both superpixel generation and semantic segmentation of VHR images within an integrated end-to-end framework. The proposed method achieved mIoU scores of 84.32, 90.13, and 55.73 on the Vaihingen, Potsdam, and LoveDA datasets, respectively. These results indicate that our model surpasses the current advanced methods, thus demonstrating the effectiveness of the proposed scheme.",VHR image,convolutional neural network,deep superpixel,semantic segmentation,"Tan, Xiangfeng","Dai, Mengdi","Kong, Dedong",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_518,"Wu, Xiaosuo","Wang, Liling","Wu, Chaoyang",Semantic Segmentation of Remote Sensing Images Using Multiway Fusion Network,SIGNAL PROCESSING,FEB 2024,7,"To effectively solve the problems of intra-class dissimilarity and inter-class similarity, this study proposes a deep learning semantic segmentation model that fuses multiple path features. It utilizes Multipath Fusion Module (MFM) to extract input image features, and dynamically fuses the features extracted from each input path. In the fusion process, the segmentation model dynamically adjusts the fuse on ratio and feature threshold of each path according to the input image, enables highly accurate image segmentation. In the upsampling stage, a guided upsampling strategy helps to avoid edge classification errors due to bilinear interpolation. The proposed network was trained and tested on the Potsdam dataset with good results, with mean intersection over union (mIoU) of 83.38%, overall accuracy (OA) of 90.21% and an F1 score of 90.86%.",Semantic segmentation,remote sensing images,Multiway Fusion Network,guided upsampling,"Guo, Cunge","Yan, Haowen","Qiao, Ze",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_519,"Wang, Xin","Zhang, Yu","Ca, Jingye",Semantic segmentation network for mangrove tree species based on UAV remote sensing images,SCIENTIFIC REPORTS,DEC 2 2024,0,"Mangroves are special vegetation that grows in the intertidal zone of the coast and has extremely high ecological and environmental value. Different mangrove species exhibit significant differences in ecological functions and environmental responses, so accurately identifying and distinguishing these species is crucial for ecological protection and monitoring. However, mangrove species recognition faces challenges, such as morphological similarity, environmental complexity, target size variability, and data scarcity. Traditional mangrove monitoring methods mainly rely on expensive and operationally complex multispectral or hyperspectral remote sensing sensors, which have high data processing and storage costs, hindering large-scale application and popularization. Although hyperspectral monitoring is still necessary in certain situations, the low identification accuracy in routine monitoring severely hinders ecological analysis. To address these issues, this paper proposes the UrmsNet segmentation network, aimed at improving identification accuracy in routine monitoring while reducing costs and complexity. It includes an improved lightweight convolution SCConv, an Adaptive Selective Attention Module (ASAM), and a Cross-Layer Feature Fusion Module (CLFFM). ASAM adaptively extracts and fuses features of different mangrove species, enhancing the network's ability to characterize mangrove species with similar morphology and in complex environments. CLFFM combines shallow details and deep semantic information to ensure accurate segmentation of mangrove boundaries and small targets.Additionally, this paper constructs a high-quality RGB image dataset for mangrove species segmentation to address the data scarcity problem. Compared to traditional methods, our approach is more precise and efficient. While maintaining relatively low parameters and computational complexity (FLOPs), it achieves excellent performance with mIoU and mPA metrics of 92.21% and 95.98%, respectively. This performance is comparable to the latest methods using multispectral or hyperspectral data but significantly reduces cost and complexity. By combining periodic hyperspectral monitoring with UrmsNet-supported routine monitoring, a more comprehensive and efficient mangrove ecological monitoring can be achieved.These research findings provide a new technical approach for large-scale, low-cost monitoring of important ecosystems such as mangroves, with significant theoretical and practical value. Furthermore, UrmsNet also demonstrates excellent performance on LoveDA, Potsdam, and Vaihingen datasets, showing potential for wider application.",Feature fusion,Mangrove species segmentation,Semantic segmentation,UAV remote sensing,"Qin, Qin","Feng, Yi","Yan, Jingke",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_520,"Wu, Honglin","Zeng, Zhaobin","Huang, Peng",CCTNet: CNN and Cross-Shaped Transformer Hybrid Network for Remote Sensing Image Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"Deep learning methods have achieved great success in the field of remote sensing image segmentation in recent years, but building a lightweight segmentation model with comprehensive local and global feature extraction capabilities remains a challenging task. In this article, we propose a convolutional neural network (CNN) and cross-shaped transformer hybrid network (CCTNet) for semantic segmentation of high-resolution remote sensing images. This model follows an encoder-decoder structure. It employs ResNet18 as an encoder to extract hierarchical feature information, and constructs a transformer decoder based on efficient cross-shaped self-attention to fully model local and global feature information and achieve lightweighting of the network. Moreover, the transformer block introduces a mixed-scale convolutional feedforward network to further enhance multiscale information extraction. Furthermore, a simplified and efficient feature aggregation module is leveraged to gradually aggregate local and global information at different stages. Extensive comparison experiments on the ISPRS Vaihingen and Potsdam datasets reveal that our method obtains superior performance compared with state-of-the-art lightweight methods.",Transformers,Feature extraction,Semantic segmentation,Semantics,"Yu, Xinyu","Zhang, Min",,,Remote sensing,Convolutional neural networks,Decoding,Computational efficiency,Data mining,Computer architecture,Convolutional neural network (CNN),cross-shaped transformer,,,,,,global contextual information,remote sensing image,semantic segmentation,,,,,,,,,
Row_521,"Li, Zhongyu","Wang, Huajun","Liu, Yang",Semantic segmentation of remote sensing image based on bilateral branch network,VISUAL COMPUTER,MAY 2024,2,"Due to the large intra-class differences between the same categories and the scale imbalance between different categories in the remote sensing image dataset, the semantic segmentation task presents the problem of small-scale object information loss, the imbalance between foreground and background, and simultaneously the background dominates, which seriously affects the performance of the network model. To solve the above problems, this paper proposes an efficient bilateral branch depth neural network model based on the U-Net depth neural network, named BBU-Net. Firstly, one branch of the network learns the distribution characteristics of the original data, and the other focuses on difficult samples. Then the two branches improve the representation and classification ability of the neural network by accumulating learning strategies. Finally, considering the geometric diversity of remote sensing images, this paper adopts test time augmentation and reflection padding strategies and proposes a balanced weighted loss function named CombineLoss to alleviate the imbalance in the training process. The depth neural network proposed in this paper was first tested on the Inria Aerial Image Labeling Dataset, and 87.53% of mean intersection over union and 97.4% of mean pixel accuracy were obtained, respectively. At the same time, to verify the model's complexity, the model proposed in this paper is compared with the neural network based on integrated learning. The comparison results show that the spatial complexity of the network proposed in this paper is much lower than the neural network obtained by integrated learning, and the parameters are also much smaller than the neural network based on integrated learning. Then use the satellite building dataset I in the WHU Building Dataset and mainstream semantic segmentation methods for multiple groups of comparative experiments. The experimental results show that the method proposed in this paper can effectively extract the semantic information of remote sensing images, significantly improve the imbalance of remote sensing image data, improve the performance of the network model, and achieve a good semantic segmentation effect, which fully proves the effectiveness of this method.",Feature extraction,Semantic segmentation,Deep neural network,Imbalance,,,,,Integrated learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_522,"Liu, Mengjia","Liu, Peng","Zhao, Lingjun",Fast semantic segmentation for remote sensing images with an improved Short-Term Dense-Connection (STDC) network,INTERNATIONAL JOURNAL OF DIGITAL EARTH,DEC 31 2024,0,"It is hard to accomplish fast semantic segmentation on large remote sensing images, since current neural networks with numerous parameters often rely on significant computational resources. Our team proposes an improved fast semantic segmentation model based on short-term dense-connection network (RepSTDC). We introduce a structure reparameterization and coordinate attention into STDC networks. By structure reparameterization, we transform the multi-branch structure into a comparable single-branch configuration during the inference process. By replacing the traditional channel attention with a coordinate attention mechanism, we enhance the attention mechanism with considering channel relationships and long-distance position information, and then it saves the memory usages. We conducted thorough experiments to assess the efficacy of network components of RepSTDC on the several benchmark datasets. Additionally, we compared our proposed approach with state-of-the-art methods. Our RepSTDC model can well balance the accuracy performances, computing speed, and memory usage in most cases. It achieves fast segmentation by significantly reducing parameters but without obviously compromising performances compared to other methods.",Convolutional neural network (CNN),structural reparameterization,remote sensing image,semantic segmentation,"Ma, Yan","Chen, Lajiao","Xu, Mengzhen",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_523,"Wang, Kai","He, Daojie","Sun, Qingqiang",A novel network for semantic segmentation of landslide areas in remote sensing images with multi-branch and multi-scale fusion,APPLIED SOFT COMPUTING,JUN 2024,2,"Landslides pose significant risks as natural disasters, highlighting the importance of accurate mapping using remote sensing images for various practical applications. However, due to the challenges arising from incomplete and inaccurate boundary information of foreground landslide polygons, existing methods can only achieve suboptimal performance. To this premise, in this paper, we propose a segmentation network called GMNet that leverages global information extraction and multi -scale feature fusion to enhance the discrimination of landslides from other objects. Specifically, by employing a multi -branch mechanism, our method effectively captures global information, while an improved multi -scale feature fusion technique addresses the issue of varying scales in landslide polygons. Furthermore, semantic enhancement enhances the semantic information of low-level features, bridging the semantic gap and enhancing fusion efficacy. Experimental results demonstrate the effectiveness of our network in segmenting landslide areas accurately within the remote sensing image dataset. Especially, our F1_scores on three benchmarks outperform existing runner-ups by notable margins of 4.81%, 1.72%, and 1.16%, showcasing the value of our method in this domain.",Landslide,Transformer,Semantic segmentation,Remote sensing,"Yi, Lizhi","Yuan, Xiaofeng","Wang, Yalin",,Feature fusion,,,,,,,,,,,,,,,,,,,,,,,,
Row_524,"Su, Yu-Chen","Liu, Tsung-Jung","Liuy, Kuan-Hsien",Multi-scale Wavelet Frequency Channel Attention for Remote Sensing Image Segmentation,DEEP LEARNING IN MEDICAL IMAGE ANALYSIS AND MULTIMODAL LEARNING FOR CLINICAL DECISION SUPPORT,2022,9,"Among recent developments in semantic segmentation, deep convolutional encoder-decoder has become the main-scheme model for remote sensing images. In this paper, we propose a architecture similar to U-Net for remote sensing image segmentation that uses wavelet frequency channel attention (WFCA) blocks as the attention mechanism to extract rich semantic features, which not only contain local information in spatial domain, but also consider frequency details in frequency domain. Then we fuse WFCA blocks with multi-scale skip connections to become multi-scale wavelet frequency channel attention (ms-WFCA) blocks for better utilizing features from different scales. Finally, the proposed method shows promising results on the Potsdam dataset.",Semantic segmentation,remote sensing,wavelet transformation,attention mechanism,,,,,multi-scale,,,,,,,,,,,,,,,,,,,,,,,,
Row_525,Ni Ruiwen,Mu Ye,Li Ji,Segmentation of Remote Sensing Images Based on U-Net Multi-Task Learning,CMC-COMPUTERS MATERIALS & CONTINUA,2022,2,"In order to accurately segment architectural features in highresolution remote sensing images, a semantic segmentation method based on U-net network multi-task learning is proposed. First, a boundary distance map was generated based on the remote sensing image of the ground truth map of the building. The remote sensing image and its truth map were used as the input in the U-net network, followed by the addition of the building ground prediction layer at the end of the U-net network. Based on the ResNet network, a multi-task network with the boundary distance prediction layer was built. Experiments involving the ISPRS aerial remote sensing image building and feature annotation data set show that compared with the full convolutional network combined with the multi-layer perceptron method, the intersection ratio of VGG16 network, VGG16 + boundary prediction, ResNet50 and the method in this paper were increased by 5.15%, 6.946%, 6.41% and 7.86%. The accuracy of the networks was increased to 94.71%, 95.39%, 95.30% and 96.10% respectively, which resulted in high-precision extraction of building features.",Multitasking learning,U-net,ResNet,remote sensing image,Zhang Tong,Luo Tianye,Feng Ruilong,Gong He,semantic segmentation,,,,,,,,Hu Tianli,,Sun Yu,Guo Ying,Li Shijun,,,,,,,,,,,,"Tyasi, Thobela Louis"
Row_526,"Li, Xin","Xu, Feng","Liu, Fan",Semantic Segmentation of Remote Sensing Images by Interactive Representation Refinement and Geometric Prior-Guided Inference,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,18,"High spatial resolution remote sensing images (HRRSIs) contain intricate details and varied spectral distributions, making their semantic segmentation a challenging task. To address this problem, it is crucial to adequately capture both local and global contexts to reduce semantic ambiguity. While self-attention modules in vision transformers capture long-range context, they tend to sacrifice local details. In this article, we propose a geometric prior-guided interactive network (GPINet), a hybrid network that refines features across encoder and decoder stages. First of all, a dual branch structure encoder with local-global interaction modules (LGIMs) is designed to fully exploit local and global contexts for feature refinement. Unlike commonly used skip connections or concatenations, the LGIMs bilaterally couple and exchange CNN features with transformer features by lossless transformation and elaborating cross-attention. Moreover, we introduce a geometric prior generation module (GPGM) that iteratively updates the randomly initialized geometric prior. Subsequently, the geometric priors are stored and used to guide feature recovery. Finally, a weighted summation is applied to the upsampled decoded features and geometric priors. By comprehensively capturing contexts and enabling lossless decoding and deterministic inference, GPINet allows the network to learn discriminative representations for accurately specifying pixel-level semantics. Experiments on three benchmark datasets demonstrate the superiority of the proposed GPINet over state-of-the-art methods. Furthermore, we validate the effectiveness of geometric priors and compare the model sizes.",Attention bias,contextual affinity,remote sensing images (RSIs),semantic segmentation,"Tong, Yao","Lyu, Xin","Zhou, Jun",,synergistic attention,,,,,,,,,,,,,,,,,,,,,,,,
Row_527,"Xu, Chunyan","Li, Chengzheng","Cui, Zhen",Hierarchical Semantic Propagation for Object Detection in Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,JUN 2020,54,"Object detection in remote sensing imagery is a critical yet challenging task in the field of computer vision due to the bird & x2019;s-eye-view perspective. Although existing object detection approaches in remote sensing imagery have achieved great advances through the utilization of deep features or rotation proposals, but they give insufficient consideration to multilevel semantic information and its propagation for guiding the learning process. Accordingly, in this article, we propose a hierarchical semantic propagation (HSP) framework to boost object detection performance in remote sensing imagery, which is better able to propagate hierarchical semantic information among different components in a unified network. Given a remote sensing image as input, the HSP framework can detect instances of semantic objects belonging to certain categories in an end-to-end way. First, the multiscale representation is captured by a basic feature pyramid network, which can hierarchically combine spatial attention details and the global semantic structure in order to learn more discriminative visual features. Second, the soft-segmentation prediction is used as an auxiliary objective in the intermediate layer of our HSP; its output instance-aware semantic information can be propagated to suppress noisy background information and thereby guide the proposal generation in the region proposal network. By further propagating this hierarchical semantic information into the region of interest module, we can then predict the object category information and the corresponding horizontal and oriented bounding boxes. Comprehensive evaluations on three benchmark data sets demonstrate the superiority of our HSP to the existing state-of-the-art methods for object detection in remote sensing imagery.",Hierarchical semantic propagation (HSP),object detection,remote sensing imagery,,"Zhang, Tong","Yang, Jian",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_528,"Fan, Xiaomin","Zhou, Wujie","Qian, Xiaohong",Progressive Adjacent-Layer coordination symmetric cascade network for semantic segmentation of Multimodal remote sensing images,EXPERT SYSTEMS WITH APPLICATIONS,MAR 15 2024,18,"Semantic segmentation of remote sensing images is a fundamental task in computer vision, with significant applications in forest and farmland cover surveys, geological disaster monitoring, and other related fields. The inclusion of digital surface models can enhance the segmentation performance compared to using unimodal imaging alone. However, most existing methods simply combine the features from both modalities without considering their differences and complementarity, leading to a loss of spatial details. In order to address this issue and improve segmentation accuracy, we propose a novel network called Progressive Adjacent-Layer Coordination Symmetric Cascade Network (PACSCNet). This network employs a two-stage fusion symmetric cascade encoder to leverage the similarities and differences between adjacent features for cross-layer fusion, thereby preserving spatial details. Additionally, our network includes a new dual-pyramid symmetric cascade decoder that extracts similarities in multimodal and cross-layer fusion features for merging. Furthermore, a pyramid residual integration module progressively integrates features at four scales to mitigate noise interference during large-scale fusion. Extensive experimental evaluations on the Vaihingen and Potsdam datasets demonstrate that PACSCNet achieves strong semantic segmentation performance, comparable to state-of-the-art approaches, in terms of accuracy and intersection-over-union. The source code and results of our proposed PACSCNet are publicly available at https://github.com/F8AoMn/PACSCNet.",Cross-layer fusion,Multimodal remote sensing image,Semantic segmentation,Symmetric cascade network,"Yan, Weiqing",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_529,"Zhang, Yuzhu","Gao, Di","Du, Yongxing",Efficient multi-scale network for semantic segmentation of fine-resolution remotely sensed images,MEASUREMENT SCIENCE AND TECHNOLOGY,SEP 1 2024,0,"Semantic segmentation of remote sensing urban scene images has diverse practical applications, including land cover mapping, urban change detection, environmental protection, and economic evaluation. However, classical semantic segmentation networks encounter challenges such as inadequate utilization of multi-scale semantic information and imprecise edge target segmentation in high-resolution remote sensing images. In response, this article introduces an efficient multi-scale network (EMNet) tailored for semantic segmentation of common features in remote sensing images. To address these challenges, EMNet integrates several key components. Firstly, the efficient atrous spatial pyramid pooling module is employed to enhance the relevance of multi-scale targets, facilitating improved extraction and processing of context information across different scales. Secondly, the efficient multi-scale attention mechanism and multi-scale jump connections are utilized to fuse semantic features from various levels, thereby achieving precise segmentation boundaries and accurate position information. Finally, an encoder-decoder structure is incorporated to refine the segmentation results. The effectiveness of the proposed network is validated through experiments conducted on the publicly available DroneDeploy image dataset and Potsdam dataset. Results indicate that EMNet achieves impressive performance metrics, with mean intersection over union (MIoU), mean precision (MPrecision), and mean recall (MRecall) reaching 75.99%, 86.76%, and 85.07%, respectively. Comparative analysis demonstrates that the network proposed in this article outperforms current mainstream semantic segmentation networks on both the DroneDeploy and Potsdam dataset.",remote sensing,deep learning,semantic segmentation,convolutional neural network,"Li, Baoshan","Qin, Ling",,,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,
Row_530,"Cai, Jiajing","Shi, Jinmei","Leau, Yu-Beng",Res50-SimAM-ASPP-Unet: A Semantic Segmentation Model for High-Resolution Remote Sensing Images,IEEE ACCESS,2024,0,"High-resolution remote sensing images contain intricate details and complex backgrounds, presenting challenges for traditional segmentation methods, which often struggle with accurate classification and contextual understanding. To address these issues, this study introduces the Res50-SimAM-ASPP-Unet model, a semantic segmentation approach for high-resolution remote sensing image processing tasks. The model integrates ResNet50 as the encoding layer of Unet for robust feature extraction, adds the SimAM attention mechanism to selectively enhance relevant details, and incorporates the ASPP module in the decoding layer to capture multi-scale contextual information. The methodology part analyzes the common ResNet model, the attention mechanism module, and the multi-scale feature extraction module, respectively, and then designs experiments to show the necessity and optimal position of adding Res50, SimAM, and ASPP. Comparative experiments on the LandCover.ai dataset demonstrate that the proposed model significantly outperforms common semantic segmentation networks, achieving a MIoU of 81.1%, MPA of 88.2%, Accuracy of 95.1%, Precision of 92.65%, and an F1 score of 90.45%. These results highlight the model's effectiveness in delivering high accuracy and adaptability across diverse remote sensing environments, establishing it as a valuable tool for applications requiring precise and scalable image segmentation.",Remote sensing,Feature extraction,Semantic segmentation,Accuracy,"Meng, Shangyu","Zheng, Xiuyan","Zhou, Jinghe",,Interpolation,Residual neural networks,Computer architecture,Computational modeling,Training,Image coding,Segmentation of high-resolution remote sensing images,multi-scale void space pyramid pool ASPP module,,,,,,attention mechanism SimAM module,Res50-SimAM-ASPP-Unet,,,,,,,,,,
Row_531,"Weng, Yijie","Li, Zongmei","Tang, Guofeng",OCNet-Based Water Body Extraction from Remote Sensing Images,WATER,OCT 2023,2,"Water body extraction techniques from remotely sensed images are crucial in water resources distribution studies, climate change studies and other work. The traditional remote sensing water body extraction has the problems of low accuracy and being time-consuming and laborious, and the water body recognition technique based on deep learning is more efficient and accurate than the traditional threshold method; however, there is the problem that the basic model of semantic segmentation is not well-adapted to complex remote sensing images. Based on this, this study adopts an OCNet feature extraction network to modify the base model of semantic segmentation, and the resulting model achieves excellent performance on water body remote sensing images. Compared with the traditional water body extraction method and the base network, the OCNet modified model has obvious improvement, and is applicable to the extraction of water bodies in true-color remote sensing images such as high-score images and unmanned aerial vehicle remote sensing images. The results show that the model in this study can realize automatic and fast extraction of water bodies from remote sensing images, and the predicted water body image accuracy (ACC) can reach 85%. This study can realize fast and accurate extraction of water bodies, which is of great significance for water resources acquisition and flood disaster prediction.",water body extraction,remote sensing image,semantic segmentation,OCNet,"Wang, Yang",,,,deep learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_532,Xu Zhaohong,Liu Yu,Wu Chen,Semantic Segmentation of Buildings in Remote Sensing Images Based on Dense Residual Learning and Channel Adaption,,2019,1,"Deep fully convolution neural network has opened a new field in semantic segmentation for remote sensing images. In this paper, an improved U-net model is proposed to extract buildings at the pixel level, so as to obtain its contour and size information. In this improved U-net model, the highly modular ResNeXt50 network is used as the encoder of U-net and a parallel dense residual module based on atrous convolution is proposed to extract multi-scale semantic information for segmentation. Moreover, the transposed convolution whose stride is 2 is used for upsampling to obtain the grayscale segmentation mask. This modified model adopts the sum of jaccard loss and binary cross entropy loss as the total loss function. Our experimental results have demonstrated that our improved U-net model has excellent performance on semantic information encoding and multi-scale feature extraction. Finally, the mean pixel precision (MPA), the mean intersection of union (MIoU), F-1 score and AUC of our improved model are 93.17%, 81.51%, 86.95% and 0.9928 respectively. The F-1 Score and AUC of our improved model are 11.3% and 11.4% higher than those of the standard U-net model respectively.",building extraction,remote sensing images,semantic segmentation,SE-Net,Guan Shihao,Zheng Ergong,Ma Yang,,U-net,,,,,,,,,2019 4TH INTERNATIONAL CONFERENCE ON ELECTROMECHANICAL CONTROL TECHNOLOGY AND TRANSPORTATION (ICECTT 2019),,,,,,,,,,,,,,,
Row_533,"Shi, Jun","Jiang, Zhiguo","Feng, Hao",SPARSE CODING-BASED TOPIC MODEL FOR REMOTE SENSING IMAGE SEGMENTATION,,2013,1,"Land cover segmentation can be viewed as topic assignment that the pixels are grouped into homogeneous regions according to different semantic topics in topic model. In this paper, we propose a novel topic model based on sparse coding for segmenting different kinds of land covers. Different from conventional topic models which generally assume each local feature descriptor is related to only one visual word of the codebook, our method utilizes sparse coding to characterize the potential correlation between the descriptor and multiple words. Therefore each descriptor can be represented by a small set of words. Furthermore, in this paper probabilistic Latent Semantic Analysis (pLSA) is applied to learn the latent relation among word, topic and document due to its simplicity and low computational cost. Experimental results on remote sensing image segmentation demonstrate the excellent superiority of our method over k-means clustering and conventional pLSA model.",remote sensing,sparse coding,pLSA,land cover segmentation,"Ma, Yibing",,,,,,,,,,,,,2013 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),,,,,,,,,,,,,,,
Row_534,"Yang, Xuan","Li, Shanshan","Chen, Zhengchao",An attention-fused network for semantic segmentation of very-high-resolution remote sensing imagery,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,AUG 2021,107,"Semantic segmentation is an essential part of deep learning. In recent years, with the development of remote sensing big data, semantic segmentation has been increasingly used in remote sensing. Deep convolutional neural networks (DCNNs) face the challenge of feature fusion: very-high-resolution remote sensing image multisource data fusion can increase the network's learnable information, which is conducive to correctly classifying target objects by DCNNs; simultaneously, the fusion of high-level abstract features and low-level spatial features can improve the classification accuracy at the border between target objects. In this paper, we propose a multipath encoder structure to extract features of multipath inputs, a multipath attention-fused block module to fuse multipath features, and a refinement attention-fused block module to fuse high-level abstract features and low-level spatial features. Furthermore, we propose a novel convolutional neural network architecture, named attention-fused network (AFNet). Based on our AFNet, we achieve state-of-the-art performance with an overall accuracy of 91.7% and a mean F1 score of 90.96% on the ISPRS Vaihingen 2D dataset and an overall accuracy of 92.1% and a mean F1 score of 93.44% on the ISPRS Potsdam 2D dataset.",Semantic segmentation,Deep learning,Very-high-resolution imagery,Attention-fused network,"Chanussot, Jocelyn","Jia, Xiuping","Zhang, Bing","Li, Baipeng",ISPRS,Convolutional neural network,,,,,,,"Chen, Pan",,,,,,,,,,,,,,,,
Row_535,"Zhang, Ming","Gu, Xin","Qi, Ji",CDEST: Class Distinguishability-Enhanced Self-Training Method for Adopting Pre-Trained Models to Downstream Remote Sensing Image Semantic Segmentation,REMOTE SENSING,APR 2024,0,"The self-supervised learning (SSL) technique, driven by massive unlabeled data, is expected to be a promising solution for semantic segmentation of remote sensing images (RSIs) with limited labeled data, revolutionizing transfer learning. Traditional 'local-to-local' transfer from small, local datasets to another target dataset plays an ever-shrinking role due to RSIs' diverse distribution shifts. Instead, SSL promotes a 'global-to-local' transfer paradigm, in which generalized models pre-trained on arbitrarily large unlabeled datasets are fine-tuned to the target dataset to overcome data distribution shifts. However, the SSL pre-trained models may contain both useful and useless features for the downstream semantic segmentation task, due to the gap between the SSL tasks and the downstream task. To adapt such pre-trained models to semantic segmentation tasks, traditional supervised fine-tuning methods that use only a small number of labeled samples may drop out useful features due to overfitting. The main reason behind this is that supervised fine-tuning aims to map a few training samples from the high-dimensional, sparse image space to the low-dimensional, compact semantic space defined by the downstream labels, resulting in a degradation of the distinguishability. To address the above issues, we propose a class distinguishability-enhanced self-training (CDEST) method to support global-to-local transfer. First, the self-training module in CDEST introduces a semi-supervised learning mechanism to fully utilize the large amount of unlabeled data in the downstream task to increase the size and diversity of the training data, thus alleviating the problem of biased overfitting of the model. Second, the supervised and semi-supervised contrastive learning modules of CDEST can explicitly enhance the class distinguishability of features, helping to preserve the useful features learned from pre-training while adapting to downstream tasks. We evaluate the proposed CDEST method on four RSI semantic segmentation datasets, and our method achieves optimal experimental results on all four datasets compared to supervised fine-tuning as well as three semi-supervised fine-tuning methods.",semantic segmentation,remote sensing (RS),transfer learning,fine-tuning method,"Zhang, Zhenshi","Yang, Hemeng","Xu, Jun","Peng, Chengli",contrastive learning,self-training,,,,,,,"Li, Haifeng",,,,,,,,,,,,,,,,
Row_536,"Hou, Jianlong","Guo, Zhi","Wu, Youming",BSNet: Dynamic Hybrid Gradient Convolution Based Boundary-Sensitive Network for Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,31,"Boundary information is essential for the semantic segmentation of remote sensing images. However, most existing methods were designed to establish strong contextual information while losing detailed information, making it challenging to extract and recover boundaries accurately. In this article, a boundary-sensitive network (BSNet) is proposed to address this problem via dynamic hybrid gradient convolution (DHGC) and coordinate sensitive attention (CSA). Specifically, in the feature extraction stage, we propose DHGC to replace vanilla convolution (VC), which adaptively aggregates one VC kernel and two gradient convolution kernels (GCKs) into a new operator to enhance boundary information extraction. The GCKs are proposed to explicitly encode boundary information, which is inspired by traditional Sobel operators. In the feature recovery stage, the CSA is introduced. This module is used to reconstruct the sharp and detailed segmentation results by adaptively modeling the boundary information and long-range dependencies in the low-level features as the assistance of high-level features. Note that DHGC and CSA are plug-and-play modules. We evaluate the proposed BSNet on three public datasets: the ISPRS 2-D semantic labeling Vaihingen, the Potsdam benchmark, and the iSAID dataset. The experimental results indicate that BSNet is a highly effective architecture that produces sharper predictions around object boundaries and significantly improves the segmentation accuracy. Our method demonstrates superior performance on the Vaihingen, the Potsdam benchmark, and the iSAID dataset in terms of the mean F-1, with improvements of 4.6%, 23%, and 2.4% over strong baselines, respectively. The code and models will be made publicly available.",Attention module,boundary information enhancement,remote sensing,semantic segmentation,"Diao, Wenhui","Xu, Tao",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_537,"Yan, Yi","Zhang, Jing","Wu, Xinjia",When zero-padding position encoding encounters linear space reduction attention: an efficient semantic segmentation Transformer of remote sensing images,INTERNATIONAL JOURNAL OF REMOTE SENSING,JAN 17 2024,0,"Semantic segmentation of remote sensing images (RSIs) is of great significance for obtaining geospatial object information. Transformers win promising effect, whereas multi-head self-attention (MSA) is expensive. We propose an efficient semantic segmentation Transformer (ESST) of RSIs that combines zero-padding position encoding with linear space reduction attention (LSRA). First, to capture the coarse-to-fine features of RSI, a zero-padding position encoding is proposed by adding overlapping patch embedding (OPE) layers and convolution feed-forward networks (CFFN) to improve the local continuity of features. Then, we replace LSRA in the attention operation to extract multi-level features to reduce the computational cost of the encoder. Finally, we design a lightweight all multi-layer perceptron (all-MLP) head decoder to easily aggregate multi-level features to generate multi-scale features for semantic segmentation. Experimental results demonstrate that our method produces a trade-off in accuracy and speed for semantic segmentation of RSIs on the Potsdam and Vaihingen datasets, respectively.",Remote sensing images,semantic segmentation,Zero-padding position encoding,linear space reduction attention,"Li, Jiafeng","Zhuo, Li",,,All-MLP,Transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_538,"Lin, Dao-Yu","Wang, Yang","Xu, Guang-Luan",SYNTHESIZING REMOTE SENSING IMAGES BY CONDITIONAL ADVERSARIAL NETWORKS,,2017,7,"Automated annotation of urban areas from overhead imagery plays an essential role in many remote sensing applications. Generative Adversarial Nets (GANs) is one of the most effective ways to handle this problem. In this manuscript, two tricks were added in conditional GANs(cGANs) which learn the mapping from input image to output remote sensing image. All the experimental results demonstrated that cGANs was a reliable way to generate high-quality remote sensing images. What's more, when this method be applied to semantic segmentation and accurate classification was made by using ISPRS 2D semantic labelling challenge dataset.",Generative adversarial nets,synthesize remote sensing images,semantic segmentation,,"Fu, Kun",,,,,,,,,,,,,2017 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),,,,,,,,,,,,,,,
Row_539,"Lang, Chunbo","Wang, Junyi","Cheng, Gong",Progressive Parsing and Commonality Distillation for Few-Shot Remote Sensing Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,41,"In recent years, few-shot segmentation (FSS) has received widespread attention from scholars by virtue of its superiority in low-data regimes. Most existing research focuses on natural image processing, and very few studies are dedicated to the practical but challenging topic of remote sensing image understanding. Related experimental results show that directly transferring the previously proposed framework to the current domain is prone to produce unsatisfactory results with incomplete objects and irrelevant distractors. Such phenomena can be attributed to the lack of modules specifically designed for the complex characteristics of remote sensing images, e.g., great intra-class diversity and low target-background contrast. In this article, we propose a conceptually simple and easy-to-implement framework to tackle the aforementioned problems. Specifically, our innovative design embodies two main aspects: 1) the support mask is progressively parsed into multiple valuable subregions that can be further exploited to compute local descriptors with segmentation cues about intractable parts; and 2) the base-class memories stored in the meta-training phase are replayed and leveraged for the distillation of novel-class prototypes, where the commonalities between classes are adequately explored, more in line with the concept of learning to learn. These two components, i.e., the progressive parsing module and commonality distillation module, contribute to each other and together constitute the proposed PCNet. We conduct extensive experiments on the standard benchmark to evaluate segmentation performance in few-shot settings. Quantitative and qualitative results illustrate that our PCNet distinctly outperforms previous FSS approaches and sets a new state-of-the-art.",Few-shot learning,few-shot segmentation (FSS),image processing,remote sensing,"Tu, Binfei","Han, Junwei",,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_540,"Hosseinpour, H. R.","Samadzadegan, F.","Javan, F. Dadrass",IMPROVING SEMANTIC SEGMENTATION OF HIGH-RESOLUTION REMOTE SENSING IMAGES USING WASSERSTEIN GENERATIVE ADVERSARIAL NETWORK,,2023,0,"Semantic segmentation of remote sensing images with high spatial resolution has many applications in a wide range of problems in this field. In recent years, the use of advanced techniques based on fully convolutional neural networks have achieved high and impressive accuracies. However, the labels of different classes are estimated independently in this method. In general, the segmentation effect is too coarse to take the relationship between pixels into account. On the other hand, due to the use of convolution filters and limitations of calculations, the field of view information of these filters will be limited in deep layers. In this study, a method based on generative adversarial network (GAN) is proposed to strengthen spatial vicinity in the output segmentation map. The segmentation model receive assistance from the GAN model in the form of a higher order potential loss. Furthermore, for better stability and performance in model training the Wasserstein GAN is used for optimization of the model. We successfully show an increase in semantic segmentation accuracy using the challenging ISPRS Vaihingen benchmark dataset.",Semantic Segmentation,Deep Learning,Wasserstein GAN,Generative Adversarial Network,"Motayyeb, S.",,,,,,,,,,,,,"ISPRS GEOSPATIAL CONFERENCE 2022, JOINT 6TH SENSORS AND MODELS IN PHOTOGRAMMETRY AND REMOTE SENSING, SMPR/ 4TH GEOSPATIAL INFORMATION RESEARCH, GIRESEARCH CONFERENCES, VOL. 48-4",,,,,,,,,,,,,,,
Row_541,"Kumar, Satyawant","Kumar, Abhishek","Lee, Dong-Gyu",RSSGLT: Remote Sensing Image Segmentation Network Based on Global-Local Transformer,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,4,"Remotely captured images possess an immense scale and object appearance variability due to the complex scene. It becomes challenging to capture the underlying attributes in the global and local context for their segmentation. Existing networks struggle to capture the inherent features due to the cluttered background. To address these issues, we propose a remote sensing image segmentation network, RSSGLT, for semantic segmentation of remote sensing images. We capture the global and local features by leveraging the benefits of the transformer and convolution mechanisms. RSSGLT is an encoder-decoder design that uses multiscale features. We construct an attention map module (AMM) to generate channelwise attention scores for fusing these features. We construct a global-local transformer block (GLTB) in the decoder network to support learning robust representations during a decoding phase. Furthermore, we designed a feature refinement module (FRM) to refine the fused output of the shallow stage encoder feature and the deepest GLTB feature of the decoder. Experimental findings on the two public datasets show the effectiveness of the proposed RSSGLT.",Transformers,Decoding,Image segmentation,Remote sensing,,,,,Convolution,Feature extraction,Semantics,Context details,multiscale features,remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,
Row_542,"Ran, Lingyan","Wang, Lushuang","Zhuo, Tao",DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image Semantic Segmentation With Unsupervised Domain Adaptation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"The semantic segmentation of remote sensing (RS) images is a challenging and hot issue due to the large amount of unlabeled data and domain variation. Unsupervised domain adaptation (UDA) has proven to be advantageous in leveraging unlabeled information from the target domain. However, traditional approaches of independently fine-tuning UDA models in the source and target domains have a limited effect on the result. In this article, we propose a hybrid training strategy that boosts self-training methods with domain fusion images. First, we introduce a novel dual-domain image fusion (DDF) strategy to effectively utilize the original image, the style-transferred image, and the intermediate-domain information. Second, to further refine the precision of pseudolabels, we present a region-specific reweighting strategy that assigns different weights to pseudolabel regions based on their spatial context. Finally, we conduct a series of extensive benchmark experiments and ablation studies on the ISPRS Vaihingen and Potsdam datasets. These results show the efficiency of our approach and establish a practical basis for implementing semantic segmentation in remote sensors.",Training,Adaptation models,Remote sensing,Semantic segmentation,"Xing, Yinghui","Zhang, Yanning",,,Task analysis,Accuracy,Semantics,Domain adaptation,feature fusion,semantic segmentation,,,,,,,,,,,,,,,,,,,
Row_543,"Li, Haifeng","Jing, Wenxuan","Wei, Guo",RiSSNet: Contrastive Learning Network with a Relaxed Identity Sampling Strategy for Remote Sensing Image Semantic Segmentation,REMOTE SENSING,JUL 2023,1,"Contrastive learning techniques make it possible to pretrain a general model in a self-supervised paradigm using a large number of unlabeled remote sensing images. The core idea is to pull positive samples defined by data augmentation techniques closer together while pushing apart randomly sampled negative samples to serve as supervised learning signals. This strategy is based on the strict identity hypothesis, i.e., positive samples are strictly defined by each (anchor) sample's own augmentation transformation. However, this leads to the over-instancing of the features learned by the model and the loss of the ability to fully identify ground objects. Therefore, we proposed a relaxed identity hypothesis governing the feature distribution of different instances within the same class of features. The implementation of the relaxed identity hypothesis requires the sampling and discrimination of the relaxed identical samples. In this study, to realize the sampling of relaxed identical samples under the unsupervised learning paradigm, the remote sensing image was used to show that nearby objects often present a large correlation; neighborhood sampling was carried out around the anchor sample; and the similarity between the sampled samples and the anchor samples was defined as the semantic similarity. To achieve sample discrimination under the relaxed identity hypothesis, the feature loss was calculated and reordered for the samples in the relaxed identical sample queue and the anchor samples, and the feature loss between the anchor samples and the sample queue was defined as the feature similarity. Through the sampling and discrimination of the relaxed identical samples, the leap from instance-level features to class-level features was achieved to a certain extent while enhancing the network's invariant learning of features. We validated the effectiveness of the proposed method on three datasets, and our method achieved the best experimental results on all three datasets compared to six self-supervised methods.",semantic segmentation,remote sensing (RS),self-supervised learning,contrastive learning,"Wu, Kai","Su, Mingming","Liu, Lu","Wu, Hao",,,,,,,,,"Li, Penglong",,"Qi, Ji",,,,,,,,,,,,,,
Row_544,"Sui, Baikai","Cao, Yungang","Bai, Xueqin",BIBED-Seg: Block-in-Block Edge Detection Network for Guiding Semantic Segmentation Task of High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,18,"Edge optimization of semantic segmentation results is a challenging issue in remote sensing image processing. This article proposes a semantic segmentation model guided by a block-in-block edge detection network named BIBED-Seg. This is a two-stage semantic segmentation model, where edges are extracted first and then segmented. We do two key works: The first work is edge detection, and we present BIBED, a block-in-block edge detection network, to extract the accurate boundary features. Here, the edge detection of multiscale feature fusion is first realized by creating the block-in-block residual network structure and devising the multilevel loss function. Second, we add the channel and spatial attention module into the residual structure to improve high-resolution remote sensing images' boundary positioning and detection accuracy by focusing on their channel and spatial dimensions. Finally, we evaluate our method on International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Vaihingen data sets and obtain ODS F-measure of 0.6671 and 0.7432, higher than other excellent edge detection methods. The second work is two-stage segmentation. First, the proposed BIBED is individually pretrained, and subsequently, the pretrained model is introduced into the entire segmentation network to extract boundary features. In the second segmentation stage, the edge detection network is used to constrain semantic segmentation results by loss cycles and feature bootstrapping. Our best model obtains the OA of 90.2%, 87.7%, and 81.5%, the IOU of 76.0%, 69.6%, and 61.3% on the ISPRS and WHDLD datasets, respectively.",Image edge detection,Feature extraction,Remote sensing,Semantic segmentation,"Zhang, Shuang","Wu, Renzhe",,,Convolution,Sensors,Semantics,Channel attention mechanism,edge detection,high-resolution remote sensing,multiple-residual convolution blocks,semantic segmentation,,,,,,spatial attention mechanism,,,,,,,,,,,
Row_545,"Nogueira, Keiller","Dalla Mura, Mauro","Chanussot, Jocelyn",Dynamic Multicontext Segmentation of Remote Sensing Images Based on Convolutional Networks,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,OCT 2019,106,"Semantic segmentation requires methods capable of learning high-level features while dealing with large volume of data. Toward such goal, convolutional networks can learn specific and adaptable features based on the data. However, these networks are not capable of processing a whole remote sensing image, given its huge size. To overcome such limitation, the image is processed using fixed size patches. The definition of the input patch size is usually performed empirically (evaluating several sizes) or imposed (by network constraint). Both strategies suffer from drawbacks and could not lead to the best patch size. To alleviate this problem, several works exploited multicontext information by combining networks or layers. This process increases the number of parameters, resulting in a more difficult model to train. In this paper, we propose a novel technique to perform semantic segmentation of remote sensing images that exploits a multicontext paradigm without increasing the number of parameters while defining, in training time, the best patch size. The main idea is to train a dilated network with distinct patch sizes, allowing it to capture multicontext characteristics from heterogeneous contexts. While processing these varying patches, the network provides a score for each patch size, helping in the definition of the best size for the current scenario. A systematic evaluation of the proposed algorithm is conducted using four high-resolution remote sensing data sets with very distinct properties. Our results show that the proposed algorithm provides improvements in pixelwise classification accuracy when compared to the state-of-the-art methods.",Convolutional networks (ConvNets),deep learning,multicontext,multiscale,"Schwartz, William Robson","dos Santos, Jefersson Alex",,,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_546,"Gao, Kuiliang","You, Xiong","Li, Ke",Attention Prompt-Driven Source-Free Adaptation for Remote Sensing Images Semantic Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,1,"Recently, remote sensing images (RSIs) domain adaptation segmentation has been extensively studied. However, existing methods generally assume that source RSIs must be available, which is obviously an overly demanding condition and will increase unnecessary costs in practice. To this end, this letter takes the lead in exploring RSIs source-free adaptation segmentation, where only the offline model pretrained on the source domain and target RSIs are available. A novel method featuring prompt learning and vision foundation models is proposed, and the novelty design includes two aspects. First, to better adapt the general-purpose knowledge in the foundation model to different target RSIs, an attention-guided prompt tuning strategy is proposed, which can dynamically steer the knowledge at different layers and positions through prompts with different weights. Second, a feature alignment strategy with similarity distance is proposed for source-free domain adaptation by taking full advantage of the representation ability of the foundation model and the flexibility of prompt learning. Extensive experiments indicate that the performance of the proposed method is significantly superior to that of existing methods. Specifically, the mIoU of target RSIs has been improved by at least 3.14%similar to 4.18%.",Foundation models,prompt learning,remote sensing images (RSIs),semantic segmentation,"Chen, Lingyu","Lei, Juan","Zuo, Xibing",,source-free adaptation,,,,,,,,,,,,,,,,,,,,,,,,
Row_547,"Yao, Hongtai","Wang, Xianpei","Zhao, Le",Semantic Segmentation for Remote Sensing Images Using Pyramid Object-Based Markov Random Field With Dual-Track Information Transmission,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,2,"Semantic segmentation is one of the most important tasks in remote sensing image processing. According to task requirements, the semantic depth given to the same remote sensing image can be different, and many people have studied it through a pyramid or multilayer structure. The Markov random field (MRF) is widely used in single-layer modeling due to its outstanding spatial relationship capturing ability and feature description ability, but it is not sufficient enough to mine the interlayer information, and the way of information transmission between layers is relatively simple direct projection segmentation results. To solve this problem, new dual-track information transmission is proposed in this letter. The proposed method first constructs a triple-multi (multiresolution, multiregion adjacency graph (RAG), and multisemantic)-pyramid (TMP) structure with the original resolution image as the middle layer in the pyramid. Then, the MRF model is defined on each layer; its likelihood function and the prior function that are related to the adjacent layer are constructed. Finally, the dual-track information transmission circulation is carried out to traverse the entire pyramid structure starting from the original resolution layer. The proposed method is tested on different remote sensing images obtained by the SPOT5, Gaofen-2, and unmanned aerial vehicle (UAV) sensors. Experimental results show that the proposed method has better segmentation performance than other multilayer MRF methods.",Image segmentation,Semantics,Remote sensing,Markov processes,"Tian, Meng","Gong, Li","Li, Bowen",,Information processing,Image resolution,Task analysis,Information transmission,Markov random field (MRF),pyramid structure,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_548,"Zhao, Lei","Qiao, Peng","Dou, Yong",Aircraft Segmentation Based On Deep Learning framework : from extreme points to remote sensing image segmentation,,2019,1,"Remote sensing image segmentation is a very important technology. Although the segmentation method based on convolutional neural networks (CNNs) has achieved promising results in natural image test set, e.g. VOC or COCO, they provide inferior performance when being transferred to remote sensing images. Due to the limits of labeled remote sensing images, fine-tuning pre-trained CNNs using remote sensing images do not benefit the image segmentation performance. Inspired by the recent works of interactive segmentation methods which exploit several extreme clicks that are fed into CNNs to improve the accuracy of the segmentation, we propose an effective method to improve the segmentation accuracy, which uses four extreme points (the top, bottom, left, and right) as the guide information. In terms of mIoU, our method achieves 84.4% on remote sensing image dataset, which outperforms the previous work by 23.1%. Compared with the previous interactive segmentation methods, the proposed method achieves superior performance. In addition, an improved method with an extra point is proposed based on the inaccurate part of results obtained by four extreme points. It is very feasible to be applied in an interactive segmentation toolbox.",Semantic segmentation,Interactive segmentation,Remote sensing images,Deep learning,,,,,,,,,,,,,,2019 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI 2019),,,,,,,,,,,,,,,
Row_549,"Ma, Xiaowen","Che, Rui","Hong, Tingfeng",SACANet: scene-aware class attention network for semantic segmentation of remote sensing images,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2023,6,"Spatial attention mechanism has been widely used in semantic segmentation of remote sensing images given its capability to model long-range dependencies. Many methods adopting spatial attention mechanism aggregate contextual information using direct relationships between pixels within an image, while ignoring the scene awareness of pixels (i.e., being aware of the global context of the scene where the pixels are located and perceiving their relative positions). Given the observation that scene awareness benefits context modeling with spatial correlations of ground objects, we design a scene-aware attention module based on a refined spatial attention mechanism embedding scene awareness. Besides, we present a local-global class attention mechanism to address the problem that general attention mechanism introduces excessive background noises while hardly considering the large intra-class variance in remote sensing images. In this paper, we integrate both scene-aware and class attentions to propose a scene-aware class attention network (SACANet) for semantic segmentation of remote sensing images. Experimental results on three datasets show that SACANet outperforms other state-of-the-art methods and validate its effectiveness. Code is available at https://github.com/xwmaxwma/rssegmentation.",Semantic Segmentation,Scene Awareness,Class Attention,,"Ma, Mengting","Zhao, Ziyan","Feng, Tian","Zhang, Wei",,,,,,,,,,,,,,,,,,,,,,,,,
Row_550,"Wang, Shanshan","Zuo, Zhiqi","Yan, Shuhao",A Novel Global-Local Feature Aggregation Framework for Semantic Segmentation of Large-Format High-Resolution Remote Sensing Images,APPLIED SCIENCES-BASEL,AUG 2024,0,"In high-resolution remote sensing images, there are areas with weak textures such as large building roofs, which occupy a large number of pixels in the image. These areas pose a challenge for traditional semantic segmentation networks to obtain ideal results. Common strategies like downsampling, patch cropping, and cascade models often sacrifice fine details or global context, resulting in limited accuracy. To address these issues, a novel semantic segmentation framework has been designed specifically for large-format high-resolution remote sensing images by aggregating global and local features in this paper. The framework consists of two branches: one branch deals with low-resolution downsampled images to capture global features, while the other branch focuses on cropped patches to extract high-resolution local details. Also, this paper introduces a feature aggregation module based on the Transformer structure, which effectively aggregates global and local information. Additionally, to save GPU memory usage, a novel three-step training method has been developed. Extensive experiments on two public datasets demonstrate the effectiveness of the proposed approach, with an IoU of 90.83% on the AIDS dataset and 90.30% on the WBDS dataset, surpassing state-of-the-art methods such as DANet, DeepLab v3+, U-Net, ViT, TransUNet, CMTFNet, and UANet.",high-resolution remote sensing images,optical large-format images,semantic segmentation,transformer,"Zeng, Weimin","Pang, Shiyan",,,building extraction,,,,,,,,,,,,,,,,,,,,,,,,
Row_551,"Zhang, Jinglin","Li, Yuxia","Tong, Zhonggui",GLCANet: Global-Local Context Aggregation Network for Cropland Segmentation from Multi-Source Remote Sensing Images,REMOTE SENSING,DEC 2024,0,"Cropland is a fundamental basis for agricultural development and a prerequisite for ensuring food security. The segmentation and extraction of croplands using remote sensing images are important measures and prerequisites for detecting and protecting farmland. This study addresses the challenges of diverse image sources, multi-scale representations of cropland, and the confusion of features between croplands and other land types in large-area remote sensing image information extraction. To this end, a multi-source self-annotated dataset was developed using satellite images from GaoFen-2, GaoFen-7, and WorldView, which was integrated with public datasets GID and LoveDA to create the CRMS dataset. A novel semantic segmentation network, the Global-Local Context Aggregation Network (GLCANet), was proposed. This method integrates the Bilateral Feature Encoder (BFE) of CNNs and Transformers with a global-local information mining module (GLM) to enhance global context extraction and improve cropland separability. It also employs a multi-scale progressive upsampling structure (MPUS) to refine the accuracy of diverse arable land representations from multi-source imagery. To tackle the issue of inconsistent features within the cropland class, a loss function based on hard sample mining and multi-scale features was constructed. The experimental results demonstrate that GLCANet improves OA and mIoU by 3.2% and 2.6%, respectively, compared to the existing advanced networks on the CRMS dataset. Additionally, the proposed method also demonstrated high precision and practicality in segmenting large-area croplands in Chongzhou City, Sichuan Province, China.",semantic segmentation,remote sensing,multi-source image,cropland,"He, Lei","Zhang, Mingheng","Niu, Zhenye","He, Haiping",deep learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_552,Zhang Yinhui,Zhang Feng,He Zifen,Remote Sensing Image Segmentation Based on Attention Guidance and Multi-Feature Fusion,ACTA OPTICA SINICA,DEC 2023,2,"Objective Remote sensing images have a large detection range, long dynamic monitoring time, and a large amount of carrying information, making the obtained ground feature information more comprehensive and rich. By extracting ground object targets from remote sensing images, more detailed and accurate ground object information in the imaging area can be obtained, providing data support for high-altitude reconnaissance, precision guidance, and terrain matching. However, with the rapid increase in data volume, the current low level of intelligent and automated target extraction methods is difficult to embrace the demand. Traditional image extraction techniques contain edge detection, threshold segmentation, and region segmentation. These methods have good segmentation performance for remote sensing targets with significant contour boundaries but lack the ability of adaptive adjustment while facing complex and ever-changing remote sensing targets. Convolutional neural networks have stronger representation ability, scalability, and robustness than traditional methods by providing multi-level semantic information in images. Due to the uneven distribution, blurred edges, and variable scales of ground objects in remote sensing images, convolutional neural networks are prone to losing edge information and multi-scale feature information during feature extraction. In addition, cloud cover of remote sensing targets in complex scenes exacerbates the loss of target edge and multi-scale information, making it more difficult for convolutional neural networks to accurately segment remote sensing ground objects. In order to solve the above problems, we propose a segmentation method that uses deep residual networks as the backbone and combines attention guidance and multi-feature fusion to enhance the network's ability to segment remote sensing image ground object edges and multi-scale objects.Methods We propose a remote sensing image semantic segmentation network called AMSNet, which combines attention guidance and multi-feature fusion. In the Encoder Section, D_ Resnet50 is applied as the backbone network to extract the main feature information from remote sensing images, which can enhance the acquisition of detailed information such as edge and small-scale targets in remote sensing images. The category guidance channel attention module is inserted into the backbone to enhance the network's segmentation ability for difficult-to-distinguish and irregularly shaped areas in remote sensing images. A feature reuse module is added to the backbone network to solve the loss of edge detail information and the disappearance of scattered small-scale targets during feature extraction. In the Decoder Section, the cross-regional feature fusion module is applied to fuse the multi-feature information, improving the acquisition of multi-scale target information. Multi-scale loss fusion module is also joined to further enhance the segmentation performance of the network for multi-scale targets.Results and Discussions From the analysis of experimental results on the remote sensing image dataset of the plateau region and the remote sensing image dataset of the plateau region under cloud interference, compared with other semantic segmentation networks, the proposed network has better segmentation performance (Table 6 and Table 7) regardless of cloud interference. In addition, the segmentation performance is less affected by cloud interference. Even under cloud interference, the segmentation accuracy of ground targets is only 1. 10 percentage points lower than that without cloud interference in mIoU, 0. 58 percentage points lower than that in mPa, and 0. 71 percentage points lower than that in mF1, which is lower than the influence of other semantic segmentation networks on segmentation effect under different cloud meteorological interference conditions. In addition, in order to verify the generalization performance of the AMSNet network segmentation effect, the International Society for Photogrammetry and Remote Sensing (ISPRS) dataset in the Vaihingen region of Germany is selected. In order to better fit the picture size, number of grouping convolutions of feature multiplexing modules in the AMSNet network is reduced to four groups. From the experimental results in Table 8, the network still performs better than other networks. This network is compared with PspNet and OCNet, with mIoU increased by 5. 09 percentage points and 5. 57 percentage points, Deeplabv3+ network with mIoU by 3. 47 percentage points, mPa by 3. 56 percentage points, and mF1 by 2. 78 percentage points. From the segmenting effect diagram of Fig. 8, this network has a lower error rate, fewer omission, and a more accurate segmenting boundary for building edges and small-scale cars than other networks.Conclusions We propose a network model based on encoding-decoding structure-AMSNet. In the encoding part, the D_Resnet50 network is applied as the backbone to extract the main feature information of remote sensing images. We also use a category-guided channel attention module to reduce the interference of channel noise on segmented objects and improve the segmentation effect of targets in difficult-to-distinguish areas. We embed a feature reuse module to compensate for the problem of target edge loss and small-scale target loss during the feature extraction process. In the decoding part, the cross-regional feature fusion module is designed to integrate multi-layer features and combine the multiscale loss fusion module to calculate the feature loss at different scales to improve the segmentation effect of the network on multi-scale targets. This network conducts experiments on the remote sensing image dataset of the plateau region, remote sensing image dataset of the plateau region under cloud interference, and a public dataset. Compared with semantic segmentation networks such as BiseNetv2, PspNet, and Deeplabv3+, the proposed network achieves better results in the evaluation indicators of mIoU, mPa, and mF1. The visualization results show that the proposed network can effectively segment the ground object targets and scattered multi-scale targets in the interlaced and hard-to-distinguish areas in the remote sensing images, and it has good segmentation performance and good robustness in cloud interference.",remote sensing image,semantic segmentation,attention mechanism,multi-scale feature,Yang Xiaogang,Lu Ruitao,Chen Guangchen,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_553,"Jin, Jidong","Lu, Wanxuan","Yu, Hongfeng",Dynamic and Adaptive Self-Training for Semi-Supervised Remote Sensing Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Remote sensing (RS) technology has made remarkable progress, providing a wealth of data for various applications, such as ecological conservation and urban planning. However, the meticulous annotation of this data is labor-intensive, leading to a shortage of labeled data, particularly in tasks like semantic segmentation. Semi-supervised methods, combining consistency regularization (CR) with self-training, offer a solution to efficiently utilize labeled and unlabeled data. However, these methods encounter challenges due to imbalanced data ratios. To tackle these challenges, we introduce a self-training approach named dynamic and adaptive self-training (DAST), which is combined with dynamic pseudo-label sampling (DPS), distribution matching (DM), and adaptive threshold updating (ATU). DPS is tailored to address the issue of class distribution imbalance by giving priority to classes with fewer samples. Meanwhile, DM and ATU aim to reduce distribution disparities by adjusting model predictions across augmented images within the framework of CR, ensuring they align with the actual data distribution. Experimental results on the Potsdam and iSAID datasets demonstrate that DAST effectively balances class distribution, aligns model predictions with data distribution, and stabilizes pseudo-labels, leading to state-of-the-art performance on both datasets. These findings highlight the potential of DAST in overcoming the challenges associated with significant disparities in labeled-to-unlabeled data ratios.",Remote sensing,Semantic segmentation,Transformers,Data models,"Rong, Xuee","Sun, Xian","Wu, Yirong",,Training,Semantics,Predictive models,Consistency regularization (CR),remote sensing (RS) image,self-training,semantic segmentation,semisupervised learning (SSL),,,,,,,,,,,,,,,,,
Row_554,"Li, Weijia","Zhao, Wenqian","Yu, Jinhua",Joint semantic-geometric learning for polygonal building segmentation from-resolution remote,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,JUL 2023,18,"As a fundamental task for geographical information updating, 3D city modeling, and other critical applications, the automatic extraction of building footprints from high-resolution remote sensing images has been substan-tially explored and received increasing attention over recent years. Among different types of building extraction methods, the polygonal segmentation methods produce vector building polygons that are in a more realistic format compared with those obtained from pixel-wise semantic labeling and contour-based methods. However, existing polygonal building segmentation methods usually require a perfect segmentation map and a complex post-processing procedure to guarantee the polygonization quality, or produce inaccurate vertex prediction results that suffer from wrong vertex sequence, self-intersections, fixed vertex quantity, etc. In our previous work, we have proposed a method for polygonal building segmentation from remote sensing images that addresses the above limitations of existing methods. In this paper, we propose PolyCity, which further extends and improves our previous work in terms of the application scenario, methodology design, and experimental results. Our proposed PolyCity contains the following three components: (1) a pixel-wise multi-task network for learning the semantic and geometric information via three tasks, i.e., building segmentation, boundary prediction, and edge orientation prediction; (2) a simple but effective vertex selection module (VSM), which effectively bridges the gap between pixel-wise and graph-based models via transforming the segmentation map into valid polygon vertices; (3) a graph-based vertex refinement network (VRN) for automatically adjusting the coordinates of VSM-generated valid polygon vertices, producing the final building polygons with more precise vertices. Results on three large-scale building extraction datasets demonstrate that our proposed PolyCity generates vector building footprints with more accurate vertices, edges, shapes, etc., achieving significant vertex score improvements while maintaining high segmentation and boundary scores compared with the current state-of-the-art. The code of PolyCity will be released at https://github.com/liweijia/polycity.",Building extraction,Semantic segmentation,Graph neural networks,High-resolution remote sensing images,"Zheng, Juepeng","He, Conghui","Fu, Haohuan","Lin, Dahua",,,,,,,,,,,,,,,,,,,,,,,,,
Row_555,"Fu, Xiaomeng","Qu, Huiming",,Research on Semantic Segmentation of High-resolution Remote Sensing Image Based on Full Convolutional Neural Network,,2018,9,"Remote sensing data is an important way to reflect the comprehensive information of surface. In this paper, based on the semantic segmentation of high-resolution remote sensing images, a segmentation method based on full convolutional neural network (FCN) is proposed. The method improves the traditional convolutional neural network (CNN) and replaces the final fully connected layer of the CNN network with a convolutional layer. And then optimize the convolution operation by using the matrix expansion technique. The experimental results show that the FCN network with sufficient training and fine-tuning can effectively perform automatic semantic segmentation of high-resolution remote sensing images. The correct segmentation accuracy is higher than 85%, which improves the efficiency of convolution operations.",semantic image segmentation,full convolutional neural network,deep learning,,,,,,,,,,,,,,,"2018 12TH INTERNATIONAL SYMPOSIUM ON ANTENNAS, PROPAGATION AND ELECTROMAGNETIC THEORY (ISAPE)",,,,,,,,,,,,,,,
Row_556,"Xin, Yi","Fan, Zide","Qi, Xiyu",Confidence-Weighted Dual-Teacher Networks With Biased Contrastive Learning for Semi-Supervised Semantic Segmentation in Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,3,"Semantic segmentation of remote sensing images is vital in remote sensing technology. High-quality models for this task require a vast amount of images, and manual annotation is a process that is time-consuming and labor-intensive. Consequently, this has catalyzed the emergence of semi-supervised semantic segmentation methods. However, the complexity of foreground categories in remote sensing images poses a challenge to maintaining prediction consistency. Moreover, inherent characteristics such as intraclass variations and interclass similarities result in a certain degree of confusion among features of different classes in the feature space. This impacts the final classification results. To improve the model's consistency and optimize the classification of categories based on features, this article proposes a new semi-supervised semantic segmentation framework that combines consistency regularization and contrastive learning (CL). In terms of consistency regularization, the proposed method incorporates dual-teacher networks, introduces ClassMix for image augmentation, and uses confidence levels to integrate the predictions from these networks. By introducing perturbations at both the network and image levels, while simultaneously maintaining consistency, the predictive prowess and generalization ability of the model are enhanced. For CL, positive-unlabeled learning (PU-Learning) is used to improve the problem of mis-sampling when selecting features. At the same time, higher biased weights are allocated to more challenging negative samples, thereby elevating the complexity of feature learning and enhancing the discriminative capability of the final feature representation space. Our extensive experiments on the ISPRS Vaihingen dataset and the challenging iSAID dataset have served to underscore the superior performance of our approach.",Remote sensing,Self-supervised learning,Perturbation methods,Semantic segmentation,"Zhang, Yidan","Li, Xinming",,,Training,Visualization,Sensors,Consistency regularization,contrastive learning (CL),remote sensing,semantic segmentation,semi-supervised learning,,,,,,,,,,,,,,,,,
Row_557,"Zhu, Jingru","Guo, Ya","Sun, Geng",Unsupervised Domain Adaptation Semantic Segmentation of High-Resolution Remote Sensing Imagery With Invariant Domain-Level Prototype Memory,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,31,"Semantic segmentation is a key technique involved in automatic interpretation of high-resolution remote sensing (HRS) imagery and has drawn much attention in the remote sensing community. Deep convolutional neural networks (DCNNs) have been successfully applied to the HRS imagery semantic segmentation task due to their hierarchical representation ability. However, the heavy dependence on a large number of training data with dense annotation and the sensitiveness to the variation of data distribution severely restrict the potential application of DCNNs for the semantic segmentation of HRS imagery. This study proposes a novel unsupervised domain adaptation semantic segmentation network (MemoryAdaptNet) for the semantic segmentation of HRS imagery. MemoryAdaptNet constructs an output space adversarial learning scheme to bridge the domain distribution discrepancy between the source domain and the target domain and to narrow the influence of domain shift. Specifically, we embed an invariant feature memory module to store invariant domain-level prototype information because the features obtained from adversarial learning only tend to represent the variant feature of current limited inputs. This module is integrated by a category attention-driven invariant domain-level memory aggregation module to current pseudo-invariant feature for further augmenting the representations. An entropy-based pseudo label filtering strategy is used to update the memory module with high-confident pseudo-invariant feature of current target images. Extensive experiments under three cross-domain tasks indicate that our proposed MemoryAdaptNet is remarkably superior to the state-of-the-art methods. Our code is available at https://github.com/RS-CSU/MemoryAdaptNet-master.",Semantic segmentation,Semantics,Remote sensing,Adversarial machine learning,"Yang, Libo","Deng, Min","Chen, Jie",,Prototypes,Memory modules,Adaptation models,Category attention,high-resolution remote sensing (HRS) imagery,invariant domain-level context,memory module,pseudo label filtering strategy,,,,,,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,
Row_558,"Zhang, Mei","Liu, Lingling","Pei, Yongtao",Semantic segmentation of multi-scale remote sensing images with contextual feature enhancement,VISUAL COMPUTER,AUG 2024,0,"Remote sensing images exhibit complex characteristics such as irregular multi-scale feature shapes, significant scale variations, and imbalanced sizes between different categories. These characteristics lead to a decrease in the accuracy of semantic segmentation in remote sensing images. In view of this problem, this paper presents a context feature-enhanced multi-scale remote sensing image semantic segmentation method. It utilizes a context aggregation module for global context co-aggregation, obtaining feature representations at different levels through self-similarity calculation and convolution operations. The processed features are input into a feature enhancement module, introducing a channel gate mechanism to enhance the expressive power of feature maps. This mechanism enhances feature representations by leveraging channel correlations and weighted fusion operations. Additionally, pyramid pooling is employed to capture multi-scale information from the enhanced features, so as to improve the performance and accuracy of the semantic segmentation model. Experimental results on the Vaihingen and Potsdam datasets (which are indeed publicly released at the URL: https://www.isprs.org/education/benchmarks/UrbanSemLab/Default.aspx) demonstrate significant improvements in the performance and accuracy of the proposed method (whose algorithm source code is indeed publicly released in Sect. 3.4), compared to previous multi-scale remote sensing image semantic segmentation approaches, verifying its effectiveness.",Context aggregation,Self-similarity calculation,Feature enhancement,Channel gate mechanism,"Xie, Guojing","Wen, Jinghua",,,Multi-scale remote sensing image,,,,,,,,,,,,,,,,,,,,,,,,
Row_559,"Jiang, Liangcun","Li, Feng","Huang, Li",TTNet: A Temporal-Transform Network for Semantic Change Detection Based on Bi-Temporal Remote Sensing Images,REMOTE SENSING,SEP 2023,4,"Semantic change detection (SCD) holds a critical place in remote sensing image interpretation, as it aims to locate changing regions and identify their associated land cover classes. Presently, post-classification techniques stand as the predominant strategy for SCD due to their simplicity and efficacy. However, these methods often overlook the intricate relationships between alterations in land cover. In this paper, we argue that comprehending the interplay of changes within land cover maps holds the key to enhancing SCD's performance. With this insight, a Temporal-Transform Module (TTM) is designed to capture change relationships across temporal dimensions. TTM selectively aggregates features across all temporal images, enhancing the unique features of each temporal image at distinct pixels. Moreover, we build a Temporal-Transform Network (TTNet) for SCD, comprising two semantic segmentation branches and a binary change detection branch. TTM is embedded into the decoder of each semantic segmentation branch, thus enabling TTNet to obtain better land cover classification results. Experimental results on the SECOND dataset show that TTNet achieves enhanced performance when compared to other benchmark methods in the SCD task. In particular, TTNet elevates mIoU accuracy by a minimum of 1.5% in the SCD task and 3.1% in the semantic segmentation task.",semantic change detection,change relationship,siamese convolutional neural network,deep learning,"Peng, Feifei","Hu, Lei",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_560,"He, Qibin","Sun, Xian","Diao, Wenhui",Multimodal Remote Sensing Image Segmentation With Intuition-Inspired Hypergraph Modeling,IEEE TRANSACTIONS ON IMAGE PROCESSING,2023,34,"Multimodal remote sensing (RS) image segmentation aims to comprehensively utilize multiple RS modalities to assign pixel-level semantics to the studied scenes, which can provide a new perspective for global city understanding. Multimodal segmentation inevitably encounters the challenge of modeling intra-and inter-modal relationships, i.e., object diversity and modal gaps. However, the previous methods are usually designed for a single RS modality, limited by the noisy collection environment and poor discrimination information. Neuropsychology and neuroanatomy confirm that the human brain performs the guiding perception and integrative cognition of multimodal semantics through intuitive reasoning. Therefore, establishing a semantic understanding framework inspired by intuition to realize multimodal RS segmentation becomes the main motivation of this work. Drived by the superiority of hypergraphs in modeling high-order relationships, we propose an intuition-inspired hypergraph network ((IH)-H-2 N) for multimodal RS segmentation. Specifically, we present a hypergraph parser to imitate guiding perception to learn intra-modal object-wise relationships. It parses the input modality into irregular hyper graphs to mine semantic clues and generate robust mono modal representations. In addition, we also design a hypergraph matcher to dynamically update the hypergraph structure from the explicit correspondence of visual concepts, similar to integrative cognition, to improve cross-modal compatibility when fusing multimodal features. Extensive experiments on two multimodal RS datasets show that the proposed I2H N outperforms the stateof-the-art models, achieving F-1/mIoU accuracy 91.4%/82.9% on the ISPRS Vaihingen dataset, and 92.1%/84.2% on the MSAW dataset.",Cognition,Semantics,Image segmentation,Remote sensing,"Yan, Zhiyuan","Yao, Fanglong","Fu, Kun",,Optical interferometry,Vegetation,Optical sensors,Multimodal remote sensing,intuitive reasoning,hypergraph learning,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_561,"Wu, Shulei","Zhao, Yuchen","Wang, Yaoru",Convolution Feature Inference-Based Semantic Understanding Method for Remote Sensing Images of Mangrove Forests,ELECTRONICS,FEB 2023,2,"The semantic segmentation and understanding of remote sensing images applying computer technology has become an important component of monitoring mangrove forests' ecological changes due to the rapid advancement of remote sensing technology. To improve the semantic segmentation capability of various surface features, this paper proposes a semantic understanding method for mangrove remote sensing images based on convolution feature inference. Firstly, the sample data is randomly selected, and next a model of convolution feature extraction is used to obtain the features of the selected sample data and build an initial feature set. Then, the convolution feature space and rule base are generated by establishing the three-dimensional color space distribution map for each class and domain similarity is introduced to construct the feature set and rules for reasoning. Next, a confidence reasoning method based on the convolution feature region growth, which introduces an improved similarity calculation, is put forward to obtain the first-time reasoning results. Finally, this approach adds a correction module, which removes the boundary information and reduces the noise from the results of the first-time reasoning as a new sample to correct the original feature set and rules, and uses the corrected feature set and rules for reasoning and understanding to obtain the final image segmentation results. It uses the corrected feature set and rules for reasoning and understanding to obtain the final image segmentation results. Experiments show that this algorithm has the benefits of a simple process, a short training time, and easy feature acquisition. The effect has been obviously improved compared to a single threshold segmentation method, K-Nearest Neighbor (KNN), Support Vector Machine (SVM), and other image segmentation methods.",convolution features,semantic understanding,remote sensing image segmentation,semantic inference,"Chen, Jinbiao","Zang, Tao","Chen, Huandong",,feature rule base,,,,,,,,,,,,,,,,,,,,,,,,
Row_562,"Du, Shouji","Du, Shihong","Liu, Bo",Incorporating DeepLabv3+and object-based image analysis for semantic segmentation of very high resolution remote sensing images,INTERNATIONAL JOURNAL OF DIGITAL EARTH,MAR 4 2021,79,"Semantic segmentation of remote sensing images is an important but unsolved problem in the remote sensing society. Advanced image semantic segmentation models, such as DeepLabv3+, have achieved astonishing performance for semantically labeling very high resolution (VHR) remote sensing images. However, it is difficult for these models to capture the precise outlines of ground objects and explore the context information that revealing relationships among image objects for optimizing segmentation results. Consequently, this study proposes a semantic segmentation method for VHR images by incorporating deep learning semantic segmentation model (DeepLabv3+) and object-based image analysis (OBIA), wherein DSM is employed to provide geometric information to enhance the interpretation of VHR images. The proposed method first obtains two initial probabilistic labeling predictions using a DeepLabv3+ network on spectral image and a random forest (RF) classifier on hand-crafted features, respectively. These two predictions are then integrated by Dempster-Shafer (D-S) evidence theory to be fed into an object-constrained higher-order conditional random field (CRF) framework to estimate the final semantic labeling results with the consideration of the spatial contextual information. The proposed method is applied to the ISPRS 2D semantic labeling benchmark, and competitive overall accuracies of 90.6% and 85.0% are achieved for Vaihingen and Potsdam datasets, respectively.",Semantic segmentation,DeepLabv3+,object-based image analysis,Dempster-Shafer evidence theory,"Zhang, Xiuyuan",,,,conditional random field,VHR images,,,,,,,,,,,,,,,,,,,,,,,
Row_563,"Liu, Yansong","Piramanayagam, Sankaranarayanan","Monteiro, Sildomar T.",SEMANTIC SEGMENTATION OF REMOTE SENSING DATA USING GAUSSIAN PROCESSES AND HIGHER-ORDER CRFS,,2017,2,"Automatic recognition for complex scenes from aerial images and other sensor data (e.g. LiDAR) has become an active topic in the remote sensing community. In this paper, we proposed a novel framework that utilizes higher-order CRFs (HCRFs) to capture the spatial contextual information for the RGB aerial images along with their co-registered LiDAR data (DSMs). Our proposed HCRFs framework exploits the spatial contextual information in two levels. The first level encourages harmonic label co-existence within one segment, which can be generated by an unsupervised superpixel algorithm. The second level takes into account the local object co-occurrence among adjacent segments. We then show that how to apply the move making graph cuts algorithm to perform efficient inference for our proposed CRFs framework. Based on the experiments on a challenging data set, our proposed higher-order CRF framework generated state-of-the-art semantic segmentation results.",Semantic segmentation,higher-order CRFs,aerial images,graph cuts,"Saber, Eli",,,,,,,,,,,,,2017 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),,,,,,,,,,,,,,,
Row_564,"Marsocci, Valerio","Scardapane, Simone",,Continual Barlow Twins: Continual Self-Supervised Learning for Remote Sensing Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,10,"In the field of earth observation (EO), continual learning (CL) algorithms have been proposed to deal with large datasets by decomposing them into several subsets and processing them incrementally. The majority of these algorithms assume that data are, first, coming from a single source, and second, fully labeled. Real-world EO datasets are instead characterized by a large heterogeneity (e.g., coming from aerial, satellite, or drone scenarios), and for the most part they are unlabeled, meaning they can be fully exploited only through the emerging self-supervised learning (SSL) paradigm. For these reasons, in this article, we present a new algorithm for merging SSL and CL for remote sensing applications that we call continual Barlow twins. It combines the advantages of one of the simplest self-supervision techniques, i.e., Barlow twins, with the elastic weight consolidation method to avoid catastrophic forgetting. In addition, we evaluate the proposed continual SSL approach on a highly heterogeneous EO dataset, showing the effectiveness of this strategy on a novel combination of three almost non-overlapping domains datasets (airborne Potsdam, satellite US3D, and drone unmanned aerial vehicle semantic segmentation dataset), on a crucial downstream task in EO, i.e., semantic segmentation. Encouraging results show the superiority of SSL in this setting, and the effectiveness of creating an incremental effective pretrained feature extractor, based on ResNet50, without the need of relying on the complete availability of all the data, with a valuable saving of time and resources.",Task analysis,Semantic segmentation,Satellites,Feature extraction,,,,,Remote sensing,Electo-optic effects,Drones,Continual learning (CL),remote sensing,self-supervised learning (SSL),semantic segmentation,,,,,,,,,,,,,,,,,,
Row_565,"Datla, Eshreddy","Vishnu, Chalavadi","Mohan, C. Krishna",A Multimodal Semantic Segmentation for Airport Runway Delineation in Panchromatic Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,3,"Monitoring airport runways in panchromatic remote sensing images is helpful for both civil and strategic communities in effective utilization of the large-area acquisitions. This paper proposes a novel multimodal semantic segmentation approach for effective delineation of the runways in panchromatic remote sensing images. The proposed approach aims to learn complementary information from two modalities, namely, panchromatic image and digital elevation model (DEM) to obtain discriminative features of the runway. The fusion of image features and the corresponding terrain information is performed by stacking the image and DEM by leveraging the merits of both Transformers and U-Net architecture. We perform the experiments on Cartosat-1 panchromatic satellite images with the corresponding Cartosat-1 DEM scenes. The experimental results demonstrate a significant contribution of terrain information to the segmentation process in achieving the contours of airport runways effectively.",Airport runway,remote sensing images,multimodal segmentation,digital elevation model,,,,,Cartosat-1,,,,,,,,,,,,,,,,,,,,,,,,
Row_566,"Ding, Lei","Lin, Dong","Lin, Shaofu",Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,82,"Long-range contextual information is crucial for the semantic segmentation of high-resolution (HR) remote sensing images (RSIs). However, image cropping operations, commonly used for training neural networks, limit the perception of long-range contexts in large RSIs. To overcome this limitation, we propose a wide-context network (WiCoNet) for the semantic segmentation of HR RSIs. Apart from extracting local features with a conventional convolutional neural network (CNN), the WiCoNet has an extra context branch to aggregate information from a larger image area. Moreover, we introduce a context transformer to embed contextual information from the context branch and selectively project it onto the local features. The context transformer extends the vision transformer, an emerging kind of neural networks, to model the dual-branch semantic correlations. It overcomes the locality limitation of CNNs and enables the WiCoNet to see the bigger picture before segmenting the land-cover/land-use (LCLU) classes. Ablation studies and comparative experiments conducted on several benchmark datasets demonstrate the effectiveness of the proposed method. In addition, we present a new Beijing Land-Use (BLU) dataset. This is a large-scale HR satellite dataset with high-quality and fine-grained reference labels, which can facilitate future studies in this field.",Transformers,Semantics,Image segmentation,Feature extraction,"Zhang, Jing","Cui, Xiaojie","Wang, Yuebin","Tang, Hao",Task analysis,Convolutional neural networks,Context modeling,Convolutional neural network,remote sensing,semantic segmentation,vision transformer (ViT),,"Bruzzone, Lorenzo",,,,,,,,,,,,,,,,
Row_567,"Wang, Xiaofeng","Kang, Menglei","Chen, Yan",Adaptive Local Cross-Channel Vector Pooling Attention Module for Semantic Segmentation of Remote Sensing Imagery,REMOTE SENSING,APR 2023,6,"Adding an attention module to the deep convolution semantic segmentation network has significantly enhanced the network performance. However, the existing channel attention module focusing on the channel dimension neglects the spatial relationship, causing location noise to transmit to the decoder. In addition, the spatial attention module exemplified by self-attention has a high training cost and challenges in execution efficiency, making it unsuitable to handle large-scale remote sensing data. We propose an efficient vector pooling attention (VPA) module for building the channel and spatial location relationship. The module can locate spatial information better by performing a unique vector average pooling in the vertical and horizontal dimensions of the feature maps. Furthermore, it can also learn the weights directly by using the adaptive local cross-channel interaction. Multiple weight learning ablation studies and comparison experiments with the classical attention modules were conducted by connecting the VPA module to a modified DeepLabV3 network using ResNet50 as the encoder. The results show that the mIoU of our network with the addition of an adaptive local cross-channel interaction VPA module increases by 3% compared to the standard network on the MO-CSSSD. The VPA-based semantic segmentation network can significantly improve precision efficiency compared with other conventional attention networks. Furthermore, the results on the WHU Building dataset present an improvement in IoU and F1-score by 1.69% and 0.97%, respectively. Our network raises the mIoU by 1.24% on the ISPRS Vaihingen dataset. The VPA module can also significantly improve the network's performance on small target segmentation.",adaptive local cross-channel interaction,vector average pooling,attention mechanism,remote sensing imagery,"Jiang, Wenxiang","Wang, Mengyuan","Weise, Thomas","Tan, Ming",semantic segmentation,deep learning,,,,,,,"Xu, Lixiang",,"Li, Xinlu","Zou, Le","Zhang, Chen",,,,,,,,,,,,
Row_568,"Lu, Hui","Liu, Qi","Liu, Xiaodong",A Survey of Semantic Construction and Application of Satellite Remote Sensing Images and Data,JOURNAL OF ORGANIZATIONAL AND END USER COMPUTING,NOV-DEC 2021,25,"With the rapid development of satellite technology, remote sensing data has entered the era of big data, and the intelligent processing of remote sensing images has been paid more attention. Through the semantic research of remote sensing data, the processing ability of remote sensing data is greatly improved. This paper aims to introduce and analyze the research and application progress of remote sensing image satellite data processing from the perspective of semantics. Firstly, it introduces the characteristics and semantic knowledge of remote sensing big data. Secondly, the semantic concept, semantic construction, and application fields are introduced in detail. Then, for remote sensing big data, the technical progress in the study field of semantic construction is analyzed from four aspects-semantic description and understanding, semantic segmentation, semantic classification, and semantic search-focusing on deep learning technology. Finally, the problems and challenges in the four aspects are discussed in detail in order to find more directions to explore.",Automatic Analysis,Deep Learning,Remote Sensing,Satellite Remote Sensing,"Zhang, Yonghong",,,,Semantic Construction,Semantic Knowledge,,,,,,,,,,,,,,,,,,,,,,,
Row_569,"Zhu, Jingru","Guo, Ya","Sun, Geng",Causal Prototype-Inspired Contrast Adaptation for Unsupervised Domain Adaptive Semantic Segmentation of High-Resolution Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Semantic segmentation of high-resolution remote sensing imagery (HRSI) suffers from the domain shift, resulting in poor performance of the model in another unseen domain. Unsupervised domain adaptive (UDA) semantic segmentation aims to adapt the semantic segmentation model trained on the labeled source domain to an unlabeled target domain. However, the existing UDA semantic segmentation models tend to align pixels or features based on statistical information related to labels in source and target domain data and make predictions accordingly, which leads to uncertainty and fragility of prediction results. In this article, we propose a causal prototype-inspired contrast adaptation (CPCA) method to explore the invariant causal mechanisms between different HRSIs domains and their semantic labels. It first disentangles causal features and bias features from the source and target domain images through a causal feature disentanglement (CFD) module. Then, a causal prototypical contrast (CPC) module is used to learn domain invariant causal features. To further de-correlate causal and bias features, a causal intervention (CI) module is introduced to intervene on the bias features to generate counterfactual unbiased samples. By forcing the causal features to meet the principles of separability, invariance, and intervention, CPCA can simulate the causal factors of source and target domains, and make decisions on the target domain based on the causal features, which can observe improved generalization ability. Extensive experiments under three cross-domain tasks indicate that CPCA is remarkably superior to the state-of-the-art methods.",Causal view,contrastive learning,counterfactuals,disentangled representation,"Hong, Liang","Chen, Jie",,,high-resolution remote sensing imagery (HRSI),semantic segmentation,unsupervised domain adaptation,Causal view,contrastive learning,counterfactuals,disentangled representation,high-resolution remote sensing imagery (HRSI),,,,,,semantic segmentation,unsupervised domain adaptation,,,,,,,,,,
Row_570,Sun Guowen,Luo Xiaobo,Zhang Kunqiang,DeepLabV3_DHC: Semantic Segmentation of Urban Unmanned Aerial Vehicle Remote Sensing Image,LASER & OPTOELECTRONICS PROGRESS,FEB 2024,0,"High-resolution unmanned aerial vehicle remote sensing images have extremely rich semantic and ground feature features, which are prone to problems such as incomplete target segmentation, missing edge information, and insufficient segmentation accuracy in semantic segmentation. To solve the above problems, based on DeepLabV3_plus model, an improved DeepLabV3_DHC is proposed. First of all, multiple backbone networks are used for down-sampling to collect low-level and high-level features of the image. Second, the atrous spatial pyramid pooling (ASPP) of the original model is replaced by a depthwise separable hybrid dilated convolution, and an adaptive coefficient is added to weaken the mesh effect. After that, the traditional sampling bilinear interpolation method is abandoned and replaced by the learnable dense upsampling convolution. Finally, cascade attention mechanism in low-level features. In this paper, a variety of backbone networks are selected for the experiment, and some images of Longchang City, Sichuan Province are selected for the dataset. The evaluation index uses the average intersection and combination ratio and the average pixel accuracy of the category as the reference basis. The experimental results show that the method in this paper not only has higher segmentation accuracy, but also reduces the amount of computation and parameters.",urban unmanned aerial vehicle remote sensing image,semantic segmentation,depthwise separable hybrid dilated convolution,dense upsampling convolution,,,,,attention mechanism,grid effect,,,,,,,,,,,,,,,,,,,,,,,
Row_571,"Chen, Yuxia","Fang, Pengcheng","Zhong, Xiaoling",Hi-ResNet: Edge Detail Enhancement for High-Resolution Remote Sensing Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"High-resolution remote sensing (HRS) semantic segmentation extracts key objects from high-resolution coverage areas. However, objects of the same category within HRS images generally show significant differences in scale and shape across diverse geographical environments, making it difficult to fit the data distribution. In addition, a complex background environment causes similar appearances of objects of different categories, which precipitates a substantial number of objects into misclassification as background. These issues make existing learning algorithms suboptimal. In this work, we solve the abovementioned problems by proposing a high-resolution remote sensing network (Hi-ResNet) with efficient network structure designs, which consists of a funnel module, a multibranch module with stacks of information aggregation (IA) blocks, and a feature refinement module, sequentially, and class-agnostic edge-aware (CEA) loss. Specifically, we propose a funnel module to downsample, which reduces the computational cost and extracts high-resolution semantic information from the initial input image. Then, we downsample the processed feature images into multiresolution branches incrementally to capture image features at different scales. Furthermore, with the design of the window multihead self-attention, squeeze-and-excitation attention, and depthwise convolution, the light-efficient IA blocks are utilized to distinguish image features of the same class with variant scales and shapes. Finally, our feature refinement module integrates the CEA loss function, which disambiguates interclass objects with similar shapes and increases the data distribution distance for correct predictions. With effective pretraining strategies, we demonstrate the superiority of Hi-ResNet over the existing prevalent methods on three HRS segmentation benchmarks.",Feature extraction,Semantics,Convolution,Transformers,"Yu, Jianhui","Zhang, Xiaoming","Li, Tianrui",,Shape,Remote sensing,Task analysis,Attention,pretraining,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_572,"Wang, Zhaoqiu","Sun, Tao","Hu, Kun",A Deep Learning Semantic Segmentation Method for Landslide Scene Based on Transformer Architecture,SUSTAINABILITY,DEC 2022,9,"Semantic segmentation technology based on deep learning has developed rapidly. It is widely used in remote sensing image recognition, but is rarely used in natural disaster scenes, especially in landslide disasters. After a landslide disaster occurs, it is necessary to quickly carry out rescue and ecological restoration work, using satellite data or aerial photography data to quickly analyze the landslide area. However, the precise location and area estimation of the landslide area is still a difficult problem. Therefore, we propose a deep learning semantic segmentation method based on Encoder-Decoder architecture for landslide recognition, called the Separable Channel Attention Network (SCANet). The SCANet consists of a Poolformer encoder and a Separable Channel Attention Feature Pyramid Network (SCA-FPN) decoder. Firstly, the Poolformer can extract global semantic information at different levels with the help of transformer architecture, and it greatly reduces computational complexity of the network by using pooling operations instead of a self-attention mechanism. Secondly, the SCA-FPN we designed can fuse multi-scale semantic information and complete pixel-level prediction of remote sensing images. Without bells and whistles, our proposed SCANet outperformed the mainstream semantic segmentation networks with fewer model parameters on our self-built landslide dataset. The mIoU scores of SCANet are 1.95% higher than ResNet50-Unet, especially.",landslide,remote sensing images,semantic segmentation,Poolformer,"Zhang, Yueting","Yu, Xiaqiong","Li, Ying",,separable channel attention,,,,,,,,,,,,,,,,,,,,,,,,
Row_573,"Li, Rui","Duan, Chenxi","Zheng, Shunyi",MACU-Net for Semantic Segmentation of Fine-Resolution Remotely Sensed Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,71,"Semantic segmentation of remotely sensed images plays an important role in land resource management, yield estimation, and economic assessment. U-Net, a deep encoder-decoder architecture, has been used frequently for image segmentation with high accuracy. In this letter, we incorporate multiscale features generated by different layers of U-Net and design a multiscale skip connected and asymmetric-convolution-based U-Net (MACU-Net), for segmentation using fine-resolution remotely sensed images. Our design has the following advantages: (1) the multiscale skip connections combine and realign semantic features contained in both low-level and high-level feature maps; (2) the asymmetric convolution block strengthens the feature representation and feature extraction capability of a standard convolution layer. Experiments conducted on two remotely sensed data sets captured by different satellite sensors demonstrate that the proposed MACU-Net transcends the U-Net, U-Netpyramid pooling layers (PPL), U-Net 3+, among other benchmark approaches. Code is available at https://github.com/lironui/MACU-Net.",Convolution,Semantics,Remote sensing,Image segmentation,"Zhang, Ce","Atkinson, Peter M.",,,Kernel,Feature extraction,Decoding,Asymmetric convolution block (ACB),fine-resolution remotely sensed images,semantic segmentation,,,,,,,,,,,,,,,,,,,
Row_574,"Li, Xinghua","Xie, Linglin","Wang, Caifeng",Boundary-enhanced dual-stream network for semantic segmentation of high-resolution remote sensing images,GISCIENCE & REMOTE SENSING,DEC 31 2024,16,"Deep convolutional neural networks (DCNNs) have been successfully used in semantic segmentation of high-resolution remote sensing images (HRSIs). However, this task still suffers from intra-class inconsistency and boundary blur due to high intra-class heterogeneity and inter-class homogeneity, considerable scale variance, and spatial information loss in conventional DCNN-based methods. Therefore, a novel boundary-enhanced dual-stream network (BEDSN) is proposed, in which an edge detection branch stream (EDBS) with a composite loss function is introduced to compensate for boundary loss in semantic segmentation branch stream (SSBS). EDBS and SSBS are integrated by highly coupled encoder and feature extractor. A lightweight multilevel information fusion module guided by channel attention mechanism is designed to reuse intermediate boundary information effectively. For aggregating multiscale contextual information, SSBS is enhanced by multiscale feature extraction module and hybrid atrous convolution module. Extensive experiments have been tested on ISPRS Vaihingen and Potsdam datasets. Results show that BEDSN can achieve significant improvements in intra-class consistency and boundary refinement. Compared with 11 state-of-the-art methods, BEDSN exhibits higher-level performance in both quantitative and visual assessments with low model complexity.",Semantic segmentation,boundary blur,intra-class inconsistency,HRSIs,"Miao, Jianhao","Shen, Huanfeng","Zhang, Liangpei",,CNN,,,,,,,,,,,,,,,,,,,,,,,,
Row_575,"Hang, Renlong","Yang, Ping","Zhou, Feng",Multiscale Progressive Segmentation Network for High-Resolution Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,58,"Semantic segmentation of high-resolution remote sensing imageries (HRSIs) is a critical task for a wide range of applications, such as precision agriculture and urban planning. Although convolutional neural networks (CNNs) have made great progress in accomplishing this task recently, there still exist some challenges to address, one of which is simultaneously segmenting objects with large-scale variations in a HRSI. Targeting at this challenge, previous CNNs often adopt multiple convolution kernels in one layer or skip-layer connections between different layers to extract multiscale representations. However, due to the limited learning capacity of each CNN, it tends to make tradeoffs in segmenting different-scale objects. This would lead to unsatisfactory segmentation results for some objects, especially the small or the large ones. In this article, we propose a multiscale progressive segmentation network to address this issue. Instead of forcing one network to deal with all scales of objects, our network attempts to cascade three subnetworks for gradually segmenting objects into small scale, large scale, and other scale. In order to make the subnetwork focus on the specific scale objects, a scale guidance module is designed. It takes advantage of segmentation results from the preceding subnetwork to guide the feature learning of the succeeding one. Additionally, to acquire the final segmentation results, we propose a position-sensitive module for adaptively combining the outputs of the three subnetworks. This module is capable of assigning combination weights of different subnetworks according to their importance. Experiments on two benchmark datasets named Vaihingen and Potsdam indicate that our proposed network can achieve considerable improvements in comparison with several state-of-the-art segmentation models.",Feature extraction,Image segmentation,Convolution,Kernel,"Liu, Qingshan",,,,Decoding,Remote sensing,Convolutional neural networks,Convolutional neural networks,high-resolution remote sensing imagery,multiscale objects,position-sensitive modules,scale guidance modules,,,,,,semantic segmentation,,,,,,,,,,,
Row_576,"Jia, Yuyu","Gao, Junyu","Huang, Wei",Holistic Mutual Representation Enhancement for Few-Shot Remote Sensing Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,7,"Few-shot segmentation (FSS) endeavors to utilize a minimal amount of annotated samples (support) to guide the segmentation of unseen objects (query). Previous techniques primarily employ a support-to-query paradigm, neglecting to sufficiently leverage the mutual representation between query and support images, which leaves models suffering from intra-class variations and background interference in remote sensing images. This article proposes a holistic mutual representation enhancement (HMRE) method to bridge these gaps. First, a dual activation (DA) module is devised to establish information symmetry between the two branches and forms the foundation for mutual representation enhancement. Subsequently, the holistic mutual enhancement is jointly constructed by the global semantic (GS) and spatial dense (SD) mutual enhancement modules. In the prediction stage for segmentation, we integrate the enhanced mutual representation into the mutual-fusion decoder to activate the homologous object regions bidirectionally. To expedite the replication of investigation in this task, we further create a corresponding benchmark Flood-3i. The whole dataset is attainable at https://drive.google.com/drive/folders/ 1FMAKf2sszoFKjq0UrUmSLnJDbwQSpfxR. Extensive experiments on two benchmarks iSAID-5i and Flood-3i demonstrate the superiority of our proposed method, which also sets a new state-of-the-art.",Few-shot semantic segmentation,mutual representation enhancement,remote sensing images,,"Yuan, Yuan","Wang, Qi",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_577,"Marsocci, Valerio","Scardapane, Simone","Komodakis, Nikos",MARE: Self-Supervised Multi-Attention REsu-Net for Semantic Segmentation in Remote Sensing,REMOTE SENSING,AUG 2021,16,"Scene understanding of satellite and aerial images is a pivotal task in various remote sensing (RS) practices, such as land cover and urban development monitoring. In recent years, neural networks have become a de-facto standard in many of these applications. However, semantic segmentation still remains a challenging task. With respect to other computer vision (CV) areas, in RS large labeled datasets are not very often available, due to their large cost and to the required manpower. On the other hand, self-supervised learning (SSL) is earning more and more interest in CV, reaching state-of-the-art in several tasks. In spite of this, most SSL models, pretrained on huge datasets like ImageNet, do not perform particularly well on RS data. For this reason, we propose a combination of a SSL algorithm (particularly, Online Bag of Words) and a semantic segmentation algorithm, shaped for aerial images (namely, Multistage Attention ResU-Net), to show new encouraging results (i.e., 81.76% mIoU with ResNet-18 backbone) on the ISPRS Vaihingen dataset.",semantic segmentation,self-supervised learning,linear attention,Vaihingen dataset,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_578,"Ulku, Irem",,,ContexNestedU-Net: Efficient Context-Aware Semantic Segmentation Architecture for Precision Agriculture Applications Based on Multispectral Remote Sensing Imagery,TRAITEMENT DU SIGNAL,OCT 2024,0,"Precision agriculture relies on semantic segmentation models to optimize crop yield and minimize environmental impact. ContexNestedU-Net is proposed to improve the capture of contextual information for efficient utilization of multispectral remote sensing images in precision agriculture applications. For this purpose, it includes a novel redesign of the convolutional blocks in the Nested U-Net model. Through the application of depthwise separable convolution in the convolution blocks, the ContexNestedU-Net efficiently preserves unique spectral information. ASubsequently, dilated convolution is applied to capture rich contextual information. Three image sets are utilized in the experiments, one from the WorldView-3 satellite and the others from aerial vehicles. Extensive experiments demonstrate that the ContexNestedU-Net outperforms other U-Net-based models for various precision agriculture tasks. When using NDVI images, the proposed architecture improves the Jaccard index by 13% for tree objects, 0.9% for crop objects, and 4.5% for wheat yellow-rust objects compared to Nested U-Net. In addition, the ContexNestedU-Net model reduces the number of trainable parameters from 36.63 to 19 compared to Nested UNet, and the computational complexity (GLOPs) decreases from 849.3 to 302.4.",remote sensing,semantic segmentation,precision agriculture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_579,"Wang, Hong","Chen, Xianzhong","Zhang, Tianxiang",CCTNet: Coupled CNN and Transformer Network for Crop Segmentation of Remote Sensing Images,REMOTE SENSING,MAY 2022,64,"Semantic segmentation by using remote sensing images is an efficient method for agricultural crop classification. Recent solutions in crop segmentation are mainly deep-learning-based methods, including two mainstream architectures: Convolutional Neural Networks (CNNs) and Transformer. However, these two architectures are not sufficiently good for the crop segmentation task due to the following three reasons. First, the ultra-high-resolution images need to be cut into small patches before processing, which leads to the incomplete structure of different categories' edges. Second, because of the deficiency of global information, categories inside the crop field may be wrongly classified. Third, to restore complete images, the patches need to be spliced together, causing the edge artifacts and small misclassified objects and holes. Therefore, we proposed a novel architecture named the Coupled CNN and Transformer Network (CCTNet), which combines the local details (e.g., edge and texture) by the CNN and global context by Transformer to cope with the aforementioned problems. In particular, two modules, namely the Light Adaptive Fusion Module (LAFM) and the Coupled Attention Fusion Module (CAFM), are also designed to efficiently fuse these advantages. Meanwhile, three effective methods named Overlapping Sliding Window (OSW), Testing Time Augmentation (TTA), and Post-Processing (PP) are proposed to remove small objects and holes embedded in the inference stage and restore complete images. The experimental results evaluated on the Barley Remote Sensing Dataset present that the CCTNet outperformed the single CNN or Transformer methods, achieving 72.97% mean Intersection over Union (mIoU) scores. As a consequence, it is believed that the proposed CCTNet can be a competitive method for crop segmentation by remote sensing images.",semantic segmentation,agricultural research,remote sensing,deep learning,"Xu, Zhiyong","Li, Jiangyun",,,CNN,Transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_580,"Xi, Zhihao","He, Xiangyu","Meng, Yu",A Multilevel-Guided Curriculum Domain Adaptation Approach to Semantic Segmentation for High-Resolution Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,9,"The semantic segmentation of high-resolution (HR) remote sensing images (RSIs) has been extensively researched in various applications. However, segmentation networks are prone to significant performance degradation on unlabeled data due to domain shift, such as data distribution shifts arising from distinct geographic locations. To address this issue, we propose a multilevel-guided curriculum domain adaptation (MuGCDA) approach for joint samplewise, categorywise, and pixelwise tasks, which facilitates the final fine-grained segmentation task by guiding the target domain to acquire samplewise and categorywise domain-robust properties. Concretely, at the sample level, we formulate a sample spatial relationship consistency guidance (SSCG) loss that guides the target domain to acquire similar sample spatial relationship properties to the source domain. At the category level, we propose a category layout structure consistency guidance (CLCG) module that guides the target domain to acquire consistent layout properties. At the pixel level, we design an adaptive hierarchical pseudolabel weight setting (AHPWS) method with a self-training (ST) paradigm to reduce the effect of label noise while improving the quality of the generated pseudolabels. Furthermore, to improve the stability of the training process, we use a momentum network (MN) as the teacher network to obtain the property knowledge and pseudolabels, and then guide the whole domain transfer process of the segmentation network, which acts as the student network. Extensive comparison and ablation experiments are conducted in several cross-space and cross-spectral scenes, and the results show that our method achieves significant performance improvements in cross-domain scenes for HR RSIs.",Domain adaptation (DA),high-resolution (HR) remote sensing,multilevel-guided curriculum learning,semantic segmentation,"Yue, Anzhi","Chen, Jingbo","Deng, Yupeng","Chen, Jiansheng",,,,,,,,,,,,,,,,,,,,,,,,,
Row_581,"Liu, Tiancheng","Hu, Qingwu","Fan, Wenlei",AMIANet: Asymmetric Multimodal Interactive Augmentation Network for Semantic Segmentation of Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"In recent years, the inherent 2-D characteristics of optical images have led to a plateau in semantic segmentation performance. The complementary nature of light detection and ranging (LiDAR) point clouds and camera images can effectively enhance semantic segmentation capabilities, and thus, research into multimodal joint semantic segmentation is garnering increasing attention. However, the domain gaps between different dimensions present challenges for the fusion of multimodal data. In this article, we introduce a novel asymmetric multimodal interaction augmented network (AMIANet), which directly processes heterogeneous data from images and point clouds. The treatment of the disparities in modal data ensures consistency in the features of both modes. Through the newly developed synergistic multimodal interaction module (SMI Module), AMIANet is capable of combining the complementary characteristics of cross-modal data. This is achieved by interactively fusing and extracting precise and rich structural information from point cloud features to enhance image characteristics. The experimental results on the N3C-California, WHU-RRDSD, and ISPRS Vaihingen datasets demonstrate that AMIANet surpasses benchmark methods and current state-of-the-art (SOTA) approaches. The code will be available at https://github.com/2012153946/AMIANet.",Point cloud compression,Feature extraction,Semantic segmentation,Semantics,"Feng, Haixia","Zheng, Daoyuan",,,Laser radar,Data mining,Deep learning,Light detection and ranging (LiDAR),multimodal fusion,remote sensing imagery,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_582,"Wang, Lunqian","Wang, Xinghua","Liu, Weilin",A unified architecture for super-resolution and segmentation of remote sensing images based on similarity feature fusion,DISPLAYS,SEP 2024,0,"The resolution of the image has an important impact on the accuracy of segmentation. Integrating super- resolution (SR) techniques in the semantic segmentation of remote sensing images contributes to the improvement of precision and accuracy, especially when the images are blurred. In this paper, a novel and efficient SR semantic segmentation network (SRSEN) is designed by taking advantage of the similarity between SR and segmentation tasks in feature processing. SRSEN consists of the multi-scale feature encoder, the SR fusion decoder, and the multi-path feature refinement block, which adaptively establishes the feature associations between segmentation and SR tasks to improve the segmentation accuracy of blurred images. Experiments show that the proposed method achieves higher segmentation accuracy on fuzzy images compared to state-of-the-art models. Specifically, the mIoU of the proposed SRSEN is 3%-6% higher than other state-of-the-art models on low-resolution LoveDa, Vaihingen, and Potsdam datasets.",Deep learning,Semantic segmentation,Super-resolution,Remote sensing image,"Ding, Hao","Xia, Bo","Zhang, Zekai","Zhang, Jinglin",,,,,,,,,"Xu, Sen",,,,,,,,,,,,,,,,
Row_583,"Ismael, Sarmad F.","Aptoula, Erchan","Kayabol, Koray",A JOINT SEMANTIC SEGMENTATION LOSS FUNCTION FOR IMBALANCED DATASETS,,2022,0,"Semantic segmentation is one of the most important applications in remote sensing image analysis. Since remote sensing datasets are often highly imbalanced in terms of class distribution, specialized loss functions such as focal loss are required. In this paper, a loss function that combines weighted focal loss with Jaccard loss has been developed. This loss function has been used to train U-Net and DeepLabV3+ semantic segmentation models on the recently introduced Landcover.ai dataset, which has a high level of class imbalance. It has been observed through our experiments that the combined loss function leads to a performance improvement.",Deep Learning,Weighted focal loss,Jaccard loss,Remote sensing imagery,,,,,Semantic segmentation,,,,,,,,,2022 IEEE MEDITERRANEAN AND MIDDLE-EAST GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (M2GARSS),,,,,,,,,,,,,,,
Row_584,"Pan, Xin","Zhao, Jian","Xu, Jun",Conditional Generative Adversarial Network-Based Training Sample Set Improvement Model for the Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,SEP 2021,17,"To achieve high segmentation quality, deep semantic segmentation neural networks (DSSNNs) need to be trained on diverse direction, location, and neighboring category combinations for each pixel in the input-output image patches. Achieving this goal requires a large sample set. However, in many practical application scenarios, a very large training sample set is too expensive to achieve, or insufficient remote sensing image data are available. These limitations directly affect the quality of the results of DSSNNs. To address the above-mentioned problem, this article proposes a conditional generative adversarial network (CGAN)-based training sample set improvement model (CGAN-TSIM) for the semantic segmentation of high-resolution remote sensing images. In CGAN-TSIM, the generator model of the CGAN can generate a sample image when a ground-truth image is an input as a ""condition."" A condition generation mechanism is designed to create ground-truth images, and these ground-truth conditions are used to drive the CGAN to generate samples containing more diverse object combinations, directions, and locations. These generated images can be added to the original training sample set to improve their spatial information diversity. Rather than simply relying on passively finding samples that contain diverse spatial information, CGAN-TSIM extracts high-level spatial information from the original training images and actively generates new sample images. Experiments show that the samples generated by CGAN-TSIM can improve the quality of the sample set. Compared with other traditional methods, CGAN-TSIM enables better classification accuracy.",Remote sensing,Image segmentation,Training,Semantics,,,,,Generative adversarial networks,Feature extraction,Generators,Condition generation,generative adversarial network (GAN),high-resolution remote sensing classification,sample generation,semantic segmentation,,,,,,,,,,,,,,,,,
Row_585,"Yang, Yang","Dong, Junwu","Wang, Yanhui",DMAU-Net: An Attention-Based Multiscale Max-Pooling Dense Network for the Semantic Segmentation in VHR Remote-Sensing Images,REMOTE SENSING,MAR 2023,4,"High-resolution remote-sensing images cover more feature information, including texture, structure, shape, and other geometric details, while the relationships among target features are more complex. These factors make it more complicated for classical convolutional neural networks to obtain ideal results when performing a feature classification on remote-sensing images. To address this issue, we proposed an attention-based multiscale max-pooling dense network (DMAU-Net), which is based on U-Net for ground object classification. The network is designed with an integrated max-pooling module that incorporates dense connections in the encoder part to enhance the quality of the feature map, and thus improve the feature-extraction capability of the network. Equally, in the decoding, we introduce the Efficient Channel Attention (ECA) module, which can strengthen the effective features and suppress the irrelevant information. To validate the ground object classification performance of the multi-pooling integration network proposed in this paper, we conducted experiments on the Vaihingen and Potsdam datasets provided by the International Society for Photogrammetry and Remote Sensing (ISPRS). We compared DMAU-Net with other mainstream semantic segmentation models. The experimental results show that the DMAU-Net proposed in this paper effectively improves the accuracy of the feature classification of high-resolution remote-sensing images. The feature boundaries obtained by DMAU-Net are clear and regionally complete, enhancing the ability to optimize the edges of features.",high-resolution remote-sensing images,ground object classification,dense connections,multiscale maximum pooling,"Yu, Bibo","Yang, Zhigang",,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_586,"Wang, Yu","Jing, Xin","Xu, Yang",Geometry-guided semantic segmentation for post-earthquake buildings using optical remote sensing images,EARTHQUAKE ENGINEERING & STRUCTURAL DYNAMICS,SEP 2023,11,"Deep-learning-based automatic recognition of post-earthquake damage for urban buildings is increasingly in demand for rapid and precise assessment of seismic hazards from optical remote sensing images. In this study, a novel loss function fusing geometric consistency constraint (GCC) with cross-entropy (CE) loss is designed for post-earthquake building segmentation with complex geometric features across multiple scales. Specifically, the GCC loss incorporates three critical components, namely, split line length, curvature, and area, and enables the exact extraction of the geometric constraints of boundary and region for damaged buildings. Through the optimization of multiple key coefficients of GCC loss, the proposed method achieves significant performance improvements in semantic segmentation, which is attributed to the enhanced ability to identify and capture the pixel relationship near the boundary. Merging GCC in the loss function enables faster and more accurate convergence of predicted values towards the ground truth during the training process, surpassing the performance of the CE loss alone. The results show that the combination of GCC and CE losses achieves the largest validation mIoU of 86.98% for damaged buildings segmentation, which facilitates post-earthquake assessment with high accuracy. Moreover, incorporating GCC leads to more precise and robust seismic damage segmentation by effectively improving edge closure, removing internal noise, and reducing false-positive and false-negative misrecognition. In addition, the GCC term further validates the effectiveness of improving segmentation tasks for other networks (e.g., DeepLabv3+). The GCC-derived method exhibits its desirable performance on segmentation accuracy, portability, and universality for building recognition with complex geometric features and post-earthquake scenes.",complex geometric features,dense post-earthquake buildings,geometric consistency loss,remote sensing images,"Cui, Liangyi","Zhang, Qiangqiang","Li, Hui",,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_587,"Zhao, Jiaqi","Zhou, Yong","Shi, Boyu",Multi-Stage Fusion and Multi-Source Attention Network for Multi-Modal Remote Sensing Image Segmentation,ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY,NOV 2021,16,"With the rapid development of sensor technology, lots of remote sensing data have been collected. It effectively obtains good semantic segmentation performance by extracting feature maps based on multi-modal remote sensing images since extra modal data provides more information. How to make full use of multi-model remote sensing data for semantic segmentation is challenging. Toward this end, we propose a new network called Multi-Stage Fusion and Multi-Source Attention Network ((MS)(2)-Net) for multi-modal remote sensing data segmentation. The multi-stage fusion module fuses complementary information after calibrating the deviation information by filtering the noise from the multi-modal data. Besides, similar feature points are aggregated by the proposed multi-source attention for enhancing the discriminability of features with different modalities. The proposed model is evaluated on publicly available multi-modal remote sensing data sets, and results demonstrate the effectiveness of the proposed method.",Semantic segmentation,multi-modal remote sensing images,attention,feature fusion,"Yang, Jingsong","Zhang, Di","Yao, Rui",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_588,"Wang, Wei","Tang, Chen","Wang, Xin",A ViT-Based Multiscale Feature Fusion Approach for Remote Sensing Image Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,22,"Semantic segmentation plays an indispensable role in automatic analysis of remote sensing image data. However, the abundant semantic information and irregular shape patterns in remote sensing images are difficult to utilize, making it hard to segment remote sensing images only using convolution and single-scale feature maps. To achieve better segmentation performance, a multiscale feature pyramid decoder (MFPD) is proposed to fuse image features extracted by vision transformer (ViT). The decoder employs a novel 2-D-to-3-D transform method to obtain multiscale feature maps that contain rich context information and fuses the multiscale feature maps by channel concatenation. Furthermore, a dimension attention module (DAM) is designed to further aggregate the context information of the extracted remote sensing image features. This approach yields superior mean intersection over union (mIoU) on the Gaofen2-CZ dataset (60.42%) and GID-5 dataset (68.21%). Experimental results indicate that the comprehensive performance of our approach exceeds the compared segmentation methods based on convolutional neural network (CNN) and ViT.",Feature extraction,Image segmentation,Transformers,Decoding,"Zheng, Bin",,,,Three-dimensional displays,Dams,Transforms,Dimension attention module (DAM),remote sensing image,semantic segmentation,vision transformer (ViT),,,,,,,,,,,,,,,,,,
Row_589,"Cao, Kangjian","Wang, Sheng","Wei, Ziheng",Unsupervised Domain Adaptation Semantic Segmentation of Remote Sensing Imagery with Scene Covariance Alignment,ELECTRONICS,DEC 2024,0,"Remote sensing imagery (RSI) segmentation plays a crucial role in environmental monitoring and geospatial analysis. However, in real-world practical applications, the domain shift problem between the source domain and target domain often leads to severe degradation of model performance. Most existing unsupervised domain adaptation methods focus on aligning global-local domain features or category features, neglecting the variations of ground object categories within local scenes. To capture these variations, we propose the scene covariance alignment (SCA) approach to guide the learning of scene-level features in the domain. Specifically, we propose a scene covariance alignment model to address the domain adaptation challenge in RSI segmentation. Unlike traditional global feature alignment methods, SCA incorporates a scene feature pooling (SFP) module and a covariance regularization (CR) mechanism to extract and align scene-level features effectively and focuses on aligning local regions with different scene characteristics between source and target domains. Experiments on both the LoveDA and Yanqing land cover datasets demonstrate that SCA exhibits excellent performance in cross-domain RSI segmentation tasks, particularly outperforming state-of-the-art baselines across various scenarios, including different noise levels, spatial resolutions, and environmental conditions.",unsupervised domain adaptation,remote sensing imagery,semantic segmentation,covariance alignment,"Chen, Kexin","Chang, Runlong","Xu, Fu",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_590,"Li, Jiangyun","Cai, Yuanxiu","Li, Qing",A review of remote sensing image segmentation by deep learning methods,INTERNATIONAL JOURNAL OF DIGITAL EARTH,DEC 31 2024,8,"Remote sensing (RS) images enable high-resolution information collection from complex ground objects and are increasingly utilized in the earth observation research. Recently, RS technologies are continuously enhanced by various characterized platforms and sensors. Simultaneously, artificial intelligence vision algorithms are also developing vigorously and playing a significant role in RS image analysis. In particular, aiming to divide images into different ground elements with specific semantic labels, RS image segmentation could realize the visual acquisition and interpretation. As one of the pioneering methods with the advantages of deep feature extraction ability, deep learning (DL) algorithms have been exploited and proved to be highly beneficial for precise segmentation in recent years. In this paper, a comprehensive review is performed on remote sensing survey systems and various kinds of specially designed deep learning architectures. Meanwhile, DL-based segmentation methods applied on four domains are also illustrated, including geography, precision agriculture, hydrology, and environmental protection issues. In the end, the existing challenges and promising research directions in RS image segmentation are discussed. It is envisioned that this review is able to provide a comprehensive and technical reference, deployment and successful exploitation of DL empowered RS image segmentation approaches.",Remote sensing,deep learning,image segmentation,,"Kou, Mingyin","Zhang, Tianxiang",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_591,"Wang, Yang","Sun, Zhaochen","Zhao, Wei",Encoder- and Decoder-Based Networks Using Multiscale Feature Fusion and Nonlocal Block for Remote Sensing Image Semantic Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,JUL 2021,11,"With the development of convolutional neural networks, the semantic segmentation of remote sensing images has been widely developed, but there are still some unsolved problems in this field due to the lack of multiscale information and the feature mismatch at the upsampling process. To solve these problems, we propose a network called multiscale feature fusion and alignment network (MFANet). MFANet is composed of an encoder and a decoder. The encoder contains a fully convolutional network, a multilevel feature fusion block (MLFFB), and a multiscale feature pyramid (MSFP). These subnetworks can obtain fine-grained feature maps that are full of multiscale and global features and improve segmentation results at multiple object scales. Moreover, MFANet uses a light convolution subnetwork, called decoder, to upsample the segmentation map stage by stage. Combining three scales of features, the decoder can promote the feature alignment at the upsampling stage. Along with the decoder, MFANet utilizes a multistage supervision loss to enhance the localization performance and boundary regression ability. Benefitting from the encoder and decoder structure and the innovative components inside encoder, MFANet is very powerful for the semantic segmentation of remote sensing images and can suit the complicated environment. We evaluate our MFANet on the Vaihingen and Potsdam data sets, and it outperforms the state-of-art methods both in the metric and visual effect.",Decoding,Feature extraction,Image segmentation,Convolution,,,,,Semantics,Remote sensing,Interpolation,Channel attention block (CAB),decoder,multilevel feature fusion block (MLFFB),nonlocal block,remote sensing,,,,,,semantic segmentation,,,,,,,,,,,
Row_592,"Wang, Yiqin",,,Remote sensing image semantic segmentation network based on ENet,JOURNAL OF ENGINEERING-JOE,SEP 2022,1,"The current image semantic segmentation methods cannot meet the requirements of high precision and high speed for remote sensing image analysis. The ENet network model builds a semantic segmentation network, which has the characteristics of few network parameters and fast operation speed. The attention mechanism module is integrated with the ENet network model, which can deeply mine image features in remote sensing datasets and ensure the accuracy of semantic segmentation. The author combines the ENet network with the attention mechanism to construct a new semantic segmentation network model. The model first constructed a remote sensing image semantic segmentation network model based on the ENet network, and simplified the model to further improve the speed of image segmentation and recognition. Then, the attention mechanism module is fused with the ENet network model, which can conduct deep and orderly mining of the image features of the remote sensing image data set. It can meet the accuracy requirements of remote sensing image semantic analysis. Simulations are performed based on three general datasets, and the experimental results show high accuracy and high speed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_593,"Yao, Hongtai","Wang, Xianpei","Zhao, Le",An Object-Based Markov Random Field with Partition-Global Alternately Updated for Semantic Segmentation of High Spatial Resolution Remote Sensing Image,REMOTE SENSING,JAN 2022,3,"The Markov random field (MRF) method is widely used in remote sensing image semantic segmentation because of its excellent spatial (relationship description) ability. However, there are some targets that are relatively small and sparsely distributed in the entire image, which makes it easy to misclassify these pixels into different classes. To solve this problem, this paper proposes an object-based Markov random field method with partition-global alternately updated (OMRF-PGAU). First, four partition images are constructed based on the original image, they overlap with each other and can be reconstructed into the original image; the number of categories and region granularity for these partition images are set. Then, the MRF model is built on the partition images and the original image, their segmentations are alternately updated. The update path adopts a circular path, and the correlation assumption is adopted to establish the connection between the label fields of partition images and the original image. Finally, the relationship between each label field is constantly updated, and the final segmentation result is output after the segmentation has converged. Experiments on texture images and different remote sensing image datasets show that the proposed OMRF-PGAU algorithm has a better segmentation performance than other selected state-of-the-art MRF-based methods.",object-based Markov random field,high spatial resolution remote sensing image,semantic segmentation,correlation assumption,"Tian, Meng","Jian, Zini","Gong, Li","Li, Bowen",,,,,,,,,,,,,,,,,,,,,,,,,
Row_594,"Wang, Shunli","Hu, Qingwu","Wang, Shaohua",Archaeological site segmentation of ancient city walls based on deep learning and LiDAR remote sensing,JOURNAL OF CULTURAL HERITAGE,MAR-APR 2024,7,"Ancient city walls, one of the most notable distinguishing features of Chinese ancient cities, are military defenses constructed of rammed earth. The ancient city walls have considerable study value because they served as the city's boundary and a symbol of power. However, as a result of natural erosion and human activities, many sites have been ruined. Existing optical remote sensing technologies, LiDAR point cloud processing algorithms, and deep learning methods are inadequate for the extraction and segmentation of ancient city wall sites. The novel semantic segmentation method for ancient city wall sites is described in this paper that extracts sites at the pixel level from LiDAR remote sensing data based on deep learning. To begin, the point cloud data collected by airborne laser scanning is processed into DEM data, and the distribution of ancient city walls in the study area is obtained through archaeological survey and expert interpretation. The dataset for deep learning semantic segmentation is then generated using image cropping and data augmentation techniques. Third, implement a U-Net semantic segmentation framework for microtopographic archaeological sites, and predict ancient city wall sites in the testing region after model training. Finally, the deep learning results are optimized using the connected component analysis method, and prediction mistakes such as holes and noise are removed. Taking Jinancheng, the capital city of the Chu kingdom, as an example, the proposed method process can identify and extract the ancient city wall sites at the pixel level, where the evaluation metrics reach 94.12% (Precision) and 81.38% (IoU). The experiment results are excellent due to improvement strategies in the dataset generation, model training, and post-processing steps. Thus, this study is significant for the current survey and protection of ancient city wall sites. The source code will be freely available at https://github.com/wshunli/Open- CHAI-CityWalls .(c) 2023 Consiglio Nazionale delle Ricerche (CNR). Published by Elsevier Masson SAS. All rights reserved.",Archaeological remote sensing,Archaeological site segmentation,Deep learning,LiDAR remote sensing,"Ai, Mingyao","Zhao, Pengcheng",,,Semantic segmentation,Archaeological survey,,,,,,,,,,,,,,,,,,,,,,,
Row_595,"Qi, Zipeng","Chen, Hao","Liu, Chenyang",Implicit Ray Transformers for Multiview Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,5,"The mainstream convolutional neural network (CNN)-based remote sensing (RS) image semantic segmentation approaches typically rely on massively labeled training data. Such a paradigm struggles with the problem of RS multiview scene segmentation with limited labeled views due to the lack of consideration of 3-D information within the scene. In this article, we propose ""implicit ray transformer (IRT)"" based on implicit neural representation (INR) for RS scene semantic segmentation with sparse labels (5% of the images being labeled). We explore a new way of introducing the multiview 3-D structure priors to the task for accurate and view-consistent semantic segmentation. The proposed method includes a two-stage learning process. In the first stage, we optimize a neural field to encode the color and 3-D structure of the RS scene based on multiview images. In the second stage, we design a ray transformer to leverage the relations between the neural field 3-D features and 2-D texture features for learning better semantic representations. Different from previous methods that only consider 3-D priors or 2-D features, we incorporate additional 2-D texture information and 3-D priors by broadcasting CNN features to different point features along the sampled ray. To verify the effectiveness of the proposed method, we construct a challenging dataset containing six synthetic sub-datasets collected from the Carla platform and three real sub-datasets from Google Maps. Experiments show that the proposed method outperforms the CNN-based methods and the state-of-the-art INR-based segmentation methods in quantitative and qualitative metrics. The ablation study shows that under a limited number of fully annotated images, the combination of the 3-D structure priors and 2-D texture can significantly improve the performance and effectively complete missing semantic information in novel views. Experiments also demonstrate that the proposed method could yield geometry-consistent segmentation results against illumination changes and viewpoint changes. Our data and code will be public.",Three-dimensional displays,Feature extraction,Semantics,Task analysis,"Shi, Zhenwei","Zou, Zhengxia",,,Remote sensing,Transformers,Annotations,Implicit neural representation (INR),remote sensing (RS),semantic segmentation,transformer,,,,,,,,,,,,,,,,,,
Row_596,"Wang, Tao","Xu, Chao","Liu, Bin",MCAT-UNet: Convolutional and Cross-Shaped Window Attention Enhanced UNet for Efficient High-Resolution Remote Sensing Image Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,3,"Semantic segmentation is a crucial step in the intelligent interpretation of high-resolution remote sensing images (HRSIs). Convolutional neural networks and transformers are widely used for semantic feature extraction in remote sensing images, but the former inevitably has limitations in modeling long-range spatial dependency information, while the latter lacks the ability to learn local semantic features. Existing remote sensing image segmentation methods are optimized and modified based on the backbone networks used in natural image processing. Despite achieving relatively good results, the complexity of their network structures leads to high computational costs and limited improvements in accuracy. These methods have limited boundary distinction for ground objects in complex environments, especially for small targets. In this article, we propose an efficient semantic segmentation architecture for HRSIs called MCAT-UNet, which utilizes multiscale convolutional attention (MSCA) and the cross-shaped window transformer (CSWT) to reconstruct UNet. The encoder stacks a sequence of MSCA to exploit the advantages of convolution attention to encode context information more effectively and enhance hierarchical multiscale representation learning. The proposed U-shaped decoder integrates three skip connections using the CSWT block to further capture long-range spatial dependency and gradually restore the size of the feature map. We benchmark MCAT-UNet on three common datasets, Potsdam, Vaihingen, and LoveDA. Comprehensive experiments and extensive ablation studies show that our proposed MCAT-UNet outperforms previous state-of-the-art methods with remarkable performance.",Transformers,Remote sensing,Feature extraction,Semantics,"Yang, Guang","Zhang, Erlei","Niu, Dangdang","Zhang, Hongming",Task analysis,Semantic segmentation,Computer vision,Convolutional attention,cross-shaped self-attention,remote sensing image,semantic segmentation,transformer,,,,,,,,,,,,,,,,,
Row_597,"Feng, Yongliang",,,SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES USING DEEP LEARNING FROM THE PERSPECTIVE OF ECOLOGICAL ENVIRONMENT PROTECTION,FRESENIUS ENVIRONMENTAL BULLETIN,2022,0,"Traditional remote sensing image semantic segmentation has problems such as low segmentation accuracy and large model parameters and calculations. A semantic segmentation network of remote sensing images using deep learning from the perspective of ecological environment protection is proposed. Firstly, the ""Gaofen-5"" remote sensing image set is preprocessed by means of data cutting and image denoising to improve the data quality. Then, the local and global diffusion blocks (LGD) is introduced to improve the EfficientNet B0 network to construct an ENet-LGD network model to improve feature learning. Finally, the conditional random field model is improved based on superpixels. That is, each small block of the super pixel reflects the boundary of the feature, which further improves the segmentation effect. The experimental results based on the Pingshuo mining area in Shanxi Province show that the proposed network can accurately reflect the surface types and their boundaries. The average segmentation accuracy is 0.865, which is better than other comparison networks. It provides a certain theoretical basis for subsequent practical applications.",Semantic segmentation of remote sensing image,From the perspective of ecological environment protection,Global local interaction module,Efficientnet B0 network,,,,,Super pixel,Conditional random field model,,,,,,,,,,,,,,,,,,,,,,,
Row_598,"Ao, Wei","Zheng, Shunyi","Meng, Yan",Few-Shot Aerial Image Semantic Segmentation Leveraging Pyramid Correlation Fusion,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,5,"Few-shot semantic segmentation (FSS) has gained significant attention due to its ability to segment novel objects using only a limited number of labeled samples, thereby addressing the problem of overfitting caused by a lack of training data. Although this technique is widely studied in the field of computer vision, there are few methods for remote-sensing images. Prevalent FSS methods can achieve remarkable results for natural images, but they are difficult to apply to remote-sensing image processing because existing methods rarely take into consideration the large-scale and resolution differences in remote-sensing images. Consequently, it is hard for them to obtain correct semantic guidance from a few annotated remote-sensing images. To tackle these problems, this article proposes the pyramid correlation fusion network (PCFNet) to promote the ability to mine helpful information by calculating multiscale pixel-wise semantic correspondence. Particularly, the dual-distance correlation (DDC) module is designed to simultaneously compute the cosine similarity and Euclidean distance between query features and support features, producing adequate guidance information to determine the category of each pixel. Moreover, to improve segmentation accuracy for small objects, the scale-aware cross-entropy loss (SACELoss) is introduced to dynamically assign loss weights according to the actual sizes of objects. This enables smaller objects to be assigned larger weight values and thus receive more attention during training. Comprehensive experiments on both the iSAID- 5(i) and DLRSD- 5(i) datasets demonstrate that our method outperforms state-of-the-art FSS methods. Our code is available at https://github.com/TinyAway/PCFNet.",Distance correlation,few-shot semantic segmentation (FSS),meta-learning,remote-sensing image processing,"Gao, Zhi",,,,semantic correspondence,,,,,,,,,,,,,,,,,,,,,,,,
Row_599,"Liu, Wenjie","Zhang, Yongjun","Fan, Haisheng",A New Multi-Channel Deep Convolutional Neural Network for Semantic Segmentation of Remote Sensing Image,IEEE ACCESS,2020,9,"The semantic segmentation of remote sensing (RS) image is a hot research field. With the development of deep learning, the semantic segmentation based on a full convolution neural network greatly improves the segmentation accuracy. The amount of information on the RS image is very large, but the sample size is extremely uneven. Therefore, even the common network can segment RS images to a certain extent, but the segmentation accuracy can still be greatly improved. The common neural network deepens the network to improve the classification accuracy, but it has a lot of loss to the target spatial features and scale features, and the existing common feature fusion methods can only solve some problems. A segmentation network is built to solve the above problems very well. The network employs the InceptionV-4 network as the backbone and improves it. We modify the network structure and introduce the changed Atrous Spatial Pyramid Pooling module to extract the multi-scale features of the target from different training stages. Without losing the depth of the network, using Inception blocks to strengthen the width of the network can obtain more abstract features. At the same time, the backbone network is used for semantic fusion of the context, it can retain more spatial features, then an effective decoder network is designed. Finally, evaluate our model on the ISPRS 2D Semantic Labeling Contest Potsdam and Inria Aerial Image Labeling Dataset. The results show that the network has very superior performance, reaching 89.62% IOU score and 94.49% F1 score on the Potsdam dataset, and the IOU score on the Inria dataset has been greatly improved.",Image segmentation,Feature extraction,Convolution,Semantics,"Zou, Yongjie","Cui, Zhongwei",,,Machine learning,Neural networks,Image classification,Semantic segmentation,neural network,remote sensing,feature fusion,,,,,,,,,,,,,,,,,,
Row_600,"Tong, Zhonggui","Li, Yuxia","Zhang, Jinglin",MSFANet: Multiscale Fusion Attention Network for Road Segmentation of Multispectral Remote Sensing Data,REMOTE SENSING,APR 2023,7,"With the development of deep learning and remote sensing technologies in recent years, many semantic segmentation methods based on convolutional neural networks (CNNs) have been applied to road extraction. However, previous deep learning-based road extraction methods primarily used RGB imagery as an input and did not take advantage of the spectral information contained in hyperspectral imagery. These methods can produce discontinuous outputs caused by objects with similar spectral signatures to roads. In addition, the images obtained from different Earth remote sensing sensors may have different spatial resolutions, enhancing the difficulty of the joint analysis. This work proposes the Multiscale Fusion Attention Network (MSFANet) to overcome these problems. Compared to traditional road extraction frameworks, the proposed MSFANet fuses information from different spectra at multiple scales. In MSFANet, multispectral remote sensing data is used as an additional input to the network, in addition to RGB remote sensing data, to obtain richer spectral information. The Cross-source Feature Fusion Module (CFFM) is used to calibrate and fuse spectral features at different scales, reducing the impact of noise and redundant features from different inputs. The Multiscale Semantic Aggregation Decoder (MSAD) fuses multiscale features and global context information from the upsampling process layer by layer, reducing information loss during the multiscale feature fusion. The proposed MSFANet network was applied to the SpaceNet dataset and self-annotated images from Chongzhou, a representative city in China. Our MSFANet performs better over the baseline HRNet by a large margin of +6.38 IoU and +5.11 F1-score on the SpaceNet dataset, +3.61 IoU and +2.32 F1-score on the self-annotated dataset (Chongzhou dataset). Moreover, the effectiveness of MSFANet was also proven by comparative experiments with other studies.",deep learning,semantic segmentation,attention mechanism,multispectral remote sensing data,"He, Lei","Gong, Yushu",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_601,"Li, Peng","Zhang, Dezheng","Wulamu, Aziguli",Semantic Relation Model and Dataset for Remote Sensing Scene Understanding,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,JUL 2021,8,"A deep understanding of our visual world is more than an isolated perception on a series of objects, and the relationships between them also contain rich semantic information. Especially for those satellite remote sensing images, the span is so large that the various objects are always of different sizes and complex spatial compositions. Therefore, the recognition of semantic relations is conducive to strengthen the understanding of remote sensing scenes. In this paper, we propose a novel multi-scale semantic fusion network (MSFN). In this framework, dilated convolution is introduced into a graph convolutional network (GCN) based on an attentional mechanism to fuse and refine multi-scale semantic context, which is crucial to strengthen the cognitive ability of our model Besides, based on the mapping between visual features and semantic embeddings, we design a sparse relationship extraction module to remove meaningless connections among entities and improve the efficiency of scene graph generation. Meanwhile, to further promote the research of scene understanding in remote sensing field, this paper also proposes a remote sensing scene graph dataset (RSSGD). We carry out extensive experiments and the results show that our model significantly outperforms previous methods on scene graph generation. In addition, RSSGD effectively bridges the huge semantic gap between low-level perception and high-level cognition of remote sensing images.",remote sensing scene understanding,semantic relation cognition,scene graph generation,multi-scale semantic fusion,"Liu, Xin","Chen, Peng",,,attentional mechanism,graph convolutional network,dilated convolution,,,,,,,,,,,,,,,,,,,,,,
Row_602,"Zeng, Wankang","Cheng, Ming","Yuan, Zhimin",Domain adaptive remote sensing image semantic segmentation with prototype guidance,NEUROCOMPUTING,MAY 1 2024,1,"Current unsupervised domain adaptation (UDA) techniques in semantic segmentation effectively decrease the domain discrepancy between the labeled source domain and unlabeled target domain, thereby enhancing the model's pixel -wise discriminative capability for target domain data. However, in remote sensing images (RSIs), our study uncovers that these approaches perform poorly in the presence of class distribution inconsistencies between the source and target domains. In this work, we propose a one -stage mean teacher framework with a novel auxiliary prototype classifier, named MTA, to address this issue. Specifically, the teacher model assigns pseudo labels at pixel level for target samples and captures knowledge from the student model via exponential moving average (EMA). With labeled source samples and target samples that have pseudo labels, the student model can alleviate the divergence between the source and target domains. In addition, the auxiliary prototype classifier (APC) reduces the performance degradation in the parametric softmax classifier of the student model caused by class distribution divergence. We also propose a prototype computation scheme to obtain each class prototype in the APC. Specifically, we build a memory bank for each class of the two domains to store feature embeddings dynamically. Then, we compute the class prototype by applying the clustering algorithm on memory banks corresponding to the class. Meanwhile, the APC reduces the intra-class domain discrepancy by optimizing the cross -entropy loss, which brings each class feature distribution of the two domains closer to the class prototype. The experimental results on RSIs UDA semantic segmentation tasks show the superiority of our approach over comparative methods.",Unsupervised domain adaptation,Semantic segmentation,Auxiliary prototype classifier,Mean teacher,"Dai, Wei","Wu, Youming","Liu, Weiquan","Wang, Cheng",,,,,,,,,,,,,,,,,,,,,,,,,
Row_603,"Zhang, Chongyu",,,Based on Multi-Feature Information Attention Fusion for Multi-Modal Remote Sensing Image Semantic Segmentation,,2021,4,"Semantic segmentation of remote sensing images are widely used in land census and agriculture. The scenes in remote sensing images are complex, easily affected by season, such as farmland. Besides, the size of the target in remote sensing image is different, the shape is irregular, and there is often the problem of missing detection, so the multi-source data information is directly fed into the neural network, resulting in fuzzy segmentation boundary, which is difficult to achieve fine segmentation. To solve this problem, we propose a Dual-way Feature attention Fusion Network (DFFNet), which consists of two branches, optical remote sensing image branch and elevation feature branch. The optical remote sensing image branch uses the spatial relationship module to learn and infer the global relationship between any two spatial positions or feature maps and then extracts the multi-level features of the image by capturing more context information and pyramid attention mechanism. The elevation feature branch strengthens the classification. Based on the boundary information, the remote sensing image segmentation is realized. Experiment results on ISPRS Vaihingen image dataset demonstrate the effectiveness of the proposed method.",Semantic segmentation,spatial attention,channel attention,feature fusion,,,,,,,,,,,,,,2021 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (IEEE ICMA 2021),,,,,,,,,,,,,,,
Row_604,"Liu, Jiang","Cheng, Shuli","Du, Anyu",ER-Swin: Feature Enhancement and Refinement Network Based on Swin Transformer for Semantic Segmentation of Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,2,"As the field of remote sensing image processing continues to advance, semantic segmentation has become a focal point in this domain. The emergence of the swin transformer (SwinT) has greatly alleviated the computational complexities associated with transformers, leading to its widespread application in the field of semantic segmentation. However, most current network models lack a feature enhancement process internally, and the model's tail lacks refinement modules to prevent category misjudgments caused by feature redundancy. To address this issue, we propose ER-Swin to explore the potential of utilizing SwinT as the backbone network for semantic segmentation in remote sensing images. Addressing the need for feature enhancement in the backbone network, we propose interactive feature enhancement attention (IFEA), which leverages diagonal information interaction to augment features. Additionally, we design the semantic selective refinement module (SSRM) to refine the rich features at the tail end of the network, thereby enhancing segmentation outcomes. We evaluated our model on the Vaihingen, Potsdam, and LoveDA datasets and achieved accuracies of 84.89%, 87.20%, and 55.1%, respectively, on the mean intersection over union (mIoU) metric. Through comparative experiments, we demonstrate the superior segmentation performance of our model, affirming its competitiveness.",Feature enhancement,refinement,semantic segmentation,swin transformer (SwinT),,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_605,"Wang, Yupei","Zhang, Haoran","Hu, Yongkang",Geometric Boundary Guided Feature Fusion and Spatial-Semantic Context Aggregation for Semantic Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON IMAGE PROCESSING,2023,7,"Semantic segmentation of remote sensing images aims to achieve pixel-level semantic category assignment for input images. This task has achieved significant advances with the rapid development of deep neural network. Most current methods mainly focus on effectively fusing the low-level spatial details and high-level semantic cues. Other methods also propose to incorporate the boundary guidance to obtain boundary preserving segmentation. However, current methods treat the multi-level feature fusion and the boundary guidance as two separate tasks, resulting in sub-optimal solutions. Moreover, due to the large inter-class difference and small intra-class consistency within remote sensing images, current methods often fail to accurately aggregate the long-range contextual cues. These critical issues make current methods fail to achieve satisfactory segmentation predictions, which severely hinder downstream applications. To this end, we first propose a novel boundary guided multi-level feature fusion module to seamlessly incorporate the boundary guidance into the multi-level feature fusion operations. Meanwhile, in order to further enforce the boundary guidance effectively, we employ a geometric-similarity-based boundary loss function. In this way, under the explicit guidance of boundary constraint, the multi-level features are effectively combined. In addition, a channel-wise correlation guided spatial-semantic context aggregation module is presented to effectively aggregate the contextual cues. In this way, subtle but meaningful contextual cues about pixel-wise spatial context and channel-wise semantic correlation are effectively aggregated, leading to spatial-semantic context aggregation. Extensive qualitative and quantitative experimental results on ISPRS Vaihingen and GaoFen-2 datasets demonstrate the effectiveness of the proposed method.",Semantic segmentation,multi-level feature fusion,contextual cues,,"Hu, Xiaoxing","Chen, Liang","Hu, Shanqing",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_606,"Sun, Yu","Bi, Fukun","Gao, Yangte",A Multi-Attention UNet for Semantic Segmentation in Remote Sensing Images,SYMMETRY-BASEL,MAY 2022,34,"In recent years, with the development of deep learning, semantic segmentation for remote sensing images has gradually become a hot issue in computer vision. However, segmentation for multicategory targets is still a difficult problem. To address the issues regarding poor precision and multiple scales in different categories, we propose a UNet, based on multi-attention (MA-UNet). Specifically, we propose a residual encoder, based on a simple attention module, to improve the extraction capability of the backbone for fine-grained features. By using multi-head self-attention for the lowest level feature, the semantic representation of the given feature map is reconstructed, further implementing fine-grained segmentation for different categories of pixels. Then, to address the problem of multiple scales in different categories, we increase the number of down-sampling to subdivide the feature sizes of the target at different scales, and use channel attention and spatial attention in different feature fusion stages, to better fuse the feature information of the target at different scales. We conducted experiments on the WHDLD datasets and DLRSD datasets. The results show that, with multiple visual attention feature enhancements, our method achieves 63.94% mean intersection over union (IOU) on the WHDLD datasets; this result is 4.27% higher than that of UNet, and on the DLRSD datasets, the mean IOU of our methods improves UNet's 56.17% to 61.90%, while exceeding those of other advanced methods.",remote sensing,image segmentation,multi-head self-attention,channel attention,"Chen, Liang","Feng, Suting",,,spatial attention,deep learning,,,,,,,,,,,,,,,,,,,,,,,
Row_607,"Zhao, Yanfeng","Yang, Zhenjian","Zhang, Yunjie",BGFNet: boundary information-aided graph structure fusion network for semantic segmentation of remote sensing images,VISUAL COMPUTER,JUN 2024,0,"Semantic segmentation of high-resolution remote sensing (RS) images faces challenges in multi-scale transformation. Although feature fusion is widely used in this task, the existing methods do not fully consider the spatial structure relationship between feature layers in the encoder stage. Firstly, this paper designs BGFNet network, combines the feature extraction in the encoder stage and the topological relationship modeling of graph structure, and proposes the graph structure-guided multi-scale feature fusion module to solve this problem. Secondly, in order to solve the problem of blurred object boundaries in RS image segmentation, we propose a multi-level deformable boundary guidance module, which emphasizes object boundaries by establishing long-range context. Finally, a shared enhanced attention module with shared parameters is proposed to enhance the characteristics of each class to improve the recognition ability of the model. The effectiveness of BGFNet is verified on Potsdam and Vaihingen public RS datasets, and its segmentation performance is obviously better than the existing mainstream methods. The source code will be freely available at https://github.com/zyf-cell/BGF_Net.",Graph structure,Boundary guidance,Attention mechanism,Remote sensing,"Chen, Yadong",,,,Semantic segmentation,Very high-resolution images,,,,,,,,,,,,,,,,,,,,,,,
Row_608,"Xie, Hongbin","Pan, Yongzhuo","Luan, Jinhua",Open-pit Mining Area Segmentation of Remote Sensing Images Based on DUSegNet,JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING,JUN 2021,27,"Remote sensing is an important technical means for monitoring and protecting mineral resources. However, because of the complex surface environment, very few good results have been achieved in the study of automatic open-pit mining area segmentation. Inspired by SegNet, UNet and D-LinkNet, this paper proposes a novel deep convolutional neural network for pixel-level semantic segmentation of optical remote sensing images termed DUSegNet. In this network, the pyramid model and upsampling method of pooling indices, similar to SegNet, are employed in the encoder-decoder architecture. In addition, the convolutional skip connection architecture, similar to UNet, is adopted to connect shallow features to the decoder. Additionally, the serial-parallel model, similar to D-LinkNet; the intensifier constructed by dilated convolution; and the classifier constructed by softmax layers are applied in the process of encoding and decoding. In the practical application stage, we present an effective open-pit mining area segmentation method for entire remote sensing images, which has great significance for practical work, such as environmental impact assessment procedures and mine management. In the experimental stage, we compared the open-pit mining area segmentation effects of SegNet, UNet, DecovNet, and DUSegNet on the same dataset manually collected from GF-2 remote sensing images and verified the advantages of DUSegNet using graphic results and optimal evaluation metrics, such as AP (0.94) and F-score (0.67).",Deep convolutional neural network,Semantic segmentation,Optical remote sensing image,Open-pit mining area,"Yang, Xue","Xi, Yawen",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_609,"Su, Yanzhou","Cheng, Jian","Bai, Haiwei",Semantic Segmentation of Very-High-Resolution Remote Sensing Images via Deep Multi-Feature Learning,REMOTE SENSING,FEB 2022,22,"Currently, an increasing number of convolutional neural networks (CNNs) focus specifically on capturing contextual features (con. feat) to improve performance in semantic segmentation tasks. However, high-level con. feat are biased towards encoding features of large objects, disregard spatial details, and have a limited capacity to discriminate between easily confused classes (e.g., trees and grasses). As a result, we incorporate low-level features (low. feat) and class-specific discriminative features (dis. feat) to boost model performance further, with low. feat helping the model in recovering spatial information and dis. feat effectively reducing class confusion during segmentation. To this end, we propose a novel deep multi-feature learning framework for the semantic segmentation of VHR RSIs, dubbed MFNet. The proposed MFNet adopts a multi-feature learning mechanism to learn more complete features, including con. feat, low. feat, and dis. feat. More specifically, aside from a widely used context aggregation module for capturing con. feat, we additionally append two branches for learning low. feat and dis. feat. One focuses on learning low. feat at a shallow layer in the backbone network through local contrast processing, while the other groups con. feat and then optimizes each class individually to generate dis. feat with better inter-class discriminative capability. Extensive quantitative and qualitative evaluations demonstrate that the proposed MFNet outperforms most state-of-the-art models on the ISPRS Vaihingen and Potsdam datasets. In particular, thanks to the mechanism of multi-feature learning, our model achieves an overall accuracy score of 91.91% on the Potsdam test set with VGG16 as a backbone, performing favorably against advanced models with ResNet101.",very-high-resolution remote sensing images,semantic segmentation,multi-feature learning,,"Liu, Haijun","He, Changtao",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_610,"Xiong, Xuan","Wang, Xiaopeng","Zhang, Jiahua",TCUNet: A Lightweight Dual-Branch Parallel Network for Sea-Land Segmentation in Remote Sensing Images,REMOTE SENSING,SEP 2023,7,"Remote sensing techniques for shoreline extraction are crucial for monitoring changes in erosion rates, surface hydrology, and ecosystem structure. In recent years, Convolutional neural networks (CNNs) have developed as a cutting-edge deep learning technique that has been extensively used in shoreline extraction from remote sensing images, owing to their exceptional feature extraction capabilities. They are progressively replacing traditional methods in this field. However, most CNN models only focus on the features in local receptive fields, and overlook the consideration of global contextual information, which will hamper the model's ability to perform a precise segmentation of boundaries and small objects, consequently leading to unsatisfactory segmentation results. To solve this problem, we propose a parallel semantic segmentation network (TCU-Net) combining CNN and Transformer, to extract shorelines from multispectral remote sensing images, and improve the extraction accuracy. Firstly, TCU-Net imports the Pyramid Vision Transformer V2 (PVT V2) network and ResNet, which serve as backbones for the Transformer branch and CNN branch, respectively, forming a parallel dual-encoder structure for the extraction of both global and local features. Furthermore, a feature interaction module is designed to achieve information exchange, and complementary advantages of features, between the two branches. Secondly, for the decoder part, we propose a cross-scale multi-source feature fusion module to replace the original UNet decoder block, to aggregate multi-scale semantic features more effectively. In addition, a sea-land segmentation dataset covering the Yellow Sea region (GF Dataset) is constructed through the processing of three scenes from Gaofen-6 remote sensing images. We perform a comprehensive experiment with the GF dataset to compare the proposed method with mainstream semantic segmentation models, and the results demonstrate that TCU-Net outperforms the competing models in all three evaluation indices: the PA (pixel accuracy), F1-score, and MIoU (mean intersection over union), while requiring significantly fewer parameters and computational resources compared to other models. These results indicate that the TCU-Net model proposed in this article can extract the shoreline from remote sensing images more effectively, with a shorter time, and lower computational overhead.",double-branch,sea-land segmentation,GF-6,CNN,"Huang, Baoxiang","Du, Runfeng",,,transformer,remote sensing,,,,,,,,,,,,,,,,,,,,,,,
Row_611,"Hao, Ting","Bai, Shuya","Wu, Tianyu",Weakly Supervised Semantic Segmentation of Remote Sensing Images Based on Progressive Mining and Saliency-Enhanced Self-Attention,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,0,"Given the high demands of effort in generating pixel-level annotations, weakly supervised semantic segmentation (WSSS) has become an important approach for remote sensing image (RSI) interpretation. However, current methods are mostly borrowed from natural scene studies, regardless of the significant variation in object sizes as well as the highly confusing intraclass heterogeneity and interclass homogeneity which are characteristic of RSIs. In this letter, we propose a WSSS method based on progressive mining and saliency-enhanced self-attention (PMSA), to efficiently segment RSIs with image-level labels. First, we exploit multiscale orientation patterns to sufficiently extract the rich texture in RSIs which can help to discern between the different classes, and combine this information with contrast and luminance features to generate fine saliency maps. Second, we design a progressive mining process to gradually discover both the large objects, representative of semantics, and the small objects, rich in patterns. Finally, we employ self-attention mechanism to capture global dependencies in RSIs for refining category areas. To inhibit the misspread of attention, we use saliency as a mask discerning between the background and the object classes. Experiments on different datasets demonstrate the competence of the proposed method, in terms of both metrical results and visual effects.",Remote sensing,saliency detection,self-attention,weakly supervised semantic segmentation (WSSS),"Zhang, Libao",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_612,"Luo, Zhenglian","He, Lingmin",,A two-stage domain adaptive remote sensing image semantic segmentation network combined with self-training,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2024,1,"Unsupervised domain adaptive (UDA) is a popular technology to solve the semantic segmentation of remote sensing images. UDA can alleviate the need for large-scale pixel-level labeling by pre-training on the labeled source domain and verifying on the target domain. However, domain offset exists between different data sets, which will greatly affect the final performance of the semantic segmentation model. Based on this, this paper proposes a segmented interdomain adaptive network framework of dual-path image transformation, which combines self-training with interdomain adaptive segmentation. Firstly, the image style conversion is used to reduce the domain offset of the image level, and the dual-path method is used to promote each other to improve the image conversion quality and the pseudo-label quality of the target domain. Secondly, the pseudo-tags of the first-stage training are used to prioritize, and the target domain is divided into easy and difficult to partition for selective retraining. By adversarial learning, the difficult-to-segment features are aligned with easily-segmented features, and the influence of domain migration is gradually reduced by adaptive repetition in the domain, thus improving the segmentation performance of the network.",Unsupervised domain adaptive,Semantic segmentation of remote sensing images,Dual path image conversion,self-training,,,,,Adversarial learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_613,"Yin, Jichong","Wu, Fang","Qi, Yuyang",Vector Mapping Method for Buildings in Remote Sensing Images Based on Joint Semantic-Geometric Learning,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,3,"An important high-precision building vector mapping method automatically delineates building polygons from high-resolution remote sensing images. Deep learning methods have greatly improved the accuracy of automatic building segmentation in remote sensing images. However, building polygons in vector forms have a compact and regular structured expression effect, which corresponds more with the application requirements of cartography and geographic information systems (GIS). We propose a vector mapping method for buildings in remote sensing images with joint semantic-geometric learning to generate building polygon vectors in remote sensing images automatically. The method, aiming to provide cartography and GIS data sources, consists of three modules: multi-task segmentation, contour regularization, and polygon optimization. To reduce missing extractions and mis-extractions and obtain a complete building segmentation mask, the multitask segmentation module performs joint semantic-geometric learning on three related tasks: building instance detection, pixel-wise contour segmentation, and edge extraction. The regularization module normalizes the segmentation mask expression using geometric constraints and image information, whereas the polygon optimization module combines geometric constraints with deep learning methods to ensure vectorization quality. The experimental results show that the proposed method adapts well to building vector extraction tasks under different scenarios and can generate building vector polygons that match the ground truth labels. This method offers significant advantages in solving problems, such as building polygon irregularity and vertex offset.",Contour regularization,joint semantic-geometric learning,multitask segmentation,polygon optimization,,,,,remote sensing images,,,,,,,,,,,,,,,,,,,,,,,,
Row_614,"Zhang, Zhaoyang","Wang, Xuying","Mei, Xiaoming",FALSE: False Negative Samples Aware Contrastive Learning for Semantic Segmentation of High-Resolution Remote Sensing Image,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,15,"Self-supervised contrastive learning (SSCL) is a potential learning paradigm for learning remote sensing image (RSI)-invariant features through the label-free method. The existing SSCL of RSI is built based on constructing positive and negative sample pairs. However, due to the richness of RSI ground objects and the complexity of the RSI contextual semantics, the same RSI patches have the coexistence and imbalance of positive and negative samples, which causes the SSCL pushing negative samples far away while pushing positive samples far away, and vice versa. We call this the sample confounding issue (SCI). To solve this problem, we propose a False negAtive sampLes aware contraStive lEarning model (FALSE) for the semantic segmentation of high-resolution RSIs. Since SSCL pretraining is unsupervised, the lack of definable criteria for false negative sample (FNS) leads to theoretical undecidability, and we designed two steps to implement the FNS approximation determination: coarse determination of FNS and precise calibration of FNS. We achieve coarse determination of FNS by the FNS self-determination (FNSD) strategy and achieve calibration of FNS by the FNS confidence calibration (FNCC) loss function. Experimental results on three RSI semantic segmentation datasets demonstrated that the FALSE effectively improves the accuracy of the downstream RSI semantic segmentation task compared with the current three models, which represent three different types of SSCL models. The mean intersection over union (mIoU) on the ISPRS Potsdam dataset is improved by 0.7% on average; on the CVPR DGLC dataset, it is improved by 12.28% on average; and on the Xiangtan dataset, this is improved by 1.17% on average. This indicates that the SSCL model has the ability to self-differentiate FNS and that the FALSE effectively mitigates the SCI in SSCL.",Calibration,Semantic segmentation,Benchmark testing,Feature extraction,"Tao, Chao","Li, Haifeng",,,Remote sensing,Semantics,Complexity theory,False negative sample (FNS),remote sensing image (RSI),self-supervised contrastive learning (SSCL),semantic segmentation,,,,,,,,,,,,,,,,,,
Row_615,"Zhu, Qiqi","Wan, Jiangqin","Zhong, Yanfei",TOPIC MODEL FOR REMOTE SENSING DATA: A COMPREHENSIVE REVIEW,,2020,2,"From text analysis to image interpretation, the topic model (TM) always plays an important role. With its powerful semantic mining capabilities, it is able to capture the latent spectral and spatial information from remote sensing (RS) images. Recent years have witnessed widespread use of TM to solve the problems in RS image interpretation, i.e., semantic segmentation, target detection, and scene classification. However, there has not yet been a study expatiating and summarizing the current situation of RS applications with TM. This paper intends to systematically summarize the application of TM in RS images and to conduct several typical experiments for comparison. Specifically, the architecture of our work can be explained as follows: 1) the theory of TM; 2) the applications of RS based on TM; 3) experimental analysis of typical TM methods to provide reference for further understanding, and 4) summary and prospects for guiding further research into TM for RS data.",Topic model,remote sensing image,scene classification,semantic segmentation,"Guan, Qingfeng","Zhang, Liangpei","Li, Deren",,,,,,,,,,,IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_616,"Liu, Xiaohui","Zhang, Lei","Wang, Rui",Cascaded CNN and global-local attention transformer network-based semantic segmentation for high-resolution remote sensing image,JOURNAL OF APPLIED REMOTE SENSING,JUL 1 2024,0,"High-resolution remote sensing images (HRRSIs) contain rich local spatial information and long-distance location dependence, which play an important role in semantic segmentation tasks and have received more and more research attention. However, HRRSIs often exhibit large intraclass variance and small interclass variance due to the diversity and complexity of ground objects, thereby bringing great challenges to a semantic segmentation task. In most networks, there are numerous small-scale object omissions and large-scale object fragmentations in the segmentation results because of insufficient local feature extraction and low global information utilization. A network cascaded by convolution neural network and global-local attention transformer is proposed called CNN-transformer cascade network. First, convolution blocks and global-local attention transformer blocks are used to extract multiscale local features and long-range location information, respectively. Then a multilevel channel attention integration block is designed to fuse geometric features and semantic features of different depths and revise the channel weights through the channel attention module to resist the interference of redundant information. Finally, the smoothness of the segmentation is improved through the implementation of upsampling using a deconvolution operation. We compare our method with several state-of-the-art methods on the ISPRS Vaihingen and Potsdam datasets. Experimental results show that our method can improve the integrity and independence of multiscale objects segmentation results.",high-resolution remote sensing images,semantic segmentation,convolution neural network,transformer,"Li, Xiaoyu","Xu, Jiyang","Lu, Xiaochen",,global-local attention transformer block,multilevel channel attention integration block,,,,,,,,,,,,,,,,,,,,,,,
Row_617,"Xi, Mengfei","Li, Jie","He, Zhilin",NRN-RSSEG: A Deep Neural Network Model for Combating Label Noise in Semantic Segmentation of Remote Sensing Images,REMOTE SENSING,JAN 2023,3,"The performance of deep neural networks depends on the accuracy of labeled samples, as they usually contain label noise. This study examines the semantic segmentation of remote sensing images that include label noise and proposes an anti-label-noise network framework, termed Labeled Noise Robust Network in Remote Sensing Image Semantic Segmentation (NRN-RSSEG), to combat label noise. The algorithm combines three main components: network, attention mechanism, and a noise-robust loss function. Three different noise rates (containing both symmetric and asymmetric noise) were simulated to test the noise resistance of the network. Validation was performed in the Vaihingen region of the ISPRS Vaihingen 2D semantic labeling dataset, and the performance of the network was evaluated by comparing the NRN-RSSEG with the original U-Net model. The results show that NRN-RSSEG maintains a high accuracy on both clean and noisy datasets. Specifically, NRN-RSSEG outperforms UNET in terms of PA, MPA, Kappa, Mean_F1, and FWIoU in the presence of noisy datasets, and as the noise rate increases, each performance of UNET shows a decreasing trend while the performance of NRN-RSSEG decreases slowly and some performances show an increasing trend. At a noise rate of 0.5, the PA (-6.14%), MPA (-4.27%) Kappa (-8.55%), Mean_F1 (-5.11%), and FWIOU (-9.75%) of UNET degrade faster; while the PA (-2.51%), Kappa (-3.33%), and FWIoU of NRN-RSSEG (-3.26) degraded more slowly, MPA (+1.41) and Mean_F1 (+2.69%) showed an increasing trend. Furthermore, comparing the proposed model with the baseline method, the results demonstrate that the proposed NRN-RSSEG anti-noise framework can effectively help the current segmentation model to overcome the adverse effects of noisy label training.",remote sensing image segmentation,noisy labels,deep learning,noise-robust network,"Yu, Minmin","Qin, Fen",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_618,"Xu, LeiLei","Shi, ShanQiu","Liu, YuJun",A large-scale remote sensing scene dataset construction for semantic segmentation,INTERNATIONAL JOURNAL OF IMAGE AND DATA FUSION,OCT 2 2023,5,"As fuelled by the advancement of deep learning for computer vision tasks, its application in other fields has been boosted. This technology has been increasingly applied to the interpretation of remote sensing image, showing high potential economic and societal significance, such as automatically mapping land cover. However, the model requires a considerable number of samples for training, and it is now adversely affected by the lack of a large-scale dataset. Moreover, labelling samples is a time-consuming and laborious task, and a complete land classification system suitable for deep learning has not been established. This limitation hinders the development and application of deep learning. To meet the data needs of deep learning in the field of remote sensing, this study develops JSsampleP, a large-scale dataset for segmentation, generating 110,170 data samples that cover various categories of scenes within Jiangsu Province, China. The existing Geographical Condition Dataset (GCD) and Basic Surveying and Mapping Dataset (BSMD) in Jiangsu were fully utilised, significantly reducing the cost of labelling samples. Furthermore, the samples were subject to a rigorous cleaning process to ensure data quality. Finally, the accuracy of the dataset is verified using the U-Net model, and the future version will be optimised continuously.",Deep learning,CNN,sample,segmentation,"Zhang, Hao","Wang, Dan","Zhang, Lu","Liang, Wan",remote sensing,,,,,,,,"Chen, Hao",,,,,,,,,,,,,,,,
Row_619,"Wang, Ming","She, Anqi","Chang, Hao",A deep inverse convolutional neural network-based semantic classification method for land cover remote sensing images,SCIENTIFIC REPORTS,MAR 27 2024,5,"The imbalance of land cover categories is a common problem. Some categories appear less frequently in the image, while others may occupy the vast majority of the proportion. This imbalance can lead the classifier to tend to predict categories with higher frequency of occurrence, while the recognition effect on minority categories is poor. In view of the difficulty of land cover remote sensing image multi-target semantic classification, a semantic classification method of land cover remote sensing image based on depth deconvolution neural network is proposed. In this method, the land cover remote sensing image semantic segmentation algorithm based on depth deconvolution neural network is used to segment the land cover remote sensing image with multi-target semantic segmentation; Four semantic features of color, texture, shape and size in land cover remote sensing image are extracted by using the semantic feature extraction method of remote sensing image based on improved sequential clustering algorithm; The classification and recognition method of remote sensing image semantic features based on random forest algorithm is adopted to classify and identify four semantic feature types of land cover remote sensing image, and realize the semantic classification of land cover remote sensing image. The experimental results show that after this method classifies the multi-target semantic types of land cover remote sensing images, the average values of Dice similarity coefficient and Hausdorff distance are 0.9877 and 0.9911 respectively, which can accurately classify the multi-target semantic types of land cover remote sensing images.",Deep inverse convolutional neural network,Land cover,Remote sensing images,Semantic classification,"Cheng, Feifei","Yang, Heming",,,Semantic segmentation,Feature extraction,,,,,,,,,,,,,,,,,,,,,,,
Row_620,"Zeng, Fuping","Yang, Bin","Zhao, Mengci",MASANet: Multi-Angle Self-Attention Network for Semantic Segmentation of Remote Sensing Images,TEHNICKI VJESNIK-TECHNICAL GAZETTE,OCT 2022,5,"As an important research direction in the field of pattern recognition, semantic segmentation has become an important method for remote sensing image information extraction. However, due to the loss of global context information, the effect of semantic segmentation is still incomplete or misclassified. In this paper, we propose a multi-angle self-attention network (MASANet) to solve this problem. Specifically, we design a multi-angle self-attention module to enhance global context information, which uses three angles to enhance features and takes the obtained three features as the inputs of self-attention to further extract the global dependencies of features. In addition, atrous spatial pyramid pooling (ASPP) and global average pooling (GAP) further improve the overall performance. Finally, we concatenate the feature maps of different scales obtained in the feature extraction stage with the corresponding feature maps output by ASPP to further extract multi-scale features. The experimental results show that MASANet achieves good segmentation performance on high-resolution remote sensing images. In addition, the comparative experimental results show that MASANet is superior to some state-of-the-art models in terms of some widely used evaluation criteria.",global context information,MASANet,multi -scale features,semantic segmentation,"Xing, Ying","Ma, Yiran",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_621,"Blaga, Bianca-Cerasela-Zelia","Nedevschi, Sergiu",,Semantic Segmentation of Remote Sensing Images With Transformer-Based U-Net and Guided Focal-Axial Attention,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"In the field of remote sensing, semantic segmentation of unmanned aerial vehicle (UAV) imagery is crucial for tasks such as land resource management, urban planning, precision agriculture, and economic assessment. Traditional methods use convolutional neural networks (CNNs) for hierarchical feature extraction but are limited by their local receptive fields, restricting comprehensive contextual understanding. To overcome these limitations, we propose a combination of transformer and attention mechanisms to improve object classification, leveraging their superior information modeling capabilities to enhance scene understanding. In this article, we present Swin-based focal axial attention network (SwinFAN), a U-Net framework featuring a Swin transformer as encoder, equipped with a novel decoder that introduces two new components for enhanced semantic segmentation of urban remote sensing images. The first proposed component is a guided focal-axial (GFA) attention module that combines local and global contextual information, enhancing the model's ability to discern intricate details and complex structures. The second component is an innovative attention-based feature refinement head (AFRH) designed to improve the precision and clarity of segmentation outputs through self-attention and convolutional techniques. Comprehensive experiments demonstrate that the accuracy of our proposed architecture significantly outperforms state-of-the-art models. More specifically, our method achieves mean intersection over union (mIoU) improvements of 1.9% on UAVid, 3.6% on Potsdam, 1.9% on Vaihingen, and 0.8% on LoveDA.",Transformers,Semantic segmentation,Remote sensing,Decoding,,,,,Computer architecture,Computational modeling,Head,Context modeling,Accuracy,Feature extraction,Deep learning,global-local attention,,,,,,remote sensing,semantic segmentation,transformer network,,,,,,,,,
Row_622,"Chen, Fenglei","Liu, Haijun","Zeng, Zhihong",BES-Net: Boundary Enhancing Semantic Context Network for High-Resolution Image Semantic Segmentation,REMOTE SENSING,APR 2022,23,"This paper focuses on the high-resolution (HR) remote sensing images semantic segmentation task, whose goal is to predict semantic labels in a pixel-wise manner. Due to the rich complexity and heterogeneity of information in HR remote sensing images, the ability to extract spatial details (boundary information) and semantic context information dominates the performance in segmentation. In this paper, based on the frequently used fully convolutional network framework, we propose a boundary enhancing semantic context network (BES-Net) to explicitly use the boundary to enhance semantic context extraction. BES-Net mainly consists of three modules: (1) a boundary extraction module for extracting the semantic boundary information, (2) a multi-scale semantic context fusion module for fusing semantic features containing objects with multiple scales, and (3) a boundary enhancing semantic context module for explicitly enhancing the fused semantic features with the extracted boundary information to improve the intra-class semantic consistency, especially in those pixels containing boundaries. Extensive experimental evaluations and comprehensive ablation studies on the ISPRS Vaihingen and Potsdam datasets demonstrate the effectiveness of BES-Net, yielding an overall improvement of 1.28/2.36/0.72 percent in mF1/mIoU/OA over FCN_8s when the BE and MSF modules are combined by the BES module. In particular, our BES-Net achieves a state-of-the-art performance of 91.4% OA on the ISPRS Vaihingen dataset and 92.9%/91.5% mF1/OA on the ISPRS Potsdam dataset.",remote sensing images,semantic segmentation,boundary enhancing semantic context,fully convolutional network,"Zhou, Xichuan","Tan, Xiaoheng",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_623,"Cui, Jian","Liu, Jiahang","Ni, Yue",FDGSNet: A Multimodal Gated Segmentation Network for Remote Sensing Image Based on Frequency Decomposition,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"Multiple modal data fusion can provide valuable and diverse information for remote sensing image segmentation. However, the existing fusion methods often lead to feature loss during the fusion of various modal data, and the complementarity among multimodal features is insufficient. To address these problems, we propose a multimodal gated segmentation network for remote sensing images based on the frequency decomposition. Complementary information from multimodal features is extracted by establishing a long-distance correlation between the low-frequency components of different modal data. In addition, high-frequency detailed features of different modal data are preserved by residual connection. The adaptive gated fusion method is then used to control the information flow between the complementary information and each modality feature map, enabling adaptive fusion between multimodal features. These operations can effectively improve the adaptability of the proposed method in various scenarios and data changes. Extensive experiments demonstrate that the proposed method has good effectiveness, robustness, and generalization and achieved state-of-the-art performance in several remote sensing image semantic segmentation tasks.",Feature extraction,Remote sensing,Semantic segmentation,Semantics,"Wang, Jinjin","Li, Manchun",,,Data mining,Accuracy,Transformers,Logic gates,Vegetation mapping,Sensors,Frequency-domain decomposition,multimodal,,,,,,remote sensing,semantic segmentation,,,,,,,,,,
Row_624,"Li, Bingnan","Gao, Jiuchong","Chen, Shuiping",POI Detection of High-Rise Buildings Using Remote Sensing Images: A Semantic Segmentation Method Based on Multitask Attention Res-U-Net,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,14,"A point-of-interest (POI) represents a specific point location that may be useful or interesting for people, and therefore, each and every building footprint in a topographic map can be recognized as a POI. Automatic extraction of building footprints using remote sensing images has become a challenging and important research topic, which is in demand for urban planning and development. Extensive studies have explored a variety of semantic segmentation methods using deep learning algorithms to achieve better performance in building footprint extraction; however, the existing algorithms were shown to have some limitations, which lead to poor segmentation results. Building roofs were recognized as building footprints in the previous studies. This is prone to error especially for high-rise buildings due to different sensor view angles. In this article, we propose a multitask Res-U-Net model with an attention mechanism for the extraction of the building roofs and the whole building shapes from remote sensing images and then use an offset vector method to detect the footprints of the high-rise buildings based on the boundaries of the corresponding building roofs and shapes. We also apply the online food delivery (OFD) data to parse the POI name of every building footprint. Several strategies are also developed in combination with the proposed model, including data augmentation and postprocessing. We conduct numerical experiments using real data of remote sensing images and OFD historical order data. Results demonstrate that our proposed model achieves a total F1-score of 77.05% and an intersection over union (IoU) of 63.55% in terms of the building roof segmentation, and an overall F1-score of 79.02% and an IoU of 66.05% for the whole building shape segmentation, which both achieve the best performance among all baseline models.",Buildings,Remote sensing,Shape,Image segmentation,"Lim, Samsung","Jiang, Hai",,,Logic gates,Global Positioning System,Multitasking,Attention mechanism,building footprint extraction,online food delivery (OFD),remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,
Row_625,"Lin, Yuting","Suzuki, Kumiko","Sogo, Shinichiro",Practical Techniques for Vision-Language Segmentation Model in Remote Sensing,,2024,0,"Traditional semantic segmentation models often struggle with poor generalizability in zero-shot scenarios such as recognizing attributes unseen in the training labels. On the other hands, language-vision models (VLMs) have shown promise in improving performance on zero-shot tasks by leveraging semantic information from textual inputs and fusing this information with visual features. However, existing VLM-based methods do not perform as effectively on remote sensing data due to the lack of such data in their training datasets. In this paper, we introduce a two-stage fine-tuning approach for a VLM-based segmentation model using a large remote sensing image-caption dataset, which we created using an existing image-caption model. Additionally, we propose a modified decoder and a visual prompt technique using a saliency map to enhance segmentation results. Through these methods, we achieve superior segmentation performance on remote sensing data, demonstrating the effectiveness of our approach.",Segmentation of Remote Sensing Data,Vision-Language Model,Fine-tuning,Visual Prompting,,,,,,,,,,,,,,"MID-TERM SYMPOSIUM THE ROLE OF PHOTOGRAMMETRY FOR A SUSTAINABLE WORLD, VOL. 48-2",,,,,,,,,,,,,,,
Row_626,"Li, Rui","Wang, Libo","Zhang, Ce",A2-FPN for semantic segmentation of fine-resolution remotely sensed images,INTERNATIONAL JOURNAL OF REMOTE SENSING,FEB 1 2022,74,"The thriving development of earth observation technology makes more and more high-resolution remote-sensing images easy to obtain. However, caused by fine-resolution, the huge spatial and spectral complexity leads to the automation of semantic segmentation becoming a challenging task. Addressing such an issue represents an exciting research field, which paves the way for scene-level landscape pattern analysis and decision-making. To tackle this problem, we propose an approach for automatic land segmentation based on the Feature Pyramid Network (FPN). As a classic architecture, FPN can build a feature pyramid with high-level semantics throughout. However, intrinsic defects in feature extraction and fusion hinder FPN from further aggregating more discriminative features. Hence, we propose an Attention Aggregation Module (AAM) to enhance multiscale feature learning through attention-guided feature aggregation. Based on FPN and AAM, a novel framework named Attention Aggregation Feature Pyramid Network (A(2)-FPN) is developed for semantic segmentation of fine-resolution remotely sensed images. Extensive experiments conducted on four datasets demonstrate the effectiveness of our A(2)-FPN in segmentation accuracy. Code is available at .",semantic segmentation,deep learning,attention mechanism,,"Duan, Chenxi","Zheng, Shunyi",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_627,"Qu, Tingting","Xu, Jindong","Chong, Qianpeng",Fuzzy neighbourhood neural network for high-resolution remote sensing image segmentation,EUROPEAN JOURNAL OF REMOTE SENSING,DEC 31 2023,9,"Remote sensing image segmentation plays an important role in many industrial-grade image processing applications. However, the problem of uncertainty caused by intraclass heterogeneity and interclass blurring is prevalent in high-resolution remote sensing images. Moreover, the complexity of information in high-resolution remote sensing images leads to a large amount of background information around objects. To solve this problem, a new fuzzy convolutional neural network is proposed in this paper. This network resolves the ambiguity and uncertainty of feature information by introducing a fuzzy neighbourhood module in the deep learning network structure. In addition, it adds a multi-attention gating module to highlight small object features and separate them from the complex background information to achieve fine segmentation of high-resolution remote sensing images. Experimental results on three different segmentation datasets suggest that the proposed method has higher segmentation accuracy and better performance than other deep learning networks, especially for complicated shadow information.",Fuzzy neighbourhood,high-resolution remote sensing image,image segmentation,multi-attention gating,"Liu, Zhaowei","Yan, Weiqing","Wang, Xuan","Song, Yongchao",,,,,,,,,"Ni, Mengying",,,,,,,,,,,,,,,,
Row_628,"Long, Jiang","Li, Mengmeng","Wang, Xiaoqin",Semantic change detection using a hierarchical semantic graph interaction network from high-resolution remote sensing images,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,MAY 2024,10,"Current semantic change detection (SCD) methods face challenges in modeling temporal correlations (TCs) between bitemporal semantic features and difference features. These methods lead to inaccurate detection results, particularly for complex SCD scenarios. This paper presents a hierarchical semantic graph interaction network (HGINet) for SCD from high-resolution remote sensing images. This multitask neural network combines semantic segmentation and change detection tasks. For semantic segmentation, we construct a multilevel perceptual aggregation network with a pyramidal architecture. It extracts semantic features that discriminate between different categories at multiple levels. We model the correlations between bitemporal semantic features using a TC module that enhances the identification of unchanged areas. For change detection, we design a semantic difference interaction module based on a graph convolutional network. It measures the interactions among bitemporal semantic features, their corresponding difference features, and the combination of both. Extensive experiments on four datasets, namely SECOND, HRSCD, Fuzhou, and Xiamen, show that HGINet performs better in identifying changed areas and categories across various scenarios and regions than nine existing methods. Compared with the existing methods applied on the four datasets, it achieves the highest F1scd values of 59.48%, 64.12%, 64.45%, and 84.93%, and SeK values of 19.34%, 14.55%, 18.28%, and 51.12%, respectively. Moreover, HGINet mitigates the influence of fake changes caused by seasonal effects, producing results with well-delineated boundaries and shapes. Furthermore, HGINet trained on the Fuzhou dataset is successfully transferred to the Xiamen dataset, demonstrating its effectiveness and robustness in identifying changed areas and categories from high-resolution remote sensing images. The code of our paper is accessible at https://github.com/long123524/HGINet-torch.",Semantic change detection,Hierarchical semantic graph interaction network,High-resolution remote sensing images,Temporal correlations,"Stein, Alfred",,,,Semantic difference interaction,,,,,,,,,,,,,,,,,,,,,,,,
Row_629,"Song, Zhenbo","Zhang, Zhenyuan","Fang, Feiyi",Deep semantic-aware remote sensing image deblurring,SIGNAL PROCESSING,OCT 2023,7,"This paper addresses the problem of blind deblurring of single remote sensing (RS) images with deep neural networks. Most existing deep learning-based methods are migrated from natural image deblurring models, disregarding the domain gap to remote sensing images. Besides, the image deblurring problem is typically considered as an independent low-level image pre-processing, taking no account of downstream tasks, such as classification and segmentation. In this paper, we first present a novel decoder with a par-allel fusion stream for fusing multi-scale RS features and expanding the receptive field. Then, to generate high-quality sharp RS images, we propose to calculate the perceptual loss on an RS-pre-trained network instead of computing on VGG19 pre-trained on natural images i.e. ImageNet. For bridging the RS im-age deblurring results to the downstream recognition tasks, we further propose a semantic loss, which is calculated on the last-layer feature map of an RS segmentation network. With extensive experiments con-ducted on public RS image datasets, we demonstrate that the proposed method improves results for RS image deblurring and achieves competitive performance both qualitatively and quantitatively. Moreover, downstream recognition experiments validate the superior quality of the recovered images over existing methods.& COPY; 2023 Elsevier B.V. All rights reserved.",Remote sensing image deblurring,Deep learning,Semantic supervision,,"Fan, Zhaoxin","Lu, Jianfeng",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_630,"Wang, Bing","Wang, Zhirui","Sun, Xian",DMML-Net: Deep Metametric Learning for Few-Shot Geographic Object Segmentation in Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,25,"Geographic object segmentation is a fundamental yet challenging problem for remote sensing image interpretation. The prevalent paradigm to solve this problem is to train deep neural networks on massive labeled samples. Although remarkable achievements have been attained, these methods suffer from the severe dependence on the large-scale dataset and require a long training process with high computation burden. To address these issues, a deep metametric learning framework, named DMML-Net, consisting of the metametric learner and the base-metric learner, is proposed for few-shot geographic object segmentation. First, DMML-Net formulates the segmentation as the metric-based pixel classification and develops a deep feature pyramid comparison network as the architecture of the metric learner for multiscale metric learning. Benefiting from this design, the segmentation can be efficiently solved, as well as being robust to deal with the scale variations of geographic objects. Second, an affinity-based fusion mechanism is introduced to adaptively reweight and fuse the semantic information across samples, effectively calibrating the deviation of prototypes induced by the intraclass variations. Third, considering the impact of the large interclass distribution divergences, DMML-Net presents a metametric training paradigm to provide the metric model with flexible scalability for fast adaptation to novel tasks. After metatraining, DMML-Net can be applied for the few-shot segmentation tasks of novel geographic objects with only a few gradient steps on the small training set. Experimental results on two benchmark remote sensing datasets demonstrate the validity and the superiority of our method in low-shot conditions where there are only one to ten labeled samples.",Task analysis,Measurement,Remote sensing,Training,"Wang, Hongqi","Fu, Kun",,,Semantics,Object segmentation,Image segmentation,Few-shot learning,few-shot segmentation,metalearning,metric learning,semantic segmentation,,,,,,,,,,,,,,,,,
Row_631,"Deng, Guohui","Wu, Zhaocong","Wang, Chengjun",CCANet: Class-Constraint Coarse-to-Fine Attentional Deep Network for Subdecimeter Aerial Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,46,"Semantic segmentation is important for the understanding of subdecimeter aerial images. In recent years, deep convolutional neural networks (DCNNs) have been used widely for semantic segmentation in the field of remote sensing. However, because of the highly complex subdecimeter resolution of aerial images, inseparability often occurs among some geographic entities of interest in the spectral domain. In addition, the semantic segmentation methods based on DCNNs mostly obtain context information using extra information within the added receptive field. However, the context information obtained this way is not explicit. We propose a novel class-constraint coarse-to-fine attentional (CCA) deep network, which enables the formation of class information constraints to obtain explicit long-range context information. Further, the performance of subdecimeter aerial image semantic segmentation can be improved, particularly for fine-structured geographic entities. Based on coarse-to-fine technology, we obtained a coarse segmentation result and constructed an image class feature library. We propose the use of the attention mechanism to obtain strong class-constrained features. Consequently, pixels of different geographic entities can adaptively match the corresponding categories in the class feature library. Additionally, we employed a novel loss function, CCA-loss to realize end-to-end training. The experimental results obtained using two popular open benchmarks, International Society for Photogrammetry and Remote Sensing (ISPRS) 2-D semantic labeling Vaihingen data set and Institute of Electrical and Electronics Engineers (IEEE) Geoscience and Remote Sensing Society (GRSS) Data Fusion Contest Zeebrugge data set, validated the effectiveness and superiority of our proposed model. The proposed method achieved state-of-the-art performance on the IEEE GRSS Data Fusion Contest Zeebrugge data set.",Image segmentation,Semantics,Remote sensing,Libraries,"Xu, Miaozhong","Zhong, Yanfei",,,Feature extraction,Automobiles,Training,Aerial imagery,context information,deep convolutional neural network (DCNN),remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,
Row_632,"Yu, Dawen","Ji, Shunping",,Long-Range Correlation Supervision for Land-Cover Classification From Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,0,"Long-range dependency modeling has been widely considered in modern deep learning-based semantic segmentation methods, especially those designed for large-size remote sensing images, to compensate the intrinsic locality of standard convolutions. However, in previous studies, the long-range dependency, modeled with an attention mechanism or transformer model, has been based on unsupervised learning, instead of explicit supervision from the objective ground truth (GT). In this article, we propose a novel supervised long-range correlation method for land-cover classification, called the supervised long-range correlation network (SLCNet), which is shown to be superior to the currently used unsupervised strategies. In SLCNet, pixels sharing the same category are considered highly correlated and those having different categories are less relevant, which can be easily supervised by the category consistency information available in the GT semantic segmentation map. Under such supervision, the recalibrated features are more consistent for pixels of the same category and more discriminative for pixels of other categories, regardless of their proximity. To complement the detailed information lacking in the global long-range correlation, we introduce an auxiliary adaptive receptive field feature extraction (ARFE) module, parallel to the long-range correlation module in the encoder, to capture finely detailed feature representations for multisize objects in multiscale remote sensing images. In addition, we apply multiscale side-output supervision and a hybrid loss function as local and global constraints to further boost the segmentation accuracy. Experiments were conducted on three public remote sensing datasets (the ISPRS Vaihingen dataset, the ISPRS Potsdam dataset, and the DeepGlobe dataset). Compared with the advanced segmentation methods from the computer vision, medicine, and remote sensing communities, the proposed SLCNet method achieved state-of-the-art performance on all the datasets. The code will be made available at gpcv.whu.edu.cn/data.",Correlation,Transformers,Remote sensing,Feature extraction,,,,,Semantics,Semantic segmentation,Computational modeling,Convolutional neural network (CNN),land-cover classification,long-range correlation supervision,remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,,
Row_633,"Yang, Yunsong","Yuan, Genji","Li, Jinjiang",Correlated Mapping Attention Cooperative Network for Urban Remote Sensing Image Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"In current remote sensing segmentation tasks, the difficulty of segmenting spectrally similar objects is a significant issue. Solving this problem is crucial for improving segmentation accuracy. Traditional image-domain segmentation methods rely on color and texture features, but spectrally similar objects have negligible color differences, leading to suboptimal segmentation results. To address this, we propose a network framework called Correlated Mapping Attention Cooperative Network (CMACNet) by extending the problem from the image domain to the feature domain. Image-domain methods depend on color and texture features, whereas feature-domain methods process higher-level abstract features, avoiding issues caused by color similarity. Specifically, CMACNet first employs an autoencoder structure. The autoencoder compresses the input data and attempts to reconstruct the original data, ensuring that the latent space representations capture essential and representative features of the input data, thereby extracting highly generalized and versatile features. Next, we introduce the correlated mapping attention mechanism, which adaptively adjusts the attention to different features based on their correlations, effectively addressing the challenge of segmenting spectrally similar objects. Furthermore, to efficiently establish global relationships among features, we design a cross global interaction layer for global feature remapping. Comprehensive experiments on the Vaihingen and Potsdam datasets demonstrate that CMACNet outperforms existing state-of-the-art methods, achieving mean intersection over union scores of 84.77% and 87.69%, respectively.",Attention mechanism,global modeling,remote sensing (RS),semantic segmentation,,,,,transformer,Attention mechanism,global modeling,remote sensing (RS),semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,
Row_634,"Ajibola, Segun","Cabral, Pedro",,A Systematic Literature Review and Bibliometric Analysis of Semantic Segmentation Models in Land Cover Mapping,REMOTE SENSING,JUN 2024,2,"Recent advancements in deep learning have spurred the development of numerous novel semantic segmentation models for land cover mapping, showcasing exceptional performance in delineating precise boundaries and producing highly accurate land cover maps. However, to date, no systematic literature review has comprehensively examined semantic segmentation models in the context of land cover mapping. This paper addresses this gap by synthesizing recent advancements in semantic segmentation models for land cover mapping from 2017 to 2023, drawing insights on trends, data sources, model structures, and performance metrics based on a review of 106 articles. Our analysis identifies top journals in the field, including MDPI Remote Sensing, IEEE Journal of Selected Topics in Earth Science, and IEEE Transactions on Geoscience and Remote Sensing, IEEE Geoscience and Remote Sensing Letters, and ISPRS Journal Of Photogrammetry And Remote Sensing. We find that research predominantly focuses on land cover, urban areas, precision agriculture, environment, coastal areas, and forests. Geographically, 35.29% of the study areas are located in China, followed by the USA (11.76%), France (5.88%), Spain (4%), and others. Sentinel-2, Sentinel-1, and Landsat satellites emerge as the most used data sources. Benchmark datasets such as ISPRS Vaihingen and Potsdam, LandCover.ai, DeepGlobe, and GID datasets are frequently employed. Model architectures predominantly utilize encoder-decoder and hybrid convolutional neural network-based structures because of their impressive performances, with limited adoption of transformer-based architectures due to its computational complexity issue and slow convergence speed. Lastly, this paper highlights existing key research gaps in the field to guide future research directions.",remote sensing,semantic segmentation,land cover mapping,deep learning,,,,,land cover classification,,,,,,,,,,,,,,,,,,,,,,,,
Row_635,"Pastorino, Martina","Moser, Gabriele","Serpico, Sebastiano B.",Semantic Segmentation of Remote-Sensing Images Through Fully Convolutional Neural Networks and Hierarchical Probabilistic Graphical Models,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,20,"Deep learning (DL) is currently the dominant approach to image classification and segmentation, but the performances of DL methods are remarkably influenced by the quantity and quality of the ground truth (GT) used for training. In this article, a DL method is presented to deal with the semantic segmentation of very-high-resolution (VHR) remote-sensing data in the case of scarce GT. The main idea is to combine a specific type of deep convolutional neural networks (CNNs), namely fully convolutional networks (FCNs), with probabilistic graphical models (PGMs). Our method takes advantage of the intrinsic multiscale behavior of FCNs to deal with multiscale data representations and to connect them to a hierarchical Markov model (e.g., making use of a quadtree). As a consequence, the spatial information present in the data is better exploited, allowing a reduced sensitivity to GT incompleteness to be obtained. The marginal posterior mode (MPM) criterion is used for inference in the proposed framework. To assess the capabilities of the proposed method, the experimental validation is conducted with the ISPRS 2D Semantic Labeling Challenge datasets on the cities of Vaihingen and Potsdam, with some modifications to simulate the spatially sparse GTs that are common in real remote-sensing applications. The results are quite significant, as the proposed approach exhibits a higher producer accuracy than the standard FCNs considered and especially mitigates the impact of scarce GTs on minority classes and small spatial details.",Semantics,Markov processes,Spatial resolution,Remote sensing,"Zerubia, Josiane",,,,Image segmentation,Training,Deep learning,Convolutional neural network (CNN),fully convolutional network (FCN),hierarchical Markov model,multiscale analysis,probabilistic graphical model (PGM),,,,,,remote sensing,semantic segmentation,,,,,,,,,,
Row_636,"Zhang, Bo","Chen, Tao","Wang, Bin",Curriculum-Style Local-to-Global Adaptation for Cross-Domain Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,36,"Although domain adaptation has been extensively studied in natural image-based segmentation tasks, the research on cross-domain segmentation for very-high-resolution (VHR) remote sensing images (RSIs) still remains underexplored. The VHR RSI-based cross-domain segmentation mainly faces two critical challenges: 1) large area land covers with many diverse object categories bring severe local patch-level data distribution deviations, thus yielding different adaptation difficulties for different local patches and 2) different VHR sensor types or dynamically changing modes cause the VHR images to go through intensive data distribution differences even for the same geographical location, resulting in different global feature-level domain gaps. To address these challenges, we propose a curriculum-style local-to-global cross-domain adaptation framework for the segmentation of VHR RSIs. The proposed curriculum-style adaptation performs the adaptation process in an easy-to-hard way according to the adaptation difficulties that can be obtained using an entropy-based score for each patch of the target domain and, thus, well aligns the local patches in a domain image. The proposed local-to-global adaptation performs the feature alignment process from the locally semantic to globally structural feature discrepancies and consists of a semantic-level domain classifier and an entropy-level domain classifier that can reduce the above cross-domain feature discrepancies. Extensive experiments have been conducted in various cross-domain scenarios, including geographic location variations and imaging mode variations, and the experimental results demonstrate that the proposed method can significantly boost the domain adaptability of segmentation networks for VHR RSIs.",Image segmentation,Semantics,Imaging,Adaptation models,,,,,Task analysis,Feature extraction,Remote sensing,Curriculum-style cross-domain adaptation (CCDA),local-to-global cross-domain adaptation,remote sensing images (RSIs),semantic segmentation,,,,,,,,,,,,,,,,,,
Row_637,"Xu, Yufen","Zhou, Shangbo","Huang, Yuhui",Transformer-Based Model with Dynamic Attention Pyramid Head for Semantic Segmentation of VHR Remote Sensing Imagery,ENTROPY,NOV 2022,4,"Convolutional neural networks have long dominated semantic segmentation of very-high-resolution (VHR) remote sensing (RS) images. However, restricted by the fixed receptive field of convolution operation, convolution-based models cannot directly obtain contextual information. Meanwhile, Swin Transformer possesses great potential in modeling long-range dependencies. Nevertheless, Swin Transformer breaks images into patches that are single-dimension sequences without considering the position loss problem inside patches. Therefore, Inspired by Swin Transformer and Unet, we propose SUD-Net (Swin transformer-based Unet-like with Dynamic attention pyramid head Network), a new U-shaped architecture composed of Swin Transformer blocks and convolution layers simultaneously through a dual encoder and an upsampling decoder with a Dynamic Attention Pyramid Head (DAPH) attached to the backbone. First, we propose a dual encoder structure combining Swin Transformer blocks and reslayers in reverse order to complement global semantics with detailed representations. Second, aiming at the spatial loss problem inside each patch, we design a Multi-Path Fusion Model (MPFM) with specially devised Patch Attention (PA) to encode position information of patches and adaptively fuse features of different scales through attention mechanisms. Third, a Dynamic Attention Pyramid Head is constructed with deformable convolution to dynamically aggregate effective and important semantic information. SUD-Net achieves exceptional results on ISPRS Potsdam and Vaihingen datasets with 92.51%mF1, 86.4%mIoU, 92.98%OA, 89.49%mF1, 81.26%mIoU, and 90.95%OA, respectively.",swin transformer,remote sensing,semantic segmentation,dynamic attention pyramid head,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_638,Cai Mi,Cui Yaqi,Lv Yafei,A New Network Structure for Semantic Segmentation of Ship Targets in Remote Sensing,,2019,0,"Accurate detection of ship targets is a research hotspot in computer vision. Most of the researches have achieved instance-level detection in the way of bounding box. But we intend to achieve more accurate detection of ship targets in pixel-level through semantic segmentation. However, there are still two main challenges: the first one is the difficulty to segment small targets caused by the difference among ship targets' scales, and the other one is the lack of localization information caused by insufficient recovery ability of the decoder part. In this paper, we propose an effective solution. First, a multi-scale pooling fusion module is proposed to fuse multi-scale feature maps and acquire more multi-scale context information, then we improve the capability of precise decoding by taking the place of convolution operation with deconvolution in the decoder part to gather more localization information. At last, we integrate above two schemes into an encoder-decoder symmetry training network with less training parameters and less training time. Furthermore, we construct a dataset for ship semantic segmentation called HRSC2016-SS by labeling HRSC2016 dataset to evaluate our solution. Experiments show that comparing with the existing methods, our proposed solution has a better performance.",HRSC2016-SS,multi-scale,deconvolution,remote sensing,Zhang Jing,Xiong Wei,Pei Jiazheng,,semantic segmentation,,,,,,,,,2019 22ND INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2019),,,,,,,,,,,,,,,
Row_639,"Gaetano, Raffaele","Scarpa, Giuseppe","Poggi, Giovanni",ADVANCES IN TEXTURE-BASED SEGMENTATION OF HIGH RESOLUTION REMOTE SENSING IMAGERY,,2009,0,"The Texture Fragmentation and Reconstruction (TFR) algorithm, recently proposed for the segmentation of textured images, has been applied with promising results to high-resolution remote-sensing images The algorithm provides a sequence of nested segmentation maps which allow the analysis at various scales of observation. Although for most test images TFR has proven able to recognize major semantic areas, some failures have also been observed due to the presence of large background regions that span the whole image and prevent the formation of distinct local texturesIn this paper we introduce a new step in the TFR processing flow which detects background regions and divides them in multiple homogeneous fragments based on their geometric level properties To this end, connected regions are first reduced to atomic components through a watershed-like transform, and then clustered again based on the features of the associated region-adjacency graph.Early experimental results prove the effectiveness of the new processing step, and its beneficial effect on the whole algorithm.",Remote-sensing images,segmentation,texture,region-adjacency-graph,,,,,,,,,,,,,,"2009 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM, VOLS 1-5",,,,,,,,,,,,,,,
Row_640,"Ahlswede, Steve","Madam, Nimisha Thekke","Schulz, Christian",WEAKLY SUPERVISED SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES FOR TREE SPECIES CLASSIFICATION BASED ON EXPLANATION METHODS,,2022,1,"The collection of a high number of pixel-based labeled training samples for tree species identification is time consuming and costly in operational forestry applications. To address this problem, in this paper we investigate the effectiveness of explanation methods for deep neural networks in performing weakly supervised semantic segmentation using only image-level labels. Specifically, we consider four methods: i) class activation maps (CAM); ii) gradient-based CAM; iii) pixel correlation module; and iv) self-enhancing maps (SEM). We compare these methods with each other using both quantitative and qualitative measures of their segmentation accuracy, as well as their computational requirements. Experimental results obtained on an aerial image archive show that: i) considered explanation techniques are highly relevant for the identification of tree species with weak supervision; and ii) the SEM outperforms the other considered methods. The code for this paper is publicly available at https://git.tu-berlin.de/rsim/rs_wsss.",Tree species mapping,weakly supervised learning,semantic segmentation,explanation methods,"Kleinschmit, Birgit","Demir, Begum",,,remote sensing,,,,,,,,,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),,,,,,,,,,,,,,,
Row_641,"Jia, Peiyan","Chen, Chen","Zhang, Delong",Semantic segmentation of deep learning remote sensing images based on band combination principle: Application in urban planning and land use,COMPUTER COMMUNICATIONS,MAR 1 2024,7,"This study investigates the relevance of semantic segmentation of remote sensing images in urban planning and land use. We introduce a novel deep learning model that leverages the principle of band combination in remote sensing images to enhance the efficiency and accuracy of semantic segmentation. Our research focuses not only on advancing the segmentation capabilities of remote sensing images but also on applying this technology in urban planning and land use to foster sustainable development in smart cities. By integrating the band combination principle into the convolution operation, our approach improves feature extraction, thereby enhancing the quality of semantic segmentation in remote sensing images. This method outperforms traditional remote sensing image analysis techniques by combining automatic feature learning and the generalization capabilities of deep learning, thereby improving the segmentation model's performance. A unique aspect of this study is the direct application of remote sensing image segmentation in urban planning and land use. Our model accurately identifies various land uses such as residential, commercial, and industrial areas, and tracks land-use change trends, aiding urban planners in future development planning. Compared to conventional methods, our model significantly reduces training time and increases computational efficiency under identical training conditions. Experimental comparisons and analyses reveal that, within the same training duration, our model's accuracy surpasses that of similar models by 10%-15%. On the ISPRS dataset, our model achieved a segmentation accuracy of 82.43% for building surfaces, and 76.54% for trees. In scenarios with relatively uniform reflective surfaces, our model outperforms similar models by approximately 10%.",Semantic segmentation,Band combination,CNN,,"Sang, Yulong","Zhang, Lei",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_642,"Zhang, Chenchen","Wang, Rongfang","Chen, Jia-Wei",A MULTI-BRANCH U-NET FOR WATER AREA SEGMENTATION WITH MULTI-MODALITY REMOTE SENSING IMAGES,,2023,1,"Water area segmentation in remote sensing images is of great importance for flood monitoring. Convolutional neural networks have been successfully applied to various computer vision tasks. Among them, a U-shaped CNN known as U-Net achieves state-of-the-art performance on various types of image segmentation, including remote sensing images. However, there are still some difficulties in the water area segmentation of remote sensing images, such as complex backgrounds, cloud shading, and rough edges. In this work, we propose a multi-branch fusion U-Net (MFU-Net) method for water area segmentation with multi-modality remote sensing images. The experimental results showed that our MFU-Net can effectively and efficiently segment water area from Sentinel-1 and Sentinel-2 images, which F1, IoU and PA on the Sen1Floods11 dataset are 91.462%, 84.598% and 98.123%, respectively.",water area semantic segmentation,multi-modality fusion,multi-branch encoder,channel attention,"Li, Weibin","Huo, Chunlei","Niu, Yi",,remote sensing,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_643,"Cai, Yuanzhi","Fan, Lei","Fang, Yuan",SBSS: Stacking-Based Semantic Segmentation Framework for Very High-Resolution Remote Sensing Image,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,12,"Semantic segmentation of very high-resolution (VHR) remote sensing images is a fundamental task for many applications. However, large variations in the scales of objects in those VHR images pose a challenge for performing accurate semantic segmentation. Existing semantic segmentation networks are able to analyze an input image at up to four resizing scales, but this may be insufficient given the diversity of object scales. Therefore, multiscale (MS) test-time data augmentation is often used in practice to obtain more accurate segmentation results, which makes equal use of the segmentation results obtained at the different resizing scales. However, it was found in this study that different classes of objects had their preferred resizing scale for more accurate semantic segmentation. Based on this behavior, a stacking-based semantic segmentation (SBSS) framework is proposed to improve the segmentation results by learning this behavior, which contains a learnable error correction module (ECM) for segmentation result fusion and an error correction scheme (ECS) for computational complexity control. Two ECS, i.e., ECS-MS and ECS-single-scale (SS), are proposed and investigated in this study. The floating-point operations (Flops) required for ECS-MS and ECS-SS are similar to the commonly used MS test and the SS test, respectively. Extensive experiments on four datasets (i.e., Cityscapes, UAVid, LoveDA, and Potsdam) show that SBSS is an effective and flexible framework. It achieved higher accuracy than MS when using ECS-MS, and similar accuracy as SS with a quarter of the memory footprint when using ECS-SS.",Error correction,Feature extraction,Semantic segmentation,Spatial resolution,,,,,Decoding,Bagging,Task analysis,Convolutional neural network,deep learning,ensemble learning,semantic segmentation,stacking,,,,,,,,,,,,,,,,,
Row_644,"Cheng, Dongcai","Meng, Gaofeng","Xiang, Shiming",FusionNet: Edge Aware Deep Convolutional Networks for Semantic Segmentation of Remote Sensing Harbor Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,DEC 2017,96,"Sea-land segmentation and ship detection are two prevalent research domains for optical remote sensing harbor images and can find many applications in harbor supervision and management. As the spatial resolution of imaging technology improves, traditional methods struggle to perform well due to the complicated appearance and background distributions. In this paper, we unify the above two tasks into a single framework and apply the deep convolutional neural networks to predict pixelwise label for an input. Specifically, an edge aware convolutional network is proposed to parse a remote sensing harbor image into three typical objects, e. g., sea, land, and ship. Two innovations are made on top of the deep structure. First, we design a multitask model by simultaneously training the segmentation and edge detection networks. Hierarchical semantic features fromthe segmentation network are extracted to learn the edge network. Second, the outputs of edge pipeline are further employed to refine entire model by adding an edge aware regularization, which helps our method to yield very desirable results that are spatially consistent and well boundary located. It also benefits the segmentation of docked ships that are quite challenging for many previous methods. Experimental results on two datasets collected fromGoogleEarth have demonstrated the effectiveness of our approach both in quantitative and qualitative performance compared with state-of-the-art methods.",Edge aware regularization,harbor images,multitask learning,semantic segmentation,"Pan, Chunhong",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_645,"Yang, Zhujun","Yan, Zhiyuan","Diao, Wenhui",Active Bidirectional Self-Training Network for Cross-Domain Segmentation in Remote-Sensing Images,REMOTE SENSING,JUL 2024,1,"Semantic segmentation with cross-domain adaptation in remote-sensing images (RSIs) is crucial and mitigates the expense of manually labeling target data. However, the performance of existing unsupervised domain adaptation (UDA) methods is still significantly impacted by domain bias, leading to a considerable gap compared to supervised trained models. To address this, our work focuses on semi-supervised domain adaptation, selecting a small subset of target annotations through active learning (AL) that maximize information to improve domain adaptation. Overall, we propose a novel active bidirectional self-training network (ABSNet) for cross-domain semantic segmentation in RSIs. ABSNet consists of two sub-stages: a multi-prototype active region selection (MARS) stage and a source-weighted class-balanced self-training (SCBS) stage. The MARS approach captures the diversity in labeled source data by introducing multi-prototype density estimation based on Gaussian mixture models. We then measure inter-domain similarity to select complementary and representative target samples. Through fine-tuning with the selected active samples, we propose an enhanced self-training strategy SCBS, designed for weighted training on source data, aiming to avoid the negative effects of interfering samples. We conduct extensive experiments on the LoveDA and ISPRS datasets to validate the superiority of our method over existing state-of-the-art domain-adaptive semantic segmentation methods.",semantic segmentation,domain adaptation,active learning,self-training network,"Ma, Yihang","Li, Xinming","Sun, Xian",,remote-sensing images,,,,,,,,,,,,,,,,,,,,,,,,
Row_646,Jian Yongsheng,Zhu Daming,Fu Zhitao,Remote Sensing Image Segmentation Network Based on Multi-Level Feature Refinement and Fusion,LASER & OPTOELECTRONICS PROGRESS,FEB 2023,1,"To accurately segment ground objects from a high-resolution remote sensing image, we propose a remote sensing image segmentation network based on multi-level feature optimization fusion that focuses on the fusion of feature maps at different levels in the feature extraction skeleton network, performs reasonable and effective extractions, and analyzes output feature map information by fusing different types of information in the network feature map. Simultaneously, layer-by-layer multi-scale coding and decoding modules are used to refine the shallow feature map that merges with the high-level feature map, and the different types of information are optimized to the high-level feature map. The hollow convolution pyramid is then used to extract the information of different receptive fields on the high-level feature map, and the output feature map of semantic segmentation is optimized. When conducting experiments on the ISPRS Vaihingen dataset, the overall accuracy of the proposed network reaches 90.34%, which effectively improves the accuracy of remote sensing image target detection when compared with the classical semantic segmentation network. Moreover, to prove the generalization of the proposed algorithm, a generalization experiment on the ISPRS Potsdam dataset is conducted; the overall accuracy of this algorithm reaches 91.47%, proving its effectiveness.",remote sensing,semantic segmentation,multi-scale encoding and decoding,feature fusion,Wen Shiya,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_647,"Geiss, Christian","Zhu, Yue","Qiu, Chunping",Deep Relearning in the Geospatial Domain for Semantic Remote Sensing Image Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,8,"We present a classification postprocessing (CPP) technique based on fully convolutional neural networks (CNNs) for semantic remote sensing image segmentation. Conventional CPP techniques aim to enhance the classification accuracy by imposing smoothness priors in the image domain. Contrary to that, here, a relearning strategy is proposed where the initial classification outcome of a CNN model is provided to a subsequent CNN model via an extended input space to guide the learning of discriminative feature representations in an end-to-end fashion. This deep relearning CNN (DRCNN) explicitly accounts for the geospatial domain by taking the spatial alignment of preliminary class labels into account. Hereby, we evaluate to learn the DRCNN in a cumulative and noncumulative way, i.e., extending the input space based on all previous or solely preceding model outputs, respectively, during an iterative procedure. Besides, the DRCNN can also be conveniently coupled with alternative CPP techniques such as object-based voting (OBV). The experimental results obtained from two test sites of WorldView-II imagery underline the beneficial performance properties of the DRCNN models. They can increase the accuracies of the initial CNN models on average from 72.64x0025; to 76.01x0025; and from 92.43x0025; to 94.52x0025; in terms of statistic. An additional increase of 1.65 and 2.84 percentage points can be achieved when combining the DRCNN models with an OBV strategy. From an epistemological point of view, our results underline that CNNs can benefit from the consideration of preliminary model outcomes and that conventional CPP techniques can profit from an upstream relearning strategy.",Image segmentation,Remote sensing,Training,Geospatial analysis,"Mou, Lichao","Zhu, Xiao Xiang","Taubenboeck, Hannes",,Computational modeling,Training data,Partitioning algorithms,Classification postprocessing (CPP),convolutional neural networks (CNNs),deep learning,relearning,,,,,,,,,,,,,,,,,,
Row_648,"Wang, Ende","Jiang, Yanmei","Li, Yong",MFCSNet: Multi-Scale Deep Features Fusion and Cost-Sensitive Loss Function Based Segmentation Network for Remote Sensing Images,APPLIED SCIENCES-BASEL,OCT 2019,12,"Semantic segmentation of remote sensing images is an important technique for spatial analysis and geocomputation. It has important applications in the fields of military reconnaissance, urban planning, resource utilization and environmental monitoring. In order to accurately perform semantic segmentation of remote sensing images, we proposed a novel multi-scale deep features fusion and cost-sensitive loss function based segmentation network, named MFCSNet. To acquire the information of different levels in remote sensing images, we design a multi-scale feature encoding and decoding structure, which can fuse the low-level and high-level semantic information. Then a max-pooling indices up-sampling structure is designed to improve the recognition rate of the object edge and location information in the remote sensing image. In addition, the cost-sensitive loss function is designed to improve the classification accuracy of objects with fewer samples. The penalty coefficient of misclassification is designed to improve the robustness of the network model, and the batch normalization layer is also added to make the network converge faster. The experimental results show that the classification performance of MFCSNet outperforms U-Net and SegNet in classification accuracy, object details and prediction consistency.",Semantic segmentation,remote sensing images,feature fusion,cost-sensitive,"Yang, Jingchao","Ren, Mengcheng","Zhang, Qingchun",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_649,"Huang, Siyuan","Dong, Kaihui","Chen, Haobing",Semantic Segmentation of Ultra-high-resolution Remote Sensing Images Based on Global-local Branch Asynchronous Feature Interaction Structure,,2024,0,"In recent years, semantic segmentation of ultra-high-resolution remote sensing images (UHRRSI) has made certain progress. UHRRSI are characterized by large spatial resolution. Using the entire UHRRSI for model training will make the computational cost almost unaffordable. Common processing methods are to downsample or crop UHRRSI. However, both methods have their own shortcomings. The downsampling operation may destroy the details of the image, and the cropping operation may damage important contextual information. Therefore, this paper proposes a structure based on global-local branch asynchronous feature interaction. The global branch is used to process the downsampled images, while the local branch focuses on processing the cropped images. During the decoding stage, the decoding features of the two branches are asynchronously interacted. In comparative experiments conducted using the Potsdam and Vaihingen datasets, our model exhibits excellent results in semantic segmentation performance.",convolutional neural network,Ultra-high resolution remote sensing images (UHRRSI),semantic segmentation,Asynchronous feature interaction,"Yao, Wei","Li, Bo","Cheng, Li",,,,,,,,,,,"39TH YOUTH ACADEMIC ANNUAL CONFERENCE OF CHINESE ASSOCIATION OF AUTOMATION, YAC 2024",,,,,,,,,,,,,,,
Row_650,"Cao, Yong","Huo, Chunlei","Xiang, Shiming",GFFNet: Global Feature Fusion Network for Semantic Segmentation of Large-Scale Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,2,"Semantic segmentation plays a pivotal role in interpreting high-resolution remote sensing images (RSIs), where contextual information is essential for achieving accurate segmentation. Despite the common practice of partitioning large RSIs into smaller patches for deep model input, existing methods often rely on adaptations from natural image semantic segmentation techniques, limiting their contextual scope to individual images. To address this limitation and harness a broader range of contextual information from original large-scale RSIs, this study introduces a global feature fusion network (GFFNet). GFFNet employs a novel approach by incorporating a group transformer structure alternated with group convolution, forming a lightweight global context learning branch. This design facilitates the extraction of global contextual features from the large-scale RSIs. In addition, we propose a cross feature fusion module that seamlessly integrates local features obtained from the convolutional network with the global contextual features. GFFNet serves as a versatile plugin for existing RSI semantic segmentation models, particularly beneficial when the target dataset involves cropping. This integration enhances the model's performance, especially in terms of segmenting large-scale objects. Experimental results on the ISPRS and GID-15 datasets validate the effectiveness of GFFNet in improving segmentation capabilities for large-scale objects in RSIs.",Cross feature fusion (CFF),global context learning,group transformer,semantic segmentation,"Pan, Chunhong",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_651,"Mao, Yong-Qiang","Jiang, Zhizhuo","Liu, Yu",Body Joint Boundary Prototype Match for Few-Shot Remote Sensing Semantic Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,0,"Deep networks require a large number of samples for optimization, so few-shot segmentation in remote sensing scenes is still an open problem. However, this challenge is exacerbated by the feature blurring and aliasing of bodies (low frequency) and boundaries (high frequency). The existing methods usually only focus on the body part of the class, that is, the low-frequency part, and ignore the critical role of boundary information, that is, high-frequency details, on feature representation. In this letter, we propose a novel body joint boundary prototype match (B2PM) approach that aims to enable prior learning of low- and high-frequency information by explicitly modeling the body and boundary features of objects. First, body-aware prototype learning (BodyPL) realizes the adaptive modeling of the body part of the object through a precise farthest point sampling (FPS) initialization algorithm and an adaptive part shift (APS) strategy, which alleviates the feature ambiguity of the body. Second, boundary-aware prototype learning (BoundPL) explicitly models boundary prototypes by building a patch division and assignment strategy to alleviate feature aliasing at boundaries. Finally, prototype match performs prior knowledge aggregation by computing the affinity between query features and support prototypes. Extensive experiments on commonly used benchmarks (iSAID and PASCAL VOC) demonstrate that B2PM improves the state of the art by significant margins.",Prototypes,Feature extraction,Vectors,Adaptation models,"Zhang, Yiming","Li, Yaowen","Yan, Chenggang","Zheng, Bolun",Computational modeling,Remote sensing,Decoding,Few-shot learning,few-shot segmentation,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_652,"Lopez, Josue","Torres, Deni","Santos, Stewart",Spectral Imagery Tensor Decomposition for Semantic Segmentation of Remote Sensing Data through Fully Convolutional Networks,REMOTE SENSING,FEB 2020,9,"This work aims at addressing two issues simultaneously: data compression at input space and semantic segmentation. Semantic segmentation of remotely sensed multi- or hyperspectral images through deep learning (DL) artificial neural networks (ANN) delivers as output the corresponding matrix of pixels classified elementwise, achieving competitive performance metrics. With technological progress, current remote sensing (RS) sensors have more spectral bands and higher spatial resolution than before, which means a greater number of pixels in the same area. Nevertheless, the more spectral bands and the greater number of pixels, the higher the computational complexity and the longer the processing times. Therefore, without dimensionality reduction, the classification task is challenging, particularly if large areas have to be processed. To solve this problem, our approach maps an RS-image or third-order tensor into a core tensor, representative of our input image, with the same spatial domain but with a lower number of new tensor bands using a Tucker decomposition (TKD). Then, a new input space with reduced dimensionality is built. To find the core tensor, the higher-order orthogonal iteration (HOOI) algorithm is used. A fully convolutional network (FCN) is employed afterwards to classify at the pixel domain, each core tensor. The whole framework, called here HOOI-FCN, achieves high performance metrics competitive with some RS-multispectral images (MSI) semantic segmentation state-of-the-art methods, while significantly reducing computational complexity, and thereby, processing time. We used a Sentinel-2 image data set from Central Europe as a case study, for which our framework outperformed other methods (included the FCN itself) with average pixel accuracy (PA) of 90% (computational time similar to 90s) and nine spectral bands, achieving a higher average PA of 91.97% (computational time similar to 36.5s), and average PA of 91.56% (computational time similar to 9.5s) for seven and five new tensor bands, respectively.",fully convolutional network,semantic segmentation,spectral image,tensor decomposition,"Atzberger, Clement",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_653,"Wei, Kan","Dai, Jinkun","Hong, Danfeng",MGFNet: An MLP-dominated gated fusion network for semantic segmentation of high-resolution multi-modal remote sensing images,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,DEC 2024,0,"The heterogeneity and complexity of multimodal data in high-resolution remote sensing images significantly challenges existing cross-modal networks infusing the complementary information of high-resolution optical and synthetic aperture radar (SAR) images for precise semantic segmentation. To address this issue, this paper proposes a multi-layer perceptron (MLP) dominated gate fusion network (MGFNet). MGFNet consists of three modules: a multi-path feature extraction network, an MLP-gate fusion (MGF) module, and a decoder. Initially, MGFNet independently extracts features from high-resolution optical and SAR images while preserving spatial information. Then, the well-designed MGF module combines the multi-modal features through channel attention and gated fusion stages, utilizing MLP as a gate to exploit complementary information and filter redundant data. Additionally, we introduce a novel high-resolution multimodal remote sensing dataset, YESegOPT-SAR, with a spatial resolution of 0.5 m. To evaluate MGFNet, we compare it with several state-of-the-art (SOTA) models using YESeg-OPT-SAR and Pohang datasets, both of which are high-resolution multi-modal datasets. The experimental results demonstrate that MGFNet achieves higher evaluation metrics compared to other models, indicating its effectiveness in multi-modal feature fusion for segmentation. The source code and data are available at https://github.com/yeyuanxin110/YESeg-OPT-SAR.",Deep learning,Multi-modal data fusion,High-resolution remote sensing images,Semantic segmentation,"Ye, Yuanxin",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_654,"Zhu, Xiaotong","Peng, Taile","Hu, Xiaobin",SN-Unetformer: a dual encoder hybrid architecture for complex targets in high-resolution remote sensing images,JOURNAL OF APPLIED REMOTE SENSING,APR 1 2023,2,"Accurately identifying the semantic information of complex objects is a challenging problem in semantic segmentation of remote sensing images. We propose a bi-encoder network for semantic segmentation of complex targets, called the SN-Unetformer. It combines ConvNeXt and Swin Transformer into a bi-encoder and constructs a feature fusion module (FFM) to fully integrate the semantic information of the bi-encoder by exploiting channel dependence. Moreover, an efficient attention mechanism has been introduced to model the global-local relationship. To the best of our knowledge, our proposed network is innovative, as it is the first method to combine two popular networks, ConvNeXt and the Swin Transformer, into a dual encoder. Our SN-Unetformer has been tested on large-scale Vaihingen and Potsdam datasets, as well as the LoveDA dataset, with significant challenges. Compared to current advanced methods for semantic segmentation for remote sensing images, our accuracy is significantly better. In particular, our method achieves 84.3% of mean intersection over union on the Vaihingen dataset, which is the best result currently available for this dataset. (c) 2023 Society of PhotoOptical Instrumentation Engineers (SPIE) [DOI: 10.1117/1.JRS.17.026512]",semantic segmentation,remote sensing,Swin Transformer,ConvNeXt,"Guo, Jia","Cao, Taotao","Wang, Hao",,dual encoder,,,,,,,,,,,,,,,,,,,,,,,,
Row_655,"Shi, Lukui","Wang, Ziyuan","Pan, Bin",An End-to-End Network for Remote Sensing Imagery Semantic Segmentation via Joint Pixel- and Representation-Level Domain Adaptation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,NOV 2021,28,"It requires pixel-by-pixel annotations to obtain sufficient training data in supervised remote sensing image segmentation, which is a quite time-consuming process. In recent years, a series of domain-adaptation methods was developed for image semantic segmentation. In general, these methods are trained on the source domain and then validated on the target domain to avoid labeling new data repeatedly. However, most domain-adaptation algorithms only tried to align the source domain and the target domain in the pixel level or the representation level, while ignored their cooperation. In this letter, we propose an unsupervised domain-adaptation method by Joint Pixel and Representation level Network (JPRNet) alignment. The major novelty of the JPRNet is that it achieves joint domain adaptation in an end-to-end manner, so as to avoid the multisource problem in the remote sensing images. JPRNet is composed of two branches, each of which is a generative-adversarial network (GAN). In one branch, pixel-level domain adaptation is implemented by the style transfer with the Cycle GAN, which could transfer the source domain to a target domain. In the other branch, the representation-level domain adaptation is realized by adversarial learning between the transferred source-domain images and the target-domain images. The experimental results on the public data sets have indicated the effectiveness of the JPRNet.",Gallium nitride,Image segmentation,Semantics,Remote sensing,"Shi, Zhenwei",,,,Feature extraction,Adaptation models,Training,Domain adaptation,generative-adversarial network (GAN),remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_656,"Hao, Rong-Rong","Sun, Hong-Mei","Wang, Rui-Xuan",A novel semantic feature enhancement network for extracting lake water from remote sensing images,INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS,SEP 2024,0,"The automatic lake water extraction method based on semantic segmentation is a research hotspot in the field of remote sensing image processing. In remote sensing images, the presence of complex noise information at the lake boundary hinders the normal expression of boundary information, which leads to methods cannot extract a coherent lake boundary. Moreover, partial small-scale lakes' texture features are weak and easily masked by the background information. To address the above issues, an end-to-end semantic segmentation network is designed. The network uses a symmetric encoder-decoder architecture to extract lake water in remote sensing images. On the one hand, a directional noise reduction filtering algorithm is proposed to reduce the impact of noise information on the network segmentation process. The algorithm utilizes a preset directional guide map to guide the nonlinear propagation of boundary noise and suppress low-contrast halo artifacts in the image, thereby better preserving the boundary sharpness of the lake. On the other hand, for the problem of missing small-scale lakes, an attention gate compression module is embedded in the skip connection. This module can adaptively integrate the correlation features between different ground objects, and selectively assign more attention to small-scale lakes, thereby improving the network's ability to recognize such lakes. In the experimental results, our method can produce more accurate lake water extraction results than the current mainstream methods. Besides it has an excellent performance in accurately identifying lake boundaries and small-scale lakes.",Semantic segmentation,Remote sensing images,Transformer,Directional noise reduction filtering,"Pan, Ang","Jia, Rui-Sheng",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_657,"Cui, Mengtian","Li, Kai","Li, Yulan",Semi-Supervised Semantic Segmentation of Remote Sensing Images Based on Dual Cross-Entropy Consistency,ENTROPY,APR 2023,3,"Semantic segmentation is a growing topic in high-resolution remote sensing image processing. The information in remote sensing images is complex, and the effectiveness of most remote sensing image semantic segmentation methods depends on the number of labels; however, labeling images requires significant time and labor costs. To solve these problems, we propose a semi-supervised semantic segmentation method based on dual cross-entropy consistency and a teacher-student structure. First, we add a channel attention mechanism to the encoding network of the teacher model to reduce the predictive entropy of the pseudo label. Secondly, the two student networks share a common coding network to ensure consistent input information entropy, and a sharpening function is used to reduce the information entropy of unsupervised predictions for both student networks. Finally, we complete the alternate training of the models via two entropy-consistent tasks: (1) semi-supervising student prediction results via pseudo-labels generated from the teacher model, (2) cross-supervision between student models. Experimental results on publicly available datasets indicate that the suggested model can fully understand the hidden information in unlabeled images and reduce the information entropy in prediction, as well as reduce the number of required labeled images with guaranteed accuracy. This allows the new method to outperform the related semi-supervised semantic segmentation algorithm at half the proportion of labeled images.",cross-entropy consistency,information entropy,semi-supervised,channel attention mechanism,"Kamuhanda, Dany","Tessone, Claudio J.",,,remote sensing image,,,,,,,,,,,,,,,,,,,,,,,,
Row_658,"Yi, Lina","Zhang, Guifeng","Wu, Zhaocong",A Scale-Synthesis Method for High Spatial Resolution Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,OCT 2012,60,"Multiscale segmentation is always needed to extract semantic meaningful objects for object-based remote sensing image analysis. Choosing the appropriate segmentation scales for distinct ground objects and intelligently combining them together are two crucial issues to get the appropriate segmentation result for target applications. With respect to these two issues, this paper proposes a simple scale-synthesis method which is highly flexible to be adjusted to meet the segmentation requirements of varying image-analysis tasks. The main idea of this method is to first divide the whole image area into multiple regions; each region consisted of ground objects that have similar optimal segmentation scale. Then, synthesize the suboptimal segmentations of each region to get the final segmentation result. The result is the combination of suboptimal scales of objects and is therefore more coherent to ground objects. To validate this method, the land-cover-category map is used to guide the scale synthesis of multiscale image segmentations for the Quickbird-image land-use classification. First, the image is coarsely divided into multiple regions; each region belongs to a certain land-cover category. Then, multiscale-segmentation results are generated by the Mumford-Shah function based region-merging method. For each land-cover category, the optimal segmentation scale is selected by the supervised segmentation-accuracy-assessment method. Finally, the optimal scales of segmentation results are synthesized under the guide of land-cover category. It is proved that the proposed scale-synthesis method can generate a more accurate segmentation result that benefits the latter classification. The land-use-classification accuracy reaches to 77.8%.",Image segmentation,multiscale,object-oriented classification,remote sensing,,,,,scale synthesis,,,,,,,,,,,,,,,,,,,,,,,,
Row_659,"Li, Yuxuan","Li, Xiang","Dai, Yimain",LSKNet: A Foundation Lightweight Backbone for Remote Sensing,INTERNATIONAL JOURNAL OF COMPUTER VISION,OCT 2024,1,"Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection, semantic segmentation and change detection, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightweight LSKNet backbone network sets new state-of-the-art scores on standard remote sensing classification, object detection, semantic segmentation and change detection benchmarks. Our comprehensive analysis further validated the significance of the identified priors and the effectiveness of LSKNet. The code is available at https://github.com/zcablii/LSKNet.",Remote sensing,CNN backbone,Large kernel,Attention,"Hou, Qibin","Liu, Li","Liu, Yongxiang","Cheng, Ming-Ming",Object detection,Semantic segmentation.,,,,,,,"Yang, Jian",,,,,,,,,,,,,,,,
Row_660,"Chen, Guangsheng","Li, Chao","Wei, Wei",Fully Convolutional Neural Network with Augmented Atrous Spatial Pyramid Pool and Fully Connected Fusion Path for High Resolution Remote Sensing Image Segmentation,APPLIED SCIENCES-BASEL,MAY 1 2019,61,"Recent developments in Convolutional Neural Networks (CNNs) have allowed for the achievement of solid advances in semantic segmentation of high-resolution remote sensing (HRRS) images. Nevertheless, the problems of poor classification of small objects and unclear boundaries caused by the characteristics of the HRRS image data have not been fully considered by previous works. To tackle these challenging problems, we propose an improved semantic segmentation neural network, which adopts dilated convolution, a fully connected (FC) fusion path and pre-trained encoder for the semantic segmentation task of HRRS imagery. The network is built with the computationally-efficient DeepLabv3 architecture, with added Augmented Atrous Spatial Pyramid Pool and FC Fusion Path layers. Dilated convolution enlarges the receptive field of feature points without decreasing the feature map resolution. The improved neural network architecture enhances HRRS image segmentation, reaching the classification accuracy of 91%, and the precision of recognition of small objects is improved. The applicability of the improved model to the remote sensing image segmentation task is verified.",semantic segmentation,remote sensing,dilated convolution,fully convolutional neural network,"Jing, Weipeng","Wozniak, Marcin","Blazauskas, Tomas","Damasevicius, Robertas",deep learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_661,"Hu, Xiaoxing","Wang, Yupei","Chen, Liang",Domain Adaptive Semantic Segmentation of Remote Sensing Images via Self-Training-Based Dual-Level Data Augmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"Semantic segmentation models experience a significant performance degradation due to domain shifts between the source and target domains. This issue is particularly prevalent in remote sensing imagery, where a semantic segmentation model trained on images from one satellite is tested on images from another. Previous research has often overlooked the role of data augmentation in enhancing a model's adaptability to target domains. In contrast, this article proposes a novel self-training framework that incorporates data augmentation at both the input and feature levels, yielding excellent results. Specifically, we introduce a regularized online self-training framework that effectively addresses the challenges of overconfidence and class imbalance inherent in self-training. Based on this framework, we implement two robust data augmentation strategies at the input and feature levels to facilitate the learning of cross-domain invariant knowledge. At the input level, we employ a large-scale domain mixing strategy, termed multidomain mixing, to enhance the model's generalization capability. At the feature level, we introduce masked feature augmentation, a masking-based perturbation technique applied to the semantic features of the target domain. This approach enhances the consistency of teacher-student network predictions in the target domain feature space, thereby improving the robustness of the model's recognition of target domain features. The integration of the proposed self-training framework with dual-level data augmentation culminates in our innovative self-training-based dual-level data augmentation (STDA) method. Extensive experimental results on the ISPRS semantic segmentation benchmark demonstrate that STDA outperforms existing state-of-the-art methods, showcasing its effectiveness.",Adaptation models,Remote sensing,Semantic segmentation,Data models,,,,,Data augmentation,Training,Semantics,Target recognition,Perturbation methods,Earth,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,
Row_662,"Kang, Yuhan","Wu, Jie","Liu, Qiang",Trans-Diff: Heterogeneous Domain Adaptation for Remote Sensing Segmentation With Transfer Diffusion,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"Domain adaptation has been demonstrated to be an important technique to reduce the expensive annotation costs for remote sensing segmentation. However, for remote sensing images (RSIs) acquired from different imaging modalities with significant differences, a model trained on one modality can hardly be utilized for images of other modalities. This leads to a greater challenge in domain adaptation, called heterogeneous domain adaptation (HDA). To address this issue, we propose a novel method called transfer diffusion (Trans-Diff), which is the first work to explore the diffusion model for HDA remote sensing segmentation. The proposed Trans-Diff constructs cross-domain unified prompts for the diffusion model. This approach enables the generation of images from different modalities with specific semantics, leading to efficient HDA segmentation. Specifically, we first propose an interrelated semantic modeling method to establish semantic interrelation between heterogeneous RSIs and annotations in a high-dimensional feature space and extract the unified features as the cross-domain prompts. Then, we construct a semantic guidance diffusion model to further improve the semantic guidance of images generated with the cross-domain prompts, which effectively facilitates the semantic transfer of RSIs from source modality to target modality. In addition, we design an adaptive sampling strategy to dynamically regulate the generated images' stylistic consistency and semantic consistency. This can effectively reduce the cross-domain discrepancies between different modalities of RSIs, ultimately significantly improving the HDA remote sensing segmentation performance. Experimental results demonstrate the superior performance of Trans-Diff over advanced methods on several heterogeneous RSI datasets.",Cross-domain prompt,diffusion model,hetero- geneous domain adaptation,hetero- geneous domain adaptation,"Yue, Jun","Fang, Leyuan",,,remote sensing segmentation,remote sensing segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_663,"Zhong, Letian","Lin, Yong","Sul, Yian",Improved U-Net Network Segmentation Method for Remote Sensing Image,,2022,1,"Semantic segmentation and extraction based on remote sensing images has important theory and significance. Deep learning has become one of the mainstream methods to extract information from remote sensing images. In this paper, based on the improvement of U-Net network structure, we combine ASPP and skip connection. Improve the residual module to improve the information extraction method. The main improvements of this paper are:0 Based on the U-Net network structure, we use the multi-scale feature detection capabilities of Pyramid to introduce. The ASPP module and the residual structure are improved, paying more attention to semantic and detail informatization, overcoming the limitations of U-Net in small target detection;20 We have improved the U-Net network, using skip connections to get more layers of information. Experiments show that the model proposed in this paper has significantly higher MPA and MIOU than the U-Net model on both the VOC dataset and the Vaihingen dataset. It means that ARU-Net can extract information better.",semantic segmentation,U-Net,Pyramid,ASPP,"Fang, Xianbao",,,,Residual block,Remote sensing image,Skip connection,,,,,,,"2022 IEEE 6TH ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC)",,,,,,,,,,,,,,,
Row_664,"Lei, Jingxiong","Liu, Xuzhi","Yang, Haolang",Dual Hybrid Attention Mechanism-Based U-Net for Building Segmentation in Remote Sensing Images,APPLIED SCIENCES-BASEL,FEB 2024,2,"High-resolution remote sensing images (HRRSI) have important theoretical and practical value in urban planning. However, current segmentation methods often struggle with issues like blurred edges and loss of detailed information due to the intricate backgrounds and rich semantics in high-resolution remote sensing images. To tackle these challenges, this paper proposes an end-to-end attention-based Convolutional Neural Network (CNN) called Double Hybrid Attention U-Net (DHAU-Net). We designed a new Double Hybrid Attention structure consisting of dual-parallel hybrid attention modules to replace the skip connections in U-Net, which can eliminate redundant information interference and enhances the collection and utilization of important shallow features. Comprehensive experiments on the Massachusetts remote sensing building dataset and the Inria aerial image labeling dataset demonstrate that our proposed method achieves effective pixel-level building segmentation in urban remote sensing images by eliminating redundant information interference and making full use of shallow features, and improves the segmentation performance without significant time costs (approximately 15%). The evaluation metrics reveal significant results, with an accuracy rate of 0.9808, precision reaching 0.9300, an F1 score of 0.9112, a mean intersection over union (mIoU) of 0.9088, and a recall rate of 0.8932.",deep learning,remote sensing image,attention mechanism,U-net,"Zeng, Zeyu","Feng, Jun",,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_665,"Zhou, Wujie","Jin, Jianhui","Lei, Jingsheng",CIMFNet: Cross-Layer Interaction and Multiscale Fusion Network for Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING,JUN 2022,50,"Semantic segmentation of remote sensing images has received increasing attention in recent years; however, using a single imaging modality limits the segmentation performance. Thus, digital surface models have been integrated into semantic segmentation to improve performance. Nevertheless, existing methods based on neural networks simply combine data from the two modalities, mostly neglecting the similarities and differences between multimodal features. Consequently, the complementarity between multimodal features cannot be exploited, and excess noise is introduced during feature processing. To solve these problems, we propose a multimodal fusion module to explore the similarities and differences between features from the two information modalities for adequate fusion. In addition, although downsampling operations such as pooling and striding can improve the feature representativeness, they discard spatial details and often lead to segmentation errors. Thus, we introduce hierarchical feature interactions to mitigate the adverse effects of downsampling and introduce a two-way interactive pyramid pooling module to extract multiscale context features for guiding feature fusion. Extensive experiments performed on two benchmark datasets show that the proposed network integrating our novel modules substantially outperforms state-of-the-art semantic segmentation methods. The code and results can be found at https://github.com/NIT-JJH/CIMFNet.",Semantics,Image segmentation,Feature extraction,Logic gates,"Yu, Lu",,,,Fuses,Data mining,Convolution,Cross-layer interaction,cross-modal fusion,high-resolution remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_666,"Wei, Youhua","Liu, Xuzhi","Lei, Jingxiong",Multiscale feature U-Net for remote sensing image segmentation,JOURNAL OF APPLIED REMOTE SENSING,JAN 1 2022,6,"The segmentation and extraction of buildings in high-resolution remote sensing images has good application prospects in military, civil, and other fields. With a depth encoder-decoder structure, U-Net is a frequently used model for high-precision image segmentation. However, the design of U-Net makes it hard to retain the detailed information of edges when processing the building segmentation. Specifically, the low-level features extracted from the shallow layer and the abstract features extracted from the deep layer cannot be completely merged, resulting in inaccurate segmentation. In response to this problem, we design a new multiscale feature extraction module that extracts target information through three convolution kernels of different scales. Taking U-Net as the baseline, by replacing skip connections with this module, we propose a multiscale feature extraction U-Net. This method can perform secondary feature extraction on the shallow feature information in the skip connection, refine the detailed information, and narrow the semantic gap between the low-level features and high-level features. It can not only improve the ability of the network to extract multiscale feature information, from a larger range to more layers to extract the edge detail information of the building in the remote sensing image, but also increase the number of skip connections to reduce network overfitting. Experimental results on Massachusetts remote sensing data and Massachusetts building data show that the method proposed offers significant improvement in terms of precision and accuracy compared with the methods full convolutional network, U-Net, SegNet, and high-resolution network, with an F1 score of 88.73%, mean IoU of 91.15%, precision of 89.74%, accuracy of 97.36%, and recall of 87.74%. (C) 2022 Society of Photo-Optical Instrumentation Engineers (SPIE)",remote sensing image,deep learning,multiscale information,building segmentation,"Yue, Ruihan","Feng, Jun",,,U-Net,,,,,,,,,,,,,,,,,,,,,,,,
Row_667,"Xu, Lewei","Hu, Zhuhua","Zhang, Chong",Remote Sensing Image Segmentation of Mariculture Cage Using Ensemble Learning Strategy,APPLIED SCIENCES-BASEL,AUG 2022,4,"Featured Application By introducing the method of deep learning, the precise segmentation of the aquaculture cages in a specific aquaculture sea area can be achieved, so as to realize the efficient statistics of the cage culture density and reduce the cost of manual statistics. In harbour areas, the irrational layout and high density of mariculture cages can lead to a dramatic deterioration of the culture's ecology. Therefore, it is important to analyze and regulate the distribution of cages using intelligent analysis based on deep learning. We propose a remote sensing image segmentation method based on the Swin Transformer and ensemble learning strategy. Firstly, we collect multiple remote sensing images of cages and annotate them, while using data expansion techniques to construct a remote sensing image dataset of mariculture cages. Secondly, the Swin Transformer is used as the backbone network to extract the remote sensing image features of cages. A strategy of alternating the local attention module and the global attention module is used for model training, which has the benefit of reducing the attention computation while exchanging global information. Then, the ensemble learning strategy is used to improve the accuracy of remote sensing cage segmentation. We carry out quantitative and qualitative analyses of remote sensing image segmentation of cages at the ports of Li'an, Xincun and Potou in Hainan Province, China. The results show that our proposed segmentation scheme has significant performance improvement compared to other models. In particular, the mIoU reaches 82.34% and pixel accuracy reaches 99.71%.",aquaculture,remote sensing image,semantic segmentation,smart agriculture,"Wu, Wei",,,,deep learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_668,"Dong, Sijun","Chen, Zhengchao",,Block Multi-Dimensional Attention for Road Segmentation in Remote Sensing Imagery,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,12,"High-resolution remote sensing image (RSI) segmentation is a relatively mature application in various deep learning projects. In this study, aiming at slender objects in road RSIs, BMDANet combines cross-layer information exchange and block multi-dimensional attention (BMDA) module and optimizes road feature extraction by using multi-dimensional information to construct a global attention module. The experimental results based on the Ottawa road dataset show that our algorithm improved the recognition results of the road in RSI, and excelled the existing RSI road segmentation algorithm and reached the state-of-the-art. In addition, based on comparative experiments, the addition of the BMDA module to different algorithms can effectively improve the accuracy of the algorithm. It has proven the effectiveness and embedding of our BMDA module in RSI road segmentation algorithms.",Roads,Feature extraction,Image segmentation,Semantics,,,,,Remote sensing,Data mining,Convolution,Block multi-dimensional attention (BMDA),cross-layer information exchange,remote sensing image (RSI),road semantic segmentation,,,,,,,,,,,,,,,,,,
Row_669,Yuan Wei,Xu Wenbo,Zhou Tian,TopPixelLoss: a loss function for semantic segmentation of remote sensing images with class imbalance,CHINESE SPACE SCIENCE AND TECHNOLOGY,DEC 25 2021,0,"Aiming at the problem that the segmentation effect of small target in remote sensing image is not ideal, a loss function named TopPixelLoss was proposed. Firstly, the cross entropy of each pixel was calculated, and then the cross entropy of all pixels was sorted from large to small. After that, a K value was determined. According to the threshold K, the pixels with the largest cross entropy of the top K were selected. Finally, the cross entropy of the K pixels was averaged as the final loss value. Experiments using PSPNet network with cross entropy, FocalLoss and TopPixelLoss were carried out respectively through Vaihingen data set of ISPRS. The results show that, for different K values, the mean intersection over union (MIoU) F1-score and accuracy (ACC) arc all higher than FocalLoss, and that the effect is the best when K is 50 000 (MIoU, F1-score and ACC arc improved by 3.0%, 5.0% and 0.1% respectively compared with FocalLoss). The proposed TopPixelLoss function is a very effective loss function for imbalanced class segmentation.",remote sensing image,semantic segmentation,deep learning,class imbalance,,,,,small target segmentation,unbalanced sample,,,,,,,,,,,,,,,,,,,,,,,
Row_670,"Zuo, Renxiang","Zhang, Guangyun","Zhang, Rongting",A Deformable Attention Network for High-Resolution Remote Sensing Images Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,25,"Deformable convolutional networks (DCNs) can mitigate the inherent limited geometric transformation. We reformulate the spatialwise attention mechanism using DCNs in this article for semantic segmentation of high-resolution remote sensing (HRRS) images. It combines the sparse spatial sampling strategy and the long-range relationship modeling capability, namely, deformable attention module (DAM). Such locality awareness, more adaptable to HRRS image structures, can capture each pixel's neighboring structural information. A reasonable multiscale deformable attention net (MDANet) is designed for the HRRS image semantic segmentation with a slightly increased computational cost based on the proposed DAM. Specifically, standard convolutional layers in the raw ResNet50 are equipped with a DAM to control sampling over a broader range of feature levels and aggregate multiscale context information. The experimental results evaluated on Vaihingen and DeepGlobe Land Cover Classification datasets show that the performance accuracy of MDANet is improved by 7.77% and 8.45% compared with the backbone network (ResNet50) in terms of Miou evaluation, respectively. Furthermore, a DAM can perform better than a global spatial attention mechanism with less computation on the 3 x 64 x 64 feature map. In addition, the added ablation studies demonstrate the effectiveness and efficiency of the DAM and multiscale strategy, respectively. Moreover, the sensitivity of critical hyperparameters is analyzed.",Deformable convolutional networks (DCNs),high-resolution remote sensing (HRRS) images,multiscale,spatialwise attention mechanism,"Jia, Xiuping",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_671,"Chong, Qianpeng","Ni, Mengying","Huang, Jianjun",Rethinking high-resolution remote sensing image segmentation not limited to technology: a review of segmentation methods and outlook on technical interpretability,INTERNATIONAL JOURNAL OF REMOTE SENSING,JUN 2 2024,0,"The intelligent segmentation of high-resolution remote sensing (HRS) image, also called as dense prediction task for HRS image, has been and will continue to be important research in the remote sensing community. In recent years, the growing wave of artificial intelligence (AI) technology has introduced innovative paradigms to this domain, yielding outstanding results and overcoming many challenges with conventional segmentation techniques. This paper provides a comprehensive review of these intelligent segmentation methodologies, including traditional pattern recognition, convolution neural network (CNN)-based, and Transformer-based techniques. However, the explosive but incomplete development of intelligent segmentation techniques also poses more challenges for earth observation experts, the most of which is the technical interpretability. Consequently, we consider these segmentation techniques in the aspect of explainable artificial intelligence (XAI). Data-centric XAI thinks the practical applications of the segmentation model while model-centric XAI will facilitate the understanding of decision-making processes and the adjustment of structural features. Moreover, this review identifies novel research questions and provides constructive insights and recommendations to HRS image segmentation tasks, which may shed new light on the intelligent segmentation methods within the remote sensing image understanding community.",AI,earth observation,remote sensing,segmentation,"Wei, Guangyi","Li, Ziyi","Xu, Jindong",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_672,"Iizuka, Reo","Xia, Junshi","Yokoya, Naoto",Frequency-Based Optimal Style Mix for Domain Generalization in Semantic Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,2,"Supervised learning methods assume that training and test data are sampled from the same distribution. However, this assumption is not always satisfied in practical situations of land cover semantic segmentation when models trained in a particular source domain are applied to other regions. This is because domain shifts caused by variations in location, time, and sensor alter the distribution of images in the target domain from that of the source domain, resulting in significant degradation of model performance. To mitigate this limitation, domain generalization (DG) has gained attention as a way of generalizing from source domain features to unseen target domains. One approach is style randomization (SR), which enables models to learn domain-invariant features through randomizing styles of images in the source domain. Despite its potential, existing methods face several challenges, such as inflexible frequency decomposition, high computational and data preparation demands, slow speed of randomization, and lack of consistency in learning. To address these limitations, we propose a frequency-based optimal style mix (FOSMix), which consists of three components: 1) full mix (FM) enhances the data space by maximally mixing the style of reference images into the source domain; 2) optimal mix (OM) keeps the essential frequencies for segmentation and randomizes others to promote generalization; and 3) regularization of consistency ensures that the model can stably learn different images with the same semantics. Extensive experiments that require the model's generalization ability, with domain shift caused by variations in regions and resolutions, demonstrate that the proposed method achieves superior segmentation in remote sensing. The source code is available at https://github.com/Reo-I/FOSMix.",Frequency-domain analysis,Data models,Training,Semantic segmentation,,,,,Task analysis,Remote sensing,Predictive models,Domain generalization (DG),semantic segmentation,style randomization (SR),,,,,,,,,,,,,,,,,,,
Row_673,"Duan, Sining","Zhao, Jingyi","Huang, Xinyi",Semantic Segmentation of Remote Sensing Data Based on Channel Attention and Feature Information Entropy,SENSORS,FEB 2024,2,"The common channel attention mechanism maps feature statistics to feature weights. However, the effectiveness of this mechanism may not be assured in remotely sensing images due to statistical differences across multiple bands. This paper proposes a novel channel attention mechanism based on feature information called the feature information entropy attention mechanism (FEM). The FEM constructs a relationship between features based on feature information entropy and then maps this relationship to their importance. The Vaihingen dataset and OpenEarthMap dataset are selected for experiments. The proposed method was compared with the squeeze-and-excitation mechanism (SEM), the convolutional block attention mechanism (CBAM), and the frequency channel attention mechanism (FCA). Compared with these three channel attention mechanisms, the mIoU of the FEM in the Vaihingen dataset is improved by 0.90%, 1.10%, and 0.40%, and in the OpenEarthMap dataset, it is improved by 2.30%, 2.20%, and 2.10%, respectively. The proposed channel attention mechanism in this paper shows better performance in remote sensing land use classification.",channel attention mechanism,land use classification,semantic segmentation,,"Zhao, Shuhe",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_674,"Cao, Yiwen","Jiang, Nanfeng","Wang, Da-Han",UAM-Net: An Attention-Based Multi-level Feature Fusion UNet for Remote Sensing Image Segmentation,"PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2023, PT IV",2024,1,"Semantic segmentation of Remote Sensing Images (RSIs) is an essential application for precision agriculture, environmental protection, and economic assessment. While UNet-based networks have made significant progress, they still face challenges in capturing long-range dependencies and preserving fine-grained details. To address these limitations and improve segmentation accuracy, we propose an effective method, namely UAM-Net (UNet with Attention-based Multi-level feature fusion), to enhance global contextual understanding and maintain fine-grained information. To be specific, UAM-Net incorporates three key modules. Firstly, the Global Context Guidance Module (GCGM) integrates semantic information from the Pyramid Pooling Module (PPM) into each decoder stage. Secondly, the Triple Attention Module (TAM) effectively addresses feature discrepancies between the encoder and decoder. Finally, the computation-effective Linear Attention Module (LAM) seamlessly fuses coarse-level feature maps with multiple decoder stages. With the corporations of these modules, UAM-Net significantly outperforms the most state-of-the-art methods on two popular benchmarks.",Semantic segmentation,U-shape architecture,Attention mechanism,Feature fusion,"Wu, Yun","Zhu, Shunzhi",,,Remote sensing images,,,,,,,,,,,,,,,,,,,,,,,,
Row_675,"Chen, Yantong","Li, Yuyang","Wang, Junsheng",Remote Sensing Image Ship Detection under Complex Sea Conditions Based on Deep Semantic Segmentation,REMOTE SENSING,FEB 2020,17,"Under complex sea conditions, ship detection from remote sensing images is easily affected by sea clutter, thin clouds, and islands, resulting in unreliable detection results. In this paper, an end-to-end convolution neural network method is introduced that combines a deep convolution neural network with a fully connected conditional random field. Based on the Resnet architecture, the remote sensing image is roughly segmented using a deep convolution neural network as the input. Using the Gaussian pairwise potential method and mean field approximation theorem, a conditional random field is established as the output of the recurrent neural network, thus achieving end-to-end connection. We compared the proposed method with other state-of-the-art methods on the dataset established by Google Earth and NWPU-RESISC45. Experiments show that the target detection accuracy of the proposed method and the ability of capturing fine details of images are improved. The mean intersection over union is 83.2% compared with other models, which indicates obvious advantages. The proposed method is fast enough to meet the needs for ship detection in remote sensing images.",remote sensing image,semantic segmentation,convolution neural network,atrous convolution,"Chen, Weinan","Zhang, Xianzhong",,,fully connected conditional random field,,,,,,,,,,,,,,,,,,,,,,,,
Row_676,"Liu, Yuheng","Zhang, Yifan","Wang, Ye",BiTSRS: A Bi-Decoder Transformer Segmentor for High-Spatial-Resolution Remote Sensing Images,REMOTE SENSING,FEB 2023,5,"Semantic segmentation of high-spatial-resolution (HSR) remote sensing (RS) images has been extensively studied, and most of the existing methods are based on convolutional neural network (CNN) models. However, the CNN is regarded to have less power in global representation modeling. In the past few years, methods using transformer have attracted increasing attention and generate improved results in semantic segmentation of natural images, owing to their powerful ability in global information acquisition. Nevertheless, these transformer-based methods exhibit limited performance in semantic segmentation of RS images, probably because of the lack of comprehensive understanding in the feature decoding process. In this paper, a novel transformer-based model named the bi-decoder transformer segmentor for remote sensing (BiTSRS) is proposed, aiming at alleviating the problem of flexible feature decoding, through a bi-decoder design for semantic segmentation of RS images. In the proposed BiTSRS, the Swin transformer is adopted as encoder to take both global and local representations into consideration, and a unique design module (ITM) is designed to deal with the limitation of input size for Swin transformer. Furthermore, BiTSRS adopts a bi-decoder structure consisting of a Dilated-Uper decoder and a fully deformable convolutional network (FDCN) module embedded with focal loss, with which it is capable of decoding a wide range of features and local detail deformations. Both ablation experiments and comparison experiments were conducted on three representative RS images datasets. The ablation analysis demonstrates the contributions of specifically designed modules in the proposed BiTSRS to performance improvement. The comparison experimental results illustrate that the proposed BiTSRS clearly outperforms some state-of-the-art semantic segmentation methods.",remote sensing,semantic segmentation,global-local modeling,bi-decoder transformer,"Mei, Shaohui",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_677,"Zhou, Wujie","Yang, Penghan","Qiu, Weiwei",STONet-S*: A Knowledge-Distilled Approach for Semantic Segmentation in Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Semantic segmentation of remote sensing images is a critical research domain. The integration of cross-modal features enhances stability in intricate environments. Despite the impressive performance of existing methods, their complexity and parameter demands remain significant. Our proposed STONet-S $<^>{\ast }$ , a stepped transmission optimization network (STONet) with knowledge distillation (KD), extracts insights from a pretrained extensive teacher network and transfers them to an untrained compact student network. Initially, a group enhancement and interaction unit (GEIU) correct for background noise influence and seamlessly integrates cross-modal features. Additionally, we introduce a stepped transmission decoder (STD) comprising a stepped capture module (SCM) and a self-reverse revision module (SRRM) to capture multiscale information from the ground up. Furthermore, leveraging the frequency domain, we employ frequency-awareness KD using a discrete cosine transform (DCT) and octave convolution to separate high and low-frequency maps, which are subsequently transferred to the student network. Last, detail-delivery and stepped-response KD (SRKD) mechanisms enhance the learning capacity of the student network. Through extensive experimentation on two datasets, STONet-S $<^>{\ast }$ demonstrates superior segmentation accuracy by achieving remarkable results with only 7.19 M parameters. The corresponding code repository can be accessed at: https://github.com/MAXHAN22/STONet.",Feature extraction,Semantics,Frequency-domain analysis,Convolution,"Qiang, Fangfang",,,,Decoding,Data mining,Semantic segmentation,Frequency-awareness knowledge distillation (FAKD),group enhancement and interaction unit (GEIU),remote sensing images (RSIs),semantic segmentation,stepped transmission decoder (STD),,,,,,,,,,,,,,,,,
Row_678,"Zhang, Boning","Zhang, Xiaokang","Pun, Man-On",PROTOTYPE-BASED CLUSTERED FEDERATED LEARNING FOR SEMANTIC SEGMENTATION OF AERIAL IMAGES,,2022,2,"Despite its impressive performance on semantic segmentation of remote sensing imagery, deep learning requires a large amount of labeled data for model training, which is both laborious and time-consuming for an individual institution. To cope with this obstacle, federated Learning (FL) has been proposed to enable multiple institutions to train a global model collaboratively without violating privacy rules. However, the performance of FL is poor in the presence of heterogeneous training data, i.e. the data is not independently and identically distributed (non-i.i.d) among participating clients, especially for remote sensing images with high spatial and spectral heterogeneity. In this paper, we propose an FL algorithm combined with prototype-based hierarchical clustering (FedPHC). Instead of updating a single global model to capture the shared knowledge of all clients, we utilize a mixture of multiple global models to handle the heterogeneity between various clients using hierarchical clustering (HC) based on the prototypical representations of clients' datasets. As a result, FedPHC can reduce the domain discrepancy within each group and obtain more representative models for heterogeneous datasets. Extensive experiments on the Inria Aerial Image Dataset confirm the effectiveness of FedPHC.",Semantic segmentation,federated learning,remote sensing images,,"Liu, Ming",,,,,,,,,,,,,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),,,,,,,,,,,,,,,
Row_679,"Ma, Siteng","Hou, Biao","Guo, Xianpeng",Unsupervised Prototype-Wise Contrastive Learning for Domain Adaptive Semantic Segmentation in Remote Sensing Image,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,2,"Labeling data in the field of remote sensing is time-consuming and labor-intensive, making domain adaptation between different domains an urgently needed solution. To address the domain gap between diverse datasets in the remote-sensing domain, numerous methods tailored for domain adaptation in high-resolution remote-sensing imagery (RSI) have emerged. Some of the existing methods focus on reducing the domain gap at either the feature level or the pixel level, often overlooking their underlying connection. To tackle this issue, we introduce a prototype-wise contrastive feature alignment (PCFA) paradigm aimed at bridging the representations between the feature and pixel levels. By dynamically updating, we acquire prototype information encompassed by different mini-batches and employ an optimal transport mechanism to reasonably apply the prototype feature distribution in guiding the learning of target domain features. We conduct extensive domain adaptation semantic segmentation (DASS) experiments on the ISPRS Vaihingen and Potsdam datasets, achieving an improvement of about 4%-5% in mean intersection over union (mIoU) compared to previous methods using the DeepLabV2 framework.",Contrastive feature alignment,domain adap-tation semantic segmentation,domain gap,feature distribution,"Wu, Zitong","Li, Zhihao","Wu, Hang","Jiao, Licheng",optimal transport mechanism,prototype,,,,,,,,,,,,,,,,,,,,,,,
Row_680,"Sun, Yihao","Wang, Mingrui","Huang, Xiaoyi",Fast Semantic Segmentation of Ultra-High-Resolution Remote Sensing Images via Score Map and Fast Transformer-Based Fusion,REMOTE SENSING,SEP 2024,0,"For ultra-high-resolution (UHR) image semantic segmentation, striking a balance between computational efficiency and storage space is a crucial research direction. This paper proposes a Feature Fusion Network (EFFNet) to improve UHR image semantic segmentation performance. EFFNet designs a score map that can be embedded into the network for training purposes, enabling the selection of the most valuable features to reduce storage consumption, accelerate speed, and enhance accuracy. In the fusion stage, we improve upon previous redundant multiple feature fusion methods by utilizing a transformer structure for one-time fusion. Additionally, our combination of the transformer structure and multibranch structure allows it to be employed for feature fusion, significantly improving accuracy while ensuring calculations remain within an acceptable range. We evaluated EFFNet on the ISPRS two-dimensional semantic labeling Vaihingen and Potsdam datasets, demonstrating that its architecture offers an exceptionally effective solution with outstanding semantic segmentation precision and optimized inference speed. EFFNet substantially enhances critical performance metrics such as Intersection over Union (IoU), overall accuracy, and F1-score, highlighting its superiority as an architectural innovation in ultra-high-resolution remote sensing image semantic segmentation.",semantic segmentation,ultra-high-resolution,multiscale feature fusion,attention mechanism,"Xin, Chengshu","Sun, Yinan",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_681,"Gao, Tianyi","Gao, Zhi","Ji, Hong",Query Adaptive Transformer and Multiprototype Rectification for Few-Shot Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Deep learning has emerged as a powerful tool for semantic segmentation tasks. However, in some data-deficient and resource-limited scenarios, networks are constrained to learn novel concepts. To tackle this problem, few-shot segmentation (FSS) has been proposed by the machine learning community, aiming at segmenting novel objects by leveraging a handful of annotated samples. However, most advanced FSS algorithms suffer from severe performance degradation when directly applied to remote sensing image (RSI) domains. Challenges arise due to the unique characteristics of RSIs. To address the large intraclass variation brought by various imaging conditions and large-scale variations, a query adaptive transformer (QAT) is proposed. QAT incorporates query priors into the feature extraction process, adapting RSI features to novel objects from query RSIs. It is noteworthy that query priors are extracted via a query prior generation (QPG) module devoid of any query label information. To alleviate interference brought by complex object distribution, a multiprototype strategy is adopted instead of representing objects with a single prototype ambiguously. Moreover, we rectify the prototypes using a prior injection module (PIM), thereby fully leveraging the advantages offered by query priors. The superiority of our method is validated through comprehensive experiments on the public iSAID-5(i) dataset and comparisons with state-of-the-art methods. Finally, we propose a novel cross-domain setting to investigate the potential and generalizability of few-shot RSI segmentation for several Earth observation applications.",Prototypes,Transformers,Feature extraction,Remote sensing,"Ao, Wei","Song, Weiwei",,,Earth,Training,Semantic segmentation,Measurement,Graphical models,Geospatial analysis,Cross-domain,data scarcity,,,,,,few-shot segmentation (FSS),remote sensing image (RSI),Transformer,,,,,,,,,
Row_682,"Chen, Jifa","Chen, Gang","Zhang, Li",Category-sensitive semi-supervised semantic segmentation framework for land-use/land-cover mapping with optical remote sensing images,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,NOV 2024,0,"High-quality land-use/land-cover mapping with optical remote sensing images yet presents significant work. Even though fully convolutional semantic segmentation models have recently contributed to popular solutions, the lack of annotation data may lead to severe degradations in their inference performance. Besides, the category confusion in high-resolution representations will further exacerbate the adverse effects. In this paper, we propose a category-sensitive semi-supervised semantic segmentation framework to address these weaknesses by employing massive unlabeled data. With the perturbations from adopted hybrid data augmentation structures, we first focus on the output space and execute regularization constraints to learn category-specific discriminative features. It is formulated with a consistency self-training procedure where a dynamic class-balanced threshold selection scheme is proposed to provide high-confident pseudo supervisions for each category. In addition, we introduce pixel-wise contrastive learning on the common embedding space from both labeled and unlabeled data domains to further facilitate the semantic dependencies among category features, in which the reliable labels are leveraged as guidance for pixel sample selection. We verify the proposed framework on two benchmark land-use/ land-cover datasets, and the experimental results demonstrate its competitive performance to other state-of-theart semi-supervised methods.",Land-use/land-cover mapping,Semantic segmentation,Semi-supervised learning,Optical remote sensing images,"Huang, Min","Luo, Jin","Ding, Mingjun","Ge, Yong",,,,,,,,,,,,,,,,,,,,,,,,,
Row_683,"Liu, Zeping","Tang, Hong",,Learning Sparse Geometric Features for Building Segmentation from Low-Resolution Remote-Sensing Images,REMOTE SENSING,APR 2023,3,"High-resolution remote-sensing imagery has proven useful for building extraction. Unfortunately, due to the high acquisition costs and infrequent availability of high-resolution imagery, low-resolution images are more practical for large-scale mapping or change tracking of buildings. However, extracting buildings from low-resolution images is a challenging task. Compared with high-resolution images, low-resolution images pose two critical challenges in terms of building segmentation: the effects of fuzzy boundary details on buildings and the lack of local textures. In this study, we propose a sparse geometric feature attention network (SGFANet) based on multi-level feature fusion to address the aforementioned issues. From the perspective of the fuzzy effect, SGFANet enhances the representative boundary features by calculating the point-wise affinity of the selected feature points in a top-down manner. From the perspective of lacking local textures, we convert the top-down propagation from local to non-local by introducing the grounding transformer harvesting the global attention of the input image. SGFANet outperforms competing baselines on remote-sensing images collected worldwide and multiple sensors at 4 and 10 m resolution, thereby, improving the IoU by at least 0.66%. Notably, our method is robust and generalizable, which makes it useful for extending the accessibility and scalability of building dynamic tracking across developing areas (e.g., the Xiong'an New Area in China) by using low-resolution images.",artificial intelligence,deep learning,remote sensing,semantic segmentation,,,,,building extraction,,,,,,,,,,,,,,,,,,,,,,,,
Row_684,"Nanni, Loris","Brahnam, Sheryl","Loreggia, Andrea",An Enhanced Loss Function for Semantic Road Segmentation in Remote Sensing Images,IEEE ACCESS,2024,0,"The analysis of road continuity in satellite images is a complex challenge. This is due to the difficulty in identifying the directional vector of road sections, especially when the satellite view of roads is obstructed by trees or other structures. Today, most research focuses on optimizing the deep learning network topology, however, the accuracy of segmentation is affected by the loss function used in training; currently, little research has been published on ad-hoc loss functions for road segmentation. To solve this problem, we proposed loss functions based on topological pixel analysis, in which more weight is given to problematic pixels representing non-real road breaks. We report the results of different tests, obtaining state-of-the-art performance among convolution neural network-based approaches. For instance, on the Massachusetts Roads dataset, our method achieved a Dice score of 75.34% and an IoU of 60.44%, compared to the best baseline scores of 74.64% and 59.51% achieved by GapLoss. Similarly, on the DeepGlobe Roads dataset, our method obtained a Dice score of 79.78% and an IoU of 66.36%, outperforming the best baseline scores of 78.62% and 64.47% by GapLoss. Both the code and information for replicating our experiments are available at https://github.com/LorisNanni/An-Enhanced-Loss-Function-for-Semantic-Road-Segmentation-in-Remote-Sensing-Images, so as to enable future reliable comparisons.",Roads,Image segmentation,Remote sensing,Task analysis,,,,,Convolution,Feature extraction,Deep learning,Convolutional neural networks,road segmentation,optimization,ensemble,,,,,,,,,,,,,,,,,,
Row_685,"Zhang, Chao","Weng, Liguo","Ding, Li",CRSNet: Cloud and Cloud Shadow Refinement Segmentation Networks for Remote Sensing Imagery,REMOTE SENSING,MAR 2023,26,"Cloud detection is a critical task in remote sensing image tasks. Due to the influence of ground objects and other noises, the traditional detection methods are prone to miss or false detection and rough edge segmentation in the detection process. To avoid the defects of traditional methods, Cloud and Cloud Shadow Refinement Segmentation Networks are proposed in this paper. The network can correctly and efficiently detect smaller clouds and obtain finer edges. The model takes ResNet-18 as the backbone to extract features at different levels, and the Multi-scale Global Attention Module is used to strengthen the channel and spatial information to improve the accuracy of detection. The Strip Pyramid Channel Attention Module is used to learn spatial information at multiple scales to detect small clouds better. Finally, the high-dimensional feature and low-dimensional feature are fused by the Hierarchical Feature Aggregation Module, and the final segmentation effect is obtained by up-sampling layer by layer. The proposed model attains excellent results compared to methods with classic or special cloud segmentation tasks on Cloud and Cloud Shadow Dataset and the public dataset CSWV.",semantic segmentation,deep learning,remote sensing imagery,attention,"Xia, Min","Lin, Haifeng",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_686,"He, Shuke","Jin, Chen","Shu, Lisheng",A new framework for improving semantic segmentation in aerial imagery,FRONTIERS IN REMOTE SENSING,MAR 19 2024,1,"High spatial resolution (HSR) remote sensing imagery presents a rich tapestry of foreground-background intricacies, rendering semantic segmentation in aerial contexts a formidable and vital undertaking. At its core, this challenge revolves around two pivotal questions: 1) Mitigating Background Interference and Enhancing Foreground Clarity. 2) Accurate Segmentation in Dense Small Object Cluster. Conventional semantic segmentation methods primarily cater to the segmentation of large-scale objects in natural scenes, yet they often falter when confronted with aerial imagery's characteristic traits such as vast background areas, diminutive foreground objects, and densely clustered targets. In response, we propose a novel semantic segmentation framework tailored to overcome these obstacles. To address the first challenge, we leverage PointFlow modules in tandem with the Foreground-Scene (F-S) module. PointFlow modules act as a barrier against extraneous background information, while the F-S module fosters a symbiotic relationship between the scene and foreground, enhancing clarity. For the second challenge, we adopt a dual-branch structure termed disentangled learning, comprising Foreground Precedence Estimation and Small Object Edge Alignment (SOEA). Our foreground saliency guided loss optimally directs the training process by prioritizing foreground examples and challenging background instances. Extensive experimentation on the iSAID and Vaihingen datasets validates the efficacy of our approach. Not only does our method surpass prevailing generic semantic segmentation techniques, but it also outperforms state-of-the-art remote sensing segmentation methods.",deep learning,aerial imagery,remote sensing segmentation,foreground saliency enhancement,"He, Xuzhi","Wang, Mingyi","Liu, Gang",,small objects semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_687,"Yang, Yunsong","Li, Jinjiang","Chen, Zheng",GVANet: A Grouped Multiview Aggregation Network for Remote Sensing Image Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"In remote sensing image segmentation tasks, various challenges arise, including difficulties in recognizing objects due to differences in perspective, difficulty in distinguishing objects with similar colors, and challenges in segmentation caused by occlusions. To address these issues, we propose a method called the grouped multiview aggregation network (GVANet), which leverages multiview information for image analysis. This approach enables global multiview expansion and fine-grained cross-layer information interaction within the network. Within this network framework, to better utilize a wider range of multiview information to tackle challenges in remote sensing segmentation, we introduce the multiview feature aggregation block for extracting multiview information. Furthermore, to overcome the limitations of same-level shortcuts when dealing with multiview problems, we propose the channel group fusion block for cross-layer feature information interaction through a grouped fusion approach. Finally, to enhance the utilization of global features during the feature reconstruction phase, we introduce the aggregation-inhibition-activation block for feature selection and focus, which captures the key features for segmentation. Comprehensive experimental results on the Vaihingen and Potsdam datasets demonstrate that GVANet outperforms current state-of-the-art methods, achieving mIoU scores of 84.5% and 87.6%, respectively.",Remote sensing,Feature extraction,Accuracy,Semantic segmentation,"Ren, Lu",,,,Semantics,Convolutional neural networks,Buildings,Attention mechanism,multiscale fusion,remote sensing,semantic segmentation,transformer,,,,,,,,,,,,,,,,,
Row_688,"Li, Xiaoxiang","Huang, Liang","Sun, Yu",Semantic segmentation of buildings in remote sensing images based on dual-path network with rich-scale features,JOURNAL OF ELECTRONIC IMAGING,SEP 1 2022,2,"To solve the problems of low utilization of spatial features and incomplete contour segmentation in building semantic segmentation of remote sensing images, a building semantic segmentation method in remote sensing images based on dual-path with rich-scale features is proposed. In the shallow spatial path of proposed method, Res2Net module and inception module are used to extract shallow rich scale features to avoid improper use of shallow features affecting segmentation results. In the deep semantic path, ResNet50 combined with hybrid dilated convolution is used as the backbone network, and the obtained high-level semantic features are pooled by a spatial pyramid to capture the deeper multi-scale features. Finally, a new feature fusion module is designed to assign weights to feature maps of different levels extracted from two paths. Experimental results on WHU and Massachusetts building datasets show that the proposed method has higher building extraction accuracy and better generalization ability compared with other semantic segmentation networks. (c) 2022 SPIE and IS&T",semantic segmentation of buildings,rich scale,dilated convolution,pyramid pooling,"Wu, Chunyan","Li, Wenguo","Ji, Xinran",,feature fusion,,,,,,,,,,,,,,,,,,,,,,,,
Row_689,"Gao, Kuiliang","Yu, Anzhu","You, Xiong",Integrating Multiple Sources Knowledge for Class Asymmetry Domain Adaptation Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,5,"In the existing unsupervised domain adaptation (UDA) methods for remote sensing images (RSIs) semantic segmentation, class symmetry is a widely followed ideal assumption, where the source and target RSIs have exactly the same class space. In practice, however, it is often very difficult to find a source RSI with exactly the same classes as the target RSI. More commonly, there are multiple source RSIs available. And there is always an intersection or inclusion relationship between the class spaces of each source-target pair, which can be referred to as class asymmetry. Nevertheless, the class asymmetry domain adaptation segmentation of RSIs with multiple sources has not yet been explored. To this end, a novel class asymmetry RSIs domain adaptation method is proposed for the first time in this article, which consists of four key components. First, a multibranch segmentation network is built to learn an expert for each source RSI. Second, a novel collaborative learning method with the cross-domain mixing strategy is proposed, to supplement the class information for each source while achieving the domain adaptation of each source-target pair. Third, a pseudolabel generation strategy is proposed to effectively combine the strengths of different experts, which can be flexibly applied to two cases where the source class union is equal to or includes the target class set. Fourth, a multiview-enhanced knowledge integration module is developed for high-level knowledge routing and transfer from multiple domains to target predictions. The experimental results of six different class settings on airborne and spaceborne RSIs show that the proposed method can effectively perform the multisource domain adaptation in the case of class asymmetry, and the obtained segmentation performance of target RSIs is significantly better than the existing relevant methods.",Class asymmetry,multiple sources,remote sensing images (RSIs),semantic segmentation,"Guo, Wenyue","Li, Ke","Huang, Ningbo",,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,,,
Row_690,"Cheng, Xu","Liu, Lihua","Song, Chen",A Cyclic Information-Interaction Model for Remote Sensing Image Segmentation,REMOTE SENSING,OCT 2021,3,"Object detection and segmentation have recently shown encouraging results toward image analysis and interpretation due to their promising applications in remote sensing image fusion field. Although numerous methods have been proposed, implementing effective and efficient object detection is still very challenging for now, especially for the limitation of single modal data. The use of a single modal data is not always enough to reach proper spectral and spatial resolutions. The rapid expansion in the number and the availability of multi-source data causes new challenges for their effective and efficient processing. In this paper, we propose an effective feature information-interaction visual attention model for multimodal data segmentation and enhancement, which utilizes channel information to weight self-attentive feature maps of different sources, completing extraction, fusion, and enhancement of global semantic features with local contextual information of the object. Additionally, we further propose an adaptively cyclic feature information-interaction model, which adopts branch prediction to decide the number of visual perceptions, accomplishing adaptive fusion of global semantic features and local fine-grained information. Numerous experiments on several benchmarks show that the proposed approach can achieve significant improvements over baseline model.",deep learning,image segmentation,transfer learning,remote sensing image,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_691,"Zhang, Yijie","Cheng, Jian","Su, Yanzhou",Global Adaptive Second-Order Transformer for Remote Sensing Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"In the domain of remote sensing (RS) image analysis, capturing global context is the key for precise semantic segmentation. Current vision transformer (ViT) advance this field by addressing convolutional neural network's (CNN) local receptive field limitations. However, ViT predominantly rely on the first-order information in image to establish global relationships, often overlooking the potential of second-order information, which is crucial for enhancing the discrimination of ground objects that exhibit high similarity and constant changes. To address this issue, we propose a global adaptive second-order transformer network (GASOT-Net). Specifically, the proposed global adaptive second-order transformer (GASOT) enhances the existing ViT structure by mining second-order information and adaptively fusing it with the first-order information during the process of establishing global dependency relationships. This approach enables the extraction of more discriminative features, thereby enriching the representation of global features. In addition, the local feature aggregation module (LFAM) is proposed to effectively aggregate features from different stages of CNN as input to the GASOT blocks. Moreover, to refine boundaries of complex ground objects, the global feature enhancement module (GFEM) is used in the decoder stage. In particular, GFEM includes two sub modules-feature shift module (FSM) and hierarchical feature fusion module (HFFM). FSM is used to enhance the local feature representation at first, and then, HFFM hierarchically aggregates local and global features from different stages. We conduct extensive experiments on four benchmark RS datasets, and the results show that our GASOT-Net outperforms other state-of-the-art methods. The code will be available at: https://github.com/j136812832/GASOT-Net.",Feature enhancement,global feature,second-order transformer,semantic segmentation,"Deng, Changjian","Xia, Ziying","Tashi, Nyima",,Feature enhancement,global feature,second-order transformer,semantic segmentation,,,,,,,,,,,,,,,,,,,,,
Row_692,"Jin, Yuanhang","Liu, Xiaosheng","Huang, Xiaobin",EMR-HRNet: A Multi-Scale Feature Fusion Network for Landslide Segmentation from Remote Sensing Images,SENSORS,JUN 2024,1,"Landslides constitute a significant hazard to human life, safety and natural resources. Traditional landslide investigation methods demand considerable human effort and expertise. To address this issue, this study introduces an innovative landslide segmentation framework, EMR-HRNet, aimed at enhancing accuracy. Initially, a novel data augmentation technique, CenterRep, is proposed, not only augmenting the training dataset but also enabling the model to more effectively capture the intricate features of landslides. Furthermore, this paper integrates a RefConv and Multi-Dconv Head Transposed Attention (RMA) feature pyramid structure into the HRNet model, augmenting the model's capacity for semantic recognition and expression at various levels. Last, the incorporation of the Dilated Efficient Multi-Scale Attention (DEMA) block substantially widens the model's receptive field, bolstering its capability to discern local features. Rigorous evaluations on the Bijie dataset and the Sichuan and surrounding area dataset demonstrate that EMR-HRNet outperforms other advanced semantic segmentation models, achieving mIoU scores of 81.70% and 71.68%, respectively. Additionally, ablation studies conducted across the comprehensive dataset further corroborate the enhancements' efficacy. The results indicate that EMR-HRNet excels in processing satellite and UAV remote sensing imagery, showcasing its significant potential in multi-source optical remote sensing for landslide segmentation.",remote sensing,landslide segmentation,HRNet,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_693,"Sheng, Jiajia","Sun, Youqiang","Huang, He",HBRNet: Boundary Enhancement Segmentation Network for Cropland Extraction in High-Resolution Remote Sensing Images,AGRICULTURE-BASEL,AUG 2022,7,"Cropland extraction has great significance in crop area statistics, intelligent farm machinery operations, agricultural yield estimates, and so on. Semantic segmentation is widely applied to remote sensing image cropland extraction. Traditional semantic segmentation methods using convolutional networks result in a lack of contextual and boundary information when extracting large areas of cropland. In this paper, we propose a boundary enhancement segmentation network for cropland extraction in high-resolution remote sensing images (HBRNet). HBRNet uses Swin Transformer with the pyramidal hierarchy as the backbone to enhance the boundary details while obtaining context. We separate the boundary features and body features from the low-level features, and then perform a boundary detail enhancement module (BDE) on the high-level features. Endeavoring to fuse the boundary features and body features, the module for interaction between boundary information and body information (IBBM) is proposed. We select remote sensing images containing large-scale cropland in Yizheng City, Jiangsu Province as the Agricultural dataset for cropland extraction. Our algorithm is applied to the Agriculture dataset to extract cropland with mIoU of 79.61%, OA of 89.4%, and IoU of 84.59% for cropland. In addition, we conduct experiments on the DeepGlobe, which focuses on the rural areas and has a diversity of cropland cover types. The experimental results indicate that HBRNet improves the segmentation performance of the cropland.",high-resolution remote sensing images,semantic segmentation,transformer,boundary refinement,"Xu, Wenyu","Pei, Haotian","Zhang, Wei","Wu, Xiaowei",cropland extraction,,,,,,,,,,,,,,,,,,,,,,,,
Row_694,"Hong, Eungi","Koo, Jamyoung","Pyo, Seongmin",Tackling Dual Gaps in Remote Sensing Segmentation: Task-Oriented Super-Resolution for Domain Adaptation,IEEE ACCESS,2024,0,"Semantic segmentation of remote sensing images plays a crucial role in various applications, such as land cover mapping and urban planning. However, the performance of semantic segmentation models often degrades when applied to images from different domains or with varying spatial resolutions. In this paper, we propose a novel task-oriented super-resolution method for domain adaptation in remote sensing semantic segmentation. Our approach aims to adapt a segmentation model trained on high-resolution images from a source domain to perform accurately on low-resolution images from a target domain. We introduce a super-resolution network that learns to enhance the spatial resolution of the target domain images while simultaneously optimizing the segmentation performance of a pre-trained and fixed segmentation model. The super-resolution network is trained using a combination of losses, including a segmentation loss, a perceptual loss, and a contrastive loss, which together ensure that the adapted images are both visually similar to the source domain images and semantically consistent with the ground-truth segmentation masks. We evaluate our method on two challenging remote sensing datasets, ISPRS Potsdam and Vaihingen, and demonstrate significant improvements in segmentation accuracy compared to state-of-the-art domain adaptation techniques. Our approach achieves mean Intersection over Union (mIoU) scores of 0.523 and 0.567 on the Potsdam and Vaihingen datasets, respectively. The proposed task-oriented super-resolution method offers a promising solution for adapting semantic segmentation models to new domains and resolutions in remote sensing applications.",Adaptation models,Superresolution,Remote sensing,Feature extraction,"Choi, Haechul","Kim, Eunkyung","Jang, Haneol",,Predictive models,Semantic segmentation,Spatial resolution,Computer architecture,Training,Semantics,Domain adaptation,remote sensing imagery,,,,,,semantic segmentation,task-oriented super-resolution,,,,,,,,,,
Row_695,"Qiu, Kevin","Bulatov, Dimitri","Budde, Lina E.",INFLUENCE OF OUT-OF-DISTRIBUTION EXAMPLES ON THE QUALITY OF SEMANTIC SEGMENTATION IN REMOTE SENSING,,2023,1,"Semantic segmentation for land cover maps follows the closed world assumption, where each pixel must be classified into a set of predefined classes. In order to fulfill this assumption, an additional class is usually introduced to describe all areas not covered by the main classes, called ""clutter"" or ""other"". Consequently, this class is extremely heterogeneous, and the classification is usually subpar. Using a common approach for uncertainty assessment of land cover classification, we analyze the influence of the clutter class being present or absent during training on the semantic segmentation. We assess the model uncertainties of two different deep learning models, U-Net and DeepLab V3+, and different training configurations by using a Monte-Carlo dropout based uncertainty metric. The corresponding uncertainty maps and histograms show a correlation between clutter class and the uncertainty metric.",Semantic segmentation,Monte-Carlo dropout,out-of-distribution,quality assessment,"Kullmann, Timo","Iwaszczuk, Dorota",,,uncertainty,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_696,"Qiao, Wenfan","Shen, Li","Wang, Jicheng",A Weakly Supervised Semantic Segmentation Approach for Damaged Building Extraction From Postearthquake High-Resolution Remote-Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,20,"Quick and accurate building damage assessment following a disaster is critical to making a preliminary estimate of losses. Remote-sensing image analysis based on convolutional neural networks (CNNs) and their relatives has shown a growing potential in this task, but faces the challenge of collecting dense pixel-level annotations. In this letter, we propose a novel weakly supervised semantic segmentation (WSSS) method based on image-level labels for pixel-wise damaged building extraction from postearthquake high-resolution remote-sensing (HRRS) images. The proposed method aims to improve the quality of the class activation map (CAM) to boost model performance. To be specific, a multiscale dependence (MSD) module and a spatial correlation refinement (SCR) module are designed by considering the special characteristics of the damaged building and are integrated into an encoder-decoder network. The former is used for complete and dense localization of damaged buildings in CAM, and the latter contributes to noise suppression. Extensive experimental evaluations over three datasets are conducted to confirm the effectiveness of the proposed approach. Both generated CAMs and extracted damaged building results of our methods are better than that of current state-of-the-art methods.",Class activation map (CAM),convolutional neural network (CNN),damaged building extraction,high-resolution remote-sensing (HRRS) image,"Yang, Xiaotian","Li, Zhilin",,,weakly supervised semantic segmentation (WSSS),,,,,,,,,,,,,,,,,,,,,,,,
Row_697,"Li, Ruoyang","Xiong, Shuping","Che, Yinchao",Research on Efficient Feature Generation and Spatial Aggregation for Remote Sensing Semantic Segmentation,ALGORITHMS,APR 2024,0,"Semantic segmentation algorithms leveraging deep convolutional neural networks often encounter challenges due to their extensive parameters, high computational complexity, and slow execution. To address these issues, we introduce a semantic segmentation network model emphasizing the rapid generation of redundant features and multi-level spatial aggregation. This model applies cost-efficient linear transformations instead of standard convolution operations during feature map generation, effectively managing memory usage and reducing computational complexity. To enhance the feature maps' representation ability post-linear transformation, a specifically designed dual-attention mechanism is implemented, enhancing the model's capacity for semantic understanding of both local and global image information. Moreover, the model integrates sparse self-attention with multi-scale contextual strategies, effectively combining features across different scales and spatial extents. This approach optimizes computational efficiency and retains crucial information, enabling precise and quick image segmentation. To assess the model's segmentation performance, we conducted experiments in Changge City, Henan Province, using datasets such as LoveDA, PASCAL VOC, LandCoverNet, and DroneDeploy. These experiments demonstrated the model's outstanding performance on public remote sensing datasets, significantly reducing the parameter count and computational complexity while maintaining high accuracy in segmentation tasks. This advancement offers substantial technical benefits for applications in agriculture and forestry, including land cover classification and crop health monitoring, thereby underscoring the model's potential to support these critical sectors effectively.",semantic segmentation,lightweight architecture,attention mechanism,linear transformation,"Shi, Lei","Ma, Xinming","Xi, Lei",,neighborhood feature optimization,,,,,,,,,,,,,,,,,,,,,,,,
Row_698,"Boulila, Wadii","Ghandorh, Hamza","Masood, Sharjeel",A transformer-based approach empowered by a self-attention technique for semantic segmentation in remote sensing,HELIYON,APR 30 2024,2,"Semantic segmentation of Remote Sensing (RS) images involves the classification of each pixel in a satellite image into distinct and non-overlapping regions or segments. This task is crucial in various domains, including land cover classification, autonomous driving, and scene understanding. While deep learning has shown promising results, there is limited research that specifically addresses the challenge of processing fine details in RS images while also considering the high computational demands. To tackle this issue, we propose a novel approach that combines convolutional and transformer architectures. Our design incorporates convolutional layers with a low receptive field to generate fine-grained feature maps for small objects in very high-resolution images. On the other hand, transformer blocks are utilized to capture contextual information from the input. By leveraging convolution and self-attention in this manner, we reduce the need for extensive downsampling and enable the network to work with full-resolution features, which is particularly beneficial for handling small objects. Additionally, our approach eliminates the requirement for vast datasets, which is often necessary for purely transformer-based networks. In our experimental results, we demonstrate the effectiveness of our method in generating local and contextual features using convolutional and transformer layers, respectively. Our approach achieves a mean dice score of 80.41%, outperforming other well-known techniques such as UNet, Fully-Connected Network (FCN), Pyramid Scene Parsing Network (PSP Net), and the recent Convolutional vision Transformer (CvT) model, which achieved mean dice scores of 78.57%, 74.57%, 73.45%, and 62.97% respectively, under the same training conditions and using the same training dataset.",Semantic segmentation,Self-attention,Vision transformer,Satellite images,"Alzahem, Ayyub","Koubaa, Anis","Ahmed, Fawad","Khan, Zahid",Remote sensing,,,,,,,,"Ahmad, Jawad",,,,,,,,,,,,,,,,
Row_699,"Li, Xin","Xu, Feng","Liu, Fan",Hybridizing Euclidean and Hyperbolic Similarities for Attentively Refining Representations in Semantic Segmentation of Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,12,"Attention mechanisms (AMs) have revolutionized the semantic segmentation network in interpreting remote sensing images (RSIs) due to their amazing ability in establishing contextual dependencies. Nevertheless, due to the complex scenes and diverse objects in RSIs, a variety of details and correlations are not available in Euclidean space. Therefore, a similarity-hybrid attention module (SHAM) is devised to attentively learn the hyperbolic and Euclidean attention maps between any two positions, followed by a weighted elementwise summation. The hybrid attention maps posses latent geometric properties of both Euclidean and hyperboloid. Taking commonly used fully convolutional network (FCN) as baseline, hybrid attention-enhanced neural network (HAENet) that embeds SHAM is presented. Experiments on International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and DeepGlobe benchmarks reveal its superiority to comparative methods. In addition, the ablation study validates the effectiveness of SHAM compared with other attention modules.",Attention mechanism (AM),hyperbolic geometry,semantic segmentation,similarity-hybrid attention,"Xia, Runliang","Tong, Yao","Li, Linyang","Xu, Zhennan",,,,,,,,,"Lyu, Xin",,,,,,,,,,,,,,,,
Row_700,"Wu, Junfeng","Tang, Zhenjie","Xu, Congan",Super-resolution domain adaptation networks for semantic segmentation via pixel and output level aligning,FRONTIERS IN EARTH SCIENCE,AUG 25 2022,4,"Recently, unsupervised domain adaptation (UDA) has attracted increasing attention to address the domain shift problem in the semantic segmentation task. Although previous UDA methods have achieved promising performance, they still suffer from the distribution gaps between source and target domains, especially the resolution discrepancy in the remote sensing images. To address this problem, this study designs a novel end-to-end semantic segmentation network, namely, Super-Resolution Domain Adaptation Network (SRDA-Net). SRDA-Net can simultaneously achieve the super-resolution task and the domain adaptation task, thus satisfying the requirement of semantic segmentation for remote sensing images, which usually involve various resolution images. The proposed SRDA-Net includes three parts: a super-resolution and segmentation (SRS) model, which focuses on recovering high-resolution image and predicting segmentation map, a pixel-level domain classifier (PDC) for determining which domain the pixel belongs to, and an output-space domain classifier (ODC) for distinguishing which domain the pixel contribution is from. By jointly optimizing SRS with two classifiers, the proposed method can not only eliminate the resolution difference between source and target domains but also improve the performance of the semantic segmentation task. Experimental results on two remote sensing datasets with different resolutions demonstrate that SRDA-Net performs favorably against some state-of-the-art methods in terms of accuracy and visual quality. Code and models are available at .",remote sensing,semantic segmentation,domain adaptation,super resolution,"Liu, Enhai","Gao, Long","Yan, Wenjun",,deep learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_701,"Yang, Yunsong","Yuan, Genji","Li, Jinjiang",SFFNet: A Wavelet-Based Spatial and Frequency Domain Fusion Network for Remote Sensing Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"To fully utilize spatial information for segmentation and address the challenge of handling areas with significant grayscale variations in remote sensing segmentation, we propose the spatial and frequency domain fusion network (SFFNet) framework. This framework employs a two-stage network design: the first stage extracts features using spatial methods to obtain features with sufficient spatial details and semantic information; the second stage maps these features in both spatial and frequency domains. In the frequency domain mapping, we introduce the wavelet transform feature decomposer (WTFD) structure, which decomposes features into low-frequency and high-frequency components using the Haar wavelet transform and integrates them with spatial features. To bridge the semantic gap between frequency and spatial features, facilitating significant feature selection to promote the combination of features from different representation domains, we design the multiscale dual-representation alignment filter (MDAF). This structure utilizes multiscale convolutions and dual-cross attentions. Comprehensive experimental results demonstrate that, compared to existing methods, SFFNet achieves superior performance in terms of mean intersection over union (mIoU), reaching 84.80% and 87.73%, respectively. The code is located at https://github.com/yysdck/SFFNet.",Image segmentation,Remote sensing,Frequency-domain analysis,Wavelet transforms,,,,,Feature extraction,Semantics,Wavelet domain,Attention mechanism,frequency domain features,global modeling,remote sensing,semantic segmentation,,,,,,wavelet transform,,,,,,,,,,,
Row_702,"Liu, Hao","Sun, Bin","Gao, Zhihai",High resolution remote sensing recognition of elm sparse forest via deep-learning-based semantic segmentation,ECOLOGICAL INDICATORS,SEP 2024,1,"Elm (Ulmus pumila L.) sparse forest plays an vital role in maintaining local ecological stability and security in the Otingdag Sandy Land area. Prior studies on elm canopy extraction have predominantly relied on manual parameter configuration, resulting in unsatisfactory levels of generalization. To meet the needs of high-precision and rapid recognition of elm sparse forests in large areas, this study proposed a recognition method for elm sparse forest that orients to high spatial resolution remote sensing imageries, using deep-learning-based semantic segmentation techniques. It can automatically learn features that are conducive to segmenting the canopy of elm trees, and retains good generalization ability on the Gaofen-2 imageries obtained in different regions. First, we constructed a dataset specialized for elm canopy semantic segmentation task, and annotated over 130,000 elm canopies based on Gaofen-2 imageries. In addition, we trained 7 deep-learning semantic segmentation model candidates. Among them, MANet showed the best performance, with its F1-score reaching 81.44%. Lastly, we applied edge detection to the elm canopy coverage area, and automatically extract the elm canopy. The proposed method can provide technical support for the investigation and monitoring of elm sparse forests, while facilitates local desertification prevention efforts in the entire Otingdag Sandy Region.",Otingdag Sandy Land,Elm Sparse Forest,Gaofen-2,Semantic Segmentation,"Chen, Zhulin","Zhu, Zhongzheng",,,Deep Learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_703,"Yu, Zhimin","Wan, Fang","Lei, Guangbo",RSLC-Deeplab: A Ground Object Classification Method for High-Resolution Remote Sensing Images,ELECTRONICS,SEP 2023,2,"With the continuous advancement of remote sensing technology, the semantic segmentation of different ground objects in remote sensing images has become an active research topic. For complex and diverse remote sensing imagery, deep learning methods have the ability to automatically discern features from image data and capture intricate spatial dependencies, thus outperforming traditional image segmentation methods. To address the problems of low segmentation accuracy in remote sensing image semantic segmentation, this paper proposes a new remote sensing image semantic segmentation network, RSLC-Deeplab, based on DeeplabV3+. Firstly, ResNet-50 is used as the backbone feature extraction network, which can extract deep semantic information more effectively and improve the segmentation accuracy. Secondly, the coordinate attention (CA) mechanism is introduced into the model to improve the feature representation generated by the network by embedding position information into the channel attention mechanism, effectively capturing the relationship between position information and channels. Finally, a multi-level feature fusion (MFF) module based on asymmetric convolution is proposed, which captures and refines low-level spatial features using asymmetric convolution and then fuses them with high-level abstract features to mitigate the influence of background noise and restore the lost detailed information in deep features. The experimental results on the WHDLD dataset show that the mean intersection over union (mIoU) of RSLC-Deeplab reached 72.63%, the pixel accuracy (PA) reached 83.49%, and the mean pixel accuracy (mPA) reached 83.72%. Compared to the original DeeplabV3+, the proposed method achieved a 4.13% improvement in mIoU and outperformed the PSP-NET, U-NET, MACU-NET, and DeeplabV3+ networks.",high-resolution remote sensing images,semantic segmentation,feature fusion,attention mechanism,"Xiong, Ying","Xu, Li","Ye, Zhiwei","Liu, Wei",,,,,,,,,"Zhou, Wen",,"Xu, Chengzhi",,,,,,,,,,,,,,
Row_704,"Ren, Yan","Long, Jie","Gao, Xiaowen",TPL-DA: A Novel Threshold-Free Pseudolabel Learning Framework for Domain Adaptive Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2025,0,"Semantic segmentation techniques for remote sensing scene understanding have significantly advanced, enhancing the refined Earth observation. However, most methods highly depend on extensive annotated data, leading to performance deterioration in complex high-resolution remote sensing cross-domain scenes, where variations in image conditions and environments are prevalent. Domain adaptive semantic segmentation (DASS) has been proposed to mitigate the reliance on dense and costly annotations, typically using stagewise training. This article addresses three key challenges in existing DASS methods: 1) insufficient warmup training, limiting potential performance gains; 2) rigid pseudolabel threshold settings in self-training (ST) result in performance bottlenecks; 3) entropy-based prediction bias alone fails to effectively identify high-confidence noise early in ST. To address these issues, we propose a novel threshold-free pseudolabel learning framework, TPL-DA. During the warmup stage, we introduce a multiview bidirectional consistency learning mechanism within a teacher-student architecture. This mechanism employs a bias-free data augmentation strategy, fostering consistent bidirectional predictions in teacher-student networks, thereby enhancing domain generalization and feature robustness. Our multiscale context-enhanced prediction module further amplifies this. In the ST stage, we propose a dynamic threshold-free pseudolabel learning strategy that utilizes well-aligned feature prototypes in the feature space to guide pseudolabel generation in the probability space, eliminating the threshold constraints. In addition, we model uncertainty using relative entropy and incorporate it into the optimization objective to manage high-confidence noise. Extensive experiments on the LoveDA, Potsdam, and Vaihingen datasets demonstrate that TPL-DA consistently outperforms existing methods and popular benchmarks, significantly enhancing DASS performance across diverse cross-domain scenes.",Domain adaptive semantic segmentation (DASS),high-resolution remote sensing (HRRS),self-training (ST),threshold,"Zhang, Ming","Liu, Guoqing","Su, Nan",,uncertainty estimation,Domain adaptive semantic segmentation (DASS),high-resolution remote sensing (HRRS),self-training (ST),threshold,uncertainty estimation,,,,,,,,,,,,,,,,,,,
Row_705,"Chen, Dong","Wang, Yuebin","Zhang, Liqiang",DA&MTSS: An End-to-End Remote Sensing Image Domain Adaptive Semantic Segmentation Framework Combining Data Augmentation and Mobile Threshold Self-Supervision,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"The application of deep learning-based semantic segmentation in remote sensing (RS) images has achieved considerable success. However, many supervised methods still heavily rely on a large amount of labeled data, requiring time-consuming and labor-intensive manual annotations. Besides, networks trained on labeled source domain data often perform poorly in inference tasks with target domain data due to the domain shift phenomenon. To address these challenges, we construct a novel end-to-end unsupervised domain adaptation (UDA) framework, named data augmentation and mobile threshold self-supervision (DA&MTSS), which integrates data augmentation with self-supervision. Specifically, we analyze the common factors that cause domain shifts in RS images and adopt different data augmentation techniques to attenuate the domain shift and enhance the generalization ability and robustness of the network in cross-domain inference. In the self-supervision phase, we design a new sample-based mobile threshold method to dynamically control the thresholds of both dominant and long-tail classes during training and generate stable pseudo-labels. Therefore, our method eliminates the need for additional training or expert knowledge and achieves the co-evolution of network parameters and pseudo-label quality in the training process. The results of comprehensive experiments on five tasks across the ISPRS Vaihingen, Potsdam, and LoveDA datasets demonstrate that this method consistently achieves higher mIoU scores, showcasing the performance advantage of DA&MTSS in UDA for the semantic segmentation of RS image.",Training,Task analysis,Semantic segmentation,Semantics,,,,,Data augmentation,Spatial resolution,Sensors,reliable pseudo-labels,remote sensing (RS) image,self-supervision,self-training,unsupervised domain adaptation (UDA) semantic segmentation,,,,,,,,,,,,,,,,,
Row_706,"Zhong, Hai-Feng","Sun, Hong-Mei","Han, Dong-Nuo",Lake water body extraction of optical remote sensing images based on semantic segmentation,APPLIED INTELLIGENCE,DEC 2022,18,"Automatically extract lake water bodies of optical remote sensing images is a very challenging task, because there are many small lakes in such images, these small lakes have the characteristics of weak target information and are easily interfered by noise information. Regarding above problems, this paper proposes an automatic extraction method of lake water based on semantic segmentation. Firstly, a multi-scale information enhancement network is designed based on the encoder-decoder structure, and the deep dilation residual structure is used in the encoder module of the network to improve the network's ability to mine the deep feature information and the context information of the lake water bodies. Secondly, the two-way channel attention mechanism is introduced into the network, which can reduce the interference of noise information on the lake boundaries and improve the accuracy of the network to the lake boundaries segmentation. Finally, the up-sampling convolution operation is used in the decoder module of the network to reduce the information loss during the up-sampling process. In this paper, the performance of the designed network is tested by using remote sensing images of lakes of different map scales and various evaluation indexes. The experimental results show that the designed network has better segmentation accuracy than other semantic segmentation networks.",Lake water body extraction,Optical remote sensing images,Weak target information,Deep dilation residual structure,"Li, Zeng-Hu","Jia, Rui-Sheng",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_707,"Liang, Zhengyin","Wang, Xili",,Semantic Segmentation of Multispectral Remote Sensing Images Based on Band-Location Adaptive Selection,LASER & OPTOELECTRONICS PROGRESS,JUL 2023,1,"Multispectral remote sensing images (MSIs) provide a substantial amount of ground object information spread over various spectral bands of the image. The quantity of information contained in different bands or different spatial locations within the same band varies significantly. How to capture useful information from MSIs is a challenging task in semantic segmentation of remote sensing images. An end -to -end semantic segmentation network (BLASeNet) based on band -location adaptive selection is proposed here. The proposed network adopts an encoder -decoder structure. In the coding phase, a band -location adaptive selection mechanism is proposed to adaptively learn the weights of different bands and different spatial locations within the same band, enhancing the effective features expression. The spectral -spatial features of 3D residual block -coded images are further proposed to make use of the band correlation of MSIs. During the decoding phase, an adaptive feature fusion module is proposed to adaptively adjust the fusion ratio of low-level detail features and high-level semantic features via network learning, as well as investigate the impact of three fusion strategies, namely, addition (BLASeNet-A), element multiplication (BLASeNet-M), and concatenation (BLASeNet-C), on the model's performance gain. Furthermore, channel attention is extended to 3D data, and the fused feature map is recalibrated on the channel dimension to produce a more accurate multi -level interactive feature map. The effectiveness of BLASeNet has been demonstrated by experimental results on ISPRS Potsdam, Qinghai and Tibet Plateau datasets.",image processing,semantic segmentation,3D convolution,band-location adaptive selection mechanism,,,,,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,
Row_708,"Lyu, Xinran","Zhang, Libao",,PROGRESSIVE REFINEMENT LEARNING BASED ON FEATURE INTERACTIVE FUSION FOR SEMANTIC SEGMENTATION OF REMOTE SENSING LIMITED DATASET,,2023,0,"Due to the labor cost and the accuracy of manual identification, it is very difficult to make a strong label dataset of remote sensing images with a large amount of data. Therefore, the limited remote sensing dataset has become a research hotspot in recent years. However, due to insufficient precision and the lack of label accuracy, these methods often have insufficient expression ability. In this paper, we proposed a semantic segmentation method for remote sensing images by progressive refinement learning. Firstly, we construct multiple classification networks to vote for label noise cleaning, and select a network to retrain. Then, the method based on hierarchical feature learning is used to realize the pixel-level pseudo label calculation. Secondly, we proposed to construct feature interactive fusion module in the multi-level codec to achieve image group semantic segmentation. Comprehensive evaluations and the comparison with 7 methods validate the superiority of the proposed model.",Remote sensing,limited dataset,progressive refinement learning,feature interactive fusion,,,,,,,,,,,,,,"2023 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",,,,,,,,,,,,,,,
Row_709,"Zhang, Chunsen","Ge, Yingwei",,Semantic Segmentation of Buildings in High Resolution Remote Sensing Images Using Conditional Random Fields,,2020,0,"This paper proposes a method of building a semantic segmentation method for high-resolution remote sensing images of conditional random fields. Through a large number of actual data operations comparison, U-Net semantic segmentation model is selected as the improved basic model in many deep convolutional neural network models. In order to improve the singularity of the upsampling operation, the U-Net semantic segmentation model is improved as follows: First, the model's crop-copy connection structure is changed to the pyramid pooling layer, and then the multi-scale representation feature image is used, and the multi-scale is used. The resampling of the feature image and the fine bilinear interpolation yield the maximum response at different scales. The improved U-Net model extracts more complete image features. The rough segmentation results are used as the initial input values of the fully connected conditional random fields (CRFs). The global pixel potential energy is inferred through the fully connected graph, and the feature images are refined. Target matching. Finally, the image features are input to the sigmoid classifier for analysis. The results show that the CRF-SUNet model with introduced conditional random field has high segmentation precision, and the boundary of the segmented building is clear, smooth and complete.",Deep learning,Semantic segmentation,Convolutional neural network,Conditional random field,,,,,,,,,,,,,,MIPPR 2019: PATTERN RECOGNITION AND COMPUTER VISION,,,,,,,,,,,,,,,
Row_710,"Gao, Kuiliang","Yu, Anzhu","You, Xiong",Prototype and Context-Enhanced Learning for Unsupervised Domain Adaptation Semantic Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,12,"In unsupervised domain adaptation (UDA) of remote sensing images (RSIs), the huge interdomain discrepancies and intradomain variances lead to complicated class-level relations. Specifically, the instances of the same class differ greatly, while instances of different classes are similar, whether across different RSIs domains or within the same RSIs domain. However, existing methods cannot fully consider these problems, limiting the performance of UDA semantic segmentation of RSIs. To this end, this article proposes a novel cross-domain multiprototypes learning method, the core idea of which is to abstract the cross-domain and intradomain class-level relations into multiple prototypes. Specifically, the multiple prototypes belonging to different classes can detailedly describe complex interclass relations, and the multiple prototypes within the same class can better model rich intraclass relations. Furthermore, the source and target samples are jointly used for prototypes calculation, to fully fuse the feature information of different RSIs. In a nutshell, utilizing the samples from different RSIs domains to learn multiple prototypes for each class can achieve better domain alignment at the class level. In addition, considering that RSIs simultaneously contain large targets with wide coverage and important small targets, two masked consistency learning strategies are designed to better explore the contextual structure of target RSIs and improve the quality of pseudo labels for prototype updating. The global consistency strategy can strengthen the utilization of global context relations, while the local consistency strategy can further improve the learning of local context details. Therefore, the proposed method is actually a prototype and context-enhanced learning method for UDA semantic segmentation of RSIs. Extensive experiments demonstrate that the proposed method can achieve better performance than existing state-of-the-art UDA methods.",Prototypes,Semantic segmentation,Semantics,Adaptation models,"Qiu, Chunping","Liu, Bing",,,Training,Task analysis,Supervised learning,Context enhancement,cross-domain multiprototypes,remote sensing images (RSIs),semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,
Row_711,"Zhang, Qing","Pan, Shulin","Min, Fan",Noisy Supervised Deep Learning for Remote Sensing Image Segmentation Using Electronic Maps,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,5,"Deep learning has made substantial progress in remote sensing image segmentation tasks. It usually requires a large number of high-quality annotation maps (i.e., clean labels), which are labor-intensive. In this letter, we propose to use abundant electronic maps (i.e., noisy labels) to supplement a small number of clean labels to solve the problem of massive label production. In addition, we propose a multistage noise supervised framework to prevent noise from deteriorating the performance of deep model. Noise-supervised deep learning framework (NSDI) consists of clean training, weight initialization, and hybrid training stages. In the clean training stage, we train a segmentation model using a small amount of clean labels and remote sensing images to compute the confusion probability matrix. In the weight initialization stage, we use the confusion probability matrix and the prediction probability to calculate the label reliability of the electronic map. In the hybrid training stage, an adaptive weighted loss function based on cross-entropy is used to dynamically update the label reliability. Then we train the model further using electronic maps with the support of the adaptive weighted loss function and label reliability. Experiments were undertaken on 2656 images of 512 x 512 pixels. Ablation studies show that NSDI improves the model robustness and the segmentation quality.",Electronic map,remote sensing image,SegFormer,semantic segmentation,"Wu, Yinghe","Zhang, Zilin","Luo, Haoran",,weakly supervision,,,,,,,,,,,,,,,,,,,,,,,,
Row_712,"Chen, Hongyu","Zhang, Hongyan","Yang, Guangyi",A Mutual Information Domain Adaptation Network for Remotely Sensed Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,10,"Although deep learning has made semantic segmentation of very-high-resolution (VHR) remote sensing (RS) images practical and efficient, its large-scale application is still limited. Given the diversity of imaging sensors, acquisition conditions, and regional styles, a deep learning network well-trained on one source domain dataset often suffers from drastic performance drops when applied to other target domain datasets. Thus, we propose a novel end-to-end mutual information domain adaptation network (MIDANet) that can shift between semantic segmentation domains by integrating multitask learning in the convolutional neural networks within an entropy adversarial learning (EAL) framework. Through the joint learning of semantic segmentation and elevation estimation, the features extracted by MIDANet can concentrate more on the elevation clues while dropping the domain-variant information (i.e., texture, spectral information). First, one encoder is applied to excavate general semantic features. Two decoders that share the same architecture are used to perform pixel-level classification and digital surface model (DSM) regression. Second, feature interaction modules (FIMs) and a mutual information attention unit (MIAU) are designed to mine the latent relationships between the two tasks and enhance their feature representations. Finally, a final MIDANet is obtained for semantic segmentation that does not require any semantic segmentation labels in the target domain after the adversarial learning of the classification entropy at the output level. Extensive comparative experiments and ablation studies were conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Vaihingen test datasets. The results show that MIDANet outperforms other state-of-the-art domain adaptation (DA) methods in both evaluation metrics and visual assessment.",Adversarial learning,domain adaptation (DA),multitask learning,semantic segmentation,"Li, Shengyang","Zhang, Liangpei",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_713,Lu Junyan,Jia Hongguang,Gao Fang,Reconstruction of Digital Surface Model of Single-view Remote Sensing Image by Semantic Segmentation Network,JOURNAL OF ELECTRONICS & INFORMATION TECHNOLOGY,APR 2021,2,"A novel method for Digital Surface Model (DSM) reconstruction of single-view remote sensing image is proposed which only relies on light detection and ranging data. Based on deep learning technology, a semantic segmentation network with an encode-decode structure is designed. The network uses Multi-scale Residual Fusion Encode and Decode (MRFED) blocks to extract semantic information from the input image, and then predicts the height value pixel by pixel, as well as adopts a strategy of skip connections with feature maps to preserves the detailed features and structural information of the input image. The model is trained and tested on a public dataset of remote sensing images containing DSM data. Experiments show that, the Mean Absolute Error ( MAE) between DSM reconstruction results and true values is 2.1e-02, the Root Mean Square Error (RMSE) is 3.8e-02, and the Structural SIMilarity (SSIM) is 92.89%, which are all better than the classic deep learning semantic segmentation networks. Experiments confirm that the method can effectively reconstruct the DSM of single-view remote sensing images with high accuracy, as well as the structure of feature distribution.",Semantic segmentation network,Encode,decode,Multi-scale residual fusion,Li Wentao,Lu Qing,,,Skip connections,Digital Surface Model (DSM),,,,,,,,,,,,,,,,,,,,,,,
Row_714,"Zhu, Xinyu","Zhang, Zhihua","He, Yi",LandslideNet: A landslide semantic segmentation network based on single-temporal optical remote sensing images,ADVANCES IN SPACE RESEARCH,NOV 15 2024,2,"Swiftly and accurately acquiring the spatial distribution, location, and magnitude of landslides while documenting them in a landslide cataloging database can furnish crucial information for precise disaster mitigation measures and secondary hazard prevention. The extraction of landslides using existing semantic segmentation algorithms may give rise to issues such as false detection and missed detection due to the diverse shape and texture features of landslides in remote sensing images, the abundance of spectral features, and the complexity of the environment. In this article, we proposed LandslideNet, a novel model specifically designed for accurate segmentation of landslides in single-temporal high spatial resolution optical remote sensing images. By constructing a landslide image dataset and employing the LandslideNet model, we successfully identify and segment landslides with high precision. Quantitative experimental results demonstrate that our LandslideNet achieves superior performance compared to widely used semantic segmentation models including U-Net, PSPNet, Deeplabv3+, HRNetv2, Segformer and GELAN-c with F1-score, mIoU, FWIoU, mPA and OA reaching 72.53 %, 78.41 %, 99.86 %, 83.33 % and 99.93 % respectively. Moreover, our model exhibits lower complexity while demonstrating improved capability in detecting landslides with complex shapes and different sizes. (c) 2024 COSPAR. Published by Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",Remote sensing,Landslide extraction,Deep learning,YOLOv8,"Wang, Wei","Yang, Shuwen","Hou, Yuhao",,Google earth,,,,,,,,,,,,,,,,,,,,,,,,
Row_715,"Li, Jiaojiao","Liu, Yuzhe","Liu, Jiachao",Feature Guide Network With Context Aggregation Pyramid for Remote Sensing Image Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,6,"In recent years, the deep learning method based on fully convolution networks has proven to be an effective method for the semantic segmentation of remote sensing images (RSIs). However, the rich information and complex content of RSIs make networks training for segmentation more challenging. Specifically, the observing distance between the space-borne cameras and the ground objects is extraordinarily far, resulting in that some smaller objects only occupy a few pixels in the image. However, due to the rapid degeneration of tiny objects during the training process, most algorithms cannot properly handle these common small objects in RSIs with satisfactory results. In this article, we propose a novel feature guide network with a context aggregation pyramid (CAP) for RSIs segmentation to conquer these issues. An innovative edge-guide feature transform module is designed to take advantage of the edge and body information of objects to strengthen edge contours and the internal consistency in homogeneous regions, which can explicitly enhance the representation of tiny objects and relieve the degradation of small objects. Furthermore, we design a CAP pooling strategy to adaptively capture optimal feature characterization that can assemble multiscale features according to the significance of different contexts. Extensive experiments on three large-scale remote sensing datasets demonstrate that our method not only can outperform the state-of-the-art methods for objects of different scales but can also achieve robust segmentation results, especially for tiny objects.",Context aggregation pyramid (CAP),deep learning,edge guide,remote sensing images (RSIs),"Song, Rui","Liu, Wei","Han, Kailiang","Du, Qian",semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_716,"Chong, Qianpeng","Ni, Mengying","Huang, Jianjun",Let the loss impartial: a hierarchical unbiased loss for small object segmentation in high-resolution remote sensing images,EUROPEAN JOURNAL OF REMOTE SENSING,DEC 31 2023,0,"The progress in optical remote sensing technology presents both a possibility and challenge for small object segmentation task. However, the gap between human vision cognition and machine behavior still poses an inherent constrains to the interpretation of small but key objects in large-scale remote sensing scenes. This paper summarizes this gap as a bias of the machine against small object segmentation task, called scale-induced bias. The scale-induced bias causes the degradation in the performance of conventional remote sensing image segmentation methods. Therefore, this paper applies a straightforward but innovative insight to mitigate the scale-induced bias. Specifically, we propose a universal impartial loss, which leverages the hierarchical approach to alleviate two sub-problems separately. The pixel-level statistical methodology is applied to remove the bias between the background and small objects, and an emendation vector is introduced to alleviate the bias between small object categories. Extensive experiments explicitly manifest that our method is fully compatible with the existing segmentation structures, armed with the hierarchical unbiased loss, these structures will achieve satisfactory improvement. The proposed method is validated on two benchmark remote sensing image datasets, where it achieved a competitive performance and could narrow the gap between the human vision cognition and machine behavior.",Hierarchical solution,remote sensing,small object,segmentation,"Wei, Guangyi","Li, Ziyi","Xu, Jindong",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_717,"Xue, Gunagkuo","Liu, Yikun","Huang, Yuwen",AANet: an attention-based alignment semantic segmentation network for high spatial resolution remote sensing images,INTERNATIONAL JOURNAL OF REMOTE SENSING,JUL 3 2022,1,"In this paper, we present an efficient network to tackle three critical problems in high spatial resolution (HSR) remote sensing image segmentation: ( i ) feature misalignment, ( i i ) insufficient contextual information extraction and ( i i i ) various class imbalance issues. In detail, we propose a novel Feature Alignment Block (FAB) to suppress misalignment issues with the guide of an anchor map. Further, to extract sufficient information, we design a Contextual Augmentation Block (CAB) to augment features of different semantic levels. Finally, we present an Annealing Online Hard Example Mining (AOHEM) strategy to handle the various class imbalance issues with a view to dynamically adjust the focus of the network. We apply the above proposed designs to FPN to form our Attention-based Alignment Network (AANet). Experimental results demonstrate that the proposed method achieves promising results on the challenging iSAID and Vaihingen datasets with a better trade-off between accuracy and complexity.",Semantic segmentation,attention,feature alignment,hard example mining,"Li, Mingsong","Yang, Gongping",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_718,"Qiu, Luyi","Yu, Dayu","Zhang, Chenxiao",A Local-Global Framework for Semantic Segmentation of Multisource Remote Sensing Images,REMOTE SENSING,JAN 2023,5,"Recently, deep learning has been widely used in the segmentation tasks of remote sensing images. However, the existing deep learning method most focus on local contextual information and has limited field of perception, which makes it difficult to capture the long-range contextual feature of objects at large scales form very-high-resolution (VHR) images. In this paper, we present a novel Local-global Framework consisting of the dual-source fusion network and local-global transformer modules, which efficiently utilize features extracted from multiple sources and fully capture features of local and global regions. The dual-source fusion network is an encoder designed to extract features from multiple sources such as spectra, synthetic aperture radar, and elevations, which selective fuse features from multiple sources and reduce the interference of redundant features. The local-global transformer module is proposed to capture fine-grained local features and coarse-grained global features, which enables the framework to focus on recognizing multiple-scale objects from the local and global regions. Moreover, we propose a pixelwise contrastive loss, which could encourage that the prediction is pulled closer to the ground truth. The Local-global Framework achieves state-of-the-art performance with 90.45% mean f1 score on the ISPRS Vaihingen dataset and 93.20% mean f1 score on the ISPRS Potsdam dataset.",semantic segmentation,deep learning,multisource image,global-local feature fusion,"Zhang, Xiaofeng",,,,contrastive learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_719,"Shi, Hao","Fan, Jiahe","Wang, Yupei",Dual Attention Feature Fusion and Adaptive Context for Accurate Segmentation of Very High-Resolution Remote Sensing Images,REMOTE SENSING,SEP 2021,11,"Land cover classification of high-resolution remote sensing images aims to obtain pixel-level land cover understanding, which is often modeled as semantic segmentation of remote sensing images. In recent years, convolutional network (CNN)-based land cover classification methods have achieved great advancement. However, previous methods fail to generate fine segmentation results, especially for the object boundary pixels. In order to obtain boundary-preserving predictions, we first propose to incorporate spatially adapting contextual cues. In this way, objects with similar appearance can be effectively distinguished with the extracted global contextual cues, which are very helpful to identify pixels near object boundaries. On this basis, low-level spatial details and high-level semantic cues are effectively fused with the help of our proposed dual attention mechanism. Concretely, when fusing multi-level features, we utilize the dual attention feature fusion module based on both spatial and channel attention mechanisms to relieve the influence of the large gap, and further improve the segmentation accuracy of pixels near object boundaries. Extensive experiments were carried out on the ISPRS 2D Semantic Labeling Vaihingen data and GaoFen-2 data to demonstrate the effectiveness of our proposed method. Our method achieves better performance compared with other state-of-the-art methods.",deep learning,land cover classification,semantic segmentation,,"Chen, Liang",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_720,"Du, Bowen","Zhao, Zirong","Hu, Xiao",Landslide susceptibility prediction based on image semantic segmentation,COMPUTERS & GEOSCIENCES,OCT 2021,46,"The visual characteristics of landslide susceptibility have not yet been fully explored. Professional or trained technicians have to take much time and effort to interpret remote sensing images and locate landslides accordingly. Although conventional machine learning methods based on hand-crafted features for landslide susceptibility prediction (LSP) have acquired remarkable performance, they have certain requirements for prior knowledge. Aiming to learn complex and inherent visual patterns of landslides through minimal manual intervention and achieve fine-grained prediction, in this paper, we define LSP as a semantic segmentation problem on optical remote sensing images. Six widely used semantic segmentation models including Fully Convolutional Network, U-Net, Pyramid Scene Parsing Network, Global Convolutional Network (GCN), DeepLab v3 and DeepLab v3+ are introduced and evaluated for LSP. As the lack of landslide datasets, an open labeled landslide dataset of remote sensing imagery is created for research. The results show that GCN and DeepLab v3 are more applicable for this problem scenario, and the best Mean Intersection-over-Union and Pixel Accuracy of models are 54.2% and 74.0% respectively, which could be further improved by more targeted network architectures. In conclusion, semantic segmentation methods are demonstrated to be effctive for predicting new potential landslides based on remote sensing images.",Landslide susceptibility prediction,Deep learning,Computer vision,Remote sensing,"Wu, Guanghui","Han, Liangzhe","Sun, Leilei","Gao, Qiang",Semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_721,"Bai, Lubin","Du, Shihong","Zhang, Xiuyuan",Domain Adaptation for Remote Sensing Image Semantic Segmentation: An Integrated Approach of Contrastive Learning and Adversarial Learning,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,30,"Although semantic segmentation models based on deep neural networks (DNNs) have achieved excellent results, generalizing well from one remote sensing dataset (source domain) to another dataset with different acquisition conditions (target domain) remains a major challenge. Many domain adaptation (DA) approaches have been proposed to address this problem. DA aims to help DNNs learn a generalizable representation space in which source and target domains have similar feature distributions, but most of the existing DA approaches have difficulty in aligning the high-dimensional image representations of two domains directly. In this study, we proposed a model integrating contrastive learning and adversarial learning in a unified framework for aligning two domains in both representation space and spatial layout. Specifically, the model consists of a semantic segmentation network for feature extraction and two branches for DA. The first branch is used for adaptation in representation space directly by a proposed pixelwise contrastive loss, while the second branch is used for adaptation in predicted results to help two domains have similar spatial layouts through a novel but simple entropy-based similarity discriminator. Additionally, a training strategy called category similarity matching sampling was proposed to provide source and target image pairs with similar category composition for each training iteration, which can help the two branches work better. Extensive experiments indicated that the two branches can benefit each other to gain a superior performance and DA pretraining by our methods can achieve impressive results with only a small number of target labeled samples.",Image segmentation,Semantics,Training,Feature extraction,"Wang, Haoyu","Liu, Bo","Ouyang, Song",,Task analysis,Adversarial machine learning,Adaptation models,Adversarial learning,contrastive learning,domain adaptation (DA),semantic segmentation,,,,,,,,,,,,,,,,,,
Row_722,"Ma, Zhanming","Xia, Min","Weng, Liguo",Local Feature Search Network for Building and Water Segmentation of Remote Sensing Image,SUSTAINABILITY,FEB 2023,28,"Extracting buildings and water bodies from high-resolution remote sensing images is of great significance for urban development planning. However, when studying buildings and water bodies through high-resolution remote sensing images, water bodies are very easy to be confused with the spectra of dark objects such as building shadows, asphalt roads and dense vegetation. The existing semantic segmentation methods do not pay enough attention to the local feature information between horizontal direction and position, which leads to the problem of misjudgment of buildings and loss of local information of water area. In order to improve this problem, this paper proposes a local feature search network (DFSNet) application in remote sensing image building and water segmentation. By paying more attention to the local feature information between horizontal direction and position, we can reduce the problems of misjudgment of buildings and loss of local information of water bodies. The discarding attention module (DAM) introduced in this paper reads sensitive information through direction and location, and proposes the slice pooling module (SPM) to obtain a large receptive field in the pixel by pixel prediction task through parallel pooling operation, so as to reduce the misjudgment of large areas of buildings and the edge blurring in the process of water body segmentation. The fusion attention up sampling module (FAUM) guides the backbone network to obtain local information between horizontal directions and positions in spatial dimensions, provide better pixel level attention for high-level feature maps, and obtain more detailed segmentation output. The experimental results of our method on building and water data sets show that compared with the existing classical semantic segmentation model, the proposed method achieves 2.89% improvement on the indicator MIoU, and the final MIoU reaches 83.73%.",semantic segmentation,building and water segmentation,local feature search,horizontal direction,"Lin, Haifeng",,,,high-resolution remote sensing image,,,,,,,,,,,,,,,,,,,,,,,,
Row_723,"Zheng, Xiaoxiong","Chen, Tao",,High spatial resolution remote sensing image segmentation based on the multiclassification model and the binary classification model,NEURAL COMPUTING & APPLICATIONS,FEB 2023,29,"Semantic segmentation technology is an important step in the interpretation of remote sensing images. High spatial resolution remote sensing images have clear features. Traditional image segmentation methods cannot fully represent the information in high spatial resolution images and tend to yield unsatisfactory segmentation accuracy. With the rapid development of deep learning, many researchers have tried to use deep learning algorithms for remote sensing image segmentation. This paper uses U-Net for multiclassification and binary classification of Gaofen-2 high spatial resolution remote sensing image data. Six types of features, which were build-up, farmland, water, meadow, forest and others, were labeled in the image. A ""neighborhood voting"" method was used to determine the category of uncertain pixels based on spatial heterogeneity and homogeneity. Through U-Net neural network multiclassification, the overall accuracy of the training data is 93.83%; the overall accuracy of the test data is 82.27%; and the test accuracy of the binary classification algorithm is 79.75%. The results show that the two models yield high accuracy and credibility in remote sensing image segmentation.",High spatial resolution remote sensing image,Semantic segmentation,U-Net,Neighborhood voting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_724,"Zhang, Panli","Zhang, Sheng","Wang, Jiquan",Identifying rice lodging based on semantic segmentation architecture optimization with UAV remote sensing imaging,COMPUTERS AND ELECTRONICS IN AGRICULTURE,DEC 2024,0,"Lodging, a prevalent issue during rice growth, detrimentally impacts both yield and quality. It also complicates the harvesting process, reducing the efficiency of mechanized collection. Existing monitoring methods, predominantly based on manual observation and satellite remote sensing, fall short in addressing the requirements of contemporary, efficient, and real-time agriculture. This research integrates image analysis techniques with advanced optimization algorithms to develop a semantic segmentation model specifically designed for detecting rice lodging in remote sensing images. The model, named MI-UConvNeXt, employs a ConvNeXt-based feature extraction network utilizing UNet architecture (UConvNeXt) and incorporates an improved multi-objective salp swarm algorithm with Latin hypercube sampling and an elite opposition-based learning strategy (ISSA-LE) to dynamically adjusting the number of UConvNeXt channels. MI-UConvNeXt achieves a balance between accuracy and complexity. Compared to seven other semantic segmentation models from the literature, MI-UConvNeXt exhibits enhanced performance, with a Pixel Accuracy (PA) of 95.59%, mean Pixel Accuracy (mPA) of 95.62%, and mean Intersection over Union ( mIoU ) of 91.91% on the validation set. This demonstrates the model's superior accuracy, lower computational resource demands, and enhanced efficiency. By integrating deep learning with intelligent optimization algorithms, this study offers a novel and effective approach for monitoring crop lodging in agricultural production, providing robust technical support for the accurate extraction of crop phenotypic information.",Rice lodging,Salp swarm algorithm,Convolutional neural network,Semantic segmentation,"Sun, Xiaobo",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_725,"Rao, Zhibo","He, Mingyi","Zhu, Zhidong",Bidirectional Guided Attention Network for 3-D Semantic Detection of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,JUL 2021,31,"Semantic segmentation and disparity estimation are in the research frontier of the computer vision and remote sensing (RS) fields. However, existing methods mostly deal with these two problems separately or use a combination of multiple models to solve these two tasks. Due to a lack of sufficient information sharing and fusion, they still have difficulties in coping with seasonal appearance differences in 3-D RS problems. In this article, we propose a novel multitask learning architecture that considers the bottomx2013;up and upx2013;bottom visual attention mechanism for 3-D semantic detection, named bidirectional guided attention network (BGA-Net). BGA-Net consists of five modules: unified backbone module (UBM), bidirectional guided attention module (BGAM), semantic segmentation module (SSM), feature matching module (FMM), and bidirectional fusion module (BFM). First, in UBM, we use a shared backbone to extract unified features and share them with three branches/modules (BGAM, SSM, and FMM). Then, SSM and FMM branches are applied to estimate segmentation and disparity maps, whereas the third branch/module (BGAM) shares the global features to guide the task-specific learning via attention mechanism. Finally, we fuse the results of the two tasks by BFM to improve the final performance. Extensive experiments demonstrate that: 1) our BGA-Net can handle the two tasks simultaneously and can be trained in an end-to-end way; 2) these modules fully take advantage of the two tasksx2019; information to share features and enhance the scene understanding ability, effectively against seasons change of RS images; and 3) BGA-Net has notable superiority and greater flexibility and also sets a new state of the art on the urban semantic 3-D (US3D) benchmark. Moreover, BGA-Net also provides insights into the intelligent interpretation of RS data images.",Task analysis,Semantics,Image segmentation,Feature extraction,"Dai, Yuchao","He, Renjie",,,Visualization,Remote sensing,Computer architecture,Bidirectional guided aggregation,remote sensing image,semantic segmentation,stereo matching,visual attention mechanism,,,,,,,,,,,,,,,,,
Row_726,"Sun, Jingxi","Li, Weihong","Zhang, Yan",Building segmentation of remote sensing images using deep neural networks and domain transform CRF,,2019,2,"Automatic building segmentation from remote sensing images is critical in the remote sensing image semantic segmentation. The success of deep neural networks has led to advances in using fully convolutional neural networks (FCN) to extract buildings from the high-resolution image. However, the downsampling processing inevitably leads to loss of details of the segmentation results. To solve this problem, some methods try to refine the results of FCN by using probability graph models such as fully connected CRF (Conditional Random Fields). Nevertheless, many fully connected CRF based methods are too time-consuming and not suitable for building segmentation tasks in some situations. In this paper, we propose a novel time- efficient end-to-end CRF model with the domain transform algorithm called DT-CRF. In the proposed model, in order to accelerate the message passing in the mean-field approximate inference algorithm, we take the edge maps as the joint image for DT-CRF and use the domain transformation algorithm to calculate the pair-wise potential instead of the Gaussian kernel function. Meanwhile, we design a multi-task network which can generate masks and edges simultaneously, and the network can make the DT-CRF to easily optimize the segmentation results using model information. The evaluation of remote sensing image datasets verifies the time and space efficiency of the proposed DTCRF and demonstrates a distinct improvement.",Remote Sensing Image,Convolutional Neural Networks,Building Segmentation,Conditional Random Field,"Gong, Weiguo",,,,Domain Transform,,,,,,,,,IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXV,,,,,,,,,,,,,,,
Row_727,"Liu, Ming","Ren, Dong","Sun, Hang",Orchard Areas Segmentation in Remote Sensing Images via Class Feature Aggregate Discriminator,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,0,"Accurate evaluation of orchard areas from remote sensing images is of great importance in economic and ecological aspects. In practice, the differences in distributions between remote sensing images and the lack of data labels make the semantic segmentation model impossible to use in new data. Unsupervised domain adaptation (UDA) methods can improve the performance of the model in the target domain by aligning the source domain and the target domain. However, due to the class mismatch problem and the interference of high-dimensional feature complexity, most UDA methods cannot achieve satisfactory results in orchard areas segmentation task. To address these issues, we propose an UDA model for orchard areas segmentation by developing a class feature aggregate discriminator (CFUDA). The class feature aggregate discriminator is designed to distinguish intradomain classes and align interdomain classes, and class feature aggregate can represent class information of different domains, which helps the model to avoid the interference of complex information. In addition, adversarial loss reweighting is introduced to the novel model, which makes the segmentation model pay more attention to the orchard areas. To verify the effectiveness of the proposed method, we conducted extensive experiments in three different remote sensing images around Yichang City. Compared to the baseline model, the proposed approach improves intersection over union (IoU) by 27.68%, and we achieve high gains of 6.07% in IoU over other UDA methods. The larger gain indicates that our proposed method has great potential in cross-domain orchard areas segmentation.",Aggregates,Task analysis,Remote sensing,Adaptation models,"Yang, Simon X.","Shao, Pan",,,Image resolution,Training,Semantics,Generative adversarial networks,remote sensing,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,
Row_728,"Song, Ahram","Kim, Yongil",,Semantic Segmentation of Remote-Sensing Imagery Using Heterogeneous Big Data: International Society for Photogrammetry and Remote Sensing Potsdam and Cityscape Datasets,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,OCT 2020,18,"Although semantic segmentation of remote-sensing (RS) images using deep-learning networks has demonstrated its effectiveness recently, compared with natural-image datasets, obtaining RS images under the same conditions to construct data labels is difficult. Indeed, small datasets limit the effective learning of deep-learning networks. To address this problem, we propose a combined U-net model that is trained using a combined weighted loss function and can handle heterogeneous datasets. The network consists of encoder and decoder blocks. The convolutional layers that form the encoder blocks are shared with the heterogeneous datasets, and the decoder blocks are assigned separate training weights. Herein, the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Cityscape datasets are used as the RS and natural-image datasets, respectively. When the layers are shared, only visible bands of the ISPRS Potsdam data are used. Experimental results show that when same-sized heterogeneous datasets are used, the semantic segmentation accuracy of the Potsdam data obtained using our proposed method is lower than that obtained using only the Potsdam data (four bands) with other methods, such as SegNet, DeepLab-V3+, and the simplified version of U-net. However, the segmentation accuracy of the Potsdam images is improved when the larger Cityscape dataset is used. The combined U-net model can effectively train heterogeneous datasets and overcome the insufficient training data problem in the context of RS-image datasets. Furthermore, it is expected that the proposed method can not only be applied to segmentation tasks of aerial images but also to tasks with various purposes of using big heterogeneous datasets.",semantic segmentation,deep learning,big dataset,ISPRS Potsdam dataset,,,,,Cityscape dataset,,,,,,,,,,,,,,,,,,,,,,,,
Row_729,Zhao Tianyu,"Xu, Jindong",,Hyperspectral Remote Sensing Image Segmentation Based on the Fuzzy Deep Convolutional Neural Network,"MEDICAL IMAGE COMPUTING AND COMPUTER-ASSISTED INTERVENTION, PT III",2020,3,"The ""synonyms spectrum"" and ""foreign body with the spectrum"" of remote sensing images have caused the traditional segmentation methods to be greatly limited. Existing segmentation methods represented by deep convolution neural network have made breakthrough progress. However, traditional deep learning is a completely deterministic model, which can not describe the data uncertainty well. To solve this problem, a new fuzzy deep neural network is proposed in this paper, called RSFCNN (Remote Sensing image segmentation with Fuzzy Convolutional Neural Network). The network integrates fuzzy unit and traditional convolution unit. Convolution unit is used to extract discriminant features with different proportions, thus providing comprehensive information for pixel-level remote sensing image segmentation. Fuzzy logic unit is used to deal with various uncertainties and provide more reliable segmentation results. In this paper, end-to-end training scheme is used to learn the parameters of fuzzy and convolution units. Experiments were carried out on the data set of ISPRS Vaihingen. According to the experimental results, the proposed method has higher segmentation accuracy and better performance than other algorithms.",remote sensing image segmentation,semantic segmentation,fuzzy neural network,convolutional neural network,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_730,"Xiang, Jianjian","Liu, Jia","Chen, Du",CTFuseNet: A Multi-Scale CNN-Transformer Feature Fused Network for Crop Type Segmentation on UAV Remote Sensing Imagery,REMOTE SENSING,FEB 2023,9,"Timely and accurate acquisition of crop type information is significant for irrigation scheduling, yield estimation, harvesting arrangement, etc. The unmanned aerial vehicle (UAV) has emerged as an effective way to obtain high resolution remote sensing images for crop type mapping. Convolutional neural network (CNN)-based methods have been widely used to predict crop types according to UAV remote sensing imagery, which has excellent local feature extraction capabilities. However, its receptive field limits the capture of global contextual information. To solve this issue, this study introduced the self-attention-based transformer that obtained long-term feature dependencies of remote sensing imagery as supplementary to local details for accurate crop-type segmentation in UAV remote sensing imagery and proposed an end-to-end CNN-transformer feature-fused network (CTFuseNet). The proposed CTFuseNet first provided a parallel structure of CNN and transformer branches in the encoder to extract both local and global semantic features from the imagery. A new feature-fusion module was designed to flexibly aggregate the multi-scale global and local features from the two branches. Finally, the FPNHead of feature pyramid network served as the decoder for the improved adaptation to the multi-scale fused features and output the crop-type segmentation results. Our comprehensive experiments indicated that the proposed CTFuseNet achieved a higher crop-type-segmentation accuracy, with a mean intersection over union of 85.33% and a pixel accuracy of 92.46% on the benchmark remote sensing dataset and outperformed the state-of-the-art networks, including U-Net, PSPNet, DeepLabV3+, DANet, OCRNet, SETR, and SegFormer. Therefore, the proposed CTFuseNet was beneficial for crop-type segmentation, revealing the advantage of fusing the features found by the CNN and the transformer. Further work is needed to promote accuracy and efficiency of this approach, as well as to assess the model transferability.",precision agriculture,UAV remote sensing,semantic segmentation,deep learning,"Xiong, Qi","Deng, Chongjiu",,,CNN,transformer,feature fusion,,,,,,,,,,,,,,,,,,,,,,
Row_731,"Qiao, Yicheng","Liu, Wei","Liang, Bin",SeMask-Mask2Former: A Semantic Segmentation Model for High Resolution Remote Sensing Images,,2023,3,"With the development of remote sensing, semantic segmentation of high-resolution remote sensing images (RSIs) is increasingly essential. At the same time, the characteristics of objects in RSIs, such as large size, variation in object scales, and complex details, make it necessary to capture both long-range context and local information. There are some methods such as Fully Convolutional Networks (FCN) and Pyramid Scene Parsing Network (PSPNet) lack the ability to capture long-range dependencies, due to the limited receptive field of Convolutional Neural Network (CNN). However, the self-attention mechanism to capture the correlation between pixels in Transformer models has remarkable capability in capturing long-range context. One of the most outstanding Transformer models is the Masked-attention Mask Transformer (Mask2Former) which adopts the mask classification method. We propose a model SeMaskMask2Former with boundary loss. Semantically Masked (SeMask) is the model's backbone and Mask2Former is the decoder. Concretely, the mask classification that generates one or even more masks for specific categories to perform the elaborate segmentation is especially suitable for handling the characteristic of large within-class and small inter-class variance of RSIs.",,,,,"Wang, Pengyun","Zhang, Haopeng","Yang, Junli",,,,,,,,,,,2023 IEEE AEROSPACE CONFERENCE,,,,,,,,,,,,,,,
Row_732,"Lang, Chunbo","Cheng, Gong","Tu, Binfei",Global Rectification and Decoupled Registration for Few-Shot Segmentation in Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,25,"Few-shot segmentation (FSS), which aims to determine specific objects in the query image given only a handful of densely labeled samples, has received extensive academic attention in recent years. However, most existing FSS methods are designed for natural images, and few works have been done to investigate more realistic and challenging applications, e.g., remote sensing image understanding. In such a setup, the complex nature of the raw images would undoubtedly further increase the difficulty of the segmentation task. To couple with potential inference failures, we propose a novel and powerful remote sensing FSS framework with global rectification (GR) and decoupled registration (DR), termed R(2)Net. Specifically, a series of dynamically updated global prototypes are utilized to provide auxiliary nontarget segmentation cues and to prevent inaccurate prototype activation resulting from the variability between query-support image pairs. The foreground (FG) and background information flows are then decoupled for more targeted and tailored object localization, avoiding unnecessary confusion from information redundancy. Furthermore, we impose additional constraints to promote interclass separability and intraclass compactness. Extensive experiments on the standard benchmark iSAID-5(i) demonstrate the superiority of the proposed R(2)Net over state-of-the-art FSS models. The code is available at https://github.com/chunbolang/R2Net.",Few-shot learning,few-shot segmentation (FSS),meta-learning,remote sensing,"Han, Junwei",,,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_733,"Gu, Haiyan","Li, Haitao","Yan, Li",An Object-Based Semantic Classification Method for High Resolution Remote Sensing Imagery Using Ontology,REMOTE SENSING,APR 2017,49,"Geographic Object-Based Image Analysis (GEOBIA) techniques have become increasingly popular in remote sensing. GEOBIA has been claimed to represent a paradigm shift in remote sensing interpretation. Still, GEOBIA-similar to other emerging paradigms-lacks formal expressions and objective modelling structures and in particular semantic classification methods using ontologies. This study has put forward an object-based semantic classification method for high resolution satellite imagery using an ontology that aims to fully exploit the advantages of ontology to GEOBIA. A three-step workflow has been introduced: ontology modelling, initial classification based on a data-driven machine learning method, and semantic classification based on knowledge-driven semantic rules. The classification part is based on data-driven machine learning, segmentation, feature selection, sample collection and an initial classification. Then, image objects are re-classified based on the ontological model whereby the semantic relations are expressed in the formal languages OWL and SWRL. The results show that the method with ontology-as compared to the decision tree classification without using the ontology-yielded minor statistical improvements in terms of accuracy for this particular image. However, this framework enhances existing GEOBIA methodologies: ontologies express and organize the whole structure of GEOBIA and allow establishing relations, particularly spatially explicit relations between objects as well as multi-scale/hierarchical relations.",geographic object-based image analysis,ontology,semantic network model,web ontology language,"Liu, Zhengjun","Blaschke, Thomas","Soergel, Uwe",,semantic web rule language,machine learning,semantic rule,land-cover classification,,,,,,,,,,,,,,,,,,,,,
Row_734,"Bai, Hao","Bai, Tingzhu","Li, Wei",A Building Segmentation Network Based on Improved Spatial Pyramid in Remote Sensing Images,APPLIED SCIENCES-BASEL,JUN 2021,1,"Building segmentation is widely used in urban planning, disaster prevention, human flow monitoring and environmental monitoring. However, due to the complex landscapes and highdensity settlements, automatically characterizing building in the urban village or cities using remote sensing images is very challenging. Inspired by the rencent deep learning methods, this paper proposed a novel end-to-end building segmentation network for segmenting buildings from remote sensing images. The network includes two branches: one branch uses Widely Adaptive Spatial Pyramid (WASP) structure to extract multi-scale features, and the other branch uses a deep residual network combined with a sub-pixel up-sampling structure to enhance the detail of building boundaries. We compared our proposed method with three state-of-the-art networks: DeepLabv3+, ENet, ESPNet. Experiments were performed using the publicly available Inria Aerial Image Labelling dataset (Inria aerial dataset) and the Satellite dataset II(East Asia). The results showed that our method outperformed the other networks in the experiments, with Pixel Accuracy reaching 0.8421 and 0.8738, respectively and with mIoU reaching 0.9034 and 0.8936 respectively. Compared with the basic network, it has increased by about 25% or more. It can not only extract building footprints, but also especially small building objects.",CNN,semantic segmentation,super resolution,remote sensing,"Liu, Xun",,,,spatial pyramid,ResNet,,,,,,,,,,,,,,,,,,,,,,,
Row_735,"Wei, Guangyi","Xu, Jindong","Chong, Qianpeng",Prior-Guided Fuzzy-Aware Multibranch Network for Remote Sensing Image Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,1,"In remote sensing images (RSIs), accurate semantic segmentation faces significant challenges due to the variation in object scales, uncertain category boundaries, and complex scenes. In view of the above challenges, we propose a prior-guided fuzzy-aware multibranch network for RSI segmentation. Specifically, a prior-feature extractor (PFE) is designed to take the local features extracted by convolution structure as prior knowledge of the network. Fuzzy-aware module (FAM) is presented to perceive and refine category boundaries with fuzzy learning, which transforms the uncertainty problem into a quantitative analysis problem by establishing fuzzy relationships between neighborhood pixels. Multibranch-feature extractor (MFE) is put forward to aggregate multiscale global context information by combining positional attention and transformer. The whole network learning process is supervised by multibranch loss. We validated our method on the Gaofen Image Dataset (GID), Potsdam, and LoveDA datasets, achieving 75.58%, 75.62%, and 53.10% mean intersection over union (mIoU), respectively, which demonstrated the superiority of our method. In addition, ablation studies further demonstrate the validity of each module in the proposed method.",Feature extraction,Transformers,Decoding,Data mining,"Huang, Jianjun","Xing, Haihua",,,Kernel,Aggregates,Remote sensing,Fuzzy learning,remote sensing,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,
Row_736,"Miao, Zuohua","Liu, Likun","Ren, Lei",Semantic Segmentation of Remote Sensing Images Based on Multi-Model Fusion,,2020,1,"Convolutional neural networks have created a new field in the research of semantic segmentation of remote sensing images. However, different network structures have different effects on the semantic segmentation of different land types. In this paper, the original data set is expanded, and an improved U-Net model is used to train a model for each type of feature target. Then combined with conditional random field (CRF) and image overlapping strategy for optimization processing; Finally, the two binary classification models obtained by training are fused to obtain multi-classified semantic segmentation images. Solve the obvious problem of large-scale remote sensing image edge stitching. The experimental results show that this method has higher accuracy in solving the segmentation of large-scale remote sensing images.",,,,,"Tang, Yang","Chen, Yong","Li, Jun",,,,,,,,,,,5TH ANNUAL INTERNATIONAL CONFERENCE ON INFORMATION SYSTEM AND ARTIFICIAL INTELLIGENCE (ISAI2020),,,,,,,,,,,,,,,
Row_737,Tian Qinglin,Zhao Yingjun,Qin Kai,Dense feature pyramid fusion deep network for building segmentation in remote sensing image,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2021,3,"It is difficult to achieve detailed segmentation since the building size varies in high-resolution remote sensing images, especially for small buildings. To address these problems, a dense feature pyramid fusion deep network is proposed in this study. First, we built an encoder-decoder structure, and combine attention mechanism and atrous convolution to improve the feature extraction results in the encoder. Second, the pyramid pooling module is selected to extract the multi-scale features from different levels. Finally, dense feature pyramid is adopted in the decoder to fuse multi-level and multi-scale features to obtain the final segmentation results. Experiments on Inria Aerial Image Labeling Dataset show that our method achieves competitive performance compared with other classical semantic segmentation networks.",remote sensing image,building segmentation,attention mechanism,feature pyramid,Li Yao,Chen Xuejiao,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_738,"Wang, Yufeng","Ding, Wenrui","Zhang, Ruiqian",Boundary-Aware Multitask Learning for Remote Sensing Imagery,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,30,"Semantic segmentation and height estimation play fundamental roles in the scene understanding of remote sensing images with their wide variety of aerial applications. Recently, deep convolutional neural networks (DCNNs) have achieved state-of-the-art performance in both tasks. However, DCNN-based methods learn to accumulate contextual information over large receptive fields while lose the local detailed information, resulting in blurry object boundaries. The complicated ground object distribution and low interclass variance further aggravate the difficulty in generating accurate predictions. To address the above-mentioned issues, we propose a novel boundary-aware multitask learning (BAMTL) framework to perform three tasks, semantic segmentation, height estimation, and boundary detection, within a unified model. The boundary detection is employed as an auxiliary task to regularize the other two master tasks at both the feature space and output space. We present a boundary attentive module to build the cross-task interaction for master tasks, which enforce the networks to filter out the confident area and focus on learning the high-frequency details. We then introduce a boundary regularized loss term to further refine the prediction maps to be locally consistent while preserving boundary structures. With these formulations, our model improves the performance of both segmentation and height tasks, especially along the boundaries. Experimental results on two publicly available remote sensing datasets demonstrate that the proposed approach performs favorably against the state-of-the-art methods.",Task analysis,Remote sensing,Semantics,Image segmentation,"Li, Hongguang",,,,Estimation,Feature extraction,Earth,Boundary regularization (BR),convolutional neural network (CNN),height estimation,multitask learning (MTL),remote sensing imagery (RSI),,,,,,scene understanding,semantic segmentation,,,,,,,,,,
Row_739,"Li, Xiang","Wen, Congcong","Wang, Lingjing",Geometry-Aware Segmentation of Remote Sensing Images via Joint Height Estimation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,26,"Recent studies have shown the benefits of using additional elevation data [e.g., digital surface model (DSM) or normalized DSM (nDSM)] for enhancing the performance of the semantic labeling of aerial images. However, previous methods mostly adopt 3-D elevation information as additional inputs, while, in many real-world applications, one does not have the corresponding DSM images at hand, and the spatial resolution of acquired DSM images usually does not match the aerial images. To alleviate this data constraint and also take advantage of 3-D elevation information, in this letter, a geometry-aware segmentation model is introduced to achieve accurate semantic labeling of aerial images via joint height estimation. Instead of using a single-stream encoder-decoder network for semantic labeling, we design a separate decoder branch to predict the height map and use the DSM images as side supervision to train this newly designed decoder branch. With the newly designed decoder branch, our model can distill the 3-D geometric features from 2-D appearance features under the supervision of ground-truth DSM images. Moreover, we develop a new geometry-aware convolution module that fuses the 3-D geometric features from the height decoder branch and the 2-D contextual features from the semantic segmentation branch. The fused feature embeddings can produce geometry-aware segmentation maps with enhanced performance. Our model is trained with DSM images as side supervision, while, in the inference stage, it does not require DSM data and directly predicts the semantic labels. Experiments on International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam data sets demonstrate the effectiveness of the proposed method for the semantic segmentation of aerial images.",Semantics,Data models,Image segmentation,Labeling,"Fang, Yi",,,,Decoding,Estimation,Convolution,Feature fusion,geometry-aware convolution (GAC),height estimation,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_740,"Dey, Madhumita","Prakash, P. S.","Aithal, Bharath Haridas",UnetEdge: A transfer learning-based framework for road feature segmentation from high-resolution remote sensing images,REMOTE SENSING APPLICATIONS-SOCIETY AND ENVIRONMENT,APR 2024,3,"Topological information is a crucial factor affecting road feature extraction using semantic segmentation. Many segmentation models have recently been developed for road feature extraction from high-resolution remote sensing imagery but have yet to achieve accurate predictions when occluded by shadows. This study proposes a transfer learning-based framework, called UnetEdge, which is designed to effectively propagate topological information into the feature map. The novel Edge module transmits edge level topological information along with contextual spatial information through the decoder layer to the final feature map. This essentially enhances the continuous flow of semantic road pixel information into the network. Moreover, to integrate heterogeneous road structure information into the network, we have leveraged the transfer learning approach to produce accurate road segmentation maps. Extensive experiments on five standard public datasets and comparative analysis against the state -of -the -art networks establish the robustness and efficiency of the model. Furthermore, the experimental results on our acquired Indian drone dataset achieved an IoU score of 70.22 and a mIoU score of 82.45% with an overall accuracy of 95.27%, validating the model's effectiveness for real -world applications.",Remote sensing,Semantic segmentation,Convolutional neural network (CNN),Indian drone dataset,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_741,"Liu, Huan","Li, Wei","Jia, Wen",Clusterformer for Pine Tree Disease Identification Based on UAV Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,7,"Pine wilt disease (PWD) is one of the most prevalent pine tree diseases, resulting in both ecological and economic havoc. Unmanned aerial vehicle (UAV) remote sensing segmentation plays a crucial role in early identifying and preventing PWD. However, deep learning segmentation models customized for PWD identification in scenarios with complex backgrounds have not received extensive exploration. In this article, we propose a novel UAV remote sensing segmentation model called Clusterformer with a conventional encoder-decoder structure. The encoder is comprised of the specially designed cluster transformer, which includes a cluster token mixer and a spatial-channel feed-forward network (SC-FFN). The cluster token mixer utilizes constructed clusters from the feature maps to represent pixels, thereby reducing redundant and interfering information. The SC-FFN extracts multiscale spatial information through depth-wise convolutions and channel information through a multilayer perceptron (MLP) in sequence. The decoder primarily consists of the specially designed D-cluster transformer. The token mixer of the D-cluster transformer employs constructed clusters from high-level decoded tokens to represent low-level encoded tokens without relying on traditional upsampling methods such as interpolation, transpose convolution, or patch expansion. Consequently, more robust and less redundant features from high-level decoded feature maps are transferred to low-level encoded feature maps. Experimental results on two PWD datasets demonstrate that Clusterformer outperforms existing state-of-the-art segmentation models. This confirms the effectiveness and efficiency of Clusterformer in PWD identification. The code is available at https://github.com/huanliu233/Clusterformer.",Cluster transformer,pine wilt identification,semantic segmentation,unmanned aerial vehicle (UAV) remote sensing,"Sun, Hong","Zhang, Mengmeng","Song, Lujie","Gui, Yuanyuan",,,,,,,,,,,,,,,,,,,,,,,,,
Row_742,"Du, Shuang","Liu, Maohua",,Class-Guidance Network Based on the Pyramid Vision Transformer for Efficient Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,3,"Small differences between classes and big variations within classes in multicategory semantic segmentation are problems that are not completely solved by the ""encoder-decoder"" structure of the fully convolutional neural network, leading to the imprecise perception of easily confused categories. To address this issue, in this article, we believe that sufficient contextual information can provide more interpretation clues to the model. Additionally, if we can mine the class-specific perceptual information for each semantic class, we can enhance the information belonging to the corresponding class in the decoding process. Therefore, we propose the class-guidance network based on the pyramid vision transformer (PVT). In detail, with the PVT as the encoder network, the following decoding process is composed of three stages. First, we design a receptive field block to expand the receptive field to different degrees using parallel branching processing and different dilatation rates. Second, we put forward a semantic guidance block to utilize the high-level features to guide the channel enhancement of low-level features. Third, we propose the class guidance block to achieve the class-aware guidance of adjacent features and achieve the refined segmentation by a progressive approach. The overall accuracy of the method is 88.91% and 88.87%, respectively, according to experimental findings on the Potsdam and Vaihingen datasets.",Class-guidance network,remote sensing images,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_743,"Zheng, Yilin","He, Lingmin","Wu, Xiangping",Self-training and Multi-level Adversarial Network for Domain Adaptive Remote Sensing Image Segmentation,NEURAL PROCESSING LETTERS,DEC 2023,0,"Unsupervised domain adaptive (UDA) image segmentation has received more and more attention in recent years. Domain adaptive methods can align the features of data in different domains, so that segmentation models can be migrated to data in other domains without incurring additional labeling costs. The traditional adversarial training uses the global alignment strategy to align the feature space, which may cause some categories to be incorrectly mapped. At the same time, in high-spatial resolution remote sensing images (RSI), the same category from different scenes (such as urban and rural areas) may have completely different distributions, which severely limits the accuracy of UDA. In order to solve these problems, in this paper: (1) a multi-level adversarial network at category-level is proposed, aiming at integrating feature information in different dimensions, studying the joint distribution at category-level, and aligning each category with adaptive adversarial loss in different dimensional spaces. (2) Use covariance regularization to optimize self-training. A method combining self-training with adversarial training is proposed, optimizes the domain adaptation effect, reduces the negative impact of false pseudo-label iteration caused by self-training. We demonstrated the latest performance of semantic segmentation on challenging LoveDA datasets. Experiments on ""urban-to-rural"" and ""rural-to-urban"" show that our method has better performance than the most advanced methods.",Domain adaptation,Semantic segmentation,Remote sensing images,Self-training,"Pan, Chen",,,,Adversarial network,,,,,,,,,,,,,,,,,,,,,,,,
Row_744,"Lu, Zili","Peng, Yuexing","Li, Wei",An Iterative Classification and Semantic Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,12,"The geological characteristics of old landslides can provide crucial information for the task of landslide protection. However, detecting old landslides from high-resolution remote sensing images (HRSIs) is of great challenge due to their partially or strongly transformed morphology over a long time and thus the limited difference with their surroundings. Additionally, small-sized datasets can restrict in-depth learning. To address these challenges, this article proposes a new iterative classification and semantic segmentation network (ICSSN), which can significantly improve both object-level and pixel-level classification performance by iteratively upgrading the feature extraction module shared by the object classification and semantic segmentation networks. To improve the detection performance on small-sized datasets, object-level contrastive learning is employed in the object classification network featuring a siamese network to realize global features extraction, and a subobject-level contrastive learning (SOCL) method is designed in the semantic segmentation network to efficiently extract salient features from boundaries of landslides. An iterative training strategy is also proposed to fuse features in the semantic space, further improving both the object-level and pixel-level classification performances. The proposed ICSSN is evaluated on a real-world landslide dataset, and experimental results show that it greatly improves both the classification and segmentation accuracy of old landslides. For the semantic segmentation task, compared to the baseline, the F1 score increases from 0.5054 to 0.5448, the mean intersection over union (mIoU) improves from 0.6405 to 0.6610, the landslide IoU grows from 0.3381 to 0.3743, the pixel accuracy (PA) is improved from 0.945 to 0.949, and the object-level detection accuracy of old landslides surges from 0.55 to 0.90. For the object classification task, the F1 score increases from 0.8846 to 0.9230, and the accuracy score is up from 0.8375 to 0.8875.",Terrain factors,Feature extraction,Semantic segmentation,Semantics,"Yu, Junchuan","Ge, Daqing","Han, Lingyi","Xiang, Wei",Reliability,Task analysis,Iterative methods,Contrastive learning,landslide detection,multitask learning,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_745,"Yang, Ruiqi","Zheng, Chen","Wang, Leiguang",MAE-BG: dual-stream boundary optimization for remote sensing image semantic segmentation,GEOCARTO INTERNATIONAL,DEC 31 2023,4,"Deep learning has achieved remarkable performance in semantically segmenting remotely sensed images. However, the high-frequency detail loss caused by continuous convolution and pooling operations and the uncertainty introduced when annotating low-contrast objects with weak boundaries induce blurred object boundaries. Therefore, a dual-stream network MAE-BG, consisting of an edge detection (ED) branch and a smooth branch with boundary guidance (BG), is proposed. The ED branch is designed to enhance the weak edges that need to be preserved, simultaneously suppressing false responses caused by local texture. This mechanism is achieved by introducing improved multiple-attention edge detection blocks (MAE). Furthermore, two specific ED branches with MAE are designed to combine with typical deep convolutional (DC) and Codec infrastructures and result in two configurations of MAE-A and MAE-B. Meanwhile, multiscale edge information extracted by MAE networks is fed into the backbone networks to complement the detail loss caused by convolution and pooling operations. This results in smooth networks with BG. After that, the segmentation results with improved boundaries are obtained by stacking the output of the ED and smooth branches. The proposed algorithms were evaluated on the ISPRS Potsdam and Inria Aerial Image Labelling datasets. Comprehensive experiments show that the proposed method can precisely locate object boundaries and improve segmentation performance. The MAE-A branch leads to an overall accuracy (OA) of 89.16%, a mean intersection over union (MIOU) of 80.25% for Potsdam, and an OA of 96.61% and MIOU of 86.63% for Inria. Compared with the results without the proposed edge optimization blocks, the OAs from the Potsdam and Inria datasets increase by 5.49% and 7.64%, respectively.",Deep learning,Boundary optimization,Squeeze and excitation,Semantic segmentation,"Zhao, Yili","Fu, Zhitao","Dai, Qinling",,Remote sensing,,,,,,,,,,,,,,,,,,,,,,,,
Row_746,"Ayala, Christian","Aranda, Carlos","Galar, Mikel",Guidelines to Compare Semantic Segmentation Maps at Different Resolutions,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Choosing the proper ground sampling distance (GSD) is a vital decision in remote sensing, which can determine the success or failure of a project. Higher resolutions may be more suitable for accurately detecting objects, but they also come with higher costs and require more computing power. Semantic segmentation is a common task in remote sensing where GSD plays a crucial role. In semantic segmentation, each pixel of an image is classified into a predefined set of classes, resulting in a semantic segmentation map. However, comparing the results of semantic segmentation at different GSDs is not straightforward. Unlike scene classification and object detection tasks, which are evaluated at scene and object level, respectively, semantic segmentation is typically evaluated at pixel level. This makes it difficult to match elements across different GSDs, resulting in a range of methods for computing metrics, some of which may not be adequate. For this reason, the purpose of this work is to set out a clear set of guidelines for fairly comparing semantic segmentation results obtained at various spatial resolutions. Additionally, we propose to complement the commonly used scene-based pixel-wise metrics with region-based pixel-wise metrics, allowing for a more detailed analysis of the model performance. The set of guidelines together with the proposed region-based metrics are illustrated with building and swimming pool detection problems. The experimental study demonstrates that by following the proposed guidelines and the proposed region-based pixel-wise metrics, it is possible to fairly compare segmentation maps at different spatial resolutions and gain a better understanding of the model's performance.",Measurement,Guidelines,Semantic segmentation,Spatial resolution,,,,,Buildings,Task analysis,Remote sensing,Error metrics,quality assessment,remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_747,"Liu, Wei","Su, Fulin",,Unsupervised Adversarial Domain Adaptation Network for Semantic Segmentation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,NOV 2020,26,"With the rapid development of deep learning technology, semantic segmentation methods have been widely used in remote sensing data. A pretrained semantic segmentation model usually cannot perform well when the testing images (target domain) have an obvious difference from the training data set (source domain), while a large enough labeled data set is almost impossible to be acquired for each scenario. Unsupervised domain adaptation (DA) techniques aim to transfer knowledge learned from the source domain to a totally unlabeled target domain. By reducing the domain shift, DA methods have shown the ability to improve the classification accuracy for the target domain. Hence, in this letter, we propose an unsupervised adversarial DA network that converts deep features into 2-D feature curves and reduces the discrepancy between curves from the source domain and curves from the target domain based on a conditional generative adversarial networks (cGANs) model. Our proposed DA network is able to improve the semantic labeling accuracy when we apply a pretrained semantic segmentation model to the target domain. To test the effectiveness of the proposed method, experiments are conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) 2-D Semantic Labeling data set. Results show that our proposed network is able to stably improve overall accuracy not only when the source and target domains are from the same city but with different building styles but also when the source and target domains are from different cities and acquired by different sensors. By comparing with a few state-of-the-art DA methods, we demonstrate that our proposed method achieves the best cross-domain semantic segmentation performance.",Feature extraction,Semantics,Image segmentation,Data models,,,,,Remote sensing,Labeling,Training,Domain adaptation (DA),generative adversarial networks (GANs),remote sensing image,semantic segmentation,transfer learning,,,,,,,,,,,,,,,,,
Row_748,"Ye, Xiaoling","Dong, Shimiao","Hu, Kai",Classification and extraction method of hidden dangers along railway lines based on semantic segmentation network,INTERNATIONAL JOURNAL OF REMOTE SENSING,OCT 2024,0,"Foreign objects invading high-speed railway lines can cause danger. One existing solution is to use remote sensing images to analyse the dangerous areas along the railway line, thereby providing a certain amount of investigation time. Considering the spatial and temporal resolution characteristics of existing remote sensing technologies in identifying floating objects and the reality of rapid land use changes, this paper identifies areas on the ground where floating objects may be generated by using semantic segmentation techniques oriented to remotely sensed imagery and provides early warnings to staff along the route. However, these regions that need to be analysed have different semantics and scales. To address these challenges, this paper proposes a Dual-branch Parallel Fusion Network (DPFNet) based on Transformer, aimed at enhancing multi-class semantic segmentation in remote sensing images. To leverage global contextual information, we introduce a Swin Transformer-based backbone network, which employs self-attention to capture a comprehensive scene context, facilitating better segmentation by considering the entire scene's context. For multi-scale semantic features, we propose one approach that involves independent branching feature expression and a Multi-scale Feature Space Fusion Module (MFSFM). The former enriches multi-scale information, while the latter fuses features across different levels to capture diverse semantic features. Experimental results demonstrate that DPFNet can effectively identify the hidden danger area, and the fusion of multi-scale features makes the network more accurately identify and segment the risk area of different sizes, improving the segmentation accuracy and robustness, and is of great significance to the formation of the 'prevention' as the core of the railway safety operation.",Foreign object intrusion,Semantic segmentation,Transformer,Remote sensing imagery,"Zhang, Yingchao","Xiong, Xiong","Zhang, Yanchao","Sheng, Tao",,,,,,,,,,,,,,,,,,,,,,,,,
Row_749,"Yuan, Wei","Xu, Wenbo",,GapLoss: A Loss Function for Semantic Segmentation of Roads in Remote Sensing Images,REMOTE SENSING,MAY 2022,7,"At present, road continuity is a major challenge, and it is difficult to extract the centerline vector of roads, especially when the road view is obstructed by trees or other structures. Most of the existing research has focused on optimizing the available deep-learning networks. However, the segmentation accuracy is also affected by the loss function. Currently, little research has been published on road segmentation loss functions. To resolve this problem, an attention loss function named GapLoss that can be combined with any segmentation network was proposed. Firstly, a deep-learning network was used to obtain a binary prediction mask. Secondly, a vector skeleton was extracted from the prediction mask. Thirdly, for each pixel, eight neighboring pixels with the same value of the pixel were calculated. If the value was 1, then the pixel was identified as the endpoint. Fourth, according to the number of endpoints within a buffered range, each pixel in the prediction image was given a corresponding weight. Finally, the weighted average value of the cross-entropy of all the pixels in the batch was used as the final loss function value. We employed four well-known semantic segmentation networks to conduct comparative experiments on three large datasets. The results showed that, compared to other loss functions, the evaluation metrics after using GapLoss were nearly all improved. From the predicted image, the road prediction by GapLoss was more continuous, especially at intersections and when the road was obscured from view, and the road segmentation accuracy was improved.",deep learning,loss function,road extraction,remote sensing image,,,,,sematic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_750,"Ding, Lei","Zhang, Jing","Guo, Haitao",Joint Spatio-Temporal Modeling for Semantic Change Detection in Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,24,"Semantic change detection (SCD) refers to the task of simultaneously extracting changed areas and their semantic categories (before and after the changes) in remote sensing images (RSIs). This is more meaningful than binary change detection (BCD) since it enables detailed change analysis in the observed areas. Previous works established triple-branch convolutional neural network (CNN) architectures as the paradigm for SCD. However, it remains challenging to exploit semantic information with a limited amount of change samples. In this work, we investigate to jointly consider the spatio-temporal dependencies to improve the accuracy of SCD. First, we propose a semantic change transformer (SCanFormer) to explicitly model the ""from-to"" semantic transitions between the bitemporal RSIs. Then, we introduce a semantic learning scheme to leverage the spatio-temporal constraints, which are coherent to the SCD task, to guide the learning of semantic changes. The resulting network semantic change network (SCanNet) significantly outperforms the baseline method in terms of both detection of critical semantic changes and semantic consistency in the obtained bitemporal results. It achieves the state-of-the-art (SOTA) accuracy on two benchmark datasets for the SCD.",Convolutional neural network (CNN),remote sensing (RS),semantic change detection (SCD),semantic segmentation,"Zhang, Kai","Liu, Bing","Bruzzone, Lorenzo",,vision transformer,,,,,,,,,,,,,,,,,,,,,,,,
Row_751,"Mei, Liye","Ye, Zhaoyi","Xu, Chuan",SCD-SAM: Adapting Segment Anything Model for Semantic Change Detection in Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,7,"Semantic change detection (SCD) has gradually emerged as a prominent research focus in remote sensing image processing due to its critical role in Earth observation applications. In view of its powerful semantic-driven feature extraction capability, the segment anything model (SAM) has demonstrated its suitability across various visual scenes. However, it suffers from significant performance degradation when confronted with remote sensing images, especially those containing various ground objects that possess significant interclass similarity and substantial intraclass variations. To address the above issues, we propose SCD-SAM, aiming to leverage the potent visual recognition capabilities of SAM for enhanced accuracy and robustness in SCD. Specifically, we introduce a contextual semantic change-aware dual encoder that combines MobileSAM and CNN to extract progressive semantic change features in parallel and inject local features into the MobileSAM encoder through depth feature interaction (DFI) to compensate for the Transformer's limitations in perceiving local semantic details. In addition, in order to utilize the strong visual feature extraction capability of MobileSAM in remote sensing images, we propose a semantic adaptor that aggregates semantic-oriented information about changing objects. To better integrate the extracted contextual semantic information, we devise a progressive feature aggregation dual decoder that aggregates binary change features and semantic change features, respectively, alleviating the semantic gap across different scales. The quantitative and visual results show that SCD-SAM outperforms the state-of-the-art SCD methods on publicly open SCD datasets (e.g., SECOND-CD and Landsat-CD). The code will be available at: https://github.com/yzygit1230/SCD-SAM.",Semantics,Feature extraction,Remote sensing,Image segmentation,"Wang, Hongzhu","Wang, Ying","Lei, Cheng","Yang, Wei",Decoding,Adaptation models,Transformers,Progressive feature aggregation,segment anything model (SAM),semantic adaptor,semantic change detection (SCD),,"Li, Yansheng",,,,,,,,,,,,,,,,
Row_752,"Wang, Baoguo","Yao, Yonghui",,Mountain Vegetation Classification Method Based on Multi-Channel Semantic Segmentation Model,REMOTE SENSING,JAN 2024,5,"With the development of satellite remote sensing technology, a substantial quantity of remote sensing data can be obtained every day, but the ability to extract information from these data remains poor, especially regarding intelligent extraction models for vegetation information in mountainous areas. Because the features of remote sensing images (such as spectral, textural and geometric features) change with changes in illumination, viewing angle, scale and spectrum, it is difficult for a remote sensing intelligent interpretation model with a single data source as input to meet the requirements of engineering or large-scale vegetation information extraction and updating. The effective use multi-source, multi-resolution and multi-type data for remote sensing classification is still a challenge. The objective of this study is to develop a highly intelligent and generalizable classification model of mountain vegetation utilizing multi-source remote sensing data to achieve accurate vegetation extraction. Therefore, a multi-channel semantic segmentation model based on deep learning, FCN-ResNet, is proposed to integrate the features and textures of multi-source, multi-resolution and multi-temporal remote sensing data, thereby enhancing the differentiation of different mountain vegetation types by capturing their characteristics and dynamic changes. In addition, several sets of ablation experiments are designed to investigate the effectiveness of the model. The method is validated on Mt. Taibai (part of the Qinling-Daba Mountains), and the pixel accuracy (PA) of vegetation classification reaches 85.8%. The results show that the proposed multi-channel semantic segmentation model can effectively discriminate different vegetation types and has good intelligence and generalization ability in different mountainous areas with similar vegetation distributions. The multi-channel semantic segmentation model can be used for the rapid updating of vegetation type maps in mountainous areas.",vegetation classification,multi-source image,remote sensing,semantic segmentation,,,,,multi-channel model,deep learning,,,,,,,,,,,,,,,,,,,,,,,
Row_753,"Ding, Cheng","Weng, Liguo","Xia, Min",Non-Local Feature Search Network for Building and Road Segmentation of Remote Sensing Image,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,APR 2021,17,"Building and road extraction from remote sensing images is of great significance to urban planning. At present, most of building and road extraction models adopt deep learning semantic segmentation method. However, the existing semantic segmentation methods did not pay enough attention to the feature information between hidden layers, which led to the neglect of the category of context pixels in pixel classification, resulting in these two problems of large-scale misjudgment of buildings and disconnection of road extraction. In order to solve these problem, this paper proposes a Non-Local Feature Search Network (NFSNet) that can improve the segmentation accuracy of remote sensing images of buildings and roads, and to help achieve accurate urban planning. By strengthening the exploration of hidden layer feature information, it can effectively reduce the large area misclassification of buildings and road disconnection in the process of segmentation. Firstly, a Self-Attention Feature Transfer (SAFT) module is proposed, which searches the importance of hidden layer on channel dimension, it can obtain the correlation between channels. Secondly, the Global Feature Refinement (GFR) module is introduced to integrate the features extracted from the backbone network and SAFT module, it enhances the semantic information of the feature map and obtains more detailed segmentation output. The comparative experiments demonstrate that the proposed method outperforms state-of-the-art methods, and the model complexity is the lowest.",semantic segmentation,building and road segmentation,self-attention,deep learning,"Lin, Haifeng",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_754,"Zhao, Rui","Shi, Zhenwei","Zou, Zhengxia",High-Resolution Remote Sensing Image Captioning Based on Structured Attention,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,76,"Automatically generating language descriptions of remote sensing images has become an emerging research hot spot in the remote sensing field. Attention-based captioning, as a representative group of recent deep learning-based captioning methods, shares the advantage of generating the words while highlighting corresponding object locations in the image. Standard attention-based methods generate captions based on coarse-grained and unstructured attention units, which fails to exploit structured spatial relations of semantic contents in remote sensing images. Although the structure characteristic makes remote sensing images widely divergent to natural images and poses a greater challenge for the remote sensing image captioning task, the key of most remote sensing captioning methods is usually borrowed from the computer vision community without considering the domain knowledge behind. To overcome this problem, a fine-grained, structured attention-based method is proposed to utilize the structural characteristics of semantic contents in high-resolution remote sensing images. Our method learns better descriptions and can generate pixelwise segmentation masks of semantic contents. The segmentation can be jointly trained with the captioning in a unified framework without requiring any pixelwise annotations. Evaluations are conducted on three remote sensing image captioning benchmark data sets with detailed ablation studies and parameter analysis. Compared with the state-of-the-art methods, our method achieves higher captioning accuracy and can generate high-resolution and meaningful segmentation masks of semantic contents at the same time.",Remote sensing,Semantics,Image segmentation,Proposals,,,,,Decoding,Task analysis,Feature extraction,Image captioning,image segmentation,remote sensing image,structured attention,,,,,,,,,,,,,,,,,,
Row_755,"Hu, Kai","Zhang, Enwei","Xia, Min",MCANet: A Multi-Branch Network for Cloud/Snow Segmentation in High-Resolution Remote Sensing Images,REMOTE SENSING,FEB 2023,33,"Because clouds and snow block the underlying surface and interfere with the information extracted from an image, the accurate segmentation of cloud/snow regions is essential for imagery preprocessing for remote sensing. Nearly all remote sensing images have a high resolution and contain complex and diverse content, which makes the task of cloud/snow segmentation more difficult. A multi-branch convolutional attention network (MCANet) is suggested in this study. A double-branch structure is adopted, and the spatial information and semantic information in the image are extracted. In this way, the model's feature extraction ability is improved. Then, a fusion module is suggested to correctly fuse the feature information gathered from several branches. Finally, to address the issue of information loss in the upsampling process, a new decoder module is constructed by combining convolution with a transformer to enhance the recovery ability of image information; meanwhile, the segmentation boundary is repaired to refine the edge information. This paper conducts experiments on the high-resolution remote sensing image cloud/snow detection dataset (CSWV), and conducts generalization experiments on two publicly available datasets (HRC_WHU and L8 SPARCS), and the self-built cloud and cloud shadow dataset. The MIOU scores on the four datasets are 92.736%, 91.649%, 80.253%, and 94.894%, respectively. The experimental findings demonstrate that whether it is for cloud/snow detection or more complex multi-category detection tasks, the network proposed in this paper can completely restore the target details, and it provides a stronger degree of robustness and superior segmentation capabilities.",multi-branch,segmentation,deep learning,remote sensing image,"Weng, Liguo","Lin, Haifeng",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_756,"Ling, Min","Cheng, Qun","Peng, Jun",Image Semantic Segmentation Method Based on Deep Learning in UAV Aerial Remote Sensing Image,MATHEMATICAL PROBLEMS IN ENGINEERING,APR 26 2022,2,"The existing semantic segmentation methods have some shortcomings in feature extraction of remote sensing images. Therefore, an image semantic segmentation method based on deep learning in UAV aerial remote sensing images is proposed. First, original remote sensing images obtained by S185 multirotor UAV are divided into smaller image blocks through sliding window and normalized to provide high-quality image set for subsequent operations. Then, the symmetric encoding-decoding network structure is improved. Bottleneck layer with 1x1 convolution is introduced to build ISegNet network model, and pooling index and convolution are used to fuse semantic information and image features. The improved encoding-decoding network gradually strengthens the extraction of details and reduces the number of parameters. Finally, based on ISegNet network, five-classification problem is transformed into five binary classification problems for network training, so as to obtain high-precision image semantic segmentation results. The experimental analysis of the proposed method based on TensorFlow framework shows that the accuracy value reaches 0.901, and the F1 value is not less than 0.83. The overall segmentation effect is better than those of other comparison methods.",,,,,"Zhao, Chenyi","Jiang, Ling",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_757,"Wen, Bin","Pan, Rumeng","Zeng, Yahui",Crop classification in time-series remote sensing images based on multi-feature extraction and attention connection semantic segmentation model,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,2024,0,"Time-series multispectral remote sensing imagery provides a dynamic representation of crop variations over time, highlighting the disparities in growth conditions among diverse crop types. This method offers a superior capability to differentiate between crop types compared to single temporal phase imagery. However, when applied to practical issues of crop segmentation, challenges persist, including low segmentation accuracy and underutilization of features. To comprehensively address these challenges, we design a new multidimensional multi-attention semantic segmentation network suitable for crop extraction from timeseries multispectral remote sensing images (TSRSnet). Specifically, we propose a multidimensional feature extraction module (MFE) for efficiently mining spatial, temporal and spectral dimensional features in multi-temporal remote sensing images. The module consists of a spatial convolutional layer with temporal modeling capabilities, and a pairwise attention mechanism for joint spatial and channel. Furthermore, we have introduced a difference skip connection module based on a multi-attention mechanism (ADC). This module can accomplish superior performance during the feature fusion phase, contingent on the significance of intra-feature and inter-feature relationships. We experimentally compare the proposed network with a typical semantic segmentation network on the dataset. The results show that the network proposed in this paper has the best segmentation performance and is well suited for the task of crop segmentation in time-series multispectral remote sensing images. The Mean Intersection over Union reached 86.37% and overall accuracy over 93.24%. Through the combination of meticulous feature extraction and multiattention mechanism, our approach improves the segmentation accuracy while better utilizing the multidimensional information of the image, thus realizing the accurate recognition and segmentation of different crops.",time-series,remote sensing images,multidimensional feature,attention connection,"Cheng, Haibo","Zou, Jialuo","Cao, Yungang","Li, Xuqing",crop classification,,,,,,,,,,,,,,,,,,,,,,,,
Row_758,"Liang, Han","Seo, Suyoung",,LPASS-Net: Lightweight Progressive Attention Semantic Segmentation Network for Automatic Segmentation of Remote Sensing Images,REMOTE SENSING,DEC 2022,2,"Semantic segmentation of remote sensing images plays a crucial role in urban planning and development. How to perform automatic, fast, and effective semantic segmentation of considerable size and high-resolution remote sensing images has become the key to research. However, the existing segmentation methods based on deep learning are complex and often difficult to apply practically due to the high computational cost of the excessive parameters. In this paper, we propose an end-to-end lightweight progressive attention semantic segmentation network (LPASS-Net), which aims to solve the problem of reducing computational costs without losing accuracy. Firstly, its backbone features are based on a lightweight network, MobileNetv3, and a feature fusion network composed of a reverse progressive attentional feature fusion network work. Additionally, a lightweight non-local convolutional attention network (LNCA-Net) is proposed to effectively integrate global information of attention mechanisms in the spatial dimension. Secondly, an edge padding cut prediction (EPCP) method is proposed to solve the problem of splicing traces in the prediction results. Finally, evaluated on the public datasets BDCI 2017 and ISPRS Potsdam, the mIoU reaches 83.17% and 88.86%, respectively, with an inference time of 0.0271 s.",lightweight network,attention mechanism,very high resolution,deep learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_759,"Zhou, Xichuan","Liang, Fu","Chen, Lihui",MeSAM: Multiscale Enhanced Segment Anything Model for Optical Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"Segment anything model (SAM) has been widely applied to various downstream tasks for its excellent performance and generalization capability. However, SAM exhibits three limitations related to remote sensing (RS) semantic segmentation task: 1) the image encoders excessively lose high-frequency information, such as object boundaries and textures, resulting in rough segmentation masks; 2) due to being trained on natural images, SAM faces difficulty in accurately recognizing objects with large-scale variations and uneven distribution in RS images; and 3) the output tokens used for mask prediction are trained on natural images and not applicable to RS image segmentation. In this article, we explore an efficient paradigm for applying SAM to the semantic segmentation of RS images. Furthermore, we propose multiscale enhanced SAM (MeSAM), a new SAM fine-tuning method more suitable for RS images to adapt it to semantic segmentation tasks. Our method first introduces an inception mixer into the image encoder to effectively preserve high-frequency features. Second, by designing a mask decoder with RS correction and incorporating multiscale connections, we make up the difference in SAM from natural images to RS images. Experimental results demonstrated that our method significantly improves the segmentation accuracy of SAM for RS images, outperforming some state-of-the-art (SOTA) methods. The code will be available at https://github.com/Magic-lem/MeSAM.",High-frequency,multiscale,remote sensing (RS),segment anything model (SAM),"Liu, Haijun","Song, Qianqian","Vivone, Gemine","Chanussot, Jocelyn",semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_760,"Shankar, Siddharth","Stearns, Leigh A.","van der Veen, C. J.",Semantic segmentation of glaciological features across multiple remote sensing platforms with the Segment Anything Model (SAM),JOURNAL OF GLACIOLOGY,NOV 2023,4,"Semantic segmentation is a critical part of observation-driven research in glaciology. Using remote sensing to quantify how features change (e.g. glacier termini, supraglacial lakes, icebergs, crevasses) is particularly important in polar regions, where glaciological features may be spatially small but reflect important shifts in boundary conditions. In this study, we assess the utility of the Segment Anything Model (SAM), released by Meta AI Research, for cryosphere research. SAM is a foundational AI model that generates segmentation masks without additional training data. This is highly beneficial in polar science because pre-existing training data rarely exist. Widely-used conventional deep learning models such as UNet require tens of thousands of training labels to perform effectively. We show that the Segment Anything Model performs well for different features (icebergs, glacier termini, supra-glacial lakes, crevasses), in different environmental settings (open water, melange, and sea ice), with different sensors (Sentinel-1, Sentinel-2, Planet, timelapse photographs) and different spatial resolutions. Due to the performance, versatility, and cross-platform adaptability of SAM, we conclude that it is a powerful and robust model for cryosphere research.",Crevasses,glacier mapping,iceberg calving,remote sensing,,,,,sea ice,,,,,,,,,,,,,,,,,,,,,,,,
Row_761,"Rao, Zhibo","He, Mingyi","Dai, Yuchao",Class Attention Network for Semantic Segmentation of Remote Sensing Images,,2020,3,"Semantic segmentation in remote sensing images is beneficial to detect objects and understand the scene in earth observation. However, classical networks always failed to obtain an accuracy segmentation map in remote sensing images due to the imbalanced labels. In this paper, we proposed a novel class attention module and decomposition-fusion strategy to cope with imbalanced labels. Based on this motivation, we investigate related architecture and strategy by follows. (1) we build a class attention module to generate multi-class attention maps, which forces the network to keep attention to small sample categories instead of being flooded by large sample data. (2) we introduce salient detection, which decomposes semantic segmentation into multi-class salient detection and then fuses them to produce a segmentation map. Extensive experiments on popular benchmarks (e.g., US3D dataset) show that our approach can serve as an efficient plug-and-play module or strategy in the previous scene parsing networks to help them cope with the problem of imbalance labels in remote sensing images.",,,,,,,,,,,,,,,,,,2020 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL SUMMIT AND CONFERENCE (APSIPA ASC),,,,,,,,,,,,,,,
Row_762,"Fan, Zhenyu","Zhan, Tao","Gao, Zhichao",Land Cover Classification of Resources Survey Remote Sensing Images Based on Segmentation Model,IEEE ACCESS,2022,10,"Land type survey is an important task of land resources survey and the basis of scientific management of land resources. With the increasingly prominent problems of population, resources, and environment, there is an urgent need for a fast and accurate classification method of large-scale land use and land cover based on remote sensing data. Traditional machine learning classification methods based on pixel classification achieved sufficient results and are widely used, such as maximum likelihood classification and random forests method. However, with the development of the novel technology of deep learning, in practical application, for multi-classified land resources, how to use the fast and effective classification method of low and medium resolution RS images needs further research. This paper takes the land resource classification of the Tonghe medium resolution RS dataset of the third land survey in China as an example to screen and compare traditional machine learning classification methods and semantic segmentation models FC-DenseNet56, GCN, BiSeNet, U-Net, DeepLabV3, AdapNet, and PSPNet, which aim to select the optimal feature extraction model. The results show that the classification accuracy of the U-Net model can reach 93.62%, which is more accurate and effective than traditional machine learning methods and other semantic segmentation models. It is suitable for multi-classification tasks of land cover resources in low and medium resolution RS images and shows a superior effect in practical application. Besides, the conclusion of this study can provide a demonstration for large-scale land cover resources investigation using low and medium resolution RS images.",Image segmentation,Semantics,Machine learning,Feature extraction,"Li, Rui","Liu, Yao","Zhang, Lianzhi","Jin, Zixiang",Radio frequency,Classification algorithms,Spatial resolution,Land use and land cover,semantic segmentation,multi-classification,deep learning,U-Net,"Xu, Supeng",,,,,,,,,,,,,,,,
Row_763,"Zhang, Yinsheng","Ji, Ru","Hu, Yuxiang",Real-Time Semantic Segmentation of Remote Sensing Images for Land Management,PHOTOGRAMMETRIC ENGINEERING AND REMOTE SENSING,JUN 2024,0,"Remote sensing image segmentation is a crucial technique in the field of land management. However, existing semantic segmentation networks require a large number of floating-point operations (FLOPs) and have long run times. In this paper, we propose a dual -path feature aggregation network (DPFANet) specifically designed for the low -latency operations required in land management applications. Firstly, we use four sets of spatially separable convolutions with varying dilation rates to extract spatial features. Additionally, we use an improved version of MobileNetV2 to extract semantic features. Furthermore, we use an asymmetric multi -scale fusion module and dual -path feature aggregation module to enhance feature extraction and fusion. Finally, a decoder is constructed to enable progressive up -sampling. Experimental results on the Potsdam data set and the Gaofen image data set (GID) demonstrate that DPFANet achieves overall accuracy of 92.2% and 89.3%, respectively. The FLOPs are 6.72 giga and the number of parameters is 2.067 million.",,,,,"Yang, Yulong","Chen, Xin","Duan, Xiuxian","Shan, Huilin",,,,,,,,,,,,,,,,,,,,,,,,,
Row_764,"Zhang, Junjie","Zhang, Qiming","Gong, Yongshun",Weakly Supervised Semantic Segmentation With Consistency-Constrained Multiclass Attention for Remote Sensing Scenes,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Obtaining image-level class labels for remote sensing (RS) images is a relatively straightforward process, sparking significant interest in weakly supervised semantic segmentation (WSSS). However, RS images present challenges beyond those encountered in generic WSSS, including complex backgrounds, densely distributed small objects, and considerable scale variations. To address the above issues, we introduce a consistency-constrained multiclass attention model, noted as CocoaNet. Specifically, CocoaNet endeavors to capture both semantic correlation and class distinctiveness using a global-local adaptive attention mechanism, which integrates the self-attention to model global correlation, complemented by a local perception branch that intensifies focus on local regions. The resulting class-specific attention weights and the patch-level pairwise affinity weights are employed to optimize the initial class activation maps (CAMs). This mechanism proves highly effective in mitigating interclass interference and managing the distribution of densely clustered small objects. Moreover, we invoke a consistency constraint to rectify activation inaccuracy. By utilizing a Siamese structure for the mutual supervision of features extracted from images at different scales, we address substantial scale variations in RS scenes. Simultaneously, a class contrast loss is adopted to enhance the discriminativeness of class-specific features. Departing from the conventional CAM optimization, which is rather complex and time-consuming, we harness the prior knowledge from the generic segment anything model (SAM) to design a joint optimization strategy (JOS) that refines target boundaries and further promotes discriminative visual features. We validate the effectiveness of our proposed approach on three benchmark datasets in multiclass RS scenarios, and the experimental results demonstrate that our model yields promising advancements compared to state-of-the-art methods.",Consistency constraint,global-local adaptive attention,weakly supervised semantic segmentation (WSSS),,"Zhang, Jian","Chen, Liang","Zeng, Dan",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_765,"Gao, Han","Zhao, Yang","Guo, Peng",Cycle and Self-Supervised Consistency Training for Adapting Semantic Segmentation of Aerial Images,REMOTE SENSING,APR 2022,11,"Semantic segmentation is a critical problem for many remote sensing (RS) image applications. Benefiting from large-scale pixel-level labeled data and the continuous evolution of deep neural network architectures, the performance of semantic segmentation approaches has been constantly improved. However, deploying a well-trained model on unseen and diverse testing environments remains a major challenge: a large gap between data distributions in train and test domains results in severe performance loss, while manual dense labeling is costly and not scalable. To this end, we proposed an unsupervised domain adaptation framework for RS image semantic segmentation that is both practical and effective. The framework is supported by the consistency principle, including the cycle consistency in the input space and self-supervised consistency in the training stage. Specifically, we introduce cycle-consistent generative adversarial networks to reduce the discrepancy between source and target distributions by translating one into the other. The translated source data then drive a pipeline of supervised semantic segmentation model training. We enforce consistency of model predictions across target image transformations in order to provide self-supervision for the unlabeled target data. Experiments and extensive ablation studies demonstrate the effectiveness of the proposed approach on two challenging benchmarks, on which we achieve up to 9.95% and 7.53% improvements in accuracy over the state-of-the-art methods, respectively.",unsupervised domain adaptation,semantic segmentation,self-supervision,remote sensing image,"Sun, Zihao","Chen, Xiuwan","Tang, Yunwei",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_766,"Liang, Chenbin","Cheng, Bo","Xiao, Baihua",Multilevel Heterogeneous Domain Adaptation Method for Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,12,"Due to more abundant data sources, more various objects of interest, and more time-consuming annotations, there is a large amount of out-of-distribution (OOD) data in the remote sensing field, on which the performance of high-accuracy image segmentation models trained under ideal experimental conditions generally degrades dramatically. Domain adaptation (DA) consequently comes into being, which aims to learn the predictor for the label-scarce target domain of interest with the help of the label-sufficient source domain in the presence of the distribution difference, namely, domain shift, between the two domains. However, the off-the-shelf DA methods for image segmentation not only struggle to cope with the more complex domain shift problems in remote sensing imagery but also almost cannot process heterogeneous data directly without information loss. While the current heterogeneous DA methods mostly still rely on some supervision information from the target domain, which is typically inaccessible in the real world. To overcome these drawbacks, we propose the multilevel heterogeneous unsupervised DA (UDA) method, termed MHDA, which unifies the instance-level DA based on cycle consistency, the feature-level DA based on contrastive learning, and the decision-level DA based on task consistency into a framework to more effectively handle the complex domain shift and heterogeneous data. After that, extensive DA experiments are conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) dataset, the BigCity dataset constructed by ourselves, and the Wuhan University (WHU) dataset, to explore the effect of each module in MHDA, the necessity of heterogeneous DA, and the effectiveness of multilevel DA. And the results demonstrate that MHDA can achieve superior performance on the remote sensing image segmentation task, compared with several state-of-the-art DA methods.",Remote sensing,Image segmentation,Task analysis,Sensors,"Dong, Yunyun","Chen, Jinfen",,,Adaptation models,Training,Spatial resolution,Contrastive learning,cycle consistency,domain adaptation (DA),semantic segmentation,task consistency,,,,,,,,,,,,,,,,,
Row_767,"Pang, Shiyan","Li, Xinyu","Chen, Jia",Prior Semantic Information Guided Change Detection Method for Bi-temporal High-Resolution Remote Sensing Images,REMOTE SENSING,MAR 2023,5,"High-resolution remote sensing image change detection technology compares and analyzes bi-temporal or multitemporal high-resolution remote sensing images to determine the change areas. It plays an important role in land cover/use monitoring, natural disaster monitoring, illegal building investigation, military target strike effect analysis, and land and resource investigation. The change detection of high-resolution remote sensing images has developed rapidly from data accumulation to algorithm models because of the rapid development of technologies such as deep learning and earth observation in recent years. However, the current deep learning-based change detection methods are strongly dependent on large sample data, and the training model has insufficient cross-domain generalization ability. As a result, a prior semantic information-guided change detection framework (PSI-CD), which alleviates the change detection model's dependence on datasets by making full use of prior semantic information, is proposed in this paper. The proposed method mainly includes two parts: one is a prior semantic information generation network that uses the semantic segmentation dataset to extract robust and reliable prior semantic information; the other is the prior semantic information guided change detection network that makes full use of prior semantic information to reduce the sample size of the change detection. To verify the effectiveness of the proposed method, we produced pixel-level semantic labels for the bi-temporal images of the public change detection dataset (LEVIR-CD). Then, we performed extensive experiments on the WHU and LEVIR-CD datasets, including comparisons with existing methods, experiments with different amounts of data, and ablation study, to show the effectiveness of the proposed method. Compared with other existing methods, our method has the highest IoU for all training samples and different amounts of training samples on WHU and LEVIR-CD, reaching a maximum of 83.25% and 83.80%, respectively.",change detection,prior semantic information,semantic segmentation,convolutional neural networks,"Zuo, Zhiqi","Hu, Xiangyun",,,high-resolution images,,,,,,,,,,,,,,,,,,,,,,,,
Row_768,"Chen, Keyan","Liu, Chenyang","Chen, Hao",RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation Based on Visual Foundation Model,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,75,"Leveraging the extensive training data from SA-1B, the segment anything model (SAM) demonstrates remarkable generalization and zero-shot capabilities. However, as a category-agnostic instance segmentation method, SAM heavily relies on prior manual guidance, including points, boxes, and coarse-grained masks. Furthermore, its performance in remote sensing image segmentation tasks remains largely unexplored and unproven. In this article, we aim to develop an automated instance segmentation approach for remote sensing images based on the foundational SAM model and incorporating semantic category information. Drawing inspiration from prompt learning, we propose a method to learn the generation of appropriate prompts for SAM. This enables SAM to produce semantically discernible segmentation results for remote sensing images, a concept that we have termed RSPrompter. We also propose several ongoing derivatives for instance segmentation tasks, drawing on recent advancements within the SAM community, and compare their performance with RSPrompter. Extensive experimental results, derived from the WHU building dataset, the NWPU VHR-10 dataset, and the SAR Ship Detection Dataset (SSDD) dataset, validate the effectiveness of our proposed method. The code for our method is publicly available at https://kychen.me/RSPrompter.",Foundation model,instance segmentation,prompt learning,remote sensing images,"Zhang, Haotian","Li, Wenyuan","Zou, Zhengxia","Shi, Zhenwei",segment anything model (SAM),,,,,,,,,,,,,,,,,,,,,,,,
Row_769,"Chen, Ziyi","Li, Dilong","Fan, Wentao",Self-Attention in Reconstruction Bias U-Net for Semantic Segmentation of Building Rooftops in Optical Remote Sensing Images,REMOTE SENSING,JUL 2021,56,"Deep learning models have brought great breakthroughs in building extraction from high-resolution optical remote-sensing images. Among recent research, the self-attention module has called up a storm in many fields, including building extraction. However, most current deep learning models loading with the self-attention module still lose sight of the reconstruction bias's effectiveness. Through tipping the balance between the abilities of encoding and decoding, i.e., making the decoding network be much more complex than the encoding network, the semantic segmentation ability will be reinforced. To remedy the research weakness in combing self-attention and reconstruction-bias modules for building extraction, this paper presents a U-Net architecture that combines self-attention and reconstruction-bias modules. In the encoding part, a self-attention module is added to learn the attention weights of the inputs. Through the self-attention module, the network will pay more attention to positions where there may be salient regions. In the decoding part, multiple large convolutional up-sampling operations are used for increasing the reconstruction ability. We test our model on two open available datasets: the WHU and Massachusetts Building datasets. We achieve IoU scores of 89.39% and 73.49% for the WHU and Massachusetts Building datasets, respectively. Compared with several recently famous semantic segmentation methods and representative building extraction methods, our method's results are satisfactory.",building extraction,U-Net,remote sensing image,building footprint,"Guan, Haiyan","Wang, Cheng","Li, Jonathan",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_770,"de Paulo, M. C. M.","Turnes, J. N.","Happ, P. N.",HOWFAR SHOULD I LOOK? A NEURAL ARCHITECTURE SEARCH STRATEGY FOR SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES,,2022,1,"Neural architecture search (NAS) is a subset of automated machine learning that tries to find the best neural network to perform a given task. In this article, a network search space is defined and applied to perform the semantic segmentation of satellite imagery. Due to the spatial nature of the data, the search space uses cells that group parallel operations with kernels of different sizes, providing options to accommodate the neighborhood information required to perform a better classification. The architecture search space follows a UNet-like network. The proposed approach uses scaled sigmoid gates, a strategy for network pruning that was adapted to search for the best operations on the cell search space. The architecture achieved by the proposed approach uses wider kernels on lower resolution feature maps, which leads to the interpretation that some pixels required information from pixels farther away than expected. The resulting network was compared to a very similar UNet-like network that only used 3x3 convolutions. The resulting network shows slightly better results on the test set.",Neural Architecture Search,Semantic Segmentation,Remote Sensing,Satellite imagery,"Ferreira, M. P.","Marques, H. A.","Feitosa, R. Q.",,Convolutional Neural Networks,,,,,,,,,"XXIV ISPRS CONGRESS: IMAGING TODAY, FORESEEING TOMORROW, COMMISSION III",,,,,,,,,,,,,,,
Row_771,"Li, Huiping",,,Multi-Scale Segmentation Method of Remote Sensing Big Data Image Using Deep Learning,JOURNAL OF INTERCONNECTION NETWORKS,NOV 2024,0,"Remote sensing image (RSI) segmentation is an effective method to interpret remote sensing information and an important means of remote sensing data information processing. Traditional RSI segmentation methods have some problems such as poor segmentation accuracy and low similarity difference measurement. Therefore, we propose a multi-scale segmentation (MSS) method for remote sensing big data image. First, the segmentation scale of RSI is divided, and the quantitative value of histogram band is used to calculate the similarity index between different objects; Second, the parameters in the same spot are improved based on the maximum area method to determine the shape factor of RSI; Finally, the object closure model is established to clarify the region conversion cost, and the RSI is dynamically segmented based on Multi-scale convolutional neural networks; The MSS algorithm of RSI is designed, and the MSS method of RSI is obtained. The results show that the maximum similarity difference measure of the proposed method is 0.648, and the similarity difference measure always remains the largest. The maximum recall of RSI is 0.954, and the highest recall is 0.988, indicating that the RSI segmentation accuracy of the proposed method is good.",Deep learning,remote sensing big data images,multi-scale segmentation,feature space,,,,,segmentation accuracy,,,,,,,,,,,,,,,,,,,,,,,,
Row_772,"Yang, Yang","Wang, Yanhui","Dong, Junwu",A Knowledge Distillation-Based Ground Feature Classification Network With Multiscale Feature Fusion in Remote-Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,2,"As a fundamental task in remote-sensing interpretation, semantic segmentation of remote-sensing images intends to allocate a definite class to each pixel in the image. Fast and efficient semantic segmentation of high-resolution remote-sensing images provides help to capture the real surface covering and plays an essential role in urban planning and dynamic monitoring. However, there are still some limitations in the previous remote-sensing image semantic segmentation model for urban scenes, such as the low weight of small target pixels and the tiny target size leading to the unsatisfactory recognition and segmentation results of the model for small target features. Meanwhile, the deeper and broader feature extraction module in the semantic segmentation network usually leads to more redundant parameters, which takes a lot of computation time. Thus, we propose a lightweight semantic segmentation network based on the knowledge distillation combined with a multiscale pyramidal pooling module and attention mechanism named KD-MSANet, which enhanced the ability to fuse and focus on shallow features. Then, we trained teacher-student models to obtain lightweight network models through a model pruning and distillation framework. Experiments on Vaihingen and Potsdam datasets demonstrated that the network we designed significantly reduces the number of parameters while ensuring almost constant accuracy. Compared with the precompression model, the student model reduced in size by 43.6% and the training efficiency was improved by 22.3%, while the accuracy reached 99.30% of the teacher model.",Feature extraction,Semantic segmentation,Semantics,Remote sensing,"Yu, Bibo",,,,Computational modeling,Knowledge engineering,Task analysis,Knowledge distillation (KD),multiscale pyramidal pooling,remote-sensing image (RSI),semantic segmentation,,,,,,,,,,,,,,,,,,
Row_773,"Jaber, Mustafa Musa","Ali, Mohammed Hasan","Abd, Sura Khalil",A Machine Learning-Based Semantic Pattern Matching Model for Remote Sensing Data Registration,JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING,DEC 2022,2,"Remote sensing image registration can benefit from a machine learning method based on the likelihood of predicting semantic spatial position distributions. Semantic segmentation of images has been revolutionized due to the accessibility of high-resolution remote sensing images and the advancement of machine learning techniques. This system captures the semantic distribution location of the matching reference picture, which ML mapped using learning-based algorithms. The affine invariant is utilized to determine the semantic template's barycenter position and the pixel's center, which changes the semantic border alignment problem into a point-to-point matching issue for the machine learning-based semantic pattern matching (ML-SPM) model. The first step examines how various factors such as template radius, training label filling form, or loss function combination affect matching accuracy. In this second step, the matching of sub-images (MSI) images is compared using heatmaps created from the expected similarity between the images' cropped sub-images. Images having radiometric discrepancies are matched with excellent accuracy by the approach. SAR-optical image matching has never been easier, and now even large-scale sceneries can be registered using this approach, which is a significant advance over previous methods. Optical satellite imaging or multi-sensor stereogrammetry can be combined with both forms of data to enhance geolocation.",Remote sensing image,Machine learning,Semantic pattern matching,Matching of sub-images,"Jassim, Mustafa Mohammed","Alkhayyat, Ahmed","Alreda, Baraa A.","Alkhuwaylidee, Ahmed Rashid",Loss function,Synthetic aperture radar (SAR),,,,,,,"Alyousif, Shahad",,,,,,,,,,,,,,,,
Row_774,"Xiu, Xiaochen","Ma, Xianping","Pun, Man-On",MDAFNET: MONOCULAR DEPTH-ASSISTED FUSION NETWORKS FOR SEMANTIC SEGMENTATION OF COMPLEX URBAN REMOTE SENSING DATA,,2023,0,"This work proposes an end-to-end Monocular Depth-Assisted Fusion Network (MDAFNet) for semantic segmentation of complex urban remote sensing data. The proposed MDAFNet consists of a Monocular Depth Estimation Network (MDENet) and a Crossmodal Fusion Network (CFNet). More specifically, the MDENet first generates the earth surface depth information while the CFNet fuses the generated depth information and RGB images to address the segmentation task. In particular, the MDENet is capable of effectively extracting features of the ground surface while overcoming artifacts such as building shadows. Furthermore, the CFNet is designed to perform segmentation by extracting and fusing semantic information from generated depth information and Red-Green-Blue (RGB) images. Extensive experiments performed on a large-scale fine-resolution remote sensing dataset named the ISPRS Vaihingen confirm that the proposed MDAFNet outperforms conventional crossmodal models equipped with Digital Surface Model information.",,,,,"Liu, Ming",,,,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_775,"Tian, Qing","Zhao, Fuhui","Zhang, Zheng",GLFFNet: A Global and Local Features Fusion Network with Biencoder for Remote Sensing Image Segmentation,APPLIED SCIENCES-BASEL,AUG 2023,2,"In recent years, semantic segmentation of high-resolution remote sensing images has been gradually applied to many important scenes. However, with the rapid development of remote sensing data acquisition technology, the existing image data processing methods are facing major challenges. Especially in the accuracy of extraction and the integrity of the edges of objects, there are often problems such as small objects being assimilated by large objects. In order to solve the above problems, based on the excellent performance of Transformer, convolution and its variants, and feature pyramids in the field of deep learning image segmentation, we designed two encoders with excellent performance to extract global high-order interactive features and low-order local feature information. These encoders are then used as the backbone to construct a global and local feature fusion network with a dual encoder (GLFFNet) to effectively complete the segmentation of remote sensing images. Furthermore, a new auxiliary training module is proposed that uses the semantic attention layer to process the extracted feature maps separately, adjust the losses, and more specifically optimize each encoder of the backbone, thus optimizing the training process of the entire network. A large number of experiments show that our model achieves 87.96% mIoU on the Potsdam dataset and 80.42% mIoU on the GID dataset, and it has superior performance compared with some state-of-the-art methods on semantic segmentation tasks in the field of remote sensing.",remote sensing image,gated convolution,transformer,atrous convolution,"Qu, Hongquan",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_776,"Kim, Minho","Dronova, Iryna","Radke, John",SEMANTIC SEGMENTATION OF ENHANCED LANDFORM MAPS USING HIGH RESOLUTION SATELLITE IMAGES,,2023,0,"High resolution fuel maps are useful for high resolution wildfire simulations and detection of hazards on the landscape. In general, high resolution Enhanced Lifeform Maps (ELMs) are used in conjunction with other data layers to create these fuel maps. However, ELMs are costly to make with substantial manual editing involved. In response, this study uses deep learning-based semantic segmentation models to generate 5-m resolution ELMs (14 classes) in Marin and San Mateo, California using high resolution remote sensing datasets. ELM classes were found to be severely imbalanced, leading to model overfitting. Sample weighted loss functions helped alleviate this issue to an extent. High resolution ELMs are bound to be more valuable with the growing fire risk and landscape heterogeneity, particularly near the wildland urban interface. All codes, future updates, and further details can be found at https://github.com/minhokim93/elm_mapping.",Enhanced lifeform map,semantic segmentation,deep learning,remote sensing,,,,,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_777,"Li, Hanlu","Li, Lei","Zhao, Liangyu",ResU-Former: Advancing Remote Sensing Image Segmentation with Swin Residual Transformer for Precise Global-Local Feature Recognition and Visual-Semantic Space Learning,ELECTRONICS,JAN 2024,4,"In the field of remote sensing image segmentation, achieving high accuracy and efficiency in diverse and complex environments remains a challenge. Additionally, there is a notable imbalance between the underlying features and the high-level semantic information embedded within remote sensing images, and both global and local recognition improvements are also limited by the multi-scale remote sensing scenery and imbalanced class distribution. These challenges are further compounded by inaccurate local localization segmentation and the oversight of small-scale features. To achieve balance between visual space and semantic space, to increase both global and local recognition accuracy, and to enhance the flexibility of input scale features while supplementing global contextual information, in this paper, we propose a U-shaped hierarchical structure called ResU-Former. The incorporation of the Swin Residual Transformer block allows for the efficient segmentation of objects of varying sizes against complex backgrounds, a common scenario in remote sensing datasets. With the specially designed Swin Residual Transformer block as its fundamental unit, ResU-Former accomplishes the full utilization and evolution of information, and the maximum optimization of semantic segmentation in complex remote sensing scenarios. The standard experimental results on benchmark datasets such as Vaihingen, Overall Accuracy of 81.5%, etc., show the ResU-Former's potential to improve segmentation tasks across various remote sensing applications.",semantic segmentation,transformer,balance between visual and semantic space,enhancement of both global and local aspects,"Liu, Fuxiang",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_778,"Zhang, Yanan","Lu, Chen","Wang, Jiao",A large-scale extraction framework for mapping urban in-formal settlements using remote sensing and semantic segmentation,GEOCARTO INTERNATIONAL,JAN 1 2024,2,"Urban informal settlements (UISs) are densely populated and poorly developed residential areas in urban areas. The mapping of UISs using remote sensing is crucial for urban planning and management. However, the large-scale extraction of UISs is impeded by the labor-intensive task of collecting numerous training samples and the lack of automatic and effective city partition. To overcome these challenges, we proposed a large-scale extraction framework for UISs based on semantic segmentation of high-resolution remote sensing images. Utilizing Deeplab V3 Plus as the foundational extraction model, the proposed framework introduces fast sample collection based on GLCM features. Besides, an automatic city partition approach combined with clustering and fine-tuning was proposed to enhance the performance on extracting a specific category of UISs. The results of the case study conducted in 36 major Chinese cities show that the proposed framework achieved good performance, with an overall F1 score of 85.76%. Furthermore, comparative assessments were performed to demonstrate the effectiveness of automatic city partition. The proposed framework offers a practical approach for the large-scale extraction of UISs, which holds great significance for sustainable development, poverty estimation, infrastructure construction, and urban planning.",Urban informal settlement,large scale,remote sensing,China,"Du, Fuguang",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_779,"Xiao, Dong","Kang, Zhihao","Fu, Yanhua",Csswin-unet: a Swin-unet network for semantic segmentation of remote sensing images by aggregating contextual information and extracting spatial information,INTERNATIONAL JOURNAL OF REMOTE SENSING,DEC 2 2023,4,"Image interpretation algorithms based on deep learning are becoming increasingly important in land cover information acquisition. We propose CSSwin-unet, a network designed for the semantic segmentation of remote sensing images. CSSwin-unet is based on Swin-unet, follows the U-shaped codec structure of U-Net, but utilizes Swin transformer blocks with superior global modelling capabilities to constitute the codec. In addition, we design a parallel branch in the encoder with a context aggregation module (CAM) to enhance contextual information extraction and alleviate the semantic ambiguity problem resulting from occlusion. To address the problem of semantic information mismatch between codecs and improve the model's ability to extract spatial information, we constructed a space extraction module (SEM) in the skip connections, which replaces the direct copying of encoder features in Swin-unet. To reduce information loss during the downsampling process and strengthen the segmentation capacity of the network, we designed a feature shrinkage module (FSM) in the downsampling session. We conducted comprehensive ablation experiments on a dataset we produced ourselves and compared the results with other advanced methods. The test results showed significant improvement, with mIoU, mF1, and OA values improving by 2.83%, 2.47%, and 2.05%, respectively, compared to the second best performing model, Swin-unet. The above results prove the excellent performance of CSSwin-unet.",Deep learning,remote sensing,segmentation,contextual information aggregation,"Li, Zhenni","Ran, Mengying",,,spatial information extraction,,,,,,,,,,,,,,,,,,,,,,,,
Row_780,"Chakravorty, Anisha","Chakraborty, Shounak",,A novel semi-supervised approach for semantic segmentation of aerial remote sensing images under limited ground-truth availability,SIGNAL IMAGE AND VIDEO PROCESSING,DEC 2024,0,"Conventional semantic segmentation techniques rely heavily on the availability of substantial ground-truth data. However, this prerequisite often proves infeasible in real-world scenarios, particularly with the labeling complexities inherent in remote sensing images. In this manuscript, a semi-supervised approach has been investigated towards semantic segmentation of remotely sensed images by addressing the challenge of limited availability of ground-truth information. For this purpose, a hybrid integration of a standard semantic segmentation model and an adversarial model has been proposed under semi-supervised setting. The former predict the masks for the unlabelled images when fine-tuned with the available labelled training images (however limited they may be); whereas the latter aids the reconstruction of original input images from the predicted soft(masks) through an adversarial mechanism. This reconstruction, further validated through a reconstruction score, assist in the identification of 'most-confident' image-mask pairs to be strategically integrated into the training set. The contribution ultimately is to utilise the unannotated images to meaningfully augment the limited training set to obtain an enhanced one. The proposed technique showcases a significant improvement, with an 11-34% enhancement over existing approaches in terms of mean intersection over union, precision, and F1-score across both the minifrance and dense labeling remote sensing dataset datasets.",Semantic segmentation,Semi-supervision,Adversarial learning,Aerial imagery,,,,,Limited ground-truth,,,,,,,,,,,,,,,,,,,,,,,,
Row_781,"Zhang, Xiuwei","Zhao, Zixu","Ran, Lingyan",FastICENet: A real-time and accurate semantic segmentation model for aerial remote sensing river ice image,SIGNAL PROCESSING,NOV 2023,5,"River ice semantic segmentation is a crucial task, which can provide us with information for river monitoring, disaster forecasting, and transportation management. Previous works mainly focus on higher accuracy acquirement, while efficiency is also important for reality usage. In this paper, a real-time and accurate river ice semantic segmentation network is proposed, named FastICENet. The general architecture consists of two branches, i.e., a shallow high-resolution spatial branch and a deep context semantic branch, which are carefully designed for the scale diversity and irregular shape of river ice in remote sensing images. Then, a novel Downsampling module and a dense connection block based on a lightweight Ghost module are adopted in the context branch to reduce the computation cost. Furthermore, a learnable upsampling strategy DUpsampling is utilized to replace the commonly used bilinear interpolation to improve the segmentation accuracy. We deploy detailed experiments on three publicly available datasets, named NWPU_YRCC_EX, NWPU_YRCC2, and Alberta River Ice Segmentation Dataset. The experimental results demonstrate that our method achieves state-of-the-art performance with competing methods, on the NWPU_YRCC_EX dataset, we can achieve the segmentation speed as 90.84FPS and the segmentation accuracy as 90.770 % mIoU, which also illustrates the good leverage between accuracy and speed. Our code is available at https://github.com/nwpulab113/FastICENet & COPY; 2023 Elsevier B.V. All rights reserved.",River ice semantic segmentation,Deep learning,Ghost module,DUpsampling,"Xing, Yinghui","Wang, Wenna","Lan, Zeze","Yin, Hanlin",,,,,,,,,"He, Houjun",,"Liu, Qixing","Zhang, Baosen","Zhang, Yanning",,,,,,,,,,,,
Row_782,"Wu, Linshan","Lu, Ming","Fang, Leyuan",Deep Covariance Alignment for Domain Adaptive Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,25,"Unsupervised domain adaptive (UDA) image segmentation has recently gained increasing attention, aiming to improve the generalization capability for transferring knowledge from the source domain to the target domain. However, in high spatial resolution remote sensing image (RSI), the same category from different domains (e.g., urban and rural) can appear to be totally different with extremely inconsistent distributions, which heavily limits the UDA accuracy. To address this problem, in this article, we propose a novel deep covariance alignment (DCA) model for UDA RSI segmentation. The DCA can explicitly align category features to learn shared domain-invariant discriminative feature representations, which enhance the ability of model generalization. Specifically, a category feature pooling (CFP) module is first used to extract category features by combining coarse outputs and deep features. Then, we leverage a novel covariance regularization (CR) to enforce the intracategory features to be closer and the intercategory features to be further separate. Compared with the existing category alignment methods, our CR aims to regularize the correlation between different dimensions of the features, and thus performs more robustly when dealing with divergent category features of imbalanced and inconsistent distributions. Finally, we propose a stagewise procedure to train the DCA to alleviate error accumulation. Experiments on both rural-to-urban and urban-to-rural scenarios of the LoveDA dataset demonstrate the superiority of our proposed DCA over other state-of-the-art UDA segmentation methods. Code is available at https://github.com/Luffy03/DCA.",Feature extraction,Training,Image segmentation,Generative adversarial networks,,,,,Task analysis,Semantics,Adaptation models,Deep covariance alignment (DCA),remote sensing image (RSI),semantic segmentation,unsupervised domain adaptive (UDA),,,,,,,,,,,,,,,,,,
Row_783,"Cui, Jian","Liu, Jiahang","Wang, Jinjin",Global Context Dependencies Aware Network for Efficient Semantic Segmentation of Fine-Resolution Remoted Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,5,"Geospatial object segmentation is a fundamental task in remote sensing image interpretation. Although deep learning has shown great potential for this task, it often suffers from limited receptive fields, insufficient global feature extraction ability, and inaccurate edge positioning, resulting in low accuracy and errors in the results. In this letter, we propose a novel global context dependency aware network (GCDNet) to achieve high-accuracy segmentation results. To overcome the limited receptive fields and promote feature extraction ability, we propose a new dot-product attention (DPA) mechanism to establish long-distance dependencies between different receptive field feature maps. To achieve more accurate object edges, we design an edge-aware optimization (EAO) module to guide the operation to directly optimize the edge details from the prediction result at the pixel level. Extensive experiments on two well-known public high-resolution remote sensing image datasets have been conducted to verify the performance of the proposed method, and the results show that the proposed method has significant advantages and maintains its robustness in different cases. Code will be available at: https://github.com/Cuiadd/GCDNet.",Edge refinement,global context,remote sensing,semantic segmentation,"Ni, Yue",,,,urban scene,,,,,,,,,,,,,,,,,,,,,,,,
Row_784,"Tian, Liang","Zhong, Xiaorou","Chen, Ming",Semantic Segmentation of Remote Sensing Image Based on GAN and FCN Network Model,SCIENTIFIC PROGRAMMING,NOV 3 2021,12,"Accurate remote sensing image segmentation can guide human activities well, but current image semantic segmentation methods cannot meet the high-precision semantic recognition requirements of complex images. In order to further improve the accuracy of remote sensing image semantic segmentation, this paper proposes a new image semantic segmentation method based on Generative Adversarial Network (GAN) and Fully Convolutional Neural Network (FCN). This method constructs a deep semantic segmentation network based on FCN, which can enhance the receptive field of the model. GAN is integrated into FCN semantic segmentation network to synthesize the global image feature information and then accurately segment the complex remote sensing image. Through experiments on a variety of datasets, it can be seen that the proposed method can meet the high-efficiency requirements of complex image semantic segmentation and has good semantic segmentation capabilities.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_785,"Zheng, Shangdong","Wu, Zebin","Du, Qian",Oriented Object Detection for Remote Sensing Images via Object-Wise Rotation-Invariant Semantic Representation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"Oriented object detection (OOD) in remote sensing images (RSIs) remains a challenging work due to an arbitrary orientation of instances. Learning rotation-invariant features is critical in modeling a fixed descriptor for instances with its rotated variants. However, most existing methods construct the descriptor from the perspectives of data or feature augmentation, but ignore the exploration of potentially useful supervision information inside the detection algorithm. In this article, we propose an object-wise rotation-invariant semantic representation (ORSR) framework, which synergizes the exploration of latent supervision, rotation-invariant learning, and guided attention mechanism into a unified network to boost the performance of OOD in RSIs. First, supervised by our constructed pseudo-ground truth of segmentation masks, a semantic segmentation branch is built along with the detection algorithm to refine the representation of backbone features. Moreover, a consistency loss function is proposed to encourage the segmentation branch to make fixed predictions for backbone features with its rotated variants. Considering that segmentation predictions remain the same affine transformations before and after rotating, we further construct a Kullback-Leibler (KL) divergence-based similarity loss function to encourage the network to model the rotation-invariant features. Finally, we separate the ""object"" descriptor from the segmentation predictions to extend the implicit constraint in our proposed semantic segmentation branch. The separated ""object"" descriptor not only involves the spatial regularizer to emphasize the high-responsive regions in the image but also can be guided by the constructed consistency loss function. We evaluate our proposed ORSR on the challenging DOTA, DIOR-R, and HRSC2016 datasets. Extensive experiments demonstrate that the proposed ORSR achieves competitive performance compared to other single-scale and multiscale (MS) detection methods.",Image segmentation,Feature extraction,Task analysis,Semantics,"Xu, Yang","Wei, Zhihui",,,Object detection,Remote sensing,Proposals,Oriented object detection (OOD),remote sensing images (RSIs),rotation-invariant learning,semantic representation,,,,,,,,,,,,,,,,,,
Row_786,"Yang, Zimeng","Wu, Qiulan","Zhang, Feng",Optimizing Spatial Relationships in GCN to Improve the Classification Accuracy of Remote Sensing Images,INTELLIGENT AUTOMATION AND SOFT COMPUTING,2023,2,"Semantic segmentation of remote sensing images is one of the core tasks of remote sensing image interpretation. With the continuous develop-ment of artificial intelligence technology, the use of deep learning methods for interpreting remote-sensing images has matured. Existing neural networks disregard the spatial relationship between two targets in remote sensing images. Semantic segmentation models that combine convolutional neural networks (CNNs) and graph convolutional neural networks (GCNs) cause a lack of feature boundaries, which leads to the unsatisfactory segmentation of various target feature boundaries. In this paper, we propose a new semantic segmentation model for remote sensing images (called DGCN hereinafter), which combines deep semantic segmentation networks (DSSN) and GCNs. In the GCN module, a loss function for boundary information is employed to optimize the learning of spatial relationship features between the target features and their relationships. A hierarchical fusion method is utilized for feature fusion and classification to optimize the spatial relationship informa-tion in the original feature information. Extensive experiments on ISPRS 2D and DeepGlobe semantic segmentation datasets show that compared with the existing semantic segmentation models of remote sensing images, the DGCN significantly optimizes the segmentation effect of feature boundaries, effectively reduces the noise in the segmentation results and improves the segmentation accuracy, which demonstrates the advancements of our model.",Remote sensing image,semantic segmentation,GCN,spatial relationship,"Chen, Xuefei","Wang, Weiqiang","Zhang, XueShen",,feature fusion,,,,,,,,,,,,,,,,,,,,,,,,
Row_787,Yuan Wei,Xu Wenbo,Zhou Tian,Remote sensing image segmentation based on PSPNet with neighborhood color difference,CHINESE SPACE SCIENCE AND TECHNOLOGY,FEB 25 2022,0,"Traditional semantic segmentation of remote sensing image is to classify the pixels with similar values by using the spectral characteristics of images, but it is unable to distinguish the same kind of objects with different spectra. Aiming at this problem, a method was proposed in which the color difference information of neighborhood is integrated into the original image as input to PSPNet. Firstly, RGB was transformed into LAB. Then CIELAB formula was used to calculate the color difference value between each pixel and eight neighboring pixels, and the average value was taken as the neighborhood color difference value of the pixel. Experiment was done by using PSPNct on WHU building dataset and Massachusetts building dataset. The results show that the MIoU, ACC and F1-score with neighborhood color difference arc better than without. Therefore, the proposed method of merging neighborhood color difference is an effective way to improve the segmentation accuracy of PSPNet.",remote sensing image,semantic segmentation,deep learning,neighborhood color difference,,,,,convolutional neural network,,,,,,,,,,,,,,,,,,,,,,,,
Row_788,"Jaber, Mustafa Musa","Ali, Mohammed Hasan","Abd, Sura Khalil",A Machine Learning-Based Semantic Pattern Matching Model for Remote Sensing Data Registration,JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING,SEP 2023,1,"Remote sensing image registration can benefit from a machine learning method based on the likelihood of predicting semantic spatial position distributions. Semantic segmentation of images has been revolutionized due to the accessibility of high-resolution remote sensing images and the advancement of machine learning techniques. This system captures the semantic distribution location of the matching reference picture, which ML mapped using learning-based algorithms. The affine invariant is utilized to determine the semantic template's barycenter position and the pixel's center, which changes the semantic border alignment problem into a point-to-point matching issue for the machine learning-based semantic pattern matching (ML-SPM) model. The first step examines how various factors such as template radius, training label filling form, or loss function combination affect matching accuracy. In this second step, the matching of sub-images (MSI) images is compared using heatmaps created from the expected similarity between the images' cropped sub-images. Images having radiometric discrepancies are matched with excellent accuracy by the approach. SAR-optical image matching has never been easier, and now even large-scale sceneries can be registered using this approach, which is a significant advance over previous methods. Optical satellite imaging or multi-sensor stereogrammetry can be combined with both forms of data to enhance geolocation.",Remote sensing image,Machine learning,Semantic pattern matching,Matching of sub-images,"Jassim, Mustafa Mohammed","Alkhayyat, Ahmed","Alreda, Baraa A.","Alkhuwaylidee, Ahmed Rashid",Loss function,Synthetic aperture radar (SAR),,,,,,,"Alyousif, Shahad",,,,,,,,,,,,,,,,
Row_789,"Zhu, Peng","Zhang, Xiangrong","Han, Xiao",High-Resolution Remote Sensing Image Segmentation With Global-Guided Normalization and Local Affinity Distillation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"In recent years, high-resolution (HR) remote sensing images (RSIs) segmentation has received growing attention. The huge number of pixels poses a challenge to the semantic segmentation algorithm, which is limited by the storage of GPUs, so the current methods for processing HR RSIs are categorized into two main categories, i.e., global methods and local methods. The former downsamples the original image and loses a lot of feature details. The latter crops the original image and fails to obtain global contextual information. Both types of methods lead to limited segmentation accuracy. In this article, we propose an end-to-end framework, called global injection network (GINet), which explores two levels of feature distribution and feature relationship to achieve tradeoff between global context and local details. In concrete terms, we propose the global-guided normalization (GGN) module, which injects global context information into local branch and modulates local features using global features to enhance the global perception of local branch. In addition, to constrain the spatial consistency of two branches, inspired by the knowledge distillation technique, we propose local affinity distillation (LAD) loss, which distills the relations in local features into global features to keep the similarity of the relationships corresponding to patches in the two branches. The comprehensive experimental results on three large-scale land-cover classification datasets, DeepGlobe (2448 x 2448), Inria Aerial (5000 x 5000), and GID-15 (7200 x 6800), confirm the effectiveness and superiority of our method in HR semantic segmentation tasks.",Remote sensing,Semantic segmentation,Semantics,Image resolution,"Chen, Puhua","Tang, Xu","Cheng, Xina","Jiao, Licheng",Context modeling,Accuracy,Transformers,Correlation,Convolution,Training,Distillation,high-resolution (HR) remote sensing images (RSIs),,,,,,semantic segmentation,,,,,,,,,,,
Row_790,"Wu, Yingxin","Liu, Yinhe","Shi, Sunan",HIGH-RESOLUTION FINE-GRAINED WETLAND MAPPING BASED ON CLASS-BALANCED DEEP SEMANTIC SEGMENTATION NETWORKS,,2023,2,"Wetlands are among the most valuable environmental resources and are crucial to achieving sustainable development strategies. However, the number, distribution, and types of wetlands are poorly understood. Widely used medium-resolution wetland products do not provide enough surface feature for fine-grained wetland mapping. To fill this gap, a high-resolution wetland remote sensing mapping dataset covering the contiguous United States was constructed. On this dataset, the performance of deep semantic segmentation networks with various backbones and architecture for wetland mapping was evaluated. Several loss functions were implemented to address the issue of class imbalance of this dataset, resulting in an improvement in classification accuracy. High spatial resolution images offer an abundance of surface texture, shape, structure, and neighborhood relationship data that can be applied to the classification of large-scale wetlands. The combination of high-resolution imagery and deep semantic segmentation models enables the automatic classification of wetlands to be refined.",Wetland mapping,high-resolution remote sensing,deep learning,semantic segmentation,"Zhong, Yanfei",,,,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_791,"Zheng, Zhuo","Zhong, Yanfei","Wang, Junjue",FarSeg plus plus : Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,NOV 1 2023,17,"Geospatial object segmentation, a fundamental Earth vision task, always suffers from scale variation, the larger intra-class variance of background, and foreground-background imbalance in high spatial resolution (HSR) remote sensing imagery. Generic semantic segmentation methods mainly focus on the scale variation in natural scenarios. However, the other two problems are insufficiently considered in large area Earth observation scenarios. In this paper, we propose a foreground-aware relation network (FarSeg++) from the perspectives of relation-based, optimization-based, and objectness-based foreground modeling, alleviating the above two problems. From the perspective of the relations, the foreground-scene relation module improves the discrimination of the foreground features via the foreground-correlated contexts associated with the object-scene relation. From the perspective of optimization, foreground-aware optimization is proposed to focus on foreground examples and hard examples of the background during training to achieve a balanced optimization. Besides, from the perspective of objectness, a foreground-aware decoder is proposed to improve the objectness representation, alleviating the objectness prediction problem that is the main bottleneck revealed by an empirical upper bound analysis. We also introduce a new large-scale high-resolution urban vehicle segmentation dataset to verify the effectiveness of the proposed method and push the development of objectness prediction further forward. The experimental results suggest that FarSeg++ is superior to the state-of-the-art generic semantic segmentation methods and can achieve a better trade-off between speed and accuracy.",Remote sensing,Feature extraction,Object segmentation,Decoding,"Ma, Ailong","Zhang, Liangpei",,,Semantic segmentation,Optimization,Semantics,Foreground modeling,object segmentation,semantic segmentation,remote sensing,deep learning,,,,,,,,,,,,,,,,,
Row_792,"Wei, Yao","Ji, Shunping",,Scribble-Based Weakly Supervised Deep Learning for Road Surface Extraction From Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,79,"Road surface extraction from remote sensing images using deep learning methods has achieved good performance, while most of the existing methods are based on fully supervised learning, which requires a large amount of training data with laborious per-pixel annotation. In this article, we propose a scribble-based weakly supervised road surface extraction method named ScRoadExtractor, which learns from easily accessible scribbles such as centerlines instead of densely annotated road surface ground truths. To propagate semantic information from sparse scribbles to unlabeled pixels, we introduce a road label propagation algorithm, which considers both the buffer-based properties of road networks and the color and spatial information of super-pixels, to produce a proposal mask with categories road, nonroad, and unknown. The proposal mask, along with the auxiliary boundary prior information detected from images, is utilized to train a dual-branch encoderx2013;decoder network which we designed for precise road surface segmentation. We perform experiments on three diverse road data sets that are comprised of high-resolution remote sensing satellite and aerial images across the world. The results demonstrate that ScRoadExtractor exceeds the classic scribble-supervised segmentation method by 20x0025; for the intersection over union (IoU) indicator and outperforms the state-of-the-art scribble-based weakly supervised methods at least 4x0025;.",Roads,Proposals,Annotations,Training,,,,,Remote sensing,Image segmentation,Semantics,Remote sensing image,road surface extraction,semantic segmentation,scribble,weakly supervised learning,,,,,,,,,,,,,,,,,
Row_793,"Liu, Guohong","Liu, Cong","Wu, Xianyun",Optimization of Remote-Sensing Image-Segmentation Decoder Based on Multi-Dilation and Large-Kernel Convolution,REMOTE SENSING,AUG 2024,0,"Land-cover segmentation, a fundamental task within the domain of remote sensing, boasts a broad spectrum of application potential. We address the challenges in land-cover segmentation of remote-sensing imagery and complete the following work. Firstly, to tackle the issues of foreground-background imbalance and scale variation, a module based on multi-dilated rate convolution fusion was integrated into a decoder. This module extended the receptive field through multi-dilated convolution, enhancing the model's capability to capture global features. Secondly, to address the diversity of scenes and background interference, a hybrid attention module based on large-kernel convolution was employed to improve the performance of the decoder. This module, based on a combination of spatial and channel attention mechanisms, enhanced the extraction of contextual information through large-kernel convolution. A convolution kernel selection mechanism was also introduced to dynamically select the convolution kernel of the appropriate receptive field, suppress irrelevant background information, and improve segmentation accuracy. Ablation studies on the Vaihingen and Potsdam datasets demonstrate that our decoder significantly outperforms the baseline in terms of mean intersection over union and mean F1 score, achieving an increase of up to 1.73% and 1.17%, respectively, compared with the baseline. In quantitative comparisons, the accuracy of our improved decoder also surpasses other algorithms in the majority of categories. The results of this paper indicate that our improved decoder achieves significant performance improvement compared with the old decoder in remote-sensing image-segmentation tasks, which verifies its application potential in the field of land-cover segmentation.",remote-sensing images,land-cover segmentation,dilated convolution,attention mechanism,"Li, Yunsong","Zhang, Xiao","Xu, Junjie",,large-kernel convolution,,,,,,,,,,,,,,,,,,,,,,,,
Row_794,"Lin, Zhiyuan","Zhu, Feng","Kong, Yanzi",SRSG and S2SG: A Model and a Dataset for Scene Graph Generation of Remote Sensing Images From Segmentation Results,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,2,"Remote sensing image analysis has drawn more attentions in the field of computer vision. At present, the methods commonly used in remote sensing image analysis are mainly at a low level semantically. The scene graph is an abstraction of objects and their relationships, which is a high-level image understanding task. To fully comprehend the meanings of remote sensing images, in this article, we propose a novel segmentation-based model to generate remote sensing image scene graphs (SRSG). In the SRSG model, a more complete and accurate scene graph is generated with the segmentation results as inputs, while the shapes of objects are reasonably coded. The morphological features of object pairs are embedded together by different branches of the SRSG model and then mapped to semantic space to predicate their relationships. Furthermore, a new dataset for scene graph generation of remote sensing images, namely, segmentation results to scene graphs (S2SG), is constructed based on pixel-level segmentation results. Experimental results demonstrate that the performance of the SRSG model is far superior to the previous methods in the task of generating remote sensing image scene graphs. The proposed SRSG model opens up new possibilities for remote sensing image analysis at a high level. Moreover, the S2SG dataset further allows for the evaluation of different approaches and is provided for the benefit of the research community.",Remote sensing,Image segmentation,Semantics,Visualization,"Wang, Qun","Wang, Jianyu",,,Task analysis,Sensors,Vegetation mapping,Dataset,relationship prediction,remote sensing image,scene graph generation,segmentation,,,,,,,,,,,,,,,,,
Row_795,"Ji, Xun","Tang, Longbin","Lu, Tongwei",DBENet: Dual-Branch Ensemble Network for Sea-Land Segmentation of Remote-Sensing Images,IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT,2023,12,"Sea-land segmentation of optical remote-sensing images holds great importance for military and civilian applications, such as coastal monitoring, target detection, and resource management. Although convolutional neural networks (CNNs) have achieved significant improvements in semantic segmentation, the challenges of efficient feature extraction, representation, fusion, and information transmission remain unsolved, which especially impacts the segmentation effectiveness of existing CNN-based models in extracting irregular and refined sea-land boundaries. In this article, a novel dual-branch ensemble network (DBENet) is proposed for pixel-level sea-land segmentation. The salient properties of the DBENet are: 1) a novel dual-branch network architecture is developed to achieve sufficient feature extraction and representation and 2) an efficient ensemble attention learning strategy suitable for the DBENet is designed to strengthen the correlation between dual branches to further facilitate feature fusion and information transmission. The comparative study with state-of-the-art methods reveals the superior performance of our approach, and the ablation study demonstrates the effectiveness of each component in the proposed network. The source code is available at https://github.com/RobertTang0/DBENet.",Convolutional neural networks (CNNs),deep learning,remote-sensing image,sea-land segmentation,"Cai, Chengtao",,,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_796,"Liu, Qi","Li, Yang","Bilal, Muhammad",CFNet: An Eigenvalue Preserved Approach to Multiscale Building Segmentation in High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,2,"In recent years, AI and deep learning (DL) methods have been widely used for object classification, recognition, and segmentation of high-resolution multispectral remote sensing images. These DL-based solutions perform better compared with the traditional spectral algorithms but still suffer from insufficient optimization of global and local features of object context. In addition, failure of code-data isolation and/or disclosure of detailed eigenvalues cause serious privacy and even secret leakage due to the sensitivity of high-resolution remote sensing data and their processing mechanisms. In this article, class feature modules have been presented in the decoder part of an attention-based CNN network to distinguish between building and nonbuilding (background) area. In this way, context features of a focused object can be extracted with more details being processed while the resolution of images is maintained. The reconstructed local and global feature values and dependencies in the proposed model are maintained by reconfiguring multiple effective attention modules with contextual dependencies to achieve better results for the eigenvalue. According to quantitative results and their visualization, the proposed model has depicted better performance over others' work using two large-scale building remote sensing datasets. The F1-score of this model reached 87.91 and 89.58 on WHU Buildings Dataset and Massachusetts Buildings Dataset, respectively, which exceeded the other semantic segmentation models.",Building extraction,class feature (CF),semantic segmentation,,"Liu, Xiaodong","Zhang, Yonghong","Wang, Huihui","Xu, Xiaolong",,,,,,,,,"Lu, Hui",,,,,,,,,,,,,,,,
Row_797,"Neupane, Bipul","Horanont, Teerayut","Aryal, Jagannath",Deep Learning-Based Semantic Segmentation of Urban Features in Satellite Images: A Review and Meta-Analysis,REMOTE SENSING,FEB 2021,119,"Availability of very high-resolution remote sensing images and advancement of deep learning methods have shifted the paradigm of image classification from pixel-based and object-based methods to deep learning-based semantic segmentation. This shift demands a structured analysis and revision of the current status on the research domain of deep learning-based semantic segmentation. The focus of this paper is on urban remote sensing images. We review and perform a meta-analysis to juxtapose recent papers in terms of research problems, data source, data preparation methods including pre-processing and augmentation techniques, training details on architectures, backbones, frameworks, optimizers, loss functions and other hyper-parameters and performance comparison. Our detailed review and meta-analysis show that deep learning not only outperforms traditional methods in terms of accuracy, but also addresses several challenges previously faced. Further, we provide future directions of research in this domain.",deep learning,remote sensing,review,semantic segmentation,,,,,urban image classification,,,,,,,,,,,,,,,,,,,,,,,,
Row_798,"Pereira, Matheus Barros","dos Santos, Jefersson Alex",,ChessMix: Spatial Context Data Augmentation for Remote Sensing Semantic Segmentation,,2021,2,"Labeling semantic segmentation datasets is a costly and laborious process if compared with tasks like image classification and object detection. This is especially true for remote sensing applications that not only work with extremely high spatial resolution data but also commonly require the knowledge of experts of the area to perform the manual labeling. Data augmentation techniques help to improve deep learning models under the circumstance of few and imbalanced labeled samples. In this work, we propose a novel data augmentation method focused on exploring the spatial context of remote sensing semantic segmentation. This method, ChessMix, creates new synthetic images from the existing training set by mixing transformed mini-patches across the dataset in a chessboard-like grid. ChessMix prioritizes patches with more examples of the rarest classes to alleviate the imbalance problems. The results in three diverse well-known remote sensing datasets show that this is a promising approach that helps to improve the networks' performance, working especially well in datasets with few available data. The results also show that ChessMix is capable of improving the segmentation of objects with few labeled pixels when compared to the most common data augmentation methods widely used.",,,,,,,,,,,,,,,,,,"2021 34TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI 2021)",,,,,,,,,,,,,,,
Row_799,"Luo, Zheng","Pan, Jianping","Hu, Yong",RS-Dseg: semantic segmentation of high-resolution remote sensing images based on a diffusion model component with unsupervised pretraining,SCIENTIFIC REPORTS,AUG 10 2024,2,"Semantic segmentation plays a crucial role in interpreting remote sensing images, especially in high-resolution scenarios where finer object details, complex spatial information and texture structures exist. To address the challenge of better extracting semantic information and ad-dressing class imbalance in multiclass segmentation, we propose utilizing diffusion models for remote sensing image semantic segmentation, along with a lightweight classification module based on a spatial-channel attention mechanism. Our approach incorporates unsupervised pretrained components with a classification module to accelerate model convergence. The diffusion model component, built on the UNet architecture, effectively captures multiscale features with rich contextual and edge information from images. The lightweight classification module, which leverages spatial-channel attention, focuses more efficiently on spatial-channel regions with significant feature information. We evaluated our approach using three publicly available datasets: Postdam, GID, and Five Billion Pixels. In the test of three datasets, our method achieved the best results. On the GID dataset, the overall accuracy was 96.99%, the mean IoU was 92.17%, and the mean F1 score was 95.83%. In the training phase, our model achieved good performance after only 30 training cycles. Compared with other models, our method reduces the number of parameters, improves the training speed, and has obvious performance advantages.",Diffusion models,Multiscale,Pretraining,Attention mechanism,"Deng, Lin","Li, Yimeng","Qi, Chen","Wang, Xunxun",Semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_800,"Cai, Jiali","Liu, Chunjuan","Yan, Haowen",Real-Time Semantic Segmentation of Remote Sensing Images Based on Bilateral Attention Refined Network,IEEE ACCESS,2021,8,"The trade-off between feature representation capability and spatial positioning accuracy is crucial to dense classification or semantic segmentation of remote sensing images. In order to better balance the low-level spatial details in the shallow network and the high-level abstract semantics in the deep network, the bilateral attention refinement lightweight network BARNet is introduced. In this way, we can use the fine-grained features in the shallow layer to further supplement and capture the deeper information of the high-level semantic features. The network employs an asymmetric encoder decoder architecture for the task of real-time semantic segmentation. Encoder part proposes a lightweight network residual unit with the split, concatenate and split bottleneck structure to achieve more light weighted, effificient and powerful feature extraction. In the decoding section, we propose an adaptive method to enhance feature representation in local attention enhancement module. In addition, the global context embedding module is introduced to divide the high-level features into two branches. One branch gets the weight vector to guide the low-level learning, and the other branch will get a semantic vector, which is used to calculate the multi-label category loss and further introduce into the overall loss function to regulate the training process better. The effectiveness and efficiency of the network are verified on ISPRS Potsdam data set and CCF data set, respectively. The results show that the models using these strategies outperform the baseline network on MIoU, PA and F1, which increase by 18.86%, 16.21% and 15.64% on the Potsdam dataset; 10.51%, 6.53% and 8.19% on the CCF dataset.",Semantics,Feature extraction,Convolution,Licenses,"Wu, Xiaosuo","Lu, Wanzhen","Wang, Xiaoyu","Sang, Changlin",Convolutional codes,Task analysis,Spatial resolution,Remote sensing image,real-time semantic segmentation,local attention enhancement module,global context embedding module,multi-label category loss,,,,,,,,,,,,,,,,,
Row_801,"Li, Jiaojiao","Zi, Shunyao","Song, Rui",A Stepwise Domain Adaptive Segmentation Network With Covariate Shift Alleviation for Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,33,"Semantic segmentation for remote sensing images (RSI) is critical for the Earth monitoring system. However, the covariate shift between RSI datasets under different capture conditions cannot be alleviated by directly using the unsupervised domain adaptation (UDA) method, which negatively affects the segmentation accuracy in RSI. We propose a stepwise domain adaptive segmentation network with covariate shift alleviation (Cov-DA) for RSI parsing to solve this issue. Specifically, to alleviate domain shift generated by different sensors, both the source and target domains are projected into a colorspace with normalized distribution through an elaborate colorspace mapping unified module (CMUM). The color distributions of these two domains tend to be more uniform. Furthermore, in the target domain, the multistatistics joint evaluation module (MJEM) is proposed to capture different statistical characteristics of subscenarios for selecting plain scenarios regarded as high-confidence segmentation results to assist the further improvement of segmentation performance. In addition, a pyramid perceptual attention module (PPAM) containing omnidirectional features without computational burdens is added to our network for effectively enhancing the multiscale feature capture ability. In the cross-city DA experiments based on the International Society for Photogrammetry and Remote Sensing (ISPRS) and aerial benchmarks, the superiority of our algorithm is significantly demonstrated. Furthermore, we release a large-scale Martian terrain dataset noted as ""Mars-Seg"" containing 5 K images with pixel-level accurate annotations regarding issues, such as the lack of semantic segmentation datasets for unknown scenes.",Image segmentation,Semantics,Feature extraction,Training,"Li, Yunsong","Hu, Yinlin","Du, Qian",,Complexity theory,Adaptive systems,Generative adversarial networks,Covariate shift alleviation,semantic segmentation,stepwise,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,
Row_802,"Sun, Yangjie","Fu, Zhongliang","Sun, Chuanxia",Deep Multimodal Fusion Network for Semantic Segmentation Using Remote Sensing Image and LiDAR Data,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,47,"Extracting semantic information from very-high-resolution (VHR) aerial images is a prominent topic in the Earth observation research. An increasing number of different sensor platforms are appearing in remote sensing, each of which can provide corresponding multimodal supplemental or enhanced information, such as optical images, light detection and ranging (LiDAR) point clouds, infrared images, or inertial measurement unit (IMU) data. However, these current deep networks for LiDAR and VHR images have not fully utilized the complete potential of multimodal data. The stacked multimodal fusion network (MFNet) ignores the structural differences between the modalities and the manual statistical characteristics within the modalities. For multimodal remote sensing data and its corresponding carefully designed handcrafted features, we designed a novel deep MFNet that can use multimodal VHR aerial images and LiDAR data and the corresponding intramodal features, such as LiDAR-derived features [slope and normalized digital surface model (NDSM)] and imagery-derived features [infrared-red-green (IRRG), normalized difference vegetation index (NDVI), and difference of Gaussian (DoG)]. Technically, we introduce the attention mechanism and multimodal learning to adaptively fuse intermodal and intramodal features. Specifically, we designed a multimodal fusion mechanism, pyramid dilation blocks, and a multilevel feature fusion module. Through these modules, our network realized the adaptive fusion of multimodal features, improved the receptive field, and enhanced the global-to-local contextual fusion effect. Moreover, we used a multiscale supervision training scheme to optimize the network. Extensive experimental results and ablation studies on the ISPRS semantic dataset and IEEE GRSS DFC Zeebrugge dataset show the effectiveness of our proposed MFNet.",Semantics,Image segmentation,Laser radar,Sensors,"Hu, Yinglei","Zhang, Shengyuan",,,Task analysis,Sun,Feature extraction,Aerial images,attention mechanism,convolutional neural network (CNN),multimodal fusion,semantic labeling,,,,,,,,,,,,,,,,,
Row_803,"Chen, Guangchen","Shi, Benjie","Zhang, Yinhui",CGSNet: Cross-consistency guiding semi-supervised semantic segmentation network for remote sensing of plateau lake,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,OCT 2024,0,"Analyzing the geographical information for the Plateau Lake region with remote sensing images (RSI) is an emerging technology to monitor the changes of the ecological environment. To alleviate the requirement of abundant labels for supervised RSI segmentation, the Cross-consistency Guiding Semi-supervised Learning (SSL) Semantic Segmentation Network is proposed, and it can perform high-quality multi-category semantic segmentation for complex remote sensing scenes with limited quantity of labeled images. Firstly, based on the SSL semantic segmentation framework, through the cross-consistency method training a teacher model with less annotated images and plentiful unannotated images, then generating higher-quality pseudo labels to guide the learning process of the student model. Secondly, dense conditional random field and mask hole repair are used to patch and fill the flaw areas of pseudo-labels based on the pixel features of position, color, and texture, further improving the granularity and reliability of the student model training dataset. Additionally, to improve the accuracy of the model, we designed a strong data augmentation (SDA) method based on a stochastic cascaded strategy, which connects multiple augmentation techniques in random order and probability cascade to generate new training samples. It mimics a variety of image transformations and noise conditions that occur in the real world to enhance the robustness in complex scenarios. To validate the effectiveness of CGSNet in complex remote sensing scenes, extended experiments are conducted on the self-built plateau lake RSI dataset and two public multi-category RSI datasets. The experiment results demonstrate that, compared with other state-of-the-art SSL methods, the proposed CGSNet achieves the highest 77.47% mIoU and 87.06% F1 scores with a limited quantity of annotated data.",Environment change monitoring,Plateau lake remote sensing,Cross-consistency guiding,Semi-supervised learning,"He, Zifen","Zhang, Pengcheng",,,Dense conditional random field,,,,,,,,,,,,,,,,,,,,,,,,
Row_804,"Qiu, Luyi","Yu, Dayu","Zhang, Xiaofeng",Efficient Remote-Sensing Segmentation With Generative Adversarial Transformer,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,0,"Most deep-learning methods that achieve high segmentation accuracy require deep network architectures that are too heavy and complex to run on embedded devices with limited storage and memory space. To address this issue, this letter proposes an efficient generative adversarial transformer (GATrans) for achieving high-precision semantic segmentation while maintaining an extremely efficient size. The framework utilizes a global transformer network (GTNet) as the generator, efficiently extracting multilevel features through residual connections. GTNet employs global transformer blocks with progressively linear computational complexity to reassign global features based on a learnable similarity function. To focus on object- and pixel-level information, the GATrans optimizes the objective function by combining structural similarity losses. We validate the effectiveness of our approach through extensive experiments on the Vaihingen dataset, achieving an average $F1$ score of 90.17% and an overall accuracy (OA) of 91.92%. Codes are available at https://github.com/qiuluyi/GATrans.",Transformers,Generators,Merging,Mathematical models,"Zhang, Chenxiao",,,,Training,Remote sensing,Feature extraction,Generative-adversarial strategy,global transformer network (GTNet),remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_805,"Hong, Danfeng","Zhang, Bing","Li, Hao",Cross-city matters: A multimodal remote sensing benchmark dataset for cross-city semantic segmentation using high-resolution domain adaptation networks,REMOTE SENSING OF ENVIRONMENT,DEC 15 2023,255,"Artificial intelligence (AI) approaches nowadays have gained remarkable success in single-modality-dominated remote sensing (RS) applications, especially with an emphasis on individual urban environments (e.g., single cities or regions). Yet these AI models tend to meet the performance bottleneck in the case studies across cities or regions, due to the lack of diverse RS information and cutting-edge solutions with high generalization ability. To this end, we build a new set of multimodal remote sensing benchmark datasets (including hyperspectral, mul-tispectral, SAR) for the study purpose of the cross-city semantic segmentation task (called C2Seg dataset), which consists of two cross-city scenes, i.e., Berlin-Augsburg (in Germany) and Beijing-Wuhan (in China). Beyond the single city, we propose a high-resolution domain adaptation network, HighDAN for short, to promote the AI model's generalization ability from the multi-city environments. HighDAN is capable of retaining the spatially topological structure of the studied urban scene well in a parallel high-to-low resolution fusion fashion but also closing the gap derived from enormous differences of RS image representations between different cities by means of adversarial learning. In addition, the Dice loss is considered in HighDAN to alleviate the class imbalance issue caused by factors across cities. Extensive experiments conducted on the C2Seg dataset show the superiority of our HighDAN in terms of segmentation performance and generalization ability, compared to state-of-the-art com-petitors. The C2Seg dataset and the semantic segmentation toolbox (involving the proposed HighDAN) will be available publicly at https://github.com/danfenghong/RSE_Cross-city.",Cross-city,Deep learning,Dice loss,Domain adaptation,"Li, Yuxuan","Yao, Jing","Li, Chenyu","Werner, Martin",High-resolution network,Land cover,Multimodal benchmark datasets,Remote sensing,Segmentation,,,,"Chanussot, Jocelyn",,"Zipf, Alexander","Zhu, Xiao Xiang",,,,,,,,,,,,,
Row_806,"Liu, Wenshu","Cui, Nan","Guo, Luo",DESformer: A Dual-Branch Encoding Strategy for Semantic Segmentation of Very-High-Resolution Remote Sensing Images Based on Feature Interaction and Multiscale Context Fusion,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Global contextual information is crucial for the semantic segmentation of remote sensing (RS) images. However, the majority of current approaches depend on convolutional neural networks (CNNs). Due to the local receptive fields inherent in convolutional operations, these networks typically capture image features within limited areas and struggle to comprehend broader contextual information in the images. In this study, a dual-branch encoding approach, DESformer, is proposed, integrating transformers with CNN, to effectively capture global multiscale context information and enhance edge feature extraction. In addition, DESformer incorporates a feature interaction module (FIM) to combine local features with global representations extracted by transformers and CNN, respectively, across different resolutions. This approach enhances the capability to capture local features in RS images and improves the understanding of extensive spatial relationships. Subsequently, we employ a novel top-down approach for global supervision of the traditional feature pyramid multilevel visual integration (MVI) module, by harnessing the clear visual center information obtained from the deepest internal features. To successfully concentrate on important information and preserve sensitivity to features at various scales, the preceding shallow features are muted. In addition, FIAB-Loss, a loss function is introduced, combining a focal loss with IOU and active boundary loss (ABL). This composite loss function strengthens the model's focus on challenging-to-distinguish categories. Extensive experiments conducted on three datasets, including the semantic segmentation of lakes in the Tibetan Plateau and the ISPRS's Vaihingen benchmark, validate the efficacy of the proposed method. The experimental results indicate that the network exhibits exceptional performance in processing VHR images and accurately extracting edge features.",Feature extraction,Convolutional neural networks,Transformers,Remote sensing,"Du, Shihong","Wang, Weiyin",,,Semantic segmentation,Visualization,Semantics,Deep learning,dual-branch coding structure,remote sensing (RS),semantic segmentation,,,,,,,,,,,,,,,,,,
Row_807,"Wu, Xinjia","Zhang, Jing","Li, Wensheng",Spatial-specific Transformer with involution for semantic segmentation of high-resolution remote sensing images,INTERNATIONAL JOURNAL OF REMOTE SENSING,FEB 16 2023,2,"High-resolution remote sensing images (HR-RSIs) have a strong dependency between geospatial objects and background. Considering the complex spatial structure and multiscale objects in HR-RSIs, how to fully mine spatial information directly determines the quality of semantic segmentation. In this paper, we focus on the Spatial-specific Transformer with involution for semantic segmentation of HR-RSIs. First, we integrate the spatial-specific involution branch with self-attention branch to form a Spatial-specific Transformer backbone to produce multilevel features with global and spatial information without additional parameters. Then, we introduce multiscale feature representation with large window attention into Swin Transformer to capture multiscale contextual information. Finally, we add a geospatial feature supplement branch in the semantic segmentation decoder to mitigate the loss of semantic information caused by down-sampling multiscale features of geospatial objects. Experimental results demonstrate that our method can achieve a competitive semantic segmentation performance of 87.61% and 80.08% mIoU on Potsdam and Vaihingen datasets, respectively.",,,,,"Li, Jiafeng","Zhuo, Li","Zhang, Jie",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_808,"Zhang, Cheng","Jiang, Wanshou","Zhao, Qing",Semantic Segmentation of Aerial Imagery via Split-Attention Networks with Disentangled Nonlocal and Edge Supervision,REMOTE SENSING,MAR 2021,15,"In this work, we propose a new deep convolution neural network (DCNN) architecture for semantic segmentation of aerial imagery. Taking advantage of recent research, we use split-attention networks (ResNeSt) as the backbone for high-quality feature expression. Additionally, a disentangled nonlocal (DNL) block is integrated into our pipeline to express the inter-pixel long-distance dependence and highlight the edge pixels simultaneously. Moreover, the depth-wise separable convolution and atrous spatial pyramid pooling (ASPP) modules are combined to extract and fuse multiscale contextual features. Finally, an auxiliary edge detection task is designed to provide edge constraints for semantic segmentation. Evaluation of algorithms is conducted on two benchmarks provided by the International Society for Photogrammetry and Remote Sensing (ISPRS). Extensive experiments demonstrate the effectiveness of each module of our architecture. Precision evaluation based on the Potsdam benchmark shows that the proposed DCNN achieves competitive performance over the state-of-the-art methods.",semantic segmentation,ResNeSt,edge constrains,disentangled non-local,,,,,depth-wise separable ASPP,remote sensing,aerial image,,,,,,,,,,,,,,,,,,,,,,
Row_809,"Wang, Falin","Ji, Jian","Wang, Yuan",DSViT: Dynamically Scalable Vision Transformer for Remote Sensing Image Segmentation and Classification,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,5,"The relationship between the foreground target and the background of remote sensing image is very complex. The vision task of remote sensing image faces the problems of complex targets and unbalanced categories. These problems make the modeling method have further improvement space. Therefore, this article proposes a dynamically scalable attention model that combines convolutional features and Transformer features. It can dynamically select the model depth according to the size of the input image, which alleviates the problem of insufficient global information extraction of the single convolution model and the computational overhead limitation of the pure Transformer model. We validated the model on two public remote sensing image classifications and two remote sensing image segmentation datasets. The accuracy and mean pixel accuracy (mPA) of the method in this article reached 96.16% and 93.44%, respectively, on the university of california (UC) Merced classification dataset. Compared with some recent work, the method has a net improvement of 5.0% and 4.82% over the pyramid vision transformer (PVT) model. On the Potsdam segmentation dataset, the accuracy and F1 of the transformer and CNN hybrid neural network (TCHNN) model are 91.5% and 92.86%, respectively. The performance of the method has improved 0.64% and 1.0%, and the other two datasets have also achieved the best results.",Transformers,Remote sensing,Feature extraction,Computational modeling,,,,,Convolutional neural networks,Convolution,Task analysis,CNN,classification,remote sensing image,semantic segmentation,transformer,,,,,,,,,,,,,,,,,
Row_810,"Han, Zheng","Fu, Bangjie","Fang, Zhenxiong",Dynahead-YOLO-Otsu: an efficient DCNN-based landslide semantic segmentation method using remote sensing images,GEOMATICS NATURAL HAZARDS & RISK,DEC 31 2024,2,"Recent advancements in deep convolutional neural networks (DCNNs) have significantly improved landslides identification using remote sensing images. Pixel-wise semantic segmentation (PSS) and object-oriented detection (OOD) are two dominant approaches, wherein PSS are better as providing detailed delineation of landslide shapes. However, PSS are limited by the difficulty in labelling training data and low segmentation speed compared to OOD. In this paper, we propose an efficient DCNN-based landslide semantic segmentation method, the so-called Dynahead-YOLO-Otsu, to perform a PSS based on the OOD results. This is achieved by locating potential landslide regions in advance using the ODD-based Dynahead-YOLO model, which enhances the capacity for detecting landslides with variable proportions and complex background in the images. The preliminary results are then processed using the Otsu binarization algorithm to cluster pixels belonging to landslides from the images of potential regions for semantic segmentation. To validate the performance, we tested the proposed method using an open-source dataset containing 950 landslide images. We compared the results with three up-to-date DCNN-and PSS- based approaches, namely DeepLab v3+, PSPnet, and Unet. Results demonstrate that the proposed method achieves comparable Recall (71.80%) and F1 scores (75.80%), with an average improvement of 22% and 16% in Precision and IoU, respectively.",Landslide,semantic segmentation,deep convolutional neural networks,Dynahead-YOLO model,"Li, Yange","Li, Jiaying","Jiang, Nan","Chen, Guangqi",Otsu binarization method,,,,,,,,,,,,,,,,,,,,,,,,
Row_811,"Guo, Ningbo","Jiang, Mingyong","Hu, Xiaoyu",NPSFF-Net: Enhanced Building Segmentation in Remote Sensing Images via Novel Pseudo-Siamese Feature Fusion,REMOTE SENSING,SEP 2024,0,"Building segmentation has extensive research value and application prospects in high-resolution remote sensing image (HRSI) processing. However, complex architectural contexts, varied building morphologies, and non-building occlusions make building segmentation challenging. Compared with traditional methods, deep learning-based methods present certain advantages in terms of accuracy and intelligence. At present, the most popular option is to first apply a single neural network to encode an HRSI, then perform a decoding process through up-sampling or using a transposed convolution operation, and then finally obtain the segmented building image with the help of a loss function. Although effective, this approach not only tends to lead to a loss of detail information, but also fails to fully utilize the contextual features. As an alternative, we propose a novel network called NPSFF-Net. First, using an improved pseudo-Siamese network composed of ResNet-34 and ResNet-50, two sets of deep semantic features of buildings are extracted with the support of transfer learning, and four encoded features at different scales are obtained after fusion. Then, information from the deepest encoded feature is enriched using a feature enhancement module, and the resolutions are recovered via the operations of skip connections and transposed convolutions. Finally, the discriminative features of buildings are obtained using the designed feature fusion algorithm, and the optimal segmentation model is obtained by fitting a cross-entropy loss function. Our method obtained intersection-over-union values of 89.45% for the Aerial Imagery Dataset, 71.88% for the Massachusetts Buildings Dataset, and 68.72% for the Satellite Dataset I.",remote sensing images,building segmentation,skip connection,,"Su, Zhijuan","Zhang, Weibin","Li, Ruibo","Luo, Jiancheng",,,,,,,,,,,,,,,,,,,,,,,,,
Row_812,"Cui, Mengtian","Li, Kai","Chen, Jianying",CM-Unet: A Novel Remote Sensing Image Segmentation Method Based on Improved U-Net,IEEE ACCESS,2023,2,"Semantic segmentation is an active research area for high-resolution (HR) remote sensing image processing. Most existing algorithms are better at segmenting different features. However, for complex scenes, many algorithms have insufficient segmentation accuracy. In this study, we propose a new method CM-Unet based on the U-Net framework to address the problems of holes, omissions, and fuzzy edge segmentation. First, we add the channel attention mechanism in the encoding network and the residual module to transmit information. Second, a multi-feature fusion mechanism is proposed in the decoding network, and an improved sub-pixel convolution method replaces the traditional upsampling operation. We conducted simulation experiments on the Potsdam, Vaihingen and GID datasets. The experimental results show that the proposed CM-Unet required segmentation time is approximately 62 ms/piece, the MIoU is 90.4% and the floating point operations (FLOPs) is 20.95 MFLOPs. Compared with U-Net, CM-Unet only increased the total number of parameters and floating point operations slightly, but achieved the best segmentation effect compared with the other models. CM-Unet can segment remote sensing images efficiently and accurately owing to its lower time consumption and space requirements; the precision of the segmentation results is better than other methods.",Remote sensing image,channel attention,multi-feature fusion,improved sub-pixel convolution,"Yu, Wei",,,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_813,"Zhang, Jie","Shao, Mingwen","Wan, Yecong",Boundary-Aware Spatial and Frequency Dual-Domain Transformer for Remote Sensing Urban Images Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"Semantic segmentation of remote sensing (RS) images refers to labeling each pixel with a class to identify objects or land cover types. Existing mainstream spatial-domain semantic segmentation methods are mainly categorized into convolutional neural network (CNN)-based and vision transformer (ViT)-based approaches. The former excels at capturing local features, while the latter is adept at extracting global features. Several recent approaches consider combining CNN and ViT to efficiently capture local and global features. However, these approaches still struggle to capture complete features of the RS images, resulting in inaccurate segmentation. To address this issue, we introduce the fast Fourier transform (FFT), which transforms images into the frequency domain for feature extraction, acquiring the image-size receptive field that can complement spatial-domain methods. Based on this, we propose a boundary-aware spatial and frequency dual-domain transformer, termed dual-domain transformer. Specifically, our dual-domain transformer incorporates a dual-domain mixer (DualM), where the spatial-domain branch combines depthwise convolution and the attention mechanism to extract local and global features effectively, while the frequency-domain branch uses FFT to extract image-size features. The two branches complement each other, enabling a more comprehensive feature extraction of RS images. Meanwhile, a boundary-guided training strategy utilizing a boundary-aware module (BAM) is devised to constrain the model extract and predict boundary detail texture, which is an auxiliary task. In addition, the decoder incorporates a scale-feature fusion module (SFM) for adaptive information fusion between the encoder and decoder. Comprehensive experiments on the Zeebrugge and ISPRS datasets, including Vaihingen and Potsdam, showcase that the dual-domain transformer significantly outperforms state-of-the-art (SOTA) methods.",Feature extraction,Frequency-domain analysis,Transformers,Semantic segmentation,"Meng, Lingzhuang","Cao, Xiangyong","Wang, Shuigen",,Semantics,Fast Fourier transforms,Training,Fast Fourier transform (FFT),frequency domain representation,remote sensing (RS) image,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_814,Yuan Wei,Xu Wenbo,Zhou Tian,A loss function of road segmentation in remote sensing image by deep learning,CHINESE SPACE SCIENCE AND TECHNOLOGY,AUG 25 2021,5,"Traditional road segmentation based on spectral features or morphological algorithms has some disadvantages such as low precision and difficulty in determining the threshold value, and the existing methods in deep learning do not consider the characteristics of roads, only using general methods to segment roads. A deep learning loss function named morphological loss function with road unique trait was proposed. Firstly, the connectivity algorithm was used to divide the prediction results into several separated connected regions, and the ratios of the region area to the circumscribed circle area was calculated respectively. Then, the average value of regions was taken as the morphological loss function of this batch of training data. Finally, the morphological loss function was summed with the cross entropy loss function according to a certain proportion to obtain the final loss function. Through the comparative experiment on open remote sensing dataset, MIoU, ACC and F1-Score were all improved by the addition of morphological loss function. According to the prediction image, the predicted road was more continuous when morphological loss function was added. The morphological loss function proposed is an effective method to improve the accuracy of road segmentation in remote sensing.",morphology,remote sensing image,road segmentation,semantic segmentation,,,,,deep learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_815,"Li, Wenyuan","Chen, Keyan","Chen, Hao",Geographical Knowledge-Driven Representation Learning for Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,53,"The proliferation of remote sensing satellites has resulted in a massive amount of remote sensing images. However, due to human and material resource constraints, the vast majority of remote sensing images remain unlabeled. As a result, it cannot be applied to currently available deep learning methods. To fully utilize the remaining unlabeled images, we propose a Geographical Knowledge-driven Representation (GeoKR) learning method for remote sensing images, improving network performance and reduce the demand for annotated data. The global land cover products and geographical location associated with each remote sensing image are regarded as geographical knowledge to provide supervision for representation learning and network pretraining. An efficient pretraining framework is proposed to eliminate the supervision noises caused by imaging times and resolutions difference between remote sensing images and geographical knowledge. A large-scale pretraining dataset Levir-KR is constructed to support network pretraining. It contains 1,431,950 remote sensing images from Gaofen series satellites with various resolutions. Experimental results demonstrate that our proposed method outperforms ImageNet pretraining and self-supervised representation learning methods and significantly reduces the burden of data annotation on downstream tasks, such as scene classification, semantic segmentation, object detection, and cloud/snow detection. It demonstrates that our proposed method can be used as a novel paradigm for pretraining neural networks. Codes will be available on https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.",Remote sensing,Task analysis,Satellites,Sensors,"Shi, Zhenwei",,,,Semantics,Annotations,Training,Cloud,snow detection,object detection,remote sensing images,representation learning,,,,,,scene classification,semantic segmentation,,,,,,,,,,
Row_816,"He, Bingnan","Wu, Dongyang","Wang, Li",FA-HRNet: A New Fusion Attention Approach for Vegetation Semantic Segmentation and Analysis,REMOTE SENSING,NOV 2024,0,"Semantic segmentation of vegetation in aerial remote sensing images is a critical aspect of vegetation mapping. Accurate vegetation segmentation effectively informs real-world production and construction activities. However, the presence of species heterogeneity, seasonal variations, and feature disparities within remote sensing images poses significant challenges for vision tasks. Traditional machine learning-based methods often struggle to capture deep-level features for the segmentation. This work proposes a novel deep learning network named FA-HRNet that leverages the fusion of attention mechanism and a multi-branch network structure for vegetation detection and segmentation. Quantitative analysis from multiple datasets reveals that our method outperforms existing approaches, with improvements in MIoU and PA by 2.17% and 4.85%, respectively, compared with the baseline network. Our approach exhibits significant advantages over the other methods regarding cross-region and cross-scale capabilities, providing a reliable vegetation coverage ratio for ecological analysis.",computer vision,aerial remote sensing images,vision models,semantic segmentation,"Xu, Sheng",,,,vegetation coverage,panoptic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_817,"Zheng, Chen","Pan, Xinxin","Chen, Xiaohui",An Object-Based Markov Random Field Model with Anisotropic Penalty for Semantic Segmentation of High Spatial Resolution Remote Sensing Imagery,REMOTE SENSING,DEC 1 2019,3,"The Markov random field model (MRF) has attracted a lot of attention in the field of remote sensing semantic segmentation. But, most MRF-based methods fail to capture the various interactions between different land classes by using the isotropic potential function. In order to solve such a problem, this paper proposed a new generalized probability inference with an anisotropic penalty for the object-based MRF model (OMRF-AP) that can distinguish the differences in the interactions between any two land classes. Specifically, an anisotropic penalty matrix was first developed to describe the relationships between different classes. Then, an expected value of the penalty information (EVPI) was developed in this inference criterion to integrate the anisotropic class-interaction information and the posteriori distribution information of the OMRF model. Finally, by iteratively updating the EVPI terms of different classes, segmentation results could be achieved when the iteration converged. Experiments of texture images and different remote sensing images demonstrated that our method could show a better performance than other state-of-the-art MRF-based methods, and a post-processing scheme of the OMRF-AP model was also discussed in the experiments.",semantic segmentation,object-based Markov random field,anisotropic penalty matrix,,"Yang, Xiaohui","Xin, Xin","Su, Limin",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_818,"Zulfiqar, Annus","Ghaffar, Muhammad M.","Shahzad, Muhammad",AI-ForestWatch: semantic segmentation based end-to-end framework for forest estimation and change detection using multi-spectral remote sensing imagery,JOURNAL OF APPLIED REMOTE SENSING,MAY 31 2021,15,"Forest change detection is crucial for sustainable forest management. The changes in the forest area due to deforestation (such as wild fires or logging due to development activities) or afforestation alter the total forest area. Additionally, it impacts the available stock for commercial purposes, climate change due to carbon emissions, and biodiversity of the forest habitat estimations, which are essential for disaster management and policy making. In recent years, foresters have relied on hand-crafted features or bi-temporal change detection methods to detect change in the remote sensing imagery to estimate the forest area. Due to manual processing steps, these methods are fragile and prone to errors and can generate inaccurate (i.e., under or over) segmentation results. In contrast to traditional methods, we present AI-ForestWatch, an end to end framework for forest estimation and change analysis. The proposed approach uses deep convolution neural network-based semantic segmentation to process multi-spectral spaceborne images to quantitatively monitor the forest cover change patterns by automatically extracting features from the dataset. Our analysis is completely data driven and has been performed using extended (with vegetation indices) Landsat-8 multi-spectral imagery from 2014 to 2020. As a case study, we estimated the forest area in 15 districts of Pakistan and generated forest change maps from 2014 to 2020, where major afforestation activity is carried out during this period. Our critical analysis shows an improvement of forest cover in 14 out of 15 districts. The AI-ForestWatch framework along with the associated dataset will be made public upon publication so that it can be adapted by other countries or regions. (C) The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License.",deep neural networks,semantic segmentation,multi-spectral remote sensing,multi-temporal forest change detection,"Weis, Christian","Malik, Muhammad, I","Shafait, Faisal","Wehn, Norbert",,,,,,,,,,,,,,,,,,,,,,,,,
Row_819,"Zhao, Yingying","Zheng, Guizhou","Xu, Zhangyan",Multiscale Feature Weighted-Aggregating and Boundary Enhancement Network for Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,4,"High-resolution remote sensing images (HRRSIs) play an important role in large area and real-time earth observation tasks. However, HRRSIs typically comprise heterogeneous objects of various sizes and complex boundary lines, which pose challenges to HRRSI segmentation. Despite the fact that deep convolutional neural networks dramatically boosted the accuracy, several limitations exist in standard models. Existing methods, mainly concatenate multiscale information to extract the various sizes of objects. However, these methods ignore differentiating information, making it difficult to take advantage of them and completely extract small objects. In addition, there have remained some difficulties in extracting boundary information with positions of uncertainty in previous works. In this article, we propose a novel multiscale feature weighted-aggregating and boundary enhancement network (MFBE-Net) for the segmentation of HRRSIs. ResNet-50, possessing a strong ability to extract features, is employed as the backbone. To fully utilize the information that was extracted, we propose a multiscale feature weighted-aggregating module, which aims to weight-integrate deep features, shallow features, and global information. The boundary enhancement module is designed to solve the blurry boundary information problems and locate its positions. Coordinate attention is also applied in the framework to coherently label size-varied ground objects from different categories and reduce information redundancy. Meanwhile, a mixed loss function is used to supervise the network training process. Finally, MFBE-Net was verified on two public HRRSI datasets, and the experimental results show that the proposed framework outperformed other existing mainstream deep learning methods and could further improve the accuracy of HRRSI segmentation.",Semantics,Image segmentation,Feature extraction,Data mining,"Qiu, Zhonghang","Chen, Zhixing",,,Deep learning,Remote sensing,Convolutional neural networks,Boundary enhancement,deep learning,feature weighted-aggregating,high-resolution remote sensing images (HRRSIs),semantic segmentation,,,,,,,,,,,,,,,,,
Row_820,"Pastorino, Martina","Moser, Gabriele","Serpico, Sebastiano B.",CRFNet: A Deep Convolutional Network to Learn the Potentials of a CRF for the Semantic Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"This article presents a method for the automatic learning of the potentials of a stochastic model, in particular a conditional random field (CRF), in a non-parametric fashion. The proposed model is based on a neural architecture, in order to leverage the modeling capabilities of deep learning (DL) approaches to directly learn semantic and spatial information from the input data. Specifically, the methodology is based on fully convolutional networks and fully connected neural networks. The idea is to access the multiscale information intrinsically extracted in the intermediate layers of a fully convolutional network through the integration of fully connected neural networks at different scales, while favoring the interpretability of the hidden layers as posterior probabilities. The potentials of the CRF are learned through an additional convolutional layer, whose kernel models the local spatial information considered. The loss function is computed as a linear combination of cross-entropy losses, accounting for the multiscale and the spatial information. To evaluate the capabilities of the proposed approach for the semantic segmentation of remote sensing images, the experimental validation was conducted with the ISPRS 2-D semantic labeling challenge Vaihingen and Potsdam datasets and with the IEEE GRSS data fusion contest Zeebruges dataset. As the ground truths of these benchmark datasets are spatially exhaustive, they have been modified to approximate the spatially sparse ground truths common in real remote sensing applications. The results are significant, as the proposed approach obtains higher average classification accuracies than recent state-of-the-art techniques considered in this article. The code is available at https://github.com/Ayana-Inria/CRFNet-RS.",Convolutional neural networks,Remote sensing,Semantic segmentation,Semantics,"Zerubia, Josiane",,,,Task analysis,Computer architecture,Conditional random fields,Conditional random fields (CRFs),convolutional neural network (CNN),fully convolutional network (FCN),remote sensing,semantic segmentation,,,,,,,,,,,,,,,,,
Row_821,"Hua, Xia","Wang, Xinqing","Rui, Ting",Cascaded panoptic segmentation method for high resolution remote sensing image,APPLIED SOFT COMPUTING,SEP 2021,17,"Great progress has been made for remote sensing image segmentation with the development of Deep Convolutional Neural Networks. However, Multiple convolutions significantly reduce the resolution and lead to the loss of many key information, the prediction accuracy of pixel categories is reduced. And DCNN accumulate context information on a large receptive field, which leads to blurred boundary segmentation of objects. This paper proposes a cascaded panoptic segmentation network to target the aforementioned problems. Firstly, a shared feature pyramid network backbone and a new hybrid task cascade framework are designed, which share the features and integrate the complementary features of different tasks in different stages, which can extract rich context information. Then, a functional module is designed to learn the mask quality of predicted instances in Mask R-CNN to calibrate the inconsistency between mask quality and mask score, thus to deal with the scale change of the object. Finally, a new Visual-saliency ranking module is designed to overcome the mutual occlusion problem between the prediction results, and strengthen robustness to illumination. The experimental results prove that our method still has significant advantages even compared with the most advanced methods, and ablation experiments also verify the effectiveness of our designed strategies. (C) 2021 Elsevier B.V. All rights reserved.",Remote sensing images,Panoptic segmentation,Instance segmentation,Semantic segmentation,"Shao, Faming","Wang, Dong",,,Deep Convolutional Neural Networks,,,,,,,,,,,,,,,,,,,,,,,,
Row_822,"de Carvalho, Osmar L. F.","de Carvalho Junior, Osmar A.","de Albuquerque, Anesmar O.",BEYOND THE VISIBLE PIXELS USING SEMANTIC AMODAL SEGMENTATION IN REMOTE SENSING IMAGES,,2022,2,"2D representations of 3D scenes generate occlusions among different targets. Understanding targets by only seeing parts of them is referred to as an amodal perception, which is still unexplored in remote sensing. Thus, we propose integrating this concept using peculiarities of remote sensing Nadir images to classify non-visible targets at a pixel level. Nadir images present a hierarchical order of occlusions, allowing us to separate different layers. We developed a dataset with 600 images and three classes (roads, vehicles, and trees) with independent labelling for each class. Any semantic segmentation model is suitable for this task, but we explored the U-net architecture with three backbones (Efficient-net-B7, ResNet-101, and ResNeXt-101). The evaluation considered the IoU metric, providing 80% for the best model (Efficient-net-B7). Future studies aim to extend this approach by introducing competing classes among each layer and increasing the number of samples and categories.",occlusion,deep learning,aerial image,,"Luiz, Argelica S.","Santana, Nickolas C.","Borges, Dibio L.",,,,,,,,,,,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),,,,,,,,,,,,,,,
Row_823,"Yang, Le","Chen, Yiming","Song, Shiji",Deep Siamese Networks Based Change Detection with Remote Sensing Images,REMOTE SENSING,SEP 2021,39,"Although considerable success has been achieved in change detection on optical remote sensing images, accurate detection of specific changes is still challenging. Due to the diversity and complexity of the ground surface changes and the increasing demand for detecting changes that require high-level semantics, we have to resort to deep learning techniques to extract the intrinsic representations of changed areas. However, one key problem for developing deep learning metho for detecting specific change areas is the limitation of annotated data. In this paper, we collect a change detection dataset with 862 labeled image pairs, where the urban construction-related changes are labeled. Further, we propose a supervised change detection method based on a deep siamese semantic segmentation network to handle the proposed data effectively. The novelty of the method is that the proposed siamese network treats the change detection problem as a binary semantic segmentation task and learns to extract features from the image pairs directly. The siamese architecture as well as the elaborately designed semantic segmentation networks significantly improve the performance on change detection tasks. Experimental results demonstrate the promising performance of the proposed network compared to existing approaches.",change detection,remote sensing,semantic segmentation,deep neural network,"Li, Fan","Huang, Gao",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_824,"Guo, Yuxuan","Wang, Zhe",,HEIGHT ESTIMATION BASED ON SEMANTIC SEGMENTATION,,2023,1,"Buildings are crucial for urban development, and building extraction and height estimation are two essential components in the process of architectural reconstruction. Nowadays, the use of instance segmentation methods to extract buildings from remote sensing imagery has become quite mature. However, accurately estimating the heights of complex buildings from images remains highly challenging. To address the task of building height estimation, this paper attempts to use semantic segmentation methods. Firstly, the height label quantification is simplified into a semantic segmentation task. Then, the segmentation results from different models are integrated through voting to improve the segmentation accuracy of buildings. Finally, the average heights predicted by different models are obtained based on the positions containing building regions to obtain the final height estimation. In the DFC2023 competition, our team ranked first in the height estimation task, sixth in the building extraction task, and third in the overall ranking.",Height Estimation,Instance segmentation,Building Extraction,,,,,,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_825,"Liu, Fang","Liu, Keming","Liu, Jia",Content-Guided and Class-Oriented Learning for VHR Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,2,"With the flourishing of remote sensing (RS) platform techniques, very high-resolution (VHR) images have become more and more popular in recent years, which benefit the task of semantic segmentation but bring new challenges as well. Small objects, such as cars and trees, only occupy a few pixels in VHR images and are usually hard to segment. Moreover, the overlap problem about similar ground objects, such as low vegetation and trees, always results in underperformance. In this article, a content-guided and class-oriented network (CGCO-Net) for VHR image semantic segmentation is proposed to tackle this problem. Specifically, an adaptive content-guided fusion (ACGF) module with deformable convolution is introduced to capture long-distance dependencies and spatial aggregation effectively. With the guidance of the high-level features, the semantic content knowledge is gradually aggregated into low-level features and the details of the original features could be preserved. In addition, a multiscale channel alignment module is introduced into the encoder-decoder structure to further extract the long-range context information and reduce the calculation consumption. In order to improve the ability of pixel-level classification, a class-oriented representation learning (CORL) way is designed with transformer blocks by class embedding and deep supervision, which gradually enhance the discrimination and benefit the final segmentation. Furthermore, a weighted loss function and a threshold optimization strategy are employed to alleviate the sample imbalance problem. Tested on three public datasets and compared with several state-of-the-art methods, the proposed CGCO-net achieves good performance in both qualitative and quantitative analysis.",Transformers,Semantics,Semantic segmentation,Feature extraction,"Yang, Jingxiang","Tang, Xu","Xiao, Liang",,Remote sensing,Convolution,Aggregates,Class-oriented,content-guided,remote sensing (RS),semantic segmentation,very high-resolution (VHR) image,,,,,,,,,,,,,,,,,
Row_826,"Li, Ziyao","Wang, Rui","Zhang, Wen",Multiscale Features Supported DeepLabV3 Optimization Scheme for Accurate Water Semantic Segmentation,IEEE ACCESS,2019,53,"In the task of using deep learning semantic segmentation model to extract water from high-resolution remote sensing images, multiscale feature sensing and extraction have become critical factors that affect the accuracy of image classification tasks. A single-scale training mode will cause one-sided extraction results, which can lead to reverse errors and imprecise detail expression. Therefore, fusing multiscale features for pixel-level classification is the key to achieving accurate image segmentation. Based on this concept, this paper proposes a deep learning scheme to achieve fine extraction of image water bodies. The process includes multiscale feature perception splitting of images, a restructured deep learning network model, multiscale joint prediction, and postprocessing optimization performed by a fully connected conditional random field (CRF). According to the scale space concept of remote sensing, we apply hierarchical multiscale splitting processing to images. Then, we improve the structure of the image semantic segmentation model DeepLabV3, an advanced image semantic segmentation model, and adjust the feature output layer of the model to multiscale features after weighted fusion. At the back end of the deep learning model, the water boundary details are optimized with the fully connected CRF. The proposed multiscale training method is well adapted to feature extraction for the different scale images in the model. In the multiscale output fusion, assigning different weights to the output features of each scale controls the influence of the various scale features on the water extraction results. We carried out a large number of water extraction experiments on GF1 remote sensing images. The results show that the method significantly improves the accuracy of water extraction and demonstrates the effectiveness of the method.",Feature extraction,Image segmentation,Semantics,Remote sensing,"Hu, Fengmin","Meng, Lingkui",,,Data mining,Deep learning,Indexes,Remote sensing,deep learning,semantic segmentation,water information extraction,multi-scales,,,,,,DeepLabV 3+,,,,,,,,,,,
Row_827,"Yu, Jie","Cai, Yang","Lyu, Xin",Boundary-Guided Semantic Context Network for Water Body Extraction from Remote Sensing Images,REMOTE SENSING,SEP 2023,2,"Automatically extracting water bodies is a significant task in interpreting remote sensing images (RSIs). Convolutional neural networks (CNNs) have exhibited excellent performance in processing RSIs, which have been widely used for fine-grained extraction of water bodies. However, it is difficult for the extraction accuracy of CNNs to satisfy the requirements in practice due to the limited receptive field and the gradually reduced spatial size during the encoder stage. In complicated scenarios, in particular, the existing methods perform even worse. To address this problem, a novel boundary-guided semantic context network (BGSNet) is proposed to accurately extract water bodies via leveraging boundary features to guide the integration of semantic context. Firstly, a boundary refinement (BR) module is proposed to preserve sufficient boundary distributions from shallow layer features. In addition, abstract semantic information of deep layers is also captured by a semantic context fusion (SCF) module. Based on the results obtained from the aforementioned modules, a boundary-guided semantic context (BGS) module is devised to aggregate semantic context information along the boundaries, thereby enhancing intra-class consistency of water bodies. Extensive experiments were conducted on the Qinghai-Tibet Plateau Lake (QTPL) and the Land-cOVEr Domain Adaptive semantic segmentation (LoveDA) datasets. The results demonstrate that the proposed BGSNet outperforms the mainstream approaches in terms of OA, MIoU, F1-score, and kappa. Specifically, BGSNet achieves an OA of 98.97% on the QTPL dataset and 95.70% on the LoveDA dataset. Additionally, an ablation study was conducted to validate the efficacy of the proposed modules.",remote sensing images,water body extraction,convolutional neural networks,boundary-guided semantic context,"Xu, Zhennan","Wang, Xinyuan","Fang, Yiwei","Jiang, Wenxuan",,,,,,,,,"Li, Xin",,,,,,,,,,,,,,,,
Row_828,"Huang, Haitao","Li, Baopu","Zhang, Yuchen",Joint Distribution Adaptive-Alignment for Cross-Domain Segmentation of High-Resolution Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,2,"Although existing unsupervised domain adaptation (UDA) methods have successfully applied to semantic segmentation tasks for high-resolution remote sensing (HRS) images, they still have some limitations that need to be addressed: 1) they mainly focus on aligning the marginal distributions while ignoring the interdomain differences in the conditional distributions, which may be suboptimal because they assume that the boundaries of category decision are identical across domains; and 2) they depend on self-supervised learning for easy-to-hard alignment, which may result in model learning erroneous knowledge from the pseudo labels. To address the above limitations, we propose a joint distribution adaptive-alignment framework (JDAF) to eliminate the distribution difference between the source and target domains, which is mainly composed of a marginal distribution alignment (MDA) module, a conditional distribution alignment (CDA) module, and an improved easy-to-hard adaptation strategy. The MDA module is used to narrow local semantic and global spatial differences between domains and first advance, and then, the CDA module that includes a category-invariant feature alignment (CFA) block and a dataset-level context aggregation (DCA) block is presented and designed, which can dynamically update and align the feature representations that are invariant to category change and adaptively incorporate dataset-level context into the features of source domain to enhance the pixel-level representation. An uncertainty-adaptive learning (UAL) method is, moreover, proposed to improve the easy-to-hard adaptation strategy by enabling the model to learn accurate knowledge from the pseudo labels, which can boost the adaptive performance of the whole JDAF. Comprehensive experiments with four cross-domain tasks on two benchmark datasets of aerospace HRS images demonstrate that the proposed JDAF achieves significant performance gains compared to the state-of-the-art cross-domain semantic segmentation methods. Our code is available at: https://github.com/maple-hx/JDAF.",Dataset-level context aggregation (DCA),high-resolution remote sensing (HRS) images,joint distribution alignment,semantic segmentation,"Chen, Tao","Wang, Bin",,,uncertainty-adaptive learning (UAL),unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,,
Row_829,"Salgueiro, Luis","Marcello, Javier","Vilaplana, Veronica",SEG-ESRGAN: A Multi-Task Network for Super-Resolution and Semantic Segmentation of Remote Sensing Images,REMOTE SENSING,NOV 2022,9,"The production of highly accurate land cover maps is one of the primary challenges in remote sensing, which depends on the spatial resolution of the input images. Sometimes, high-resolution imagery is not available or is too expensive to cover large areas or to perform multitemporal analysis. In this context, we propose a multi-task network to take advantage of the freely available Sentinel-2 imagery to produce a super-resolution image, with a scaling factor of 5, and the corresponding high-resolution land cover map. Our proposal, named SEG-ESRGAN, consists of two branches: the super-resolution branch, that produces Sentinel-2 multispectral images at 2 m resolution, and an encoder-decoder architecture for the semantic segmentation branch, that generates the enhanced land cover map. From the super-resolution branch, several skip connections are retrieved and concatenated with features from the different stages of the encoder part of the segmentation branch, promoting the flow of meaningful information to boost the accuracy in the segmentation task. Our model is trained with a multi-loss approach using a novel dataset to train and test the super-resolution stage, which is developed from Sentinel-2 and WorldView-2 image pairs. In addition, we generated a dataset with ground-truth labels for the segmentation task. To assess the super-resolution improvement, the PSNR, SSIM, ERGAS, and SAM metrics were considered, while to measure the classification performance, we used the IoU, confusion matrix and the F1-score. Experimental results demonstrate that the SEG-ESRGAN model outperforms different full segmentation and dual network models (U-Net, DeepLabV3+, HRNet and Dual_DeepLab), allowing the generation of high-resolution land cover maps in challenging scenarios using Sentinel-2 10 m bands.",multi-task network,super-resolution,semantic segmentation,Sentinel-2,,,,,WorldView-2,,,,,,,,,,,,,,,,,,,,,,,,
Row_830,"Zhou, Ruixue","Yuan, Zhiqiang","Rong, Xuee",Weakly Supervised Semantic Segmentation in Aerial Imagery via Cross-Image Semantic Mining,REMOTE SENSING,FEB 2023,6,"Weakly Supervised Semantic Segmentation (WSSS) with only image-level labels reduces the annotation burden and has been rapidly developed in recent years. However, current mainstream methods only employ a single image's information to localize the target and do not account for the relationships across images. When faced with Remote Sensing (RS) images, limited to complex backgrounds and multiple categories, it is challenging to locate and differentiate between the categories of targets. As opposed to previous methods that mostly focused on single-image information, we propose CISM, a novel cross-image semantic mining WSSS framework. CISM explores cross-image semantics in multi-category RS scenes for the first time with two novel loss functions: the Common Semantic Mining (CSM) loss and the Non-common Semantic Contrastive (NSC) loss. In particular, prototype vectors and the Prototype Interactive Enhancement (PIE) module were employed to capture semantic similarity and differences across images. To overcome category confusions and closely related background interferences, we integrated the Single-Label Secondary Classification (SLSC) task and the corresponding single-label loss into our framework. Furthermore, a Multi-Category Sample Generation (MCSG) strategy was devised to balance the distribution of samples among various categories and drastically increase the diversity of images. The above designs facilitated the generation of more accurate and higher-granularity Class Activation Maps (CAMs) for each category of targets. Our approach is superior to the RS dataset based on extensive experiments and is the first WSSS framework to explore cross-image semantics in multi-category RS scenes and obtain cutting-edge state-of-the-art results on the iSAID dataset by only using image-level labels. Experiments on the PASCAL VOC2012 dataset also demonstrated the effectiveness and competitiveness of the algorithm, which pushes the mean Intersection-Over-Union (mIoU) to 67.3% and 68.5% on the validation and test sets of PASCAL VOC2012, respectively.",weakly supervised semantic segmentation,remote sensing images,image-level labels,,"Ma, Weicong","Sun, Xian","Fu, Kun","Zhang, Wenkai",,,,,,,,,,,,,,,,,,,,,,,,,
Row_831,"Zhang, Lili","Xu, Mengqi","Wang, Gaoxu",SiameseNet Based Fine-Grained Semantic Change Detection for High Resolution Remote Sensing Images,REMOTE SENSING,DEC 2023,0,"Change detection in high resolution (HR) remote sensing images faces more challenges than in low resolution images because of the variations of land features, which prompts this research on faster and more accurate change detection methods. We propose a pixel-level semantic change detection method to solve the fine-grained semantic change detection for HR remote sensing image pairs, which takes one lightweight semantic segmentation network (LightNet), using the parameter-sharing SiameseNet, as the architecture to carry out pixel-level semantic segmentations for the dual-temporal image pairs and achieve pixel-level change detection based directly on semantic comparison. LightNet consists of four long-short branches, each including lightweight dilated residual blocks and an information enhancement module. The feature information is transmitted, fused, and enhanced among the four branches, where two large-scale feature maps are fused and then enhanced via the channel information enhancement module. The two small-scale feature maps are fused and then enhanced via a spatial information enhancement module, and the four upsampling feature maps are finally concatenated to form the input of the Softmax. We used high resolution remote sensing images of Lake Erhai in Yunnan Province in China, collected by GF-2, to make one dataset with a fine-grained semantic label and a dual-temporal image-pair label to train our model, and the experiments demonstrate the superiority of our method and the accuracy of LightNet; the pixel-level semantic change detection methods are up to 89% and 86%, respectively.",change detection,dual-temporal remote sensing images,information enhancement,Siamese network,"Shi, Rui","Xu, Yi","Yan, Ruijie",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_832,"Wang, Yiqin",,,Remote Sensing Image Semantic Segmentation Algorithm Based on Improved ENet Network,SCIENTIFIC PROGRAMMING,OCT 4 2021,9,"A remote sensing image semantic segmentation algorithm based on improved ENet network is proposed to improve the accuracy of segmentation. First, dilated convolution and decomposition convolution are introduced in the coding stage. They are used in conjunction with ordinary convolution to increase the receptive field of the model. Each convolution output contains a larger range of image information. Second, in the decoding stage, the image information of different scales is obtained through the upsampling operation and then through the compression, excitation, and reweighting operations of the Squeeze and Excitation (SE) module. The weight of each feature channel is recalibrated to improve the accuracy of the network. Finally, the Softmax activation function and the Argmax function are used to obtain the final segmentation result. Experiments show that our algorithm can significantly improve the accuracy of remote sensing image semantic segmentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_833,"Wang, Biao","Jiang, Zhenghao","Ma, Weichun",Dual-Dimension Feature Interaction for Semantic Change Detection in Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"Remote sensing semantic change detection (SCD) involves extracting information about changes in land cover/land use (LCLU) within the same area at different times. This issue is of crucial significance in many Earth observation tasks, such as precise urban planning and natural resource management. However, the current methods primarily focus on spatial feature extraction, lacking awareness of temporal features. Consequently, there are challenges in extracting change features, making distinguishing intraclass and interclass differences difficult. This also contributes to pseudochange, posing challenges for SCD tasks. To overcome the limitations of existing methods, we present a dual-dimension feature interaction network (DFINet) for SCD. First, to enhance the assessment and perceptual abilities related to intraclass and interclass differences, we introduce a temporal difference feature enhancement (TDFE) module. This module comprehensively captures features from the temporal dimension. Then, to address the interrelation between multitemporal and multilevel features, we investigate the feature selection interaction (FSIA) and interaction attention modules (IAM), which enable multidimensional deep fusion and interaction of change features. This enhances the capacity for information transfer and integration among the features within multitemporal remote sensing images (RSIs). The experimental results demonstrate that, compared to existing methods, the proposed architecture achieves a significant improvement in accuracy. Additionally, the design enhancements added to DFINet boost the practicality of remote sensing SCD, underscoring its substantial research value.",Feature extraction,Semantics,Task analysis,Remote sensing,"Xu, Xiao","Zhang, Peng","Wu, Yanlan","Yang, Hui",Sensors,Semantic segmentation,Convolution,Dual-dimension,interclass,intraclass,remote sensing images (RSIs),semantic change detection (SCD),,,,,,,,,,,,,,,,,
Row_834,"Liu, Wei","Su, Fulin","Jin, Xinfei",Bispace Domain Adaptation Network for Remotely Sensed Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,22,"Supervised learning for semantic segmentation has achieved impressive success in remote sensing, while this normally has a high demand on pixel-level ground truth from the testing images (target domain). Labeling data for semantic segmentation is labor-intensive and time-consuming. To reduce the workload of manual labeling, domain adaptation (DA) utilizes preexisting labeled images from other sources (source domain) to classify the images in the target domain. In this article, we propose a bispace alignment network for DA named BSANet. BSANet is designed to have a dual-branch structure which is able to extract features in the image domain and the wavelet domain simultaneously. To minimize the discrepancy between the source and target domains, we propose a bispace adversarial learning strategy. Specifically, BSANet employs two discriminators in different spaces, one aligning the source and target feature distributions, and the other helping the classification outputs render reasonable spatial layouts. The proposed method shows the ability to train an end-to-end network for semantic segmentation without using any label in the target domain. Extensive experiments and ablation studies are conducted in cross-city scenarios. Comparative experiments with several state-of-the-art DA methods show that our method achieves the best performance.",Semantics,Feature extraction,Image segmentation,Wavelet domain,"Li, Hongxu","Qin, Rongjun",,,Generators,Training,Loss measurement,Adversarial learning,domain adaptation (DA),semantic segmentation,transfer learning (TL),wavelet transform,,,,,,,,,,,,,,,,,
Row_835,"Cui, Liangyi","Jing, Xin","Wang, Yu",Improved Swin Transformer-Based Semantic Segmentation of Postearthquake Dense Buildings in Urban Areas Using Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,42,"Timely acquiring the earthquake-induced damage of buildings is crucial for emergency assessment and post-disaster rescue. Optical remote sensing is a typical method for obtaining seismic data due to its wide coverage and fast response speed. Convolutional neural networks (CNNs) are widely applied for remote sensing image recognition. However, insufficient extraction and expression ability of global correlations between local image patches limit the performance of dense building segmentation. This paper proposes an improved Swin Transformer to segment dense urban buildings from remote sensing images with complex backgrounds. The original Swin Transformer is used as a backbone of the encoder, and a convolutional block attention module is employed in the linear embedding and patch merging stages to focus on significant features. Hierarchical feature maps are then fused to strengthen the feature extraction process and fed into the UPerNet (as the decoder) to obtain the final segmentation map. Collapsed and non-collapsed buildings are labeled from remote sensing images of the Yushu and Beichuan earthquakes. Data augmentations of horizontal and vertical flipping, brightness adjustment, uniform fogging, and non-uniform fogging are performed to simulate actual situations. The effectiveness and superiority of the proposed method over the original Swin Transformer and several mature CNN-based segmentation models are validated by ablation experiments and comparative studies. The results show that the mean intersection-over-union of the improved Swin Transformer reaches 88.53%, achieving an improvement of 1.3% compared to the original model. The stability, robustness, and generalization ability of dense building recognition under complex weather disturbances are also validated.",Attention mechanism,complex weather disturbances,dense seismic building segmentation,feature fusion,"Huan, Yixuan","Xu, Yang","Zhang, Qiangqiang",,improved Swin Transformer,remote sensing images,,,,,,,,,,,,,,,,,,,,,,,
Row_836,"Wang, Fang","Piao, Shihao","Xie, Jindong",CSE-HRNet: A Context and Semantic Enhanced High-Resolution Network for Semantic Segmentation of Aerial Imagery,IEEE ACCESS,2020,14,"Semantic segmentation of high-resolution aerial images is a concerning issue of remote sensing applications. To address the issues of intra-class heterogeneity and inter-class homogeneity, a novel end-to-end semantic segmentation network, namely Context and Semantic Enhanced High-Resolution Network (CSE-HRNet), is proposed in this paper. Two procedures are considered comprehensively, which are multi-scale contextual feature extractor and multi-level semantic feature producer. Nested Dilated Residual Block (NDRB) is designed firstly, which could enhance the representational power of multi-scale contexts and tackle the issue of intra-class heterogeneity. The pyramidal feature hierarchy is introduced secondly, by which multi-level feature fusions could be utilized to enlarge inter-class semantic differences. Experimental results verify that, based on the Potsdam and Vaihingen benchmarks, the proposed CSE-HRNet can achieve competitive performance compared with other state-of-the-art methods.",Semantic segmentation,image analysis,machine learning,remote sensing image,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_837,"Shuangpeng, Zheng","Ta, Fang","Hong, Huo",Farmland Recognition of High Resolution Multispectral Remote Sensing Imagery using Deep Learning Semantic Segmentation Method,,2019,5,"Farmland mapping is an important step for estimating grain yields. However extraction of farmland from multispectral remote sensing images (RSIs) is still a challenging work, as farmland is located on not only plains but also mountains, which displays divergent and confusing characteristics in RSIs. To solve the problem of lacking the multispectral remote sensing image dataset for pretraining, we extend Deep Feature Aggregation Net (DFANet) with fewer network parameters, a semantic segmentation network, to automatically map farmland from 3-band to multispectral images in a pixel-wise strategy. In this network, we first utilize more information aggregation. The fully-connected attention module is then replaced by a proposed convolution attention module. Finally, a new proposed decoder is used to recover the details of the feature map. Experimental results show that the model with multispectral RSIs outperforms the baselines.",Deep Learing,Remote Sensing,Farmland Recognition,High-resolution Multispectral Image,,,,,Agriculture,,,,,,,,,PROCEEDINGS OF 2019 INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE (PRAI 2019),,,,,,,,,,,,,,,
Row_838,"Yu, Minmin","Qin, Fen",,Research on the Applicability of Transformer Model in Remote-Sensing Image Segmentation,APPLIED SCIENCES-BASEL,FEB 2023,6,"Transformer models have achieved great results in the field of computer vision over the past 2 years, drawing attention from within the field of remote sensing. However, there are still relatively few studies on this model in the field of remote sensing. Which method is more suitable for remote-sensing segmentation? In particular, how do different transformer models perform in the face of high-spatial resolution and the multispectral resolution of remote-sensing images? To explore these questions, this paper presents a comprehensive comparative analysis of three mainstream transformer models, including the segmentation transformer (SETRnet), SwinUnet, and TransUnet, by evaluating three aspects: a visual analysis of feature-segmentation results, accuracy, and training time. The experimental results show that the transformer structure has obvious advantages for the feature-extraction ability of large-scale remote-sensing data sets and ground objects, but the segmentation performance of different transfer structures in different scales of remote-sensing data sets is also very different. SwinUnet exhibits better global semantic interaction and pixel-level segmentation prediction on the large-scale Potsdam data set, and the SwinUnet model has the highest accuracy metrics for KAPPA, MIoU, and OA in the Potsdam data set, at 76.47%, 63.62%, and 85.01%, respectively. TransUnet has better segmentation results in the small-scale Vaihingen data set, and the three accuracy metrics of KAPPA, MIoU, and OA are the highest, at 80.54%, 56.25%, and 85.55%, respectively. TransUnet is better able to handle the edges and details of feature segmentation thanks to the network structure together built by its transformer and convolutional neural networks (CNNs). Therefore, TransUnet segmentation accuracy is higher when using a small-scale Vaihingen data set. Compared with SwinUnet and TransUnet, the segmentation performance of SETRnet in different scales of remote-sensing data sets is not ideal, so SETRnet is not suitable for the research task of remote-sensing image segmentation. In addition, this paper discusses the reasons for the performance differences between transformer models and discusses the differences between transformer models and CNN. This study further promotes the application of transformer models in remote-sensing image segmentation, improves the understanding of transformer models, and helps relevant researchers to select a more appropriate transformer model or model improvement method for remote-sensing image segmentation.",transformer,multihead attention,remote-sensing image segmentation,deep learning,,,,,SwinUner,TransUnet,SETRnet,visual classification,,,,,,,,,,,,,,,,,,,,,
Row_839,"Yang, Yunsong","Yuan, Genji","Li, Jinjiang",Multielement-Feature-Based Hierarchical Context Integration Network for Remote Sensing Image Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,1,"In the current remote sensing segmentation tasks, we identify issues of insufficient accuracy in segmenting objects and types with similar colors, along with a lack of adequate smoothness and coherence in edge segmentation. To address these challenges, we propose a network framework called the multielement-feature-based hierarchical context integration network (MHCINet). This framework achieves deep integration of global information, local information, multiscale information, and edge information. First, we introduce an Edge and Levels Grouped Aggregator to fuse shallow features, deep features, and edge information, enhancing foreground saliency. Finally, to better identify instances with similar colors during the feature reconstruction stage, we design a constant multivariate feature integrator to fully exploit multiscale information and global context, thereby improving the segmentation model's performance. Comprehensive experimental results on the Vaihingen and Potsdam datasets demonstrate that MHCINet outperforms existing state-of-the-art methods, achieving mean intersection over union of 84.8% and 87.6% on the Vaihingen and Potsdam datasets, respectively.",Remote sensing,Image edge detection,Image color analysis,Semantics,,,,,Semantic segmentation,Feature extraction,Task analysis,Edge fusion,multiscale fusion,remote sensing,semantic segmentation,transformer,,,,,,,,,,,,,,,,,
Row_840,"Yang, Wanying","Cheng, Yali","Xu, Wenbo",Remote Sensing Semantic Change Detection Model for Improving Objects Completeness,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2025,0,"Semantic change detection (SCD) extends beyond binary change detection by not only discerning the locations of change areas, but also offering the alterations in land-cover/land-use types. This refined change information is concernful for various applications. While deep learning methods have made significant progress in SCD, accurately capturing the integrity of targets remains challenging in intricate scenarios. Therefore, this article proposes a deformable multiscale composite transformer network (DMCTNet). This network effectively models relevant semantic information and spatio-temporal dependencies. DMCTNet leverages a variant vision foundation models encoder to learn specific knowledge, facilitating effective visual representation in remote sensing images. A multiscale feature aggregator module is developed to discern both the ""what"" and ""where"" of changes by integrating features across different scales. Subsequently, a masked decoder through queries to convey rich semantic change information, guided by conspicuous change potential locations to decode. A substantial volume of experimental results consistently demonstrate that this model achieves more accurate and reliable results in change areas, improving the intersection over the union of change by 2.65% and 1.67% on the SECOND and Landsat-SCD datasets, respectively. Code will be made available.",Semantics,Transformers,Feature extraction,Tuning,,,,,Correlation,Visualization,Decoding,Accuracy,Remote sensing,Computational modeling,Remote sensing (RS),semantic change detection (SCD),,,,,,semantic segmentation,visual foundation models (VFMs),,,,,,,,,,
Row_841,"Xiong, Wei","Cai, Mi","Lv, Yafei",FA-Net: feature attention network for semantic segmentation of ship port,GEOCARTO INTERNATIONAL,MAR 19 2022,1,"Accurate understanding of the scene of ship ports is important in a broad range of military and civilian applications, such as maritime management, maritime safety, fisheries management, maritime situational awareness (MSA), and ocean traffic surveillance. Semantic segmentation, which implements pixel-level classification, can achieve an exhaustive analysis of ship ports. However, in the earlier time, the state-of-the-art methods of semantic segmentation were mostly based on the study of natural images. Subsequently, semantic segmentation has been gradually widely used in remote sensing, but still few of them has focused on the parsing of ship ports in remote sensing. In order to realize a detailed analysis of ship ports, a novel framework (called Feature Attention Network) is proposed for the accurate segmentation of multiple targets in a ship port in this paper. In this framework, a multi-label classification auxiliary network is first designed to solve the problem of confused multiple prediction for one target by capturing more global context information. Then, an attention model is introduced to solve the problem of error segmentation between similar targets with different labels. Finally, a feature aggregation model is presented to obtain more contextual information. In addition, we construct a data set for the semantic segmentation of ship ports (called HRSC2016-SP) by labeling the HRSC2016 data set to evaluate our proposed framework. Our approach has achieved a state-of-the-art result (82.16% mIoU) on the test set of HRSC2016-SP.",FA-Net,semantic segmentation,ship port,remote sensing,"Pei, Jiazheng",,,,HRSC2016-SP,,,,,,,,,,,,,,,,,,,,,,,,
Row_842,"Wang, Hongzhen","Wang, Ying","Zhang, Qian",Gated Convolutional Neural Network for Semantic Segmentation in High-Resolution Images,REMOTE SENSING,MAY 2017,168,"Semantic segmentation is a fundamental task in remote sensing image processing. The large appearance variations of ground objects make this task quite challenging. Recently, deep convolutional neural networks (DCNNs) have shown outstanding performance in this task. A common strategy of these methods (e.g., SegNet) for performance improvement is to combine the feature maps learned at different DCNN layers. However, such a combination is usually implemented via feature map summation or concatenation, indicating that the features are considered indiscriminately. In fact, features at different positions contribute differently to the final performance. It is advantageous to automatically select adaptive features when merging different-layer feature maps. To achieve this goal, we propose a gated convolutional neural network to fulfill this task. Specifically, we explore the relationship between the information entropy of the feature maps and the label-error map, and then a gate mechanism is embedded to integrate the feature maps more effectively. The gate is implemented by the entropy maps, which are generated to assign adaptive weights to different feature maps as their relative importance. Generally, the entropy maps, i.e., the gates, guide the network to focus on the highly-uncertain pixels, where detailed information from lower layers is required to improve the separability of these pixels. The selected features are finally combined to feed into the classifier layer, which predicts the semantic label of each pixel. The proposed method achieves competitive segmentation accuracy on the public ISPRS 2D Semantic Labeling benchmark, which is challenging for segmentation by only using the RGB images.",semantic segmentation,CNN,deep learning,ISPRS,"Xiang, Shiming","Pan, Chunhong",,,remote sensing,gate,,,,,,,,,,,,,,,,,,,,,,,
Row_843,"Nalinipriya, G.","Lydia, E. Laxmi","Alshenaifi, Reem",A Two-Tiered Bidirectional Atrous Spatial Pyramid Pooling-Based Semantic Segmentation Model for Landslide Classification Using Remote Sensing Images,IEEE ACCESS,2024,0,"Effective landslide representation from great spatial resolution images is significant in numerous applications. Many research works and techniques have been advertised. Still, these methods are very challenging to relate in real time since they depend on remotely sensing landslides from a solitary sensor with an exact spatial resolution. Precisely identifying landslides over a vast region with intricate background entities is difficult. Machine Learning (ML) and Deep Learning (DL) have attained extraordinary performance in classifying images utilizing remotely sensed images from numerous platforms. Moreover, techniques built within DL architectures tend to implement encoder-decoder network structures, where constant convolutions effortlessly strain out numerous landslide features. This study develops a Bidirectional Atrous Spatial Pyramid Pooling-Based Semantic Segmentation and Classification Model (BASPP-SSCM) technique for landslide Remote Sensing Images. The main goal of the BASPP-SSCM technique is to segment and classify the landslide areas. In the preprocessing stage, the BASPP-SSCM model employs an adaptive Wiener filtering (AWF) technique to eliminate the noise. Next, for the semantic segmentation method, the BASPP-SSCM technique utilizes the DeepLabV3 method with the backbone of the ConvNeXtLarge model for determining the landslide region. Furthermore, the CapsNet model is utilized for the feature extraction process. Besides, the Rigdelet neural network (RNN) technique is employed for the landslide classification process. At last, the pelican optimization algorithm (POA) methodology is implemented to fine-tune the parameters involved in the RNN model. A wide range of investigations is performed to highlight the superiority of the BASPP-SSCM method using a benchmark dataset. The performance validation of the BASPP-SSCM method underscored a superior accuracy value of 98.23% of other existing approaches.",Terrain factors,Feature extraction,Remote sensing,Semantic segmentation,"Kavuri, Radhika","Ishak, Mohamad Khairi",,,Adaptation models,Accuracy,Image recognition,Computational modeling,Spatial resolution,Reliability,landslide remote sensing images,pelican optimization algorithm,,,,,,feature extraction,atrous spatial pyramid pooling,,,,,,,,,,
Row_844,"El Rai, Marwa Chendeb","Aburaed, Nour","Al-Saad, Mina",Integrating Deep Learning with Active Contour Models in Remote Sensing Image Segmentation,,2020,1,"Semantic image segmentation using deep learning is a crucial step in remote sensing and image processing. It has been exploited in oil spill identification in this work. Remote sensing Synthetic Aperture Radar (SAR) images have been used to identify oil spills due to their capability to cover wide scenery irrespective of the weather and illumination conditions. Oil spills can be seen by radar sensors as black spots. Nonetheless, the discrimination between the oil spills and looks-alike is challenging in the case of semantic segmentation at pixel level. To overcome this problem, the active contour without edges models take into account the length of boundaries, the areas inside and outside the region of interest to be integrated in the deep learning image segmentation model. For this purpose, a loss function, which includes the area and the length of object, is back propagated into the semantic segmentation architecture to optimize the deep learning process. The method is evaluated on a publicly available oil spill dataset. The experiments show that the proposed approach outperforms other state-of-the-art methods in terms of Intersection over Union (IoU).",Oil Spill detection,Synthetic Aperture Radar,Semantic Segmentation,Deep Learning,"Al-Ahmad, Hussain","Al Mansoori, Saeed","Marshall, Stephen",,Active Contour Models,,,,,,,,,"2020 27TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS, CIRCUITS AND SYSTEMS (ICECS)",,,,,,,,,,,,,,,
Row_845,"Li, Xin","Xu, Feng","Xia, Runliang",Encoding Contextual Information by Interlacing Transformer and Convolution for Remote Sensing Imagery Semantic Segmentation,REMOTE SENSING,AUG 2022,23,"Contextual information plays a pivotal role in the semantic segmentation of remote sensing imagery (RSI) due to the imbalanced distributions and ubiquitous intra-class variants. The emergence of the transformer intrigues the revolution of vision tasks with its impressive scalability in establishing long-range dependencies. However, the local patterns, such as inherent structures and spatial details, are broken with the tokenization of the transformer. Therefore, the ICTNet is devised to confront the deficiencies mentioned above. Principally, ICTNet inherits the encoder-decoder architecture. First of all, Swin Transformer blocks (STBs) and convolution blocks (CBs) are deployed and interlaced, accompanied by encoded feature aggregation modules (EFAs) in the encoder stage. This design allows the network to learn the local patterns and distant dependencies and their interactions simultaneously. Moreover, multiple DUpsamplings (DUPs) followed by decoded feature aggregation modules (DFAs) form the decoder of ICTNet. Specifically, the transformation and upsampling loss are shrunken while recovering features. Together with the devised encoder and decoder, the well-rounded context is captured and contributes to the inference most. Extensive experiments are conducted on the ISPRS Vaihingen, Potsdam and DeepGlobe benchmarks. Quantitative and qualitative evaluations exhibit the competitive performance of ICTNet compared to mainstream and state-of-the-art methods. Additionally, the ablation study of DFA and DUP is implemented to validate the effects.",semantic segmentation,Swin Transformer,local patterns and distant dependencies,feature aggregation,"Li, Tao","Chen, Ziqi","Wang, Xinyuan","Xu, Zhennan",well-rounded context,,,,,,,,"Lyu, Xin",,,,,,,,,,,,,,,,
Row_846,"Jiang, Jie","Lyu, Chengjin","Liu, Siying",RWSNet: a semantic segmentation network based on SegNet combined with random walk for remote sensing,INTERNATIONAL JOURNAL OF REMOTE SENSING,JAN 17 2020,36,"Semantic segmentation methods based on deep learning considerably improve the segmentation performance of remote sensing images. However, with the extensive application of high-resolution remote sensing images, additional details introduce considerable interference to the learning process for classification, thereby diminishing the accuracy of segmentation and resulting in blurry object boundaries. To address this problem, this study designed Random-Walk-SegNet (RWSNet), a semantic segmentation network based on SegNet combined with random walk. First, SegNet is used as the basic architecture with the sliding window strategy that optimizes the network output to improve the continuity and smoothness of segmentation. Second, seed regions of the random walk are selected in accordance with the classification output of SegNet. Third, the weights of the undirected graph edge are determined by fusing the gradient of the original image and probability map of SegNet. Finally, random walk is implemented on the entire image, thus reducing edge blur and realizing high-performance semantic segmentation of remote sensing images. In comparison with mainstream and other improved methods, the proposed network has lower complexity but better performance, and the algorithm is state-of-the-art and robust.",,,,,"He, Yongqiang","Hao, Xuetao",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_847,"Chen, Kaiqiang","Fu, Kun","Yan, Menglong",Semantic Segmentation of Aerial Images With Shuffling Convolutional Neural Networks,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,FEB 2018,71,"Semantic segmentation of aerial images refers to assigning one land cover category to each pixel. This is a challenging task due to the great differences in the appearances of ground objects. Many attempts have been made during the past decades. In recent years, convolutional neural networks (CNNs) have been introduced in the remote sensing field, and various solutions have been proposed to realize dense semantic labeling with CNNs. In this letter, we propose shuffling CNNs to realize semantic segmentation of aerial images in a periodic shuffling manner. This approach is a supplement to current methods for semantic segmentation of aerial images. We propose a naive version and a deeper version of this method, and both are adept at detecting small objects. Additionally, we propose a method called field-of-view (FoV) enhancement that can enhance the predictions. This method can be applied to various networks, and our experiments verify its effectiveness. The final results are further improved through an ensemble method that averages the score maps generated by the models at different checkpoints of the same network. We evaluate our models using the ISPRS Vaihingen and Potsdam data sets, and we acquire promising results using these two data sets.",Aerial images,convolutional neural networks (CNNs),deep learning,remote sensing,"Gao, Xin","Sun, Xian","Wei, Xin",,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_848,"Grau, Marc","Lontke, Alexander","Jiang, Xuemei",SELF SUPERVISED LEARNING IN REMOTE SENSING: QUANTIFYING APPROACHES EFFECTIVENESS ACROSS DOWNSTREAM TASKS,,2023,0,"In the remote sensing field, vast amounts of data are available. However, labeling such data is expensive. Self-supervised learning makes it possible to leverage unlabeled data for the training of deep neural network models. This work focuses on the effectiveness of self-supervised pretext tasks for different supervised downstream tasks. Therefore, we compare generative, contrastive, and generative-contrastive pretext tasks across classification and semantic segmentation downstream tasks. Our results show that the contrastive setup is beneficial for remote sensing image classification, whereas the generative-contrastive setup shows the best results for the semantic segmentation downstream task. Therefore, our work indicates that the choice of self-supervised pretext task is an important consideration to optimize downstream task performance.",Remote Sensing,Deep Learning,Self-Supervised Learning,Classification,"Scheibenreif, Linus",,,,Segmentation,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_849,"Chen, Suting","Wu, Chaoqun","Mukherjee, Mithun",HA-MPPNet: Height Aware-Multi Path Parallel Network for High Spatial Resolution Remote Sensing Image Semantic Seg-Mentation,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,OCT 2021,2,"Semantic segmentation of remote sensing images (RSI) plays a significant role in urban management and land cover classification. Due to the richer spatial information in the RSI, existing convolutional neural network (CNN)-based methods cannot segment images accurately and lose some edge information of objects. In addition, recent studies have shown that leveraging additional 3D geometric data with 2D appearance is beneficial to distinguish the pixels' category. However, most of them require height maps as additional inputs, which severely limits their applications. To alleviate the above issues, we propose a height aware-multi path parallel network (HA-MPPNet). Our proposed MPPNet first obtains multi-level semantic features while maintaining the spatial resolution in each path for preserving detailed image information. Afterward, gated high-low level feature fusion is utilized to complement the lack of low-level semantics. Then, we designed the height feature decode branch to learn the height features under the supervision of digital surface model (DSM) images and used the learned embeddings to improve semantic context by height feature guide propagation. Note that our module does not need a DSM image as additional input after training and is end-to-end. Our method outperformed other state-of-the-art methods for semantic segmentation on publicly available remote sensing image datasets.",remote sensing image,semantic segmentation,high spatial resolution,gated feature fusion,"Zheng, Yujie",,,,digital surface model (DSM),height features,,,,,,,,,,,,,,,,,,,,,,,
Row_850,"Bello, Inuwa Mamuda","Zhang, Ke","Su, Yu",Densely multiscale framework for segmentation of high resolution remote sensing imagery,COMPUTERS & GEOSCIENCES,OCT 2022,6,"Semantic segmentation has gained research attention in recent times, especially within the remote sensing community. The deep neural network has proven to be the most effective approach for segmentation applications due to its automatic feature extraction capability. Research results indicate that the multiscale segmentation frameworks are more suitable for high-level feature extraction, especially from complex remote sensing images. However, most existing multiscale frameworks are either complex or highly parameterized, making them inefficient for real-time remote sensing applications. In this work, we propose an accurate and highly efficient densely multiscale segmentation network specifically for real-time segmentation of remotely sensed imagery. We significantly improve the representation capability of the network by embedding its structure with the dense connection, which allows gradient to flow with ease through the network. The proposed network with few trainable parameters performed significantly on two publicly available challenging datasets, making it suitable for deployment on resource-constrained devices for real-time remote sensing applications.",Segmentation,Dense convolution,Multiscale,Neural network,"Wang, Jingyu","Aslam, Muhammad Azeem",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_851,"Xu, Yizhe","Yan, Liangliang","Jiang, Jie",EI-HCR: An Efficient End-to-End Hybrid Consistency Regularization Algorithm for Semisupervised Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,3,"Recently, remote sensing image (RSI) semantic segmentation technology has advanced greatly, with the fully supervised process achieving particularly strong performance. However, the technology depends heavily on dataset labels, leading to high annotation costs. To alleviate this problem, we propose a novel efficient end-to-end hybrid consistency regularization algorithm (EI-HCR) for the semisupervised semantic segmentation of RSI, wherein only a few labeled images and a large number of unlabeled images are effectively used. First, we devise data perturbation (DP) consistency regularization (CR), which includes a data mix-up method to combine unlabeled and labeled images. Then, we employ teacher and student networks to conduct model perturbation (MP) CR. Both segmentation results are regarded as pseudo-labels for each other. In the end, the semisupervised loss is composed of DP and MP consistency loss and supervises network training along with the fully supervised loss. More importantly, we first combine the characteristics of knowledge distillation to make the student network more lightweight, efficiently reducing the model inference time. Experimental results demonstrate the effectiveness of EI-HCR on the ISPRS Vaihingen and Massachusetts Buildings datasets. With only 5% of the labeled images, EI-HCR can achieve the same accuracy as the fully supervised training with 50% of the labeled images, and the number of student model parameters is only 9.64 M, indicating the method's great advantages over other algorithms.",Consistency regularization (CR),knowledge distillation,remote sensing image (RSI),semantic segmentation,,,,,semisupervised learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_852,"Wei, Yao","Zhang, Kai","Ji, Shunping",Simultaneous Road Surface and Centerline Extraction From Large-Scale Remote Sensing Images Using CNN-Based Segmentation and Tracing,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,DEC 2020,96,"Accurate and up-to-date road maps are of great importance in a wide range of applications. Unfortunately, automatic road extraction from high-resolution remote sensing images remains challenging due to the occlusion of trees and buildings, discriminability of roads, and complex backgrounds. To address these problems, especially road connectivity and completeness, in this article, we introduce a novel deep learning-based multistage framework to accurately extract the road surface and road centerline simultaneously. Our framework consists of three steps: boosting segmentation, multiple starting points tracing, and fusion. The initial road surface segmentation is achieved with a fully convolutional network (FCN), after which another lighter FCN is applied several times to boost the accuracy and connectivity of the initial segmentation. In the multiple starting points tracing step, the starting points are automatically generated by extracting the road intersections of the segmentation results, which then are utilized to track consecutive and complete road networks through an iterative search strategy embedded in a convolutional neural network (CNN). The fusion step aggregates the semantic and topological information of road networks by combining the segmentation and tracing results to produce the final and refined road segmentation and centerline maps. We evaluated our method utilizing three data sets covering various road situations in more than 40 cities around the world. The results demonstrate the superior performance of our proposed framework. Specifically, our method's performance exceeded the other methods by 7% and 40% for the connectivity indicator for road surface segmentation and for the completeness indicator for centerline extraction, respectively.",Roads,Image segmentation,Remote sensing,Boosting,,,,,Feature extraction,Surface topography,Semantics,Convolutional neural network (CNN),remote sensing images,road extraction,segmentation,tracing,,,,,,,,,,,,,,,,,
Row_853,"Ji, Wei","Fang, Zhou","Feng, Decai",Semantic segmentation of Arctic Sea ice in summer from remote sensing satellite images based on BAU-NET,JOURNAL OF APPLIED REMOTE SENSING,OCT 2022,1,"To effectively solve the accurate identification of gray ice, melt ponds water, floe, brash ice, and thin ice in the melting state of the Arctic Sea ice during summer, we propose adding a batch normalization layer and adaptive moment estimation optimizer of a U-NET (BAU-NET) method for Arctic Sea ice semantic segmentation in summer from remote sensing satellite optical images. The U-NET network structure is optimized to 18 convolution layers, and a batch normalization layer and a nonlinear activation function rectified linear unit are added behind each convolution layer. Then the hyper-parameters of the network structure are adjusted. The cross-entropy loss function based on SOFTMAX and L2 regularization are used in training the model, and the adaptive moment estimation optimizer is used for iterative training until the error convergence. The experimental results show that the accuracy, precision, recall, and F1 score of sea ice extraction results reaches more than 97.26%. Compared with the DeepLabv3 and U-Net methods, the sea ice prediction time efficiency is improved by 90.99s and 1.57s, respectively, and the accuracy is improved by 6.59% and 8.05%, respectively, which indicate that the sea ice prediction time efficiency and accuracy of the BAU-NET method are significantly improved. (c) 2022 Society of Photo-Optical Instrumentation Engineers (SPIE)",deep learning,batch normalization layer and adaptive moment estimation optimizer of U-NET,remote sensing satellite images,arctic sea ice in summer,"Ge, Xizhi",,,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_854,"Zhang, Liangji","Yang, Zaichun","Zhou, Guoxiong",MDMASNet: A dual-task interactive semi-supervised remote sensing image segmentation method,SIGNAL PROCESSING,NOV 2023,6,"Remote sensing image (RSIs) segmentation is widely used in urban planning, natural disaster detection and many other fields. Compared with natural scene images, RSIs have higher resolution, complex imaging, and diverse object shapes and sizes, while semantic segmentation methods based on deep learning often require many data labels. In this paper, we propose a semi-supervised RSIs segmentation network with multi-scale deformable threshold feature extraction module and mixed attention (MDMANet). First, a pyramid ensemble structure is used, which incorporates deformable convolution and bole convolution, to extract features of objects with different shapes and sizes and reduce the influence of redundant features. Meanwhile, a mixed attention (MA) is proposed to aggregate long-range contextual relationships and fuse low-level features with high-level features. Second, an FCN-based full convolution discriminator task network is designed to help evaluate the feasibility of unlabeled image prediction results. We performed experimental validation on three datasets, and the results show that MDMANet segmentation provides more significant improvement in accuracy and better generalization than existing segmentation networks. & COPY; 2023 Published by Elsevier B.V.",Semi-supervised learning,GAN,Attention mechanism,Semantic segmentation,"Lu, Chao","Chen, Aibin","Ding, Yao","Wang, Yanfeng",,,,,,,,,"Li, Liujun",,"Cai, Weiwei",,,,,,,,,,,,,,
Row_855,"Zhang, Yunfeng","Chi, Mingmin",,Mask-R-FCN: A Deep Fusion Network for Semantic Segmentation,IEEE ACCESS,2020,20,"Remote sensing image classification plays a significant role in urban applications, precision agriculture, water resource management. The task of classification in the field of remote sensing is to map raw images to semantic maps. Typically, fully convolutional network (FCN) is one of the most effective deep neural networks for semantic segmentation. However, small objects in remote sensing images can be easily overlooked and misclassified as the majority label, which is often the background of the image. Although many works have attempted to deal with this problem, making a trade-off between background semantics and edge details is still a problem. This is mainly because they are based on a single neural network model. To deal with this problem, a convolutional deep network with regions (R-CNN), which is highly effective for object detection is leveraged as a complementary component in our work. A learning-based and decision-level strategy is applied to fuse both semantic maps from a semantic model and an object detection model. The proposed network is referred to as Mask-R-FCN. Experimental results on real remote sensing images from the Zurich dataset, Gaofen Image Dataset (GID), and DataFountain2017 show that the proposed network can obtain higher accuracy than single deep neural networks and other machine learning algorithms. The proposed network achieved better average accuracies, which are approximately 2% higher than those of any other single deep neural networks on the Zurich, GID, and DataFoundation2017 datasets.",Deep fusion,deep semantic segmentation,fully convolutional network,object detection,,,,,remote sensing,,,,,,,,,,,,,,,,,,,,,,,,
Row_856,"Zhang, Xiangrong","Weng, Zhenhang","Zhu, Peng",ESDINet: Efficient Shallow-Deep Interaction Network for Semantic Segmentation of High-Resolution Aerial Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"Semantic segmentation of high-resolution remote sensing images is essential in many fields. Nevertheless, in practical applications, constrained by limited computational resources and complex network structures, many advanced models on semantic segmentation often fail to show efficient performance, prompting research on lightweight models. For lightweight semantic segmentation models, the two-branch architecture has been shown to work well in speed and performance. However, such two-branch architectures usually do not utilize enough information for shallow structures to efficiently provide richer multiscale information for the two branches. The lightweight modules it uses are difficult to extract the global context information of the features effectively. Compared with the current advanced semantic segmentation models, lightweight models still have some differences in performance. In order to solve these problems, we propose a new lightweight dual-branch architecture efficient shallow-deep interaction network (ESDINet), which can quickly extract low-level spatial and high-level semantic information of images through the detail branch and semantic branch. Specifically, we have constructed an efficient double-branch structure with shallow and deep different interactions to achieve multiscale information interaction. At the same time, we optimize the semantic branch and propose a new linear attention block to effectively improve the global perception of the semantic branch. We performed extensive experiments and the results show that our model achieves a good balance between segmentation accuracy and inference speed. In particular, ESDINet achieves 82.03% mean intersection over union (mIoU) on the Vaihingen test set, while the proposed model achieves an inference speed of 116 frames/s (FPS) for 512 chi 512 inputs on a single NVIDIA GTX 2080Ti GPU.",High-resolution remote sensing image,lightweight,multiscale,semantic segmentation,"Han, Xiao","Zhu, Jin","Jiao, Licheng",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_857,"Chen, Canyu","Zhu, Guobin","Chen, Xiliang",Wetland Scene Segmentation of Remote Sensing Images Based on Lie Group Feature and Graph Cut Model,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2025,0,"Given the increasingly severe destruction of wetlands in recent years, research and monitoring for wetland protection are urgently needed. However, wetland monitoring still faces significant challenges such as high manual costs and low image monitoring accuracy, primarily due to the complex composition of wetland terrain and diverse spectral characteristics, especially located in remote suburban areas. With their rich semantic and spectral features, studying wetlands as scenes in remote sensing images and segmenting them effectively can address these challenges. To address the issue of low segmentation accuracy on remote sensing image with GrabCut, this study proposes a remote sensing image segmentation extraction method, named SceneCut, based on feature extraction and multifeature joint graph cuts. The method consists of three parts: feature extraction, feature fusion, and multifeature joint graph cuts. In the first part, image features are calculated using a sliding window approach. In the second part, region covariance calculation and matrix eigenvalue decomposition are used to generate a multichannel feature image from the extracted feature vectors. Finally, in the third part, GrabCut is applied to the multichannel image to perform graph cuts, considering multiple features, and the segmentation result is mapped back to the original image to generate the segmented image. Experimental analysis and field exploration validation of wetland images conducted in this study demonstrate that compared to algorithms that only use RGB features for segmentation or those that do not consider the relationship between multiple features, SceneCut performs well in wetland scene extraction, especially in extracting boundaries with significant blending.",Wetlands,Feature extraction,Image segmentation,Remote sensing,,,,,Monitoring,Semantics,Accuracy,Object oriented modeling,Lie groups,Deep learning,Graph cut model,lie group,,,,,,remote sensing,scene segmentation,wetland,,,,,,,,,
Row_858,"Lian, Renbao","Huang, Liqin",,Weakly Supervised Road Segmentation in High-Resolution Remote Sensing Images Using Point Annotations,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,19,"Road segmentation methods based on deep neural networks have achieved great success in recent years, but creating accurate pixel-wise training labels is still a boring and expensive task, especially for large-scale high-resolution remote sensing images (HRSIs). Inspired by the stacked hourglass model for human joints detection, we propose a weakly supervised road segmentation method using point annotations in this article. First, we design a patch-based deep convolutional neural network (DCNN) model for road seeds and background points detection and train the model using point annotations. Then, in the process of road segmentation, the DCNN model detects a series of road and background points that are used to train a Support Vector Machine Classifier (SVC) for classifying each pixel into road or nonroad. According to the local geometry of road and the inaccurate classification of SVC, a multiscale and multidirection Gabor filter (MMGF) is put forward to estimate the road potential. Finally, the active contour model based on local binary fitting energy (LBF-Snake) is introduced to extract the road regions from the inhomogeneous road potential. Qualitative and quantitative comparisons show that our method achieves results close to the fully supervised semantic methods without considering the annotation cost and outperforms them given a fixed budget.",Roads,Image segmentation,Annotations,Semantics,,,,,Training,Remote sensing,Estimation,Active contour model,Gabor filter,point annotation,remote sensing images,road segmentation,,,,,,weakly supervised segmentation,,,,,,,,,,,
Row_859,"Huan, Hai","Liu, Yuan","Xie, Yaqin",MAENet: Multiple Attention Encoder-Decoder Network for Farmland Segmentation of Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,14,"With the rapid development of computer vision, semantic segmentation as an important part of the technology has made some achievements in different applications. However, in the farmland segmentation scenario of remote sensing images, the capability of common semantic segmentation methods in restoring the farmland edge and identifying narrow farmland ridges needs to be improved. Therefore, in this letter a semantic segmentation method-multiple attention encoder-decoder network (MAENet)-for farmland segmentation is proposed. The design of a dual-pooling efficient channel attention (DPECA) module and its embedment in the backbone to improve the efficiency of feature extraction is described; secondly, a dual-feature attention (DFA) module is proposed to extract contextual information of high-level features; finally, a global-guidance information upsample (GIU) module is added to the decoder to reduce the influence of redundant information on feature fusion. We use three self-made farmland image datasets representing UAV data to train MAENet and compare them with other methods. The results show that the performances of segmentation and generalization of MAENet are improved compared with other methods. The MIoU and Kappa coefficient in the farmland multi-classification test set can reach 93.74% and 96.74%.",Feature extraction,Image segmentation,Semantics,Data mining,"Wang, Chao","Xu, Dongdong","Zhang, Yi",,Convolution,Remote sensing,Decoding,Attention module,farmland segmentation,feature fusion,pyramid,UAV images,,,,,,,,,,,,,,,,,
Row_860,"Li, Shuo","Liu, Fang","Jiao, Licheng",Mask-Guided Correlation Learning for Few-Shot Segmentation in Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,4,"Few-shot segmentation aims to segment specific objects in a query image based on a few densely annotated images and has been extensively studied in recent years. In remote sensing, image segmentation faces challenges such as less training data, large intraclass diversity, and low foreground-background contrast. In this work, we propose a novel few-shot segmentation method in remote sensing imagery based on mask-guided correlation learning (MGCL) to alleviate the above challenges. In our MGCL, a novel mask-guided feature enhancement (MGFE) module is proposed, which makes features have intramask consistency by leveraging oversegmented masks. In order to enhance the contrast between foreground and background, a novel foreground-background correlation (FBC) module is proposed, which enhances background correlation representation by learning foreground correlation and background correlation separately. Furthermore, a novel mask-guided correlation decoder (MGCD) module is proposed to guide the decoder to focus on the consistency within the mask, thereby learning how to segment complete objects and improving segmentation accuracy. Sufficient experiments on the iSAID-5(i )and DLRSD-5(i )datasets show that our MGCL outperforms all comparative methods. In particular, in the one-shot setting of the iSAID-5(i )dataset, we achieve an mIoU of 39.92 based on ResNet50, which is an improvement of 4.25 over the state-of-the-art (SOAT) method. The visualization of features before and after the MGFE module further concretely demonstrates the motivation and advantages of our MGCL. The code is available at https://github.com/LiShuo1001/MGCL.",Image segmentation,Correlation,Remote sensing,Feature extraction,"Liu, Xu","Chen, Puhua","Li, Lingling",,Prototypes,Accuracy,Semantics,Correlation learning,few-shot learning,few-shot segmentation,segment anything,semantic segmentation,,,,,,,,,,,,,,,,,
Row_861,Wang Xi,Yu Ming,Ren Hong-e,Remote sensing image semantic segmentation combining UNET and FPN,CHINESE JOURNAL OF LIQUID CRYSTALS AND DISPLAYS,MAR 2021,10,"The traditional remote sensing image segmentation method is inefficient and the segmentation fineness is not enough in complex scenes. The UNET model is well-known for its good segmentation effect, but it does not perform well for the smaller objects contained in the image and the edge segmentation of larger objects. In order to solve this problem, a method combining UNET structure with FPN structure is proposed in this paper to improve the ability of UNET model to integrate multi-scale information. At the same time, the BLR loss function which can better capture the edge of the target edge is used to improve the segmentation effect of UNET model on the target boundary. The experimental results show that the method used in this paper effectively improves the accuracy of semantic segmentation and alleviates the problem of poor edge segmentation of small-scale targets and large-scale targets. The target edge segmentation can be more accurate to achieve better segmentation results.",deep learning,UNET,FPN,BLR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_862,"Sun, Weiwei","Wang, Ruisheng",,Fully Convolutional Networks for Semantic Segmentation of Very High Resolution Remotely Sensed Images Combined With DSM,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,MAR 2018,225,"Recently, approaches based on fully convolutional networks (FCN) have achieved state-of-the-art performance in the semantic segmentation of very high resolution (VHR) remotely sensed images. One central issue in this method is the loss of detailed information due to downsampling operations in FCN. To solve this problem, we introduce the maximum fusion strategy that effectively combines semantic information from deep layers and detailed information from shallow layers. Furthermore, this letter develops a powerful backend to enhance the result of FCN by leveraging the digital surface model, which provides height information for VHR images. The proposed semantic segmentation scheme has achieved an overall accuracy of 90.6% on the ISPRS Vaihingen benchmark.",Fully convolutional networks (FCN),deep learning,semantic segmentation,remote sensing,,,,,very high resolution (VHR),,,,,,,,,,,,,,,,,,,,,,,,
Row_863,"Ma, Xianping","Zhang, Xiaokang","Pun, Man-On",MSFNET: MULTI-STAGE FUSION NETWORK FOR SEMANTIC SEGMENTATION OF FINE-RESOLUTION REMOTE SENSING DATA,,2022,3,"This work proposes a Multi-Stage Fusion Network (MSFNet) for semantic segmentation of fine-resolution remote sensing data by exploiting a multi-stage transformer architecture. The proposed MSFNet fuses information of different scales and modalities using a multi-stage scheme based on cross-attention mechanism. More specifically, the proposed MSFNet is composed of two Multi-Level Transformers (ML-Trans), one Crossmodal Fusion Transformer (CFTrans) and one Global-Context Augmented Transformer (GCATrans). DMLTrans and CFTrans are designed to fuse features in different levels in each modality and high-level crossmodal abstract features, respectively, whereas GCATrans enhances the fusion feature of the main modal. Capitalizing on MSFNet, this work demonstrates the fusion of red-green-blue (RGB) remote sensing images and digital surface model (DSM) data. Extensive experiments on large-scale fine-resolution remote sensing data sets, namely the ISPRS Vaihingen, confirm the excellent performance of the proposed architecture as compared to conventional multimodal methods.",,,,,"Liu, Ming",,,,,,,,,,,,,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),,,,,,,,,,,,,,,
Row_864,"Ma, Xianping","Zhang, Xiaokang","Wang, Zhiguo",Unsupervised Domain Adaptation Augmented by Mutually Boosted Attention for Semantic Segmentation of VHR Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,31,"This work investigates unsupervised domain adaptation (UDA)-based semantic segmentation of very high-resolution (VHR) remote sensing (RS) images from different domains. Most existing UDA methods resort to generative adversarial networks (GANs) to cope with the domain shift problem caused by the discrepancies across different domains. However, these GAN-based UDA methods directly align two domains in the appearance, latent, or output space based on convolutional neural networks (CNNs), making them ineffective in exploiting long-range dependencies across the high-level feature maps derived from different domains. Unfortunately, such high-level features play an essential role in characterizing RS images with complex content. To circumvent this obstacle, a mutually boosted attention transformer (MBATrans) is proposed to capture cross-domain dependencies of semantic feature representations in this work. Compared with conventional UDA methods, MBATrans can significantly reduce domain discrepancies by capturing transferable features using global attention. More specifically, MBATrans utilizes a novel mutually boosted attention (MBA) module to align cross-domain feature maps while enhancing domain-general features. Furthermore, a novel GAN-based network with improved discriminative capability is devised by integrating an additional discriminator to learn domain-specific features. Extensive experiments on two large-scale VHR RS datasets, namely, International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Vaihingen, confirm the superior performance of the proposed MBATrans-augmented GAN (MBATA-GAN) architecture. The source code in this work is available at https://github.com/sstary/SSRS.",Generative adversarial networks (GANs),mutually boosted attention (MBA),remote sensing (RS) image,unsupervised domain adaptation (UDA),"Pun, Man-On",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_865,"Guo, Rui","Liu, Jianbo","Li, Na",Pixel-Wise Classification Method for High Resolution Remote Sensing Imagery Using Deep Neural Networks,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,MAR 2018,34,"Considering the classification of high spatial resolution remote sensing imagery, this paper presents a novel classification method for such imagery using deep neural networks. Deep learning methods, such as a fully convolutional network (FCN) model, achieve state-of-the-art performance in natural image semantic segmentation when provided with large-scale datasets and respective labels. To use data efficiently in the training stage, we first pre-segment training images and their labels into small patches as supplements of training data using graph-based segmentation and the selective search method. Subsequently, FCN with atrous convolution is used to perform pixel-wise classification. In the testing stage, post-processing with fully connected conditional random fields (CRFs) is used to refine results. Extensive experiments based on the Vaihingen dataset demonstrate that our method performs better than the reference state-of-the-art networks when applied to high-resolution remote sensing imagery classification.",high resolution imagery,remote sensing,convolution neural network,semantic segmentation,"Liu, Shibin","Chen, Fu","Cheng, Bo","Duan, Jianbo",data augmentation,deep learning,,,,,,,"Li, Xinpeng",,"Ma, Caihong",,,,,,,,,,,,,,
Row_866,"He, Shuang","Lu, Xia","Gu, Jason",RSI-Net: Two-Stream Deep Neural Network for Remote Sensing Images-Based Semantic Segmentation,IEEE ACCESS,2022,16,"For semantic segmentation of remote sensing images (RSI), trade-off between representation power and location accuracy is quite important. How to get the trade-off effectively is an open question, where current approaches of utilizing very deep models result in complex models with large memory consumption. In contrast to previous work that utilizes dilated convolutions or deep models, we propose a novel two-stream deep neural network for semantic segmentation of RSI (RSI-Net) to obtain improved performance through modeling and propagating spatial contextual structure effectively and a decoding scheme with image-level and graph-level combination. The first component explicitly models correlations between adjacent land covers and conduct flexible convolution on arbitrarily irregular image regions by using graph convolutional network, while densely connected atrous convolution network (DenseAtrousCNet) with multi-scale atrous convolution can expand the receptive fields and obtain image global information. Extensive experiments are implemented on the Vaihingen, Potsdam and Gaofen RSI datasets, where the comparison results demonstrate the superior performance of RSI-Net in terms of overall accuracy (91.83%, 93.31% and 93.67% on three datasets, respectively), F1 score (90.3%, 91.49% and 89.35% on three datasets, respectively) and kappa coefficient (89.46%, 90.46% and 90.37% on three datasets, respectively) when compared with six state-of-the-art RSI semantic segmentation methods.",Semantics,Image segmentation,Feature extraction,Decoding,"Tang, Haitong","Yu, Qin","Liu, Kaiyue","Ding, Haozhou",Convolutional neural networks,Neural networks,Streaming media,Convolutional neural network (CNN),graph convolutional network (GCN),encoder,decoder,feature fusion,"Chang, Chunqi",,"Wang, Nizhuan",,,semantic segmentation,,,,,,,,,,,
Row_867,"Zhang, Tong","Zhuang, Yin","Wang, Guanqun",Multiscale Semantic Fusion-Guided Fractal Convolutional Object Detection Network for Optical Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,44,"Optical remote sensing object detection is a challenging task, because of the complex background interference, ambiguous appearances of tiny objects, densely arranged circumstances, and multiclass object with vaster scale variances and irregular aspect ratios. The performance of object detection is seriously restricted. Thus, in this article, inspired by the anchor-free object detection framework, and aiming to solve these difficulties to improve the optical remote sensing object detection performance, a powerful one-stage detector of multiscale semantic fusion-guided fractal convolution network (MSFC-Net) is proposed. First, facing these strong-coupled semantic relations in each complex scene, a compound semantic feature fusion (CSFF) way is designed for generating an effective semantic description, which is a benefit to pixel-wise object center point interpretation. In addition, it can be easily extended into a semantic segmentation task. Second, in view of accurate multiclass pixel-wise center point predictions based on an effective compound semantic description, a novel fractal convolution (FC) regression layer is designed, which adaptively achieves the regression of multiscale bounding boxes (bboxes) with irregular aspect ratio under no priori information. Third, related to the set up FC regression layer, a specific hybrid loss is designed to make the proposed MSFC-Net converge better. Finally, the extensive experiments on challenge data sets of large-scale dataset for object detection in aerial images (DOTA) and object detection in optical remote sensing images (DIOR) datasets are carried out, and comparisons indicate that the proposed MSFC-Net can perform the remarkable performance than other state-of-the-art one-stage detectors, as it can reach 80.26% mean average precision (mAP) and 79.33% mF1 on DOTA and 70.08% mAP and 73.45% mF1 on DIOR. Then, our work is available at https://github.com/ZhAnGToNG1/MSFC-Net.",Object detection,Detectors,Semantics,Remote sensing,"Dong, Shan","Chen, He","Li, Lianlin",,Optical imaging,Optical detectors,Feature extraction,Anchor-free,compound semantic description,fractal convolution (FC),multiscale feature fusion,object detection,,,,,,optical remote sensing,,,,,,,,,,,
Row_868,"Li, Xuemei","Wang, Xing","Ye, Huping",Multinetwork Algorithm for Coastal Line Segmentation in Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"The demarcation between the sea and the land, commonly referred to as the coastline, is of paramount importance for the dynamic monitoring of its alterations. This monitoring is essential for the effective utilization of marine resources and the conservation of the ecological environment. Addressing the challenges posed by the extensive expanse of coastal lines, which can complicate their acquisition and processing, this study utilizes remote sensing imagery to introduce an algorithm for coastal line segmentation. The algorithm integrates multiple networks to enhance its effectiveness. Innovations encompass the development of an extraction algorithm for coastal lines that are as follows. First, utilize an attention-guided conditional generative adversarial network (AC-GAN) model, which redefines the task of image segmentation by framing it as a style transformation problem. Second, a strategy for coastal line segmentation utilizes Dense Swin Transformer Unet (DSTUnet) to construct a densely structured model. This approach integrates Transformer to prioritize focal regions, thereby enhancing image and semantic interpretation. Third, a transfer learning framework is proposed to integrate multiple features, leveraging the strengths of different networks to achieve accurate segmentation of coastal lines. The study introduced two datasets, and the experimental results confirm that parallel network configurations and asymmetric weighting are superior in achieving optimal results, with an area overlap measure (AOM) score of 85%, outperforming the Unet by 5%.",Sea measurements,Image segmentation,Feature extraction,Remote sensing,"Qiu, Shi","Liao, Xiaohan",,,Transformers,Training,Generators,Coastal line,generative adversarial network (GAN),remote sensing,segmentation,transformer,,,,,,Unet,,,,,,,,,,,
Row_869,"Ding, Lei","Guo, Haitao","Liu, Sicong",Bi-Temporal Semantic Reasoning for the Semantic Change Detection in HR Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,89,"Semantic change detection (SCD) extends the multiclass change detection (MCD) task to provide not only the change locations but also the detailed land-cover/land-use (LCLU) categories before and after the observation intervals. This fine-grained semantic change information is very useful in many applications. Recent studies indicate that the SCD can be modeled through a triple-branch convolutional neural network (CNN), which contains two temporal branches and a change branch. However, in this architecture, the communications between the temporal branches and the change branch are insufficient. To overcome the limitations in existing methods, we propose a novel CNN architecture for the SCD, where the semantic temporal features are merged in a deep CD unit. Furthermore, we elaborate on this architecture to reason the bi-temporal semantic correlations. The resulting bi-temporal semantic reasoning network (Bi-SRNet) contains two types of semantic reasoning blocks to reason both single-temporal and cross-temporal semantic correlations, as well as a novel loss function to improve the semantic consistency of change detection results. Experimental results on a benchmark dataset show that the proposed architecture obtains significant accuracy improvements over the existing approaches, while the added designs in the Bi-SRNet further improve the segmentation of both semantic categories and the changed areas. The codes in this article are accessible at https://github.com/ggsDing/Bi-SRNet.",Semantics,Convolutional neural networks,Task analysis,Feature extraction,"Mou, Lichao","Zhang, Jing","Bruzzone, Lorenzo",,Image segmentation,Data models,Correlation,Change detection (CD),convolutional neural network (CNN),remote sensing,semantic change detection (SCD),semantic segmentation (SS),,,,,,,,,,,,,,,,,
Row_870,"Cui, Wei","Feng, Zhanyun","Chen, Jiale",Long-Tailed Effect Study in Remote Sensing Semantic Segmentation Based on Graph Kernel Principles,REMOTE SENSING,APR 2024,1,"The performance of semantic segmentation in remote sensing, based on deep learning models, depends on the training data. A commonly encountered issue is the imbalanced long-tailed distribution of data, where the head classes contain the majority of samples while the tail classes have fewer samples. When training with long-tailed data, the head classes dominate the training process, resulting in poorer performance in the tail classes. To address this issue, various strategies have been proposed, such as resampling, reweighting, and transfer learning. However, common resampling methods suffer from overfitting to the tail classes while underfitting the head classes, and reweighting methods are limited in the extreme imbalanced case. Additionally, transfer learning tends to transfer patterns learned from the head classes to the tail classes without rigorously validating its generalizability. These methods often lack additional information to assist in the recognition of tail class objects, thus limiting performance improvements and constraining generalization ability. To tackle the abovementioned issues, a graph neural network based on the graph kernel principle is proposed for the first time. By leveraging the graph kernel, structural information for tail class objects is obtained, serving as additional contextual information beyond basic visual features. This method partially compensates for the imbalance between tail and head class object information without compromising the recognition accuracy of head classes objects. The experimental results demonstrate that this study effectively addresses the poor recognition performance of small and rare targets, partially alleviates the issue of spectral confusion, and enhances the model's generalization ability.",graph neural network,long-tailed distribution,graph kernel,remote sensing,"Xu, Xing","Tian, Yueling","Zhao, Huilin","Wang, Chenglei",,,,,,,,,,,,,,,,,,,,,,,,,
Row_871,"Zang, Ning","Cao, Yun","Wang, Yuebin",Land-Use Mapping for High-Spatial Resolution Remote Sensing Image Via Deep Learning: A Review,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,39,"Land-use mapping (LUM) using high-spatial resolution remote sensing images (HSR-RSIs) is a challenging and crucial technology. However, due to the characteristics of HSR-RSIs, such as different image acquisition conditions and massive, detailed information, and performing LUM faces unique scientific challenges. With the emergence of new deep learning (DL) algorithms in recent years, methods to LUM with DL have achieved huge breakthroughs, which offer novel opportunities for the development of LUM for HSR-RSIs. This article aims to provide a thorough review of recent achievements in this field. Existing high spatial resolution datasets in the research of semantic segmentation and single-object segmentation are presented first. Next, we introduce several basic DL approaches that are frequently adopted for LUM. After reviewing DL-based LUM methods comprehensively, which highlights the contributions of researchers in the field of LUM for HSR-RSIs, we summarize these DL-based approaches based on two LUM criteria. Individually, the first one has supervised learning, semisupervised learning, or unsupervised learning, while another one is pixel-based or object-based. We then briefly review the fundamentals and the developments of the development of semantic segmentation and single-object segmentation. At last, quantitative results that experiment on the dataset of ISPRS Vaihingen and ISPRS Potsdam are given for several representative models such as fully convolutional network (FCN) and U-Net, following up with a comparison and discussion of the results.",Semantics,Meters,Buildings,Satellites,"Huang, Bo","Zhang, Liqiang","Mathiopoulos, P. Takis",,Remote sensing,Image segmentation,Spatial resolution,Deep learning (DL),high-spatial resolution remote sensing images (HSR-RSIs),land-use mapping (LUM),semantic segmentation,,,,,,,,,,,,,,,,,,
Row_872,"Lu, Tingyu","Gao, Meixiang","Wang, Lei",Crop classification in high-resolution remote sensing images based on multi-scale feature fusion semantic segmentation model,FRONTIERS IN PLANT SCIENCE,AUG 1 2023,7,"The great success of deep learning in the field of computer vision provides a development opportunity for intelligent information extraction of remote sensing images. In the field of agriculture, a large number of deep convolutional neural networks have been applied to crop spatial distribution recognition. In this paper, crop mapping is defined as a semantic segmentation problem, and a multi-scale feature fusion semantic segmentation model MSSNet is proposed for crop recognition, aiming at the key problem that multi-scale neural networks can learn multiple features under different sensitivity fields to improve classification accuracy and fine-grained image classification. Firstly, the network uses multi-branch asymmetric convolution and dilated convolution. Each branch concatenates conventional convolution with convolution nuclei of different sizes with dilated convolution with different expansion coefficients. Then, the features extracted from each branch are spliced to achieve multi-scale feature fusion. Finally, a skip connection is used to combine low-level features from the shallow network with abstract features from the deep network to further enrich the semantic information. In the experiment of crop classification using Sentinel-2 remote sensing image, it was found that the method made full use of spectral and spatial characteristics of crop, achieved good recognition effect. The output crop classification mapping was better in plot segmentation and edge characterization of ground objects. This study can provide a good reference for high-precision crop mapping and field plot extraction, and at the same time, avoid excessive data acquisition and processing.",remote sensing,crop classification,deep learning,convolutional neural network,,,,,multi-scale feature,,,,,,,,,,,,,,,,,,,,,,,,
Row_873,"Wu, Hao","Dong, Hongli","Wang, Zhibao",SEMANTIC SEGMENTATION OF OIL WELL SITES USING SENTINEL-2 IMAGERY,,2023,1,"The number and geographical location of oil well sites can reflect the local oil production situation and there is a growing interest in automatically identifying oil well sites from remote sensing images. Traditionally, visual interpretation was employed to extract oil well sites locations from remotely sensing images. However, this approach is time-consuming and heavily dependent on domain experts. Advancements in remote sensing satellite technology and the widespread use of deep learning algorithms have enabled the automated extraction of oil well sites from remote sensing images. In this paper, we established the Northeast Petroleum University Oil Well Sites Dataset Version 1.0 (NEPU-OWS V1.0), and to evaluate its usability by comparing several different deep learning models based on semantic segmentation algorithms for optical remote sensing images. Experimental results show that current advanced deep learning models achieve high accuracy on this dataset, demonstrating great potential for remote sensing detection in oil well sites.",Sentinel 2,semantic segmentation,oil well sites,deep learning,"Bai, Lu","Huo, Fengcai","Tao, Jinhua","Chen, Liangfu",,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_874,"He, Chen","Liu, Yalan","Wang, Dacheng",Automatic Extraction of Bare Soil Land from High-Resolution Remote Sensing Images Based on Semantic Segmentation with Deep Learning,REMOTE SENSING,MAR 2023,7,"Accurate monitoring of bare soil land (BSL) is an urgent need for environmental governance and optimal utilization of land resources. High-resolution imagery contains rich semantic information, which is beneficial for the recognition of objects on the ground. Simultaneously, it is susceptible to the impact of its background. We propose a semantic segmentation model, Deeplabv3+-M-CBAM, for extracting BSL. First, we replaced the Xception of Deeplabv3+ with MobileNetV2 as the backbone network to reduce the number of parameters. Second, to distinguish BSL from the background, we employed the convolutional block attention module (CBAM) via a combination of channel attention and spatial attention. For model training, we built a BSL dataset based on BJ-2 satellite images. The test result for the F1 of the model was 88.42%. Compared with Deeplabv3+, the classification accuracy improved by 8.52%, and the segmentation speed was 2.34 times faster. In addition, compared with the visual interpretation, the extraction speed improved by 11.5 times. In order to verify the transferable performance of the model, Jilin-1GXA images were used for the transfer test, and the extraction accuracies for F1, IoU, recall and precision were 86.07%, 87.88%, 87.00% and 95.80%, respectively. All of these experiments show that Deeplabv3+-M-CBAM achieved efficient and accurate extraction results and a well transferable performance for BSL. The methodology proposed in this study exhibits its application value for the refinement of environmental governance and the surveillance of land use.",bare soil land,high-resolution remote sensing imagery,semantic segmentation,deep learning,"Liu, Shufu","Yu, Linjun","Ren, Yuhuan",,Deeplabv3+,CBAM,,,,,,,,,,,,,,,,,,,,,,,
Row_875,"Hasan, Kazi Rakib","Tuli, Anamika Biswas","Khan, Md Al-Masrur",Deep-Learning-Based Semantic Segmentation for Remote Sensing: A Bibliometric Literature Review,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,3,"Deep learning (DL) has emerged as a powerful technique for a wide range of computer vision applications. Consequently, DL is also being adopted to process geospatial and remote sensing (RS) images. As these methods are sporadic over different studies, many review papers have also been published to gather the approaches and summarize the existing models in this field. However, a state-of-the-art review paper is still scarce in this field that will present a bibliometric analysis as well as a critical analysis of the recent works. Therefore, this article aims to spur the researchers with a bibliometric analysis to identify the current research trend. As a research sample, in total, 281 related papers were collected from the Web of Science source, and bibliometric analysis was accomplished using VOSviewer software. Among the collection of associated works from the database, 28 papers were selected according to the defined criteria for detailed analysis. Besides this, a few research questions were generated to extract necessary information from the literature for extracting the pros and cons of the selected works. DL techniques were applied in these works and achieved results. Furthermore, the papers were also categorized based on the addressed RS application domain.",Bibliometric analysis,deep learning,remote sensing (RS),segmentation,"Kee, Seong-Hoon","Samad, Md Abdus","Nahid, Abdullah-Al",,VOSviewer,,,,,,,,,,,,,,,,,,,,,,,,
Row_876,"Zhao, Junqi","Du, Dongsheng","Chen, Lifu",HA-Net for Bare Soil Extraction Using Optical Remote Sensing Images,REMOTE SENSING,AUG 2024,0,"Bare soil will cause soil erosion and contribute to air pollution through the generation of dust, making the timely and effective monitoring of bare soil an urgent requirement for environmental management. Although there have been some researches on bare soil extraction using high-resolution remote sensing images, great challenges still need to be solved, such as complex background interference and small-scale problems. In this regard, the Hybrid Attention Network (HA-Net) is proposed for automatic extraction of bare soil from high-resolution remote sensing images, which includes the encoder and the decoder. In the encoder, HA-Net initially utilizes BoTNet for primary feature extraction, producing four-level features. The extracted highest-level features are then input into the constructed Spatial Information Perception Module (SIPM) and the Channel Information Enhancement Module (CIEM) to emphasize the spatial and channel dimensions of bare soil information adequately. To improve the detection rate of small-scale bare soil areas, during the decoding stage, the Semantic Restructuring-based Upsampling Module (SRUM) is proposed, which utilizes the semantic information from input features and compensate for the loss of detailed information during downsampling in the encoder. An experiment is performed based on high-resolution remote sensing images from the China-Brazil Resources Satellite 04A. The results show that HA-Net obviously outperforms several excellent semantic segmentation networks in bare soil extraction. The average precision and IoU of HA-Net in two scenes can reach 90.9% and 80.9%, respectively, which demonstrates the excellent performance of HA-Net. It embodies the powerful ability of HA-Net for suppressing the interference from complex backgrounds and solving multiscale issues. Furthermore, it may also be used to perform excellent segmentation tasks for other targets from remote sensing images.",deep learning,semantic segmentation,remote sensing images,bare soil extraction,"Liang, Xiujuan","Chen, Haoda","Jin, Yuchen",,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,
Row_877,"Li, Zhenghong","Chen, Hao","Jing, Ning",RemainNet: Explore Road Extraction from Remote Sensing Image Using Mask Image Modeling,REMOTE SENSING,SEP 2023,10,"Road extraction from a remote sensing image is a research hotspot due to its broad range of applications. Despite recent advancements, achieving precise road extraction remains challenging. Since a road is thin and long, roadside objects and shadows cause occlusions, thus influencing the distinguishment of the road. Masked image modeling reconstructs masked areas from unmasked areas, which is similar to the process of inferring occluded roads from nonoccluded areas. Therefore, we believe that mask image modeling is beneficial for indicating occluded areas from other areas, thus alleviating the occlusion issue in remote sensing image road extraction. In this paper, we propose a remote sensing image road extraction network named RemainNet, which is based on mask image modeling. RemainNet consists of a backbone, image prediction module, and semantic prediction module. An image prediction module reconstructs a masked area RGB value from unmasked areas. Apart from reconstructing original remote sensing images, a semantic prediction module of RemainNet also extracts roads from masked images. Extensive experiments are carried out on the Massachusetts Roads dataset and DeepGlobe Road Extraction dataset; the proposed RemainNet improves 0.82-1.70% IoU compared with other state-of-the-art road extraction methods.",remote sensing,road extraction,semantic segmentation,masked image modeling,"Li, Jun",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_878,"Lin, Rui","Zhang, Ying","Zhu, Xue",Local-Global Feature Capture and Boundary Information Refinement Swin Transformer Segmentor for Remote Sensing Images,IEEE ACCESS,2024,3,"Semantic segmentation of urban remote sensing images is a highly challenging task. Due to the complex background, occlusion overlap, and small-scale targets in urban remote sensing images, the semantic segmentation results suffer from deficiencies such as similar target confusion, blurred target boundaries, and small-scale target omission. To solve the above problems, a local-global feature capture and boundary information refinement Swin Transformer segmentor (LGBSwin) is proposed. First, the dual linear attention module (DLAM) utilizes spatial linear attention and channel linear attention mechanisms for strengthening global modeling capabilities to improve the segmentation ability of similar targets. Second, boundary-aware enhancement (BAE) adaptively mines the boundary semantic information through the effective integration of high-level and low-level features to alleviate blurred boundaries. Finally, feature refinement aggregation (FRA) establishes information relationships between different layers, reduces the loss of local information, and enhances local-global dependence, thus significantly improving the recognition ability of small targets. Experimental results demonstrate the effectiveness of LGBSwin, with an F1 of 91.02% on the ISPRS Vaihingen dataset and 93.35% on the ISPRS Potsdam dataset.",Boundary information,dual linear attention,feature capture,remote sensing images,"Chen, Xueyun",,,,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_879,"Ismael, Sarmad F.","Kayabol, Koray","Aptoula, Erchan",Unsupervised Domain Adaptation for the Semantic Segmentation of Remote Sensing Images via One-Shot Image-to-Image Translation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,7,"Domain adaptation is one of the prominent strategies for handling both the scarcity of pixel-level ground truth and the domain shift, that is widely encountered in large-scale land use/cover map calculation. Studies focusing on adversarial domain adaptation via re-styling source domain samples, commonly through generative adversarial networks (GANs), have reported varying levels of success, yet they suffer from semantic inconsistencies, visual corruptions, and often require a large number of target domain samples. In this letter, we propose a new lightweight unsupervised domain adaptation (UDA) method for the semantic segmentation of very high-resolution remote sensing images, based on an image-to-image translation (I2IT) approach, via an encoder-decoder strategy where latent content representations are mixed across domains, and a perceptual network module and loss function enforce visual semantic consistency. We show through cross-domain comparative experiments that it: 1) leads to semantically consistent images; 2) can operate with a single target domain sample (i.e., one-shot); and 3) at a fraction of the number of parameters required from the state-of-the-art methods, while still outperforming them. Code is available at github.com/Sarmadfismael/RSOS_I2I.",Image translation,one-shot learning,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_880,"Geng, Xinzhe","Lei, Tao","Chen, Qi",GLOBAL EVOLUTION NEURAL NETWORK FOR SEGMENTATION OF REMOTE SENSING IMAGES,,2022,0,"The popular convolutional neural networks (CNNs) have been successfully used in very high-resolution remote sensing image semantic segmentation. However, these networks often suffer from performance limitations. First, although deeper networks usually provide better feature representation, they may cause parameter redundancy and the inefficient use of prior knowledge. Secondly, attention-based networks often only focus on weighting different features of a single sample but ignore the correlation of all samples in training set, thus leading to the loss of global information. To address above issues, we propose two simple yet effective global evolution strategies. The first is knowledge enhancement. This strategy can reactivate invalid convolutional kernels through convergence of different models and make full use of prior knowledge from the network to improve its feature representation. The second is a dict-attention module that greatly enhances the generalization of networks by learning and inferring the global relationship among different samples through the dictionary unit. As a result, a novel global evolution network (GENet) is designed based on knowledge enhancement and dict-attention for remote sensing image semantic segmentation. Experiments demonstrate that the proposed GENet is not only superior to popular networks in segmentation accuracy.",Deep learning,image segmentation,knowledge enhancement,attention mechanism,"Su, Jian","He, Xi","Wang, Qi","Nandi, Asoke K.",,,,,,,,,,"2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP)",,,,,,,,,,,,,,,
Row_881,"Song, Zhishuang","Zhang, Zhitao","Yang, Shuqin",Identifying sunflower lodging based on image fusion and deep semantic segmentation with UAV remote sensing imaging,COMPUTERS AND ELECTRONICS IN AGRICULTURE,DEC 2020,78,"Sunflower lodging is a common agricultural disorder taking place in the middle and late sunflower growth periods. This disorder reduces the sunflower seed yield, damages the seed quality, and hence usually causes great losses in both crop quantity and quality. Sunflower lodging is mainly caused by extreme and destructive weather events, which have been recently occurring more frequently. This is why it is highly crucial to develop methods for fast and accurate identification of sunflower lodging. In this work, an efficient method for sunflower lodging identification is proposed based on image fusion and deep semantic segmentation of remote sensing images obtained from an unmanned aerial vehicle (UAV). First, the resolution of low-resolution multispectral images was enhanced through matching their features with those of high-resolution visible-range images. Then, for effective lodging assessment, high-quality multispectral images with rich spectral information and high spatial resolution were obtained through fusing the visible-range images and the enhanced multispectral ones. Subsequently, in order to refine the identification outcomes, a variant of the segmentation network (SegNet) deep architecture was developed for semantic segmentation. This variant has skip connections, separable convolution, and a conditional random field. Experimental evaluation shows that the fusion-based approaches clearly outperform the no-fusion ones in terms of the lodging identification accuracy for all compared architectures including support vector machine (SVM), fully convolutional network (FCN), SegNet, and the proposed SegNet variant. Meanwhile, the deep semantic segmentation methods consistently outperform the classical SVM one with hand-crafted features. As well, the improved SegNet method outperformed all of the compared methods and achieved the best accuracies of 84.4% and 89.8% without and with image fusion, respectively, on one test. The corresponding accuracies on another test set were 76.6% and 83.3%, respectively. Moreover, the proposed method can also identify the sunflower lodging and non-lodging patterns and separate them from the background. These capabilities are highly beneficial for lodging hazard assessment and sunflower harvest survey. Overall, the proposed method effectively exploited UAV remote sensing image data with fusion and deep semantic segmentation modules in order to provide a useful reference for sunflower lodging assessment and mapping.",Sunflower lodging identification,Image fusion,Deep learning,Unmanned aerial vehicle remote sensing image,"Ding, Dianyuan","Ning, Jifeng",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_882,"Kemker, Ronald","Salvaggio, Carl","Kanan, Christopher",Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,NOV 2018,372,"Deep convolutional neural networks (DCNNs) have been used to achieve state-of-the-art performance on many computer vision tasks (e.g., object recognition, object detection, semantic segmentation) thanks to a large repository of annotated image data. Large labeled datasets for other sensor modalities, e.g., multispectral imagery (MSI), are not available due to the large cost and manpower required. In this paper, we adapt state-of-the-art DCNN frameworks in computer vision for semantic segmentation for MSI imagery. To overcome label scarcity for MSI data, we substitute real MSI for generated synthetic MSI in order to initialize a DCNN framework. We evaluate our network initialization scheme on the new RIT-18 dataset that we present in this paper. This dataset contains very-high resolution MSI collected by an unmanned aircraft system. The models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work.",Deep learning,Convolutional neural network,Semantic segmentation,Multispectral,,,,,Unmanned aerial system,Synthetic imagery,,,,,,,,,,,,,,,,,,,,,,,
Row_883,"Audebert, Nicolas","Le Saux, Bertrand","Lefevre, Sebastien",HOW USEFUL IS REGION-BASED CLASSIFICATION OF REMOTE SENSING IMAGES IN A DEEP LEARNING FRAMEWORK ?,,2016,27,"In this paper, we investigate the impact of segmentation algorithms as a preprocessing step for classification of remote sensing images in a deep learning framework. Especially, we address the issue of segmenting the image into regions to be classified using pre-trained deep neural networks as feature extractors for an SVM-based classifier. An efficient segmentation as a preprocessing step helps learning by adding a spatially-coherent structure to the data. Therefore, we compare algorithms producing superpixels with more traditional remote sensing segmentation algorithms and measure the variation in terms of classification accuracy. We establish that superpixel algorithms allow for a better classification accuracy as a homogenous and compact segmentation favors better generalization of the training samples.",Remote sensing,Segmentation algorithms,Image classification,Deep learning,,,,,Superpixels,,,,,,,,,2016 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),,,,,,,,,,,,,,,
Row_884,"Ma, Jiabao","Zhou, Wujie","Qian, Xiaohong",Deep-Separation Guided Progressive Reconstruction Network for Semantic Segmentation of Remote Sensing Images,REMOTE SENSING,NOV 2022,5,"The success of deep learning and the segmentation of remote sensing images (RSIs) has improved semantic segmentation in recent years. However, existing RSI segmentation methods have two inherent problems: (1) detecting objects of various scales in RSIs of complex scenes is challenging, and (2) feature reconstruction for accurate segmentation is difficult. To solve these problems, we propose a deep-separation-guided progressive reconstruction network that achieves accurate RSI segmentation. First, we design a decoder comprising progressive reconstruction blocks capturing detailed features at various resolutions through multi-scale features obtained from various receptive fields to preserve accuracy during reconstruction. Subsequently, we propose a deep separation module that distinguishes various classes based on semantic features to use deep features to detect objects of different scales. Moreover, adjacent middle features are complemented during decoding to improve the segmentation performance. Extensive experimental results on two optical RSI datasets show that the proposed network outperforms 11 state-of-the-art methods.",digital surface model,multimodal,multi-scale supervision,feature separation,"Yu, Lu",,,,reconstruction refinement,,,,,,,,,,,,,,,,,,,,,,,,
Row_885,"Wu, Tong","Hu, Yuan","Peng, Ling",Improved Anchor-Free Instance Segmentation for Building Extraction from High-Resolution Remote Sensing Images,REMOTE SENSING,SEP 2020,30,"Building extraction from high-resolution remote sensing images plays a vital part in urban planning, safety supervision, geographic databases updates, and some other applications. Several researches are devoted to using convolutional neural network (CNN) to extract buildings from high-resolution satellite/aerial images. There are two major methods, one is the CNN-based semantic segmentation methods, which can not distinguish different objects of the same category and may lead to edge connection. The other one is CNN-based instance segmentation methods, which rely heavily on pre-defined anchors, and result in the highly sensitive, high computation/storage cost and imbalance between positive and negative samples. Therefore, in this paper, we propose an improved anchor-free instance segmentation method based on CenterMask with spatial and channel attention-guided mechanisms and improved effective backbone network for accurate extraction of buildings in high-resolution remote sensing images. Then we analyze the influence of different parameters and network structure on the performance of the model, and compare the performance for building extraction of Mask R-CNN, Mask Scoring R-CNN, CenterMask, and the improved CenterMask in this paper. Experimental results show that our improved CenterMask method can successfully well-balanced performance in terms of speed and accuracy, which achieves state-of-the-art performance at real-time speed.",building extraction,improved anchor-free instance segmentation,high-resolution remote sensing images,deep learning,"Chen, Ruonan",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_886,"Nie, Jie","Wang, Jingyu","Jing, Niantai",Bidirectional Layout-Semantic-Pixel Joint Decoupling and Embedding Network for Remote Sensing Colorization,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"In recent years, there has been a growing demand for the colorization of remote sensing images due to their inherent limitations caused by remote sensors, such as hazy or noisy atmospheric conditions. These factors result in the captured images needing to be clarified. Compared to ordinary images, remote sensing images present unique challenges in color recovery due to their imbalanced spatial distribution of objects. In this article, we propose a novel bidirectional layout-semantic-pixel joint decoupling and embedding network (BDEnet) following the idea of human painting to generate highly saturated color images with strong spatial consistency and object salience. The proposed BDEnet model emulates the process of human painting through a step-by-step approach. It begins by determining the overall tone of a large macroscopic region and progressively refining the local color based on this initial assessment. Specifically, BDEnet incorporates finer-grained semantics and pixel color information into a colored layout that represents a wide range of continuous areas, thereby accomplishing the colorization task. The BDEnet model operates at three scales, namely the layout (macro), semantic (medium), and pixel (micro) scales. It comprises three key modules: the multiscale feature decoupling (MFD) module, the layout-semantic-pixel multigranularity learning (MGL) module, and the semantic-pixel embedding (SPE) module. MFD module effectively reduces redundant noise from the semantic and layout scales by employing scale decoupling. This process ensures the extraction of efficient features essential for MGL. In the MGL module, three branches with different scales are employed to achieve layout division, semantic segmentation, and pixel coloring. To address the issue of insufficient category label guidance in layouts, we propose a novel approach called similar semantic merging (SSM) using a weakly supervised scheme to accomplish layout division. Finally, the SPE module incorporates stable semantic and pixel information into the layout features. This integration results in the generation of color images that exhibit strong spatial consistency, emphasize object salience, and possess high color saturation.",Colorization,decoupling,multiscale,remote sensing,"Zuo, Zijie","Chen, Shuguo","Liang, Xinyue",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_887,"Shen, Qian","Huang, Jiru","Wang, Min",Semantic feature-constrained multitask siamese network for building change detection in high-spatial-resolution remote sensing imagery,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,JUL 2022,53,"In the field of remote sensing applications, semantic change detection (SCD) simultaneously identifies changed areas and their change types by jointly conducting bitemporal image classification and change detection. It facilitates change reasoning and provides more application value than binary change detection (BCD), which offers only a binary map of the changed/unchanged areas. In this study, we propose a multitask Siamese network, named the semantic feature-constrained change detection (SFCCD) network, for building change detection in bitemporal high-spatial-resolution (HSR) images. SFCCD conducts feature extraction, semantic segmentation and change detection simultaneously, where change detection and semantic segmentation are the main and auxiliary tasks, respectively. For the segmentation task, ResNet50 is used to conduct image feature extraction, and the extracted semantic features are provided to execute the change detection task via a series of jump connections. For the change detection task, a global channel attention (GCA) module and a multiscale feature fusion (MSFF) module are designed, where high-level features offer training guidance to the low-level feature maps, and multiscale features are fused with multiple convolutions that possess different receptive fields. In bitemporal HSR images with different view angles, high-rise buildings have different directional height displacements, which generally cause serious false alarms for common change detection methods. However, known public building change detection datasets often lack buildings with height displacement. We thus create the Nanjing Dataset (NJDS) and design the aforementioned network structures and modules to target this issue. Experiments for method validation and comparison are conducted on the NJDS and two additional public datasets, i.e., the WHU Building Dataset (WBDS) and Google Dataset (GDS). Ablation experiments on the NJDS show that the joint utilization of the GCA and MSFF modules performs better than several classic modules, including atrous spatial pyramid pooling (ASPP), efficient spatial pyramid (ESP), channel attention block (CAB) and global attention upsampling (GAU) modules, in dealing with building height displacement. Furthermore, SFCCD achieves higher accuracy in terms of the OA, recall, F1-score and mIoU measures than several state-of-the-art change detection methods, including deeply supervised image fusion network (DSIFN), the dual-task constrained deep Siamese convolutional network (DTCDSCN), and multitask U-Net (MTU-Net).",Multitask learning,Height displacement,High-spatial-resolution remote sensing,Semantic segmentation,"Tao, Shikang","Yang, Rui","Zhang, Xin",,Change detection,Siamese network,,,,,,,,,,,,,,,,,,,,,,,
Row_888,"Ji, Xun","Tang, Longbin","Chen, Long",Toward efficient and lightweight sea-land segmentation for remote sensing images,ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,SEP 2024,0,"Sea-land segmentation is of great significance for autonomous coastline monitoring, which is fundamental research in the remote sensing community. Due to the diverse contents and easily confused sea-land boundaries contained in remote sensing images, it is always challenging to achieve precise sea-land segmentation for complex scenarios. Although existing deep learning -based methods have exhibited promising performance, excessive computational load and insufficient use of hierarchical features remain unresolved. In this paper, we contribute to addressing the problems by developing an efficient and lightweight convolutional neural network (CNN) termed E -Net. On the one hand, the proposed network adopts a novel E -shaped architecture that reforms the conventional U-codec structure to make full use of hierarchical features at different depths, so that the sea-land segmentation effect can be significantly improved without excessive computational load. On the other hand, a contextual aggregation attention mechanism (CA2M) is designed to further facilitate efficient aggregation and transmission of contextual information, so that the fuzzy and irregular sea-land boundaries can be accurately distinguished. Extensive experiments reveal that our approach not only produces superior sea-land segmentation effect but also demonstrates promising computational efficiency. Specifically, the proposed E -Net achieves state-of-the-art sea-land segmentation performance with 92.78% and 93.62% mean Intersection over Union (mIoU) on the SLSD and HRSC2016 datasets, respectively, while the frames per second (FPS) reaches 108.032 with as low as 52.287G floating point operations per second (FLOPs).",Sea-land segmentation,Convolutional neural network (CNN),Remote sensing images,Deep learning,"Hao, Li-Ying","Guo, Hui",,,Attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,
Row_889,"Zhu, Hongchun","Lu, Zhiwei","Zhang, Chao",Remote Sensing Classification of Offshore Seaweed Aquaculture Farms on Sample Dataset Amplification and Semantic Segmentation Model,REMOTE SENSING,SEP 2023,1,"Satellite remote sensing provides an effective technical means for the precise extraction of information on aquacultural areas, which is of great significance in realizing the scientific supervision of the aquaculture industry. Existing optical remote sensing methods for the extraction of aquacultural area information mostly focus on the use of image spatial features and research on classification methods of single aquaculture patterns. Accordingly, the comprehensive utilization of a combination of spectral information and deep learning automatic recognition technology in the feature expression and discriminant extraction of aquaculture areas needs to be further explored. In this study, using Sentinel-2 remote sensing images, a method for the accurate extraction of different algae aquaculture zones combined with spectral information and deep learning technology was proposed for the characteristics of small samples, multidimensions, and complex water components in marine aquacultural areas. First, the feature expression ability of the aquaculture area target was enhanced through the calculation of the normalized difference aquaculture water index (NDAWI). Second, on this basis, the improved deep convolution generative adversarial network (DCGAN) algorithm was used to amplify the samples and create the NDAWI dataset. Finally, three semantic segmentation methods (UNet, DeepLabv3, and SegNet) were used to design models for classifying the algal aquaculture zones based on the sample amplified time series dataset and comprehensively compare the accuracy of the model classifications for achieving accurate extraction of different algal aquaculture information within the seawater aquaculture zones. The results show that the improved DCGAN amplification exhibited a better effect than the generative adversarial networks (GANs) and DCGAN under the indexes of structural similarity (SSIM) and peak signal-to-noise ratio (PSNR). The UNet classification model constructed on the basis of the improved DCGAN-amplified NDAWI dataset achieved better classification results (Lvshunkou: OA = 94.56%, kappa = 0.905; Jinzhou: OA = 94.68%, kappa = 0.913). The algorithmic model in this study provides a new method for the fine classification of marine aquaculture area information under small sample conditions.",Sentinel-2,normalized difference aquaculture water index (NDAWI),sample amplification,semantic segmentation,"Yang, Yanrui","Zhu, Guocan","Zhang, Yining","Liu, Haiying",classification of aquaculture seas,,,,,,,,,,,,,,,,,,,,,,,,
Row_890,Zhao Xinwei,Li Haichang,Wang Rui,Semantic Segmentation of Remote Sensing Images via Stepwise-Refined Large-Kernel Deconvolutional Networks,,2019,0,"Deep CNN based semantic segmentation has been developed for several years and many models are proposed. However, most of them are designed for natural scene images such as PASCAL VOC, and cannot perform very well on remote sensing images, in which objects are much smaller and more densely distributed than those in natural scene images. In this paper, we demonstrate the importance of high-resolution feature maps and the problem of large dilated convolutional kernels in semantic segmentation of remote sensing images. Furthermore, we propose a Stepwise-Refined Large-Kernel Deconvolutional Network with a focus on small and densely-distributed objects such as houses and buildings, or long and narrow ones such as roads and rivers. Experiments on a public available ISPRS Vaihingen Challenge Dataset and our self-compiled Fujian Dataset show that our model outperforms the state-of-the-art models in semantic segmentation of remote sensing images.",,,,,Zheng Changwen,Shi Song,,,,,,,,,,,,"2019 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, AUTOMATION AND CONTROL TECHNOLOGIES (AIACT 2019)",,,,,,,,,,,,,,,
Row_891,"Lu, Wanzhen","Liang, Longxue","Wu, Xiaosuo",An Adaptive Multiscale Fusion Network Based on Regional Attention for Remote Sensing Images,IEEE ACCESS,2020,4,"With the widespread application of semantic segmentation in remote sensing images with high-resolution, how to improve the accuracy of segmentation becomes a research goal in the remote sensing field. An innovative Fully Convolutional Network (FCN) is proposed based on regional attention for improving the performance of the semantic segmentation framework for remote sensing images. The proposed network follows the encoder-decoder architecture of semantic segmentation and includes the following three strategies to improve segmentation accuracy. The enhanced GCN module is applied to capture the semantic features of remote sensing images. MGFM is proposed to capture different contexts by sampling at different densities. Furthermore, RAM is offered to assign large weights to high-value information in different regions of the feature map. Our method is assessed on two datasets: ISPRS Potsdam dataset and CCF dataset. The results indicate that our model with those strategies outperforms baseline models (DCED50) concerning F1, mean IoU and PA, 10.81%,19.11%, and 11.36% on the Potsdam dataset and 29.26%, 27.64% and 13.57% on the CCF dataset.",Convolution,Feature extraction,Remote sensing,Image segmentation,"Wang, Xiaoyu","Cai, Jiali",,,Semantics,Random access memory,Kernel,Remote sensing,fully convolutional networks,semantic segmentation,encoder-decoder architecture,regional attention,,,,,,Potsdam dataset,CCF dataset,,,,,,,,,,
Row_892,"Chen, Jing","Xia, Min","Wang, Dehao",Double Branch Parallel Network for Segmentation of Buildings and Waters in Remote Sensing Images,REMOTE SENSING,MAR 2023,40,"The segmentation algorithm for buildings and waters is extremely important for the efficient planning and utilization of land resources. The temporal and space range of remote sensing pictures is growing. Due to the generic convolutional neural network's (CNN) insensitivity to the spatial position information in remote sensing images, certain location and edge details can be lost, leading to a low level of segmentation accuracy. This research suggests a double-branch parallel interactive network to address these issues, fully using the interactivity of global information in a Swin Transformer network, and integrating CNN to capture deeper information. Then, by building a cross-scale multi-level fusion module, the model can combine features gathered using convolutional neural networks with features derived using Swin Transformer, successfully extracting the semantic information of spatial information and context. Then, an up-sampling module for multi-scale fusion is suggested. It employs the output high-level feature information to direct the low-level feature information and recover the high-resolution pixel-level features. According to experimental results, the proposed networks maximizes the benefits of the two models and increases the precision of semantic segmentation of buildings and waters.",double branch,CNN,semantic segmentation,buildings and waters,"Lin, Haifeng",,,,deep learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_893,"Wang, Sheng","Huang, Xiaohui","Han, Wei",Lithological mapping of geological remote sensing via adversarial semi-supervised segmentation network,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,DEC 2023,11,"Geological remote sensing interpretation (GRSI), which aims to recognize multiple geological elements based on their characteristics on satellite remote sensing images, is vital in large-scale regional lithological mapping. However, due to the influence of long-term geological movements, the spatial distribution of geological elements (such as lithology, glaciers, and soils) on the image is often complex and highly fragmented. In addition, the characteristics of high inter-class similarity and severe homogenization make the annotation of geological element samples require significant cost and expert knowledge. These lead to insufficient interpretation accuracy and limited annotation samples in GRSI. To alleviate the dependence of labeled samples and promote the performance of GRSI, we propose an adversarial semi-supervised segmentation network with object-context and global-attention (AdvSemi-OCGNet) for the GRSI, which achieves effective segmentation results in the case of limited labeled samples. Under the architecture of adversarial learning, a proposed baseline network OCGNet and a full convolution discriminator (FCD) are integrated to conduct semi-supervised segmentation. OCGNet, as the generator, aims to confuse FCD by predicting probability maps. Then FCD selects trustworthy regions with high-confidence of unlabeled sample to generate pseudo-labels for semi-supervised segmentation of OCGNet. Iterative adversarial learning is employed to simulate the process of expert interpretation of geological elements through continuous discrimination and correction. Finally, a fully connected conditional random field eliminates holes and isolated areas of segmented results caused by misclassification. Two study areas are selected, which include various geological elements such as multiple lithologies, soils, rock glaciers, and surface water. Numerous experiments have revealed the superiority of the proposed model in GRSI with limited annotation samples.",Geological remote sensing,Lithological mapping,Semi-supervised semantic segmentation,Adversarial learning,"Li, Jun","Zhang, Xiaohan","Wang, Lizhe",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_894,"Liu, Chenfang","Sun, Yuli","Xu, Yanjie",A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,1,"With the advent of the era of high-resolution remote sensing, semantic segmentation methods for solving pixel-level classification have been widely studied. Deep learning has significantly advanced deep feature extraction methods, becoming widely employed in remote sensing image analysis. Deep feature fusion methods are able to effectively combine features from different sources. Optical and synthetic aperture radar (SAR) images stand out as primary data sources in remote sensing, offering complementary and consistent information. Fusion of deep semantic features of optical and SAR images can alleviate the limitations of single-source images in application and improve semantic segmentation accuracy. Therefore, this article reviews the research on deep fusion of optical and SAR images in semantic segmentation tasks from four aspects. First, we provide a summary of challenges and research methods pertinent to semantic segmentation of remote sensing images. Then the challenges and urgent needs of deep feature fusion of optical and SAR images are analyzed, and current research is summarized from the perspective of structural design by studying various feature fusion strategies. In addition, the compilation and in-depth analysis of open-source optical and SAR datasets suitable for semantic segmentation are undertaken, serving as fundamental resources for future research endeavors. Finally, the article identifies the major challenges summarized from the literature review in this field, outlining expectations and potential future directions for researchers.",Optical sensors,Remote sensing,Optical imaging,Semantic segmentation,"Sun, Zhongzhen","Zhang, Xianghui","Lei, Lin","Kuang, Gangyao",Radar polarimetry,Adaptive optics,Feature extraction,Deep feature fusion,optical images,review,semantic segmentation,synthetic aperture radar (SAR) images,,,,,,,,,,,,,,,,,
Row_895,"Tan, Xiaowei","Xiao, Zhifeng","Wan, Qiao",Scale Sensitive Neural Network for Road Segmentation in High-Resolution Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,MAR 2021,34,"Road segmentation in remote sensing images has been widely used in many fields. Semantic segmentation, based on deep learning, has become a hot topic for road segmentation. With the deepening of convolutional neural network (CNN) structures, features in the convolution layer that has more semantic information become more important for road segmentation. However, the spatial resolution of the convolutional layer reduced as the CNN network deepens, which causes the extracted roads to lose some important location information. To solve this problem, this letter proposes a novel end-to-end road segmentation method to effectively utilize the different levels of convolutional layers to enhance the model's ability to precisely perceive road edges and shapes. The model includes an encoder and a decoder. The encoder encodes the image to obtain the features of different levels and scales. The decoder consists of two modules: scale fusion module and scale sensitive module. In the scale fusion module, features in pooling layers of different scales are fused to obtain a fusion feature. In a scale sensitive module, a weight tensor at the end of the network is learned to evaluate the importance of fusion features. This road segmentation network has been experimentally verified using public data sets, which greatly improves the road segmentation accuracy and achieves good performance.",Roads,Image segmentation,Semantics,Convolution,"Shao, Weiping",,,,Spatial resolution,Decoding,Tensile stress,Convolutional neural network~(CNN),multiscale feature,remote sensing,road segmentation,,,,,,,,,,,,,,,,,,
Row_896,"Hou, Jianlong","Guo, Zhi","Feng, Yingchao",SPANet: Spatial Adaptive Convolution Based Content-Aware Network for Aerial Image Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,9,"Semantic segmentation of remote sensing images encounters four significant difficulties: 1) complex backgrounds, 2) large-scale differences, 3) numerous small objects, and 4) extreme foreground-background imbalance. However, the existing generic semantic segmentation models mainly focus on the modeling context information and rarely focus on these four issues. This article presents an enhanced remote sensing image semantic segmentation framework to solve these problems through the hierarchical atrous pyramid (HASP) module and spatial-adaptive convolution-based FPN decoder framework. On the one hand, HASP solved the problem of complex backgrounds and large-scale differences by further enlarging the receptive field of the network through the cascade of atrous convolution with various rates. On the other hand, spatial adaptive convolution is embedded in FPN decoder framework step by step to solve the problems of numerous small objects and extreme foreground-background imbalance. Besides, a boundary-based loss function is constructed to help the network optimize the relevant segmentation results. Extensive experiments over iSAID and ISPRS Vaihingen datasets reflect the superiority of the presented structure to conventional the state-of-the-art semantic segmentation approaches.",Remote sensing,Convolution,Semantic segmentation,Feature extraction,"Wu, Youming","Diao, Wenhui",,,Decoding,Semantics,Deep learning,Attention module,remote sensing,semantic segmentation,spatial adaptive,,,,,,,,,,,,,,,,,,
Row_897,"Qi, Qingqing","Zhao, Jinghao","Lin, Lu",Combined multi-level context aggregation and attention mechanism method for photovoltaic panel extraction from high resolution remote sensing images,INTERNATIONAL JOURNAL OF REMOTE SENSING,JUN 2 2024,0,"In the context of global carbon emission reduction, solar photovoltaic (PV) technology is experiencing rapid development. Using high-resolution remote sensing images to accurately obtain PV information over a large region, including location and size, has the advantages of high statistical efficiency and timely data update for the PV energy management. Due to the intra-class diversity of PV panels and the intricate variability in their deployment environments, existing semantic segmentation methods often have problems such as under-segmentation and mis-segmentation. To alleviate these problems, this paper proposes an improved DeepLabv3+ semantic segmentation network to more accurately extract PV panels from high-resolution remote sensing images. With the aim of alleviating under-segmentation, a multi-level context aggregation module is developed. This module can enhance the model's ability to learn the characteristics of PV panels and their surrounding environment by aggregating rich contextual information from multi-scale and semantic levels. To alleviate the problem of mis-segmentation, a hybrid attention module is introduced. This module sequentially and adaptively adjusts the weight distribution in both the channel and spatial dimensions, thus enabling the model to focus more on the feature information and spatial positions of PV objects. Experiments conducted on a self-constructed Beijing PV segmentation dataset show that the method in this paper has advantages of completeness and accuracy in extracting PV panels compared to the baseline model and current mainstream semantic segmentation network. In addition, the results of experiments on extracting PV panels in real region show that our model also has good stability and generalization capability.",Photovoltaic panels,remote sensing images,semantic segmentation,context modelling,"Zhang, Xiaoqing","Tian, Yajun",,,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,
Row_898,"Chaurasia, Kuldeep","Nandy, Rijul","Pawar, Omkar",Semantic segmentation of high-resolution satellite images using deep learning,EARTH SCIENCE INFORMATICS,DEC 2021,8,"The increasing common use of incidental unrectified satellite images have many applications for mapping of earth for coastal and ocean applications. Hazard assessment and natural resource management can also be done via this process. Remote sensing is being used extensively due to the increase in the number of satellites in space. It is also the future of optimization of GPS systems and the internet. To demonstrate the semantic segmentation process, this study presents proposed solutions along with their evaluation metrics adapted from fully connected neural networks such as UNet and PSPNet. UNet architecture based deep learning model has outperformed PSPNet based architecture with overall Mean-IOU score of 0.51 on the test set in the semantic segmentation. The overall accuracy of the model can further be improved by providing homogeneous features to train the model, balance classes and by incorporating more data set for semantic segmentation. The developed model can be useful to the authorities for smart city planning and landuse mapping.",Deep Learning,Semantic Segmentation,Feature extraction,UNet Architecture,"Singh, Ravi Ranjan","Ahire, Meghana",,,Remote Sensing,,,,,,,,,,,,,,,,,,,,,,,,
Row_899,"Zhang, Tianjian","Xue, Zhaohui","Su, Hongjun",Deformable Transformer and Spectral U-Net for Large-Scale Hyperspectral Image Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"Remote sensing semantic segmentation tasks aim to automatically extract land cover types by accurately classifying each pixel. However, large-scale hyperspectral remote sensing images possess rich spectral information, complex and diverse spatial distributions, significant scale variations, and a wide variety of land cover types with detailed features, which pose significant challenges for segmentation tasks. To overcome these challenges, this study introduces a U-shaped semantic segmentation network that combines global spectral attention and deformable Transformer for segmenting large-scale hyperspectral remote sensing images. First, convolution and global spectral attention are utilized to emphasize features with the richest spectral information, effectively extracting spectral characteristics. Second, deformable self-attention is employed to capture global-local information, addressing the complex scale and distribution of objects. Finally, deformable cross-attention is used to aggregate deep and shallow features, enabling comprehensive semantic information mining. Experiments conducted on a large-scale hyperspectral remote sensing dataset (WHU-OHS) demonstrate that: first, in different cities including Changchun, Shanghai, Guangzhou, and Karamay, DTSU-Net achieved the highest performance in terms of mIoU compared to the baseline methods, reaching 56.19%, 37.89%, 52.90%, and 63.54%, with an average improvement of 7.57% to 34.13%, respectively; second, module ablation experiments confirm the effectiveness of our proposed modules, and deformable Transformer significantly reduces training costs compared to conventional Transformers; third, our approach achieves the highest mIoU of 57.22% across the entire dataset, with a balanced trade-off between accuracy and parameter efficiency, demonstrating an improvement of 1.65% to 56.58% compared to the baseline methods.",Feature extraction,Transformers,Data mining,Hyperspectral imaging,,,,,Semantics,Semantic segmentation,Convolution,Sensors,Land surface,Decoding,Deep learning,hyperspectral remote sensing,,,,,,large-scale,semantic segmentation,transformer,,,,,,,,,
Row_900,"Yu, Xizi","Li, Shuang","Zhang, Yu",Incorporating convolutional and transformer architectures to enhance semantic segmentation of fine-resolution urban images,EUROPEAN JOURNAL OF REMOTE SENSING,DEC 31 2024,1,"Though convolutional neural networks (CNN) exhibit promise in image semantic segmentation, they have limitations in capturing global context information, resulting in inaccuracies in segmenting small object features and object boundaries. This study introduces a hybrid network, ICTANet, which incorporate convolutional and Transformer architectures to improve the segmentation performance of fine-resolution remote sensing urban imagery. The ICTANet model is essentially a Transformer-based encoder-decoder structure. The dual-encoder architecture, which combines CNN and Swin Transformer modules, is designed to extract both global and local detail information. The feature information at various stages is collected by the Feature Extraction and Fusion modules (FEF), enabling multi-scale contextual information fusion. In addition, an Auxiliary Boundary Detection (ABD) module is introduced at the end of the decoder to enhance the model's ability to capture object boundary information. Numerous ablation experiments have been conducted to demonstrate the efficacy of various components within the network. The testing results have proven that the proposed model can achieve satisfactory performance on the ISPRS Vaihingen and Potsdam dataset, with overall accuracies of 91.9% and 92.0%, respectively. Simultaneously, the proposed model is also compared to the current state-of-the-art methods, exhibiting competitive performance, particularly in the segmentation of diminutive objects like cars and trees.",Image semantic segmentation,transformers,convolutional neural networks,feature extraction,,,,,remote sensing,,,,,,,,,,,,,,,,,,,,,,,,
Row_901,"Yu, Chih-Chang","Chen, Yuan-Di","Cheng, Hsu-Yung",Semantic Segmentation of Satellite Images for Landslide Detection Using Foreground-Aware and Multi-Scale Convolutional Attention Mechanism,SENSORS,OCT 2024,0,"Advancements in satellite and aerial imagery technology have made it easier to obtain high-resolution remote sensing images, leading to widespread research and applications in various fields. Remote sensing image semantic segmentation is a crucial task that provides semantic and localization information for target objects. In addition to the large-scale variation issues common in most semantic segmentation datasets, aerial images present unique challenges, including high background complexity and imbalanced foreground-background ratios. However, general semantic segmentation methods primarily address scale variations in natural scenes and often neglect the specific challenges in remote sensing images, such as inadequate foreground modeling. In this paper, we present a foreground-aware remote sensing semantic segmentation model. The model introduces a multi-scale convolutional attention mechanism and utilizes a feature pyramid network architecture to extract multi-scale features, addressing the multi-scale problem. Additionally, we introduce a Foreground-Scene Relation Module to mitigate false alarms. The model enhances the foreground features by modeling the relationship between the foreground and the scene. In the loss function, a Soft Focal Loss is employed to focus on foreground samples during training, alleviating the foreground-background imbalance issue. Experimental results indicate that our proposed method outperforms current state-of-the-art general semantic segmentation methods and transformer-based methods on the LS dataset benchmark.",remote sensing,semantic segmentation,convolutional attention mechanism,multi-scale features fusion,"Jiang, Chi-Lun",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_902,"Wang, Qixiong","Yin, Jihao","Jiang, Hongxiang",Disentangled Foreground-Semantic Adapter Network for Generalized Aerial Image Few-Shot Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Semantic segmentation of remote sensing imagery requires extensive annotated samples for training, facing challenges in adapting to novel classes with few annotations. Few-shot semantic segmentation (FS-Seg) employs a support-to-query paradigm, which encounters many practical constraints. Recently, generalized few-shot semantic segmentation (GFS-Seg) has been proposed to align with general semantic segmentation paradigms, enabling the segmentation of all classes (both base and novel classes) in the image. However, existing GFS-Seg methods struggle with a large intra-class variance of background, degradation on base classes, and overfitting on novel classes during fine-tuning in aerial imagery. To address the above issues, we propose the disentangled foreground-semantic adapter network (DFSA-Net) for generalized aerial image FS-Seg. Specifically, to reduce the interference from background features, DFSA-Net employs a foreground-semantic decoder (FSD) to decompose semantic segmentation into foreground aggregation and multiclass refinement. To mitigate the base classes degradation and novel class overfitting during fine-tuning, we propose disentangled low-rank adapter (DLA) for fine-tuning phase, designed to preserve the base parameters while ensuring efficient adaptation to novel classes. Finally, we introduce an inference ensemble strategy that merges base and novel decoder prediction to achieve final output. Experimental results on NWPU and iSAID datasets demonstrate the superiority of our DFSA-Net over other compared methods.",Semantic segmentation,Decoding,Semantics,Training,"Feng, Jiaqi","Zhang, Guangyun",,,Prototypes,Benchmark testing,Interference,Adapter tuning,aerial imagery,deep learning,disentanglement representation,few-shot semantic segmentation (FS-Seg),,,,,,,,,,,,,,,,,
Row_903,"Zhang, Yonghong","Lu, Huanyu","Ma, Guangyi",MU-Net: Embedding MixFormer into Unet to Extract Water Bodies from Remote Sensing Images,REMOTE SENSING,JUL 2023,10,"Water bodies extraction is important in water resource utilization and flood prevention and mitigation. Remote sensing images contain rich information, but due to the complex spatial background features and noise interference, problems such as inaccurate tributary extraction and inaccurate segmentation occur when extracting water bodies. Recently, using a convolutional neural network (CNN) to extract water bodies is gradually becoming popular. However, the local property of CNN limits the extraction of global information, while Transformer, using a self-attention mechanism, has great potential in modeling global information. This paper proposes the MU-Net, a hybrid MixFormer architecture, as a novel method for automatically extracting water bodies. First, the MixFormer block is embedded into Unet. The combination of CNN and MixFormer is used to model the local spatial detail information and global contextual information of the image to improve the ability of the network to capture semantic features of the water body. Then, the features generated by the encoder are refined by the attention mechanism module to suppress the interference of image background noise and non-water body features, which further improves the accuracy of water body extraction. The experiments show that our method has higher segmentation accuracy and robust performance compared with the mainstream CNN- and Transformer-based semantic segmentation networks. The proposed MU-Net achieves 90.25% and 76.52% IoU on the GID and LoveDA datasets, respectively. The experimental results also validate the potential of MixFormer in water extraction studies.",attention mechanism,convolutional neural network,MixFormer,remote sensing,"Zhao, Huajun","Xie, Donglin","Geng, Sutong","Tian, Wei",semantic segmentation,Transformer,,,,,,,"Sian, Kenny Thiam Choy Lim Kam",,,,,,,,,,,,,,,,
Row_904,"He, Mingyuan","Zhang, Jie","He, Yang",Annotated Dataset for Training Cloud Segmentation Neural Networks Using High-Resolution Satellite Remote Sensing Imagery,REMOTE SENSING,OCT 2024,1,"The integration of satellite data with deep learning has revolutionized various tasks in remote sensing, including classification, object detection, and semantic segmentation. Cloud segmentation in high-resolution satellite imagery is a critical application within this domain, yet progress in developing advanced algorithms for this task has been hindered by the scarcity of specialized datasets and annotation tools. This study addresses this challenge by introducing CloudLabel, a semi-automatic annotation technique leveraging region growing and morphological algorithms including flood fill, connected components, and guided filter. CloudLabel v1.0 streamlines the annotation process for high-resolution satellite images, thereby addressing the limitations of existing annotation platforms which are not specifically adapted to cloud segmentation, and enabling the efficient creation of high-quality cloud segmentation datasets. Notably, we have curated the Annotated Dataset for Training Cloud Segmentation (ADTCS) comprising 32,065 images (512 x 512) for cloud segmentation based on CloudLabel. The ADTCS dataset facilitates algorithmic advancement in cloud segmentation, characterized by uniform cloud coverage distribution and high image entropy (mainly 5-7). These features enable deep learning models to capture comprehensive cloud characteristics, enhancing recognition accuracy and reducing ground object misclassification. This contribution significantly advances remote sensing applications and cloud segmentation algorithms.",high-resolution remote sensing images,cloud segmentation,annotated dataset,CloudLabel annotation technique,"Zuo, Xinjie","Gao, Zebin",,,ground truth label data,,,,,,,,,,,,,,,,,,,,,,,,
Row_905,"Han, Jing","Wang, Weiyu","Lin, Yuqi",MRU-Net: A remote sensing image segmentation network for enhanced edge contour Detection,KSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS,DEC 31 2023,0,"Remote sensing image segmentation plays an important role in realizing intelligent city construction. The current mainstream segmentation networks effectively improve the segmentation effect of remote sensing images by deeply mining the rich texture and semantic features of images. But there are still some problems such as rough results of small target region segmentation and poor edge contour segmentation. To overcome these three challenges, we propose an improved semantic segmentation model, referred to as MRU-Net, which adopts the U-Net architecture as its backbone. Firstly, the convolutional layer is replaced by BasicBlock structure in U-Net network to extract features, then the activation function is replaced to reduce the computational load of model in the network. Secondly, a hybrid multi scale recognition module is added in the encoder to improve the accuracy of image segmentation of small targets and edge parts. Finally, test on Massachusetts Buildings Dataset and WHU Dataset the experimental results show that compared with the original network the ACC,mIoU and F1 value are improved, and the imposed network shows good robustness and portability in different datasets.",Convolutional neural network,Image processing,Hybrid multiscale identification module,Micro residual structure,"Lyu, Xueqiang",,,,Remote sensing image segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_906,"Zhu, Yuqian","Chen, Weitao","He, Wenxi",CUG_MISDataset: A Remote Sensing Instance Segmentation Dataset for Improved Wide-Area High-Precision Mining Land Occupation Recognition,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"The effective and rapid acquisition of wide-area mine occupation information is crucial for ecological geo-environmental protection and sustainable development. Remote sensing instance segmentation technology based on deep learning is a promising solution. However, there are two significant challenges including insufficient training datasets and unsuitable segmentation models. To overcome these issues, this study provides a large-scale remote sensing instance segmentation dataset for mining land occupation (CUG_MISDataset). The CUG_MISDataset comprises 1426 image blocks and more than 3000 instances, covering all 150 types of mines found in China's Hubei province. It features multiple mine types, various land occupations, and complex instance scales. First, this study compares the performance of seven mainstream remote sensing instance segmentation models using the proposed CUG_MISDataset. The results show that all seven models achieve high segmentation accuracy. It indicates that the constructed CUG_MISDataset is robust and can serve as a valuable benchmark for remote sensing instance segmentation of mining areas. Second, aiming at the difficulty of large scale variation in this dataset, we propose a multiscale dilation feature pyramid network (MSD-FPN), which introduces a dynamic weight allocation mechanism to give more weight to important semantic information, while convolution with different dilation rates is used in the module to enhance the expression of mines' multiscale features. The proposed MSD-FPN can achieve a 2.0% average precision improvement on the CUG_MISDataset.",Deep learning,instance segmentation,mining area,mining land occupation,"Wang, Ruizhen","Li, Xianju","Wang, Lizhe",,remote sensing,,,,,,,,,,,,,,,,,,,,,,,,
Row_907,"Lan, Lingxiang","Wu, Dong","Chi, Mingmin",Multi-temporal Change Detection based on Deep Semantic Segmentation Networks,,2019,3,"The task of change detection (CD) can be considered as a pixel-level classification problem and can be fulfilled by a semantic segmentation task. In this paper, we propose an end-to-end change detection framework based on a deep semantic segmentation network, in particular, fully convolutional networks in terms of the difference image or the concatenated one of multi-temporal images. Experimental results illustrate that our framework can obtain a state-of-the-art result on the real-world high resolution CD dataset.",Remote sensing,change detection,deep learning,semantic segmentation,,,,,,,,,,,,,,2019 10TH INTERNATIONAL WORKSHOP ON THE ANALYSIS OF MULTITEMPORAL REMOTE SENSING IMAGES (MULTITEMP),,,,,,,,,,,,,,,
Row_908,"Shamsolmoali, Pourya","Zareapoor, Masoumeh","Wang, Ruili",A Novel Deep Structure U-Net for Sea-Land Segmentation in Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,SEP 2019,91,"Sea-land segmentation is an important process for many key applications in remote sensing. Proper operative sea-land segmentation for remote sensing images remains a challenging issue due to complex and diverse transition between sea and land. Although several convolutional neural networks (CNNs) have been developed for sea-land segmentation, the performance of these CNNs is far from the expected target. This paper presents a novel deep neural network structure for pixel-wise sea-land segmentation, a residual Dense U-Net (RDU-Net), in complex and high-density remote sensing images. RDU-Net is a combination of both downsampling and upsampling paths to achieve satisfactory results. In each downsampling and upsampling path, in addition to the convolution layers, several densely connected residual network blocks are proposed to systematically aggregate multiscale contextual information. Each dense network block contains multilevel convolution layers, short-range connections, and an identity mapping connection, which facilitates features reuse in the network and makes full use of the hierarchical features from the original images. These proposed blocks have a certain number of connections that are designed with shorter distance backpropagation between the layers and can significantly improve segmentation results while minimizing computational costs. We have performed extensive experiments on two real datasets, Google-Earth and ISPRS, and compared the proposed RDU-Net against several variations of dense networks. The experimental results show that RDU-Net outperforms the other state-of-the-art approaches on the sea-land segmentation tasks.",Deep neural network (DNN),dense network (DenseNet),remote sensing images,sea-land segmentation,"Zhou, Huiyu","Yang, Jie",,,U-Net,,,,,,,,,,,,,,,,,,,,,,,,
Row_909,"Wang, Pan","Zhao, Hengqian","Yang, Zihan",Fast Tailings Pond Mapping Exploiting Large Scene Remote Sensing Images by Coupling Scene Classification and Sematic Segmentation Models,REMOTE SENSING,JAN 2023,6,"In the process of extracting tailings ponds from large scene remote sensing images, semantic segmentation models usually perform calculations on all small-size remote sensing images segmented by the sliding window method. However, some of these small-size remote sensing images do not have tailings ponds, and their calculations not only affect the model accuracy, but also affect the model speed. For this problem, we proposed a fast tailings pond extraction method (Scene-Classification-Sematic-Segmentation, SC-SS) that couples scene classification and semantic segmentation models. The method can map tailings ponds rapidly and accurately in large scene remote sensing images. There were two parts in the method: a scene classification model, and a semantic segmentation model. Among them, the scene classification model adopted the lightweight network MobileNetv2. With the help of this network, the scenes containing tailings ponds can be quickly screened out from the large scene remote sensing images, and the interference of scenes without tailings ponds can be reduced. The semantic segmentation model used the U-Net model to finely segment objects from the tailings pond scenes. In addition, the encoder of the U-Net model was replaced by the VGG16 network with stronger feature extraction ability, which improves the model's accuracy. In this paper, the Google Earth images of Luanping County were used to create the tailings pond scene classification dataset and tailings pond semantic segmentation dataset, and based on these datasets, the training and testing of models were completed. According to the experimental results, the extraction accuracy (Intersection Over Union, IOU) of the SC-SS model was 93.48%. The extraction accuracy of IOU was 15.12% higher than the U-Net model, while the extraction time was shortened by 35.72%. This research is of great importance to the remote sensing dynamic observation of tailings ponds on a large scale.",tailings ponds,large scene remote sensing images,deep learning,fast mapping,"Jin, Qian","Wu, Yanhua","Xia, Pengjiu","Meng, Lingxuan",,,,,,,,,,,,,,,,,,,,,,,,,
Row_910,"Long, Jiang","Li, Mengmeng","Wang, Xiaoqin",Integrating Spatial Details With Long-Range Contexts for Semantic Segmentation of Very High-Resolution Remote-Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,13,"This letter presents a cross-learning network (i.e., CLCFormer) integrating fine-grained spatial details within long-range global contexts based upon convolutional neural networks (CNNs) and transformer, for semantic segmentation of very high-resolution (VHR) remote-sensing images. More specifically, CLCFormer comprises two parallel encoders, derived from the CNN and transformer, and a CNN decoder. The encoders are backboned on SwinV2 and EfficientNet-B3, from which the extracted semantic features are aggregated at multiple levels using a bilateral feature fusion module (BiFFM). First, we used attention gate (ATG) modules to enhance feature representation, improving segmentation results for objects with various shapes and sizes. Second, we used an attention residual (ATR) module to refine spatial features's learning, alleviating boundary blurring of occluded objects. Finally, we developed a new strategy, called auxiliary supervise strategy (ASS), for model optimization to further improve segmentation performance. Our method was tested on the WHU, Inria, and Potsdam datasets, and compared with CNN-based and transformer-based methods. Results showed that our method achieved state-of-the-art performance on the WHU building dataset (92.31% IoU), Inria building dataset (83.71% IoU), and Potsdam dataset (80.27% MIoU). We concluded that CLCFormer is a flexible, robust, and effective method for the semantic segmentation of VHR images. The codes of the proposed model are available at https://github.com/long123524/CLCFormer.",Feature extraction,Transformers,Semantics,Convolutional neural networks,,,,,Convolution,Buildings,Tiles,Auxiliary supervise,CLCFormer,convolutional neural networks (CNNs),semantic segmentation,transformer,,,,,,very high-resolution (VHR) images,,,,,,,,,,,
Row_911,"Wang, Yiwen","Lyn, Ye","Cao, Yanpeng",DEEP LEARNING FOR SEMANTIC SEGMENTATION OF UAV VIDEOS,,2019,4,"As one of the key problems in both remote sensing and computer vision, video semantic segmentation has been attracting increasing amounts of attention. Using video segmentation technique for Unmanned Aerial Vehicle (UAV) data processing is also a popular application. Previous methods extended single image segmentation approaches to multiple frames. The temporal dependencies are ignored in these methods. This paper proposes a novel segmentation method to solve this problem. Combining the fully convolutional networks (FCN) and the Convolution Long Short Term Memory (Conv-LSTM) together, we segment the sequence of the video frames instead of segmenting each individual frame separately. FCN serves as the frame-based segmentation method. Conv-LSTM makes use of the temporal information between consecutive frames. Experimental results show the superiority of this method especially in some classes compared to the single image segmentation model using video dataset from UAV.",FCN,Conv-LSTM,video semantic segmentation,UAV,"Yang, Michael Ying",,,,,,,,,,,,,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),,,,,,,,,,,,,,,
Row_912,"Tang, Kai","Xu, Fei","Chen, Xuehong",The ClearSCD model: Comprehensively leveraging semantics and change relationships for semantic change detection in high spatial resolution remote sensing imagery,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,MAY 2024,7,"The Earth has been undergoing continuous anthropogenic and natural change. High spatial resolution (HSR) remote sensing imagery provides a unique opportunity to accurately reveal these changes on a planetary scale. Semantic change detection (SCD) with HSR imagery has become a common technique for tracking the evolution of land surface types at a semantic level. However, existing SCD methods rarely model the dependency between semantics and changes, resulting in suboptimal accuracy in detecting complicated surface changes. To address this limitation, we propose ClearSCD, a multi-task learning model that leverages the mutual gain relationship between semantics and change through three innovative modules. The first module interprets semantic features at different times into posterior probabilities for surface types to detect binary change information; the second module learns the correlation between surface types over time and the binary change information; a semantic augmented contrastive learning module is used as the third module to improve the performance of the other two modules. We tested ClearSCD's performance against state-of-the-art methods on benchmark datasets and a real- world scenario (named LsSCD dataset), showing that ClearSCD outperformed the alternatives on mIoUscmetrics sc metrics by 1.23% to 19.34%. Furthermore, ablation experiments demonstrated the unique contribution of the three innovative modules to performance improvement. The high computational efficiency and robust performance over diverse landscapes demonstrate that ClearSCD is an operational tool for detecting detailed land surface changes from HSR imagery. Code and LsSCD dataset available at https://github.com/tangkai-RS/ClearSCD.",Remote sensing,Change detection,Semantic change detection,Semantic segmentation,"Dong, Qi","Yuan, Yuheng","Chen, Jin",,Deep learning,Multi-task learning,,,,,,,,,,,,,,,,,,,,,,,
Row_913,"Zhang, Di","Zhao, Jiaqi","Chen, Jingyang",Edge-aware and spectral-spatial information aggregation network for multispectral image semantic segmentation,ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,SEP 2022,13,"Semantic segmentation is a fundamental task in the field of remote sensing image intelligent interpretation and computer vision. Multispectral remote sensing images have attracted more and more researchers' attention because they can accurately describe different types of reflection spectra. However, inaccurate multispectral feature description leads to edge semantic ambiguity and misclassification of small objects. In this article, we propose a novel network named edge-aware and spectral-spatial information aggregation net (ESSANet) to capture both high-level semantic features and low-level edge details for semantic segmentation of remote sensing images. Specifically, on the one hand, in order to improve the representation ability of discriminant features, we design a two-stream spectral-spatial feature extraction network via 3D hybrid convolution and multi-level aggregation network. On the other hand, in order to eliminate the effect of edge semantic ambiguity, we develop a siamese edge-aware structure and multi-stage edge loss function. Experimental results show that our method achieved 3.5% and 4.09% mean intersection over union (mIoU) score improvements and 2.59% and 3.32% Kappa score improvements compared with the competitive baseline algorithm on the SEN12MS and US3D datasets, respectively. In addition, the method proposed in this paper also achieves a better trade-off between speed and accuracy.",Spectral&ndash,spatial information,Edge-aware,Remote sensing,"Zhou, Yong","Shi, Boyu","Yao, Rui",,Multispectral semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_914,"Wang, Guoying","Chen, Jiahao","Mo, Lufeng",Border-Enhanced Triple Attention Mechanism for High-Resolution Remote Sensing Images and Application to Land Cover Classification,REMOTE SENSING,AUG 2024,2,"With the continuous development and popularization of remote sensing technology, remote sensing images have been widely used in the field of land cover classification. Since remote sensing images have complex spatial structure and texture features, it is becoming a challenging problem to accurately categorize them. Land cover classification has practical application value in various fields, such as environmental monitoring and protection, urban and rural planning and management, and climate change research. In recent years, remote sensing image classification methods based on deep learning have been rapidly developed, in which semantic segmentation technology has become one of the mainstream methods for land cover classification using remote sensing image. Traditional semantic segmentation algorithms tend to ignore the edge information, resulting in poor classification of the edge part in land cover classification, and there are numerous attention mechanisms to make improvements for these problems. In this paper, a triple attention mechanism, BETAM (Border-Enhanced Triple Attention Mechanism), for edge feature enhancement of high-resolution remote sensing images is proposed. Furthermore, a new model on the basis of the semantic segmentation network model DeeplabV3+ is also introduced, which is called DeepBETAM. The triple attention mechanism BETAM is able to capture feature dependencies in three dimensions: position, space, and channel, respectively. Through feature importance weighting, modeling of spatial relationships, and adaptive learning capabilities, the model BETAM pays more attention to edge features, thus improving the accuracy of edge detection. A remote sensing image dataset SMCD (Subject Meticulous Categorization Dataset) is constructed to verify the robustness of the attention mechanism BETAM and the model DeepBETAM. Extensive experiments were conducted on the two self-built datasets FRSID and SMCD. Experimental results showed that the mean Intersection over Union (mIoU), mean Pixel Accuracy (mPA), and mean Recall (mRecall) of DeepBETAM are 63.64%, 71.27%, and 71.31%, respectively. These metrics are superior to DeeplabV3+, DeeplabV3+(SENet), DeeplabV3+(CBAM), DeeplabV3+(SAM), DeeplabV3+(ECANet), and DeeplabV3+(CAM), which are network models that incorporate different attention mechanisms. The reason is that BETAM has better edge segmentation results and segmentation accuracy. Meanwhile, on the basis of the self-built dataset, the four main classifications of buildings, cultivated land, water bodies and vegetation were subdivided and detected, and good experimental results were obtained, which verified the robustness of the attention mechanism BETAM and the model DeepBETAM. The method has broad application prospects and can provide favorable support for research and application in the field of surface classification.",semantic segmentation,remote sensing image,BETAM,triple attention mechanism,"Wu, Peng","Yi, Xiaomei",,,edge detection,,,,,,,,,,,,,,,,,,,,,,,,
Row_915,"Nunes de Castro, Heitor da Rocha","de Carvalho Junior, Osmar Abilio","Ferreira de Carvalho, Osmar Luiz",DETECTION OF KARST DEPRESSIONS IN BRAZIL USING DEEP SEMANTIC SEGMENTATION,,2023,0,"This research aims to investigate the use of semantic segmentation and Shuttle Radar Topography Mission (SRTM) data in detecting natural karst depressions developed on the carbonate rocks of the Neoproterozoic Bambui Group in Western Bahia, Brazil. The study area is a karst landscape containing depressions enclosed in limestone, many forming lakes. The methodology had the following steps: (a) visual interpretation of karst depressions from Sentinel-2 and OLI-Landsat 8 images; (b) generation of DEM-based sink depth plus nine morphometric attributes; (c) selection of 128x128-pixel samples for training (1600), validation (400), and testing (400) considering two channels (DEM and sink depth based on DEM) and eleven channels (the two previous ones and the morphometric attributes); and (d) semantic segmentation using U-Net architecture with EfficientNet-B7 backbone. The accuracy metrics were 98.26, 72.82, 79.50, 79.16, and 65.51 for OA, precision, recall, F-score, and IoU when considering SRTM plus morphometric attributes (11 channels).",Semantic segmentation,sparse annotation,iterative learning,remote sensing,"Trancoso Gomes, Roberto Arnaldo","Fontes Guimaraes, Renato",,,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_916,"Ye, Wenhui","Lei, Weimin","Zhang, Wei",GFSCompNet: remote sensing image compression network based on global feature-assisted segmentation,MULTIMEDIA TOOLS AND APPLICATIONS,JAN 2024,1,"The proliferation of remote sensing image data in recent years has posed a pressing need for efficient compression techniques due to constrained transmission bandwidth. While lossless compression preserves image fidelity, it falls short of meeting real-time demands. Conversely, conventional lossy compression methods can attain high compression ratios for real-time applications, but often introduce issues like block artifacts, blurring, and distortions in the decompressed images. Hence, we propose the Global Feature-Assisted Segmentation Compression Network (GFSCompNet) as a solution for high compression ratio lossy compression. Initially, we design a segmentation network utilizing a dual-branch global feature-assisted segmentation approach to precisely detect small targets in remote sensing images. On the compression side of the network, we leverage an attention mechanism and code rate allocation technique to seamlessly merge the segmented small target information with the original image, thereby allocating a higher compression code rate to the small target region. Furthermore, a joint hyper-priority decoding and entropy coding estimation network is proposed to further remove the redundancy in the potential representation and improve the compression ratio. Experimental results conducted under conditions of high compression ratios and comparable bit rates demonstrate that our approach yields higher-quality reconstructed images compared to the JPEG algorithm and outperforms other deep learning-based image compression methods. Additionally, it effectively preserves small target information, thereby enhancing the interpretability of machine learning models.",Remote sensing image,Image compression,Semantic segmentation,Feature fusion,"Yu, Tingting","Feng, Xiang",,,Separate hyperprior decoders,,,,,,,,,,,,,,,,,,,,,,,,
Row_917,"Shen, Zhengwei","Shang, Yongheng","Zhang, Xiaoyu",CMMSNet:A Multi-modal Semantic Segmentation Network for Rooftop Extraction based on SAR and Optical Images,,2024,0,"Distributed rooftop photovoltaic systems hold immense potential for renewable energy generation, and accurate extraction of building roofs from high-resolution remote sensing imagery is crucial for their development. While current semantic segmentation methods primarily rely on single-modality optical images, Synthetic Aperture Radar (SAR) offers complementary ground information that can significantly enhance segmentation accuracy. However, the modality disparities arising from different imaging mechanisms pose challenges in feature fusion between SAR and optical images, existing approaches rely on simplistic fusion methods to exploit the complementary information of each modality, ignoring the correlative information between the different modalities during feature extraction, this results in an insufficient integration of complementary information.To address these challenges, we introduce CMMSNet, a novel multi-modal fusion semantic segmentation network specifically designed for building roof extraction. The main architecture of CMMSNet is constituted by three three core modules: the feature extraction encoder module, the heterogeneous modality alignment module, and the modality fusion decoder module. Initially, dual independent pyramid-structured encoders are employed by CMMSNet to separately extract feature pyramids from SAR and optical images at various scales, this strategy is intended to capture multi-scale semantic contexts and address the issue of large spatial scale variations among different objects in remote sensing images. Furthermore, an Adaptive Feature Alignment Module (AFAM) is introduced, tasked with identifying correlative information between the two modalities from a spatial dimension and aligning the modal features accordingly, this process is crucial for facilitating cross-modal learning and in enhancing the feature representation of each modality. In addition, a Cross-Modal Multi-Scale Feature Fusion (CMMSFF) module is designed to effectively integrates multi-scale and multi-modal heterogeneous features from both modalities, this module employs a channel self-attention mechanism, which adaptively fuses discriminative features by applying weights to each modality and selectively discarding irrelevant components, thus enhancing the selection and fusion of key channels within the multimodal features set. This innovative approach allows us to harness the complementary information provided by SAR and optical images, enhancing the overall segmentation performance. A series of comprehensive experiments conducted on the DFC23 dataset demonstrate that our proposed CMMSNet outperforms other existing mainstream semantic segmentation methods in both stability and effectiveness, including both single-modal and multi-modal approaches. This achievement sets a new benchmark for the extraction of building rooftop through the use of multi-modal remote sensing images. Our findings highlight the importance of leveraging multi-modal data fusion in addressing real-world challenges in remote sensing image analysis and offer valuable insights for future research in this domain.",Building extraction,Multi-modal fusion,Remote sensing,semantic segmentation,"Yin, Jianwei","Han, Jun","Cai, Chao",,,,,,,,,,,"2024 12TH INTERNATIONAL CONFERENCE ON AGRO-GEOINFORMATICS, AGRO-GEOINFORMATICS 2024",,,,,,,,,,,,,,,
Row_918,"Lekavicius, Justinas","Gruzauskas, Valentas",,Data Augmentation with Generative Adversarial Network for Solar Panel Segmentation from Remote Sensing Images,ENERGIES,JUL 2024,2,"With the popularity of solar energy in the electricity market, demand rises for data such as precise locations of solar panels for efficient energy planning and management. However, these data are not easily accessible; information such as precise locations sometimes does not exist. Furthermore, existing datasets for training semantic segmentation models of photovoltaic (PV) installations are limited, and their annotation is time-consuming and labor-intensive. Therefore, for additional remote sensing (RS) data creation, the pix2pix generative adversarial network (GAN) is used, enriching the original resampled training data of varying ground sampling distances (GSDs) without compromising their integrity. Experiments with the DeepLabV3 model, ResNet-50 backbone, and pix2pix GAN architecture were conducted to discover the advantage of using GAN-based data augmentations for a more accurate RS imagery segmentation model. The result is a fine-tuned solar panel semantic segmentation model, trained using transfer learning and an optimal amount-60% of GAN-generated RS imagery for additional training data. The findings demonstrate the benefits of using GAN-generated images as additional training data, addressing the issue of limited datasets, and increasing IoU and F1 metrics by 2% and 1.46%, respectively, compared with classic augmentations.",deep learning,solar panels,semantic segmentation,data augmentation,,,,,generative adversarial network,remote sensing,transfer learning,,,,,,,,,,,,,,,,,,,,,,
Row_919,"Xiao, Tao","Liu, Yikun","Huang, Yuwen",MFRNet: A Multipath Feature Refinement Network for Semantic Segmentation in High-Resolution Remote Sensing Images,REMOTE SENSING LETTERS,DEC 2 2022,3,"Deep convolutional neural networks have made significant progress in the field of intelligent analysis of remote-sensing images. However, the semantic segmentation task in high-resolution remote-sensing (HRRS) images always faces the problem of large-scale variation and complex background samples, which causes difficulties in distinguishing confusable ground objects. In this letter, we propose a novel multipath feature refinement network (MFRNet) to alleviate the above problems. We design the feature refinement module (FRM) to fuse features at various scales, which helps to capture different levels of spatial information. It also alleviates the boundary ambiguity problem by enhancing the learning of features with boundary information. The multiscale feature attention module (MFAM) combines atrous convolution and non-local block to obtain larger receptive fields and long-range contextual information, while the feature fusion module (FFM) balances semantic and spatial information, further improving the embedding of locally discriminative features. Experimental results on ISPRS Potsdam and LoveDA datasets indicate that the proposed MFRNet outperforms other semantic segmentation methods and excels in the accuracy and consistency of object boundary segmentation.",,,,,"Yang, Gongping",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_920,"Guan, Renchu","Wang, Mingming","Bruzzone, Lorenzo",Lightweight Attention Network for Very High-Resolution Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,14,"Semantic segmentation is one of the most challenging tasks for very high-resolution (VHR) remote sensing applications. Deep convolutional neural networks (DCNNs) based on the attention mechanism have shown outstanding performance in VHR remote sensing images semantic segmentation. However, the existing attention-guided methods require the estimation of a large number of parameters that are affected by the limited number of available labeled samples that results in underperforming segmentation results. In this article, we propose a multistage feature fusion lightweight (MSFFL) model to greatly reduce the number of parameters and improve the accuracy of semantic segmentation. In this model, two parallel enhanced attention modules, i.e., the spatial attention module (SAM) and the channel attention module (CAM), are designed by introducing encoding position information. Then, a covariance calculation strategy is adopted to recalibrate the generated attention maps. The integration of enhanced attention modules into the proposed lightweight module results in an efficient lightweight attention network (LiANet). The performance of the proposed LiANet is assessed on two benchmark datasets. Experimental results demonstrate that LiANet can achieve promising performance with a small number of parameters.",Semantic segmentation,Remote sensing,Semantics,Feature extraction,"Zhao, Haishi","Yang, Chen",,,Task analysis,Computational modeling,Covariance matrices,Covariance,lightweight,position information,remote sensing,semantic segmentation,,,,,,very high-resolution (VHR) images,,,,,,,,,,,
Row_921,"Huang, Xin","Wang, Wenrui","Li, Jiayi",A Stepwise Refining Image-Level Weakly Supervised Semantic Segmentation Method for Detecting Exposed Surface for Buildings (ESB) From Very High-Resolution Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,6,"Exposed surface for buildings (ESB), which refers to exposed surfaces with traces of building construction, often leads to urban dust. Accurate ESB detection is important for planning urban development and improving urban environment. Fine-grained monitoring of ESB typically needs massive high-quality pixel-level labels, which are demanding and expensive. In contrast, obtaining cost-efficient image-level labels is more promising. Most image-level weakly supervised methods can extract pixel-level pseudo labels using the class activation map (CAM) generated by the classification network. Subsequently, these labels are applied to train the semantic segmentation network. However, the CAM is easy to miss fine-grained information, which leads to label noise. Moreover, the downsampling in the segmentation networks will further loss the spatial information. Furthermore, the sparse distribution and irregular shape of ESB pose additional challenges. Given these problems, we propose a stepwise refining image-level weakly supervised semantic segmentation method (SRIWS): 1) we introduce a new data augmentation method called SRMix to oversample the classification dataset; 2) we propose a two-branch network with a superpixel pooling layer (SPNet) as the semantic segmentation network to capture both global semantic information and spatial details; and 3) to alleviate the impact of potential noise in the initial labels, we design the high-confidence sample filtering operation (HSF) during the SPNet training. The evaluation experiments for the SRIWS were performed on three datasets. The results confirm that our proposed SRIWS presents a superior performance in recognizing ESB compared with existing state-of-the-art methods. In addition, numerous ablation experimental results indicate the effectiveness and robustness of our SRIWS.",Exposed surface for buildings (ESB),image-level samples,very high-resolution remote sensing (VHR) images,weakly supervised semantic segmentation,"Wang, Leiguang","Xie, Xing",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_922,"Lumban-Gaol, Y. A.","Rizaldy, A.","Murtiyoso, A.",COMPARISON OF DEEP LEARNING ARCHITECTURES FOR THE SEMANTIC SEGMENTATION OF SLUM AREAS FROM SATELLITE IMAGES,,2023,1,"The mapping of slum areas is an important task when considering the necessity for an inclusive, safe and resilient cities. While many methods exist in this regard, the use of machine learning and more specifically deep learning has gained traction in recent years. In this paper, we present a systematic comparison of existing deep learning architectures and backbones. The experiments in the paper investigate the question of which architecture and backbone combination and which configuration of dataset preparation is best for use in slum mapping. In another experiment we implemented the trained model to predict slums in existing open data. The experiments in the paper used public open data provided by Helber et al. (2018). Results show that FPN with vgg16 backbone showed the most potential in this particular application. The results of the semantic segmentation also shows promise, although the discrepancy in slum characteristic still hinders a proper generalization of its use.",deep learning,semantic segmentation,slums,remote sensing,,,,,,,,,,,,,,"GEOSPATIAL WEEK 2023, VOL. 48-1",,,,,,,,,,,,,,,
Row_923,"Song, A.",,,Semantic Segmentation of UAV image using Combined U-net and heterogeneous UAV imagery datasets,,2022,0,"Semantic segmentation of urban areas can provide useful information for analyzing and detecting changes in urban development. Recently, numerous remote sensing image datasets from various platforms have been acquired, and various semantic segmentation studies using them have been conducted. However, they do not contain many images because of their large data capacity and difficulty in constructing label data. Furthermore, it is difficult to use them simultaneously because each dataset has a different spatial resolution, shooting angle, and meaningful objects. In this study, two different UAV image datasets, such as UAVid semantic segmentation and semantic drone datasets, were used to train a combined U-net model to use heterogeneous remote sensing datasets for semantic segmentation tasks simultaneously. The UAVid dataset has a flight height of 50 m and 300 images with eight classes. However, the semantic drone dataset was acquired at an altitude of 5-30 m above the ground and contains 598 images with 20 classes. The combined U-net model is based on the U-net architecture, but it receives input from two different data sources. The experimental results showed that learning two datasets with a combined U-net improved semantic segmentation accuracy more than learning each data with a U-net. This study confirms the ability to train two different datasets acquired from different places and platforms simultaneously; thus, evaluating the applicability of semantic segmentation studies using heterogeneous remote sensing datasets.",Semantic segmentation,UAV,combined U-net,heterogeneous dataset,,,,,,,,,,,,,,REMOTE SENSING TECHNOLOGIES AND APPLICATIONS IN URBAN ENVIRONMENTS VII,,,,,,,,,,,,,,,
Row_924,"Zhang, Junping","Li, Tong","Lu, Xiaochen",Semantic Classification of High-Resolution Remote-Sensing Images Based on Mid-level Features,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,JUN 2016,37,"With the resolution improvement of the remote-sensing images, more details are shown clearly. The challenge that comes along is how to boost the relatively low classification accuracy caused by using pixel-based image classification approaches and low-level visual structure. The low-level features (LLF) may not well describe the image due to the semantic gap between low-level visual features and high-level semantics of images. The bag-of-visual-words (BOV) model which generates mid-level features was proposed to bridge the two levels. However, it generally neglects the context information between local patches. In this paper, an object-oriented semantic classification algorithm that combines BOV with the optimal segmentation scale is presented. In this algorithm, BOV addresses the problem of the representation of mid-level for scenes, while the optimal segmentation scale intends to overcome the defect of conventional BOV in lacking of relationship between image patches and to give more thorough description. The object-based BOV is presented to construct mid-level representations for object description instead of LLF, and histogram intersection kernel (HIK) is introduced in support vector machine (SVM) for classification. The experiments conducted on three datasets testify the superiority of the proposed algorithm.",Bag-of-visual-words (BOV),mid-level representations,multiscale segmentation,object-oriented,"Cheng, Zhen",,,,semantic classification,,,,,,,,,,,,,,,,,,,,,,,,
Row_925,"Yu, Chuang","Hu, Zhuhua","Li, Ruoqing",Segmentation and density statistics of mariculture cages from remote sensing images using mask R-CNN,INFORMATION PROCESSING IN AGRICULTURE,SEP 2022,11,"The normal growth of fishes is closely relevant to the density of mariculture. It is of great significance to accurately calculate the breeding area of specific sea area from satellite remote sensing images. However, there are no reports about cage segmentation and density detection based on remote sensing images so far. And the accurate segmentation of cages faces challenges from very large high-resolution images. Firstly, a new public mariculture cage data set is built. Secondly, the training set is augmented via sample variations to improve the robustness of the model. Then, for cage segmentation and density statistics, a new methodology based on Mask R-CNN is proposed. Using dividing and stitching technologies, the entire remote sensing test images of the cage can be accurately segmented. Finally, using the trained model, the object detection features and segmentation characteristics can be obtained at the same time. Considering only the area within the target detection frame, the proposed method can count the pixels in the segmented area, which can obtain accurate area and density while reducing time-consuming. Experimental results demonstrate that, compared with traditional contour extraction method and U-Net based scheme, the proposed scheme can significantly improve segmentation precision and model's robustness. The relative error of the actual area is only 1.3%.(c) 2021 China Agricultural University. Production and hosting by Elsevier B.V. on behalf of KeAi. This is an open access article under the CC BY-NC-ND license (http://creativecommons. org/licenses/by-nc-nd/4.0/).",Deep learning,Mask R-CNN,Image segmentation,Remote sensing,"Xia, Xin","Zhao, Yaochi","Fan, Xiang","Bai, Yong",,,,,,,,,,,,,,,,,,,,,,,,,
Row_926,"Wang, Zhengxin","Zhao, Longlong","Meng, Jintao",Deep Learning-Based Cloud Detection for Optical Remote Sensing Images: A Survey,REMOTE SENSING,DEC 2024,0,"In optical remote sensing images, the presence of clouds affects the completeness of the ground observation and further affects the accuracy and efficiency of remote sensing applications. Especially in quantitative analysis, the impact of cloud cover on the reliability of analysis results cannot be ignored. Therefore, high-precision cloud detection is an important step in the preprocessing of optical remote sensing images. In the past decade, with the continuous progress of artificial intelligence, algorithms based on deep learning have become one of the main methods for cloud detection. The rapid development of deep learning technology, especially the introduction of self-attention Transformer models, has greatly improved the accuracy of cloud detection tasks while achieving efficient processing of large-scale remote sensing images. This review provides a comprehensive overview of cloud detection algorithms based on deep learning from the perspective of semantic segmentation, and elaborates on the research progress, advantages, and limitations of different categories in this field. In addition, this paper introduces the publicly available datasets and accuracy evaluation indicators for cloud detection, compares the accuracy of mainstream deep learning models in cloud detection, and briefly summarizes the subsequent processing steps of cloud shadow detection and removal. Finally, this paper analyzes the current challenges faced by existing deep learning-based cloud detection algorithms and the future development direction of the field.",cloud detection,deep learning,semantic segmentation,optical satellite imagery,"Han, Yu","Li, Xiaoli","Jiang, Ruixia","Chen, Jinsong",remote sensing,,,,,,,,"Li, Hongzhong",,,,,,,,,,,,,,,,
Row_927,"Weng, Liguo","Pang, Kai","Xia, Min",Sgformer: A Local and Global Features Coupling Network for Semantic Segmentation of Land Cover,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,20,"With the introduction of Earth observation satellites, the classification technology through high-definition remote sensing images appeared. After decades of evolution, the land cover classification method in high-definition satellite maps has been gradually improved. Recently, high-definition remote sensing maps have been applied to land cover classification. Nowadays, classification methods using high-definition maps have these following problems. First, the traditional land cover classification methods cannot process the rich details in high-definition maps. Second, there are different acquisition conditions in the maps of different regions, which leads to distortion, deformation, and illumination blur of remote sensing images. Third, the existing methods are unable to provide a good generalization performance. To address these issues, a dual-branch parallel network structure is proposed, called Sgformer, to improve the performance of the transformer in the context of high-definition remote sensing maps. The network enhances perceptual learning with convolution operators that extract local features and a self-attention module that captures global representations. Local information and global representations with semantic divergence are fused through a feature coupling module. At last, a decoder is designed to maximize the preservation of local features and global representations and to better recover high-definition feature maps. The results of semantic segmentation experiments show that the methodology in this study has higher accuracy than the other methodologies.",Deep learning,land cover,neural network,remote sensing,"Lin, Haifeng","Qian, Ming","Zhu, Changjie",,semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_928,"He, Yawen","Jin, Feng","Li, Yongheng",Integrating semantic segmentation and edge detection for agricultural greenhouse extraction,JOURNAL OF APPLIED REMOTE SENSING,APR 1 2024,0,"Agricultural greenhouses have a negative impact on the ecological environment while bringing huge economic and social benefits. Therefore, it is of great significance to obtain greenhouse information in a timely and accurate manner. Due to the complex spectral characteristics and dense spatial distribution characteristics of greenhouses, although the extraction of greenhouses based on a single semantic segmentation model can extract the area with high precision, the segmentation process has a serious problem of boundary adhesion between greenhouses, which makes it difficult to accurately obtain the quantity of greenhouses. To address this, our study proposes a method for greenhouse extraction that integrates semantic segmentation and edge constraints, using high-spatial-resolution remote sensing images to accurately extract the area and quantity of greenhouses. This method employs an improved semantic segmentation model (AtDy-D-LinkNet) to extract the greenhouse area, which embeds a convolutional attention module into the D-LinkNet and adopts a dynamic upsampling strategy, achieving precise greenhouse extraction. Experiments demonstrate that the improved model increased the recall, precision, F1 score, and intersection over union by 1.68%, 2.27%, 1.93%, and 3.54%, respectively, compared to the original model. To address the significant edge adhesion issue in semantic segmentation and accurately extract the quantity of greenhouses, we developed an edge constraint approach. This approach uses an edge detection model to extract greenhouse boundaries, further constrains the greenhouse surfaces, separates adhered greenhouses, and outputs vector patches representing individual greenhouses, thereby achieving precise greenhouse quantity extraction. The experiments show that this method effectively combines the advantages of semantic segmentation and edge detection. It not only ensures the accuracy of greenhouse area extraction but also effectively solves the boundary adhesion issue, significantly improving quantity extraction accuracy, resulting in vector patches that align with the actual area, quantity, and spatial distribution of greenhouses. This can provide a data foundation for greenhouse management and planning in agriculture.",agricultural greenhouse,high spatial resolution remote sensing,semantic segmentation,edge detection,,,,,edge adhesion,,,,,,,,,,,,,,,,,,,,,,,,
Row_929,"Zhang, Mi","Hu, Xiangyun","Zhao, Like",Translation-aware semantic segmentation via conditional least-square generative adversarial networks,JOURNAL OF APPLIED REMOTE SENSING,DEC 23 2017,11,"Semantic segmentation has recently made rapid progress in the field of remote sensing and computer vision. However, many leading approaches cannot simultaneously translate label maps to possible source images with a limited number of training images. The core issue is insufficient adversarial information to interpret the inverse process and proper objective loss function to overcome the vanishing gradient problem. We propose the use of conditional least squares generative adversarial networks (CLS-GAN) to delineate visual objects and solve these problems. We trained the CLS-GAN network for semantic segmentation to discriminate dense prediction information either from training images or generative networks. We show that the optimal objective function of CLS-GAN is a special class of f-divergence and yields a generator that lies on the decision boundary of discriminator that reduces possible vanished gradient. We also demonstrate the effectiveness of the proposed architecture at translating images from label maps in the learning process. Experiments on a limited number of high resolution images, including close-range and remote sensing datasets, indicate that the proposed method leads to the improved semantic segmentation accuracy and can simultaneously generate high quality images from label maps. (C) 2017 Society of Photo-Optical Instrumentation Engineers (SPIE)",generative adversarial network,deep learning,semantic segmentation,divergence,"Pang, Shiyan","Gong, Jinqi","Luo, Min",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_930,"Ye, Wenhui","Zhang, Wei","Lei, Weimin",Remote sensing image instance segmentation network with transformer and multi-scale feature representation,EXPERT SYSTEMS WITH APPLICATIONS,DEC 30 2023,12,"The goal of remote sensing image (RSI) instance segmentation is to perform instance-level semantic parsing of its contents. Aside from classifying and locating regions of interest (RoI), it also requires assigning finer pixel -wise annotations to objects. However, RSI often suffers from cluttered backgrounds, variable object scales, and complex object edge contours, making the instance segmentation task more challenging. In this work, we analytically customize an instance segmentation model that is more suitable for RSI. Specifically, we propose three novel modules for a region-based instance segmentation framework, namely Channel-Spatial Attention Module (CSA), Multi-Scale Aware Module (MSA), and Semantic Relation Learning Module (SRL). Among them, feature calibration performed by CSA can alleviate the semantic gap between low-level features and high-level semantics in both channel and spatial dimensions. Inheriting the capabilities of both the convolutional neural network (CNN) and the Transformer, SRL can help the network integrate both neighborhood features and long-range dependencies for instance semantic prediction. The MSA module designs a cascaded residual structure with different receptive fields to model the scale variation of objects in RSI. Experimental results on challenging ISAID, NWPU VHR-10, SSDD, BITTC and HRSID datasets demonstrate the superiority of our method, achieving mask APs of 40.2%, 68.2%, 68.4%, 50.4% and 55.8% respectively. Code and pretrained models are available at https://github.com/Sherlock1018/RSIISN.",Remote sensing image,Instance segmentation,Region-based,Long-range dependencies,"Zhang, Wenchao","Chen, Xinyi","Wang, Yanwen",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_931,"Cai, Miaoxin","Chen, He","Zhang, Tong",Consistency Regularization Based on Masked Image Modeling for Semisupervised Remote Sensing Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,1,"Semisupervised semantic segmentation aims to effectively leverage both unlabeled and scare labeled images, reducing the reliance on labor-intensive pixel-level labeling for extensive training processes. The leading semisupervised learning method, consistency regularization, employs weak and strong data augmentations to diversify input representations. Ultimately the model is compelled to maintain consistent predictions across different input views, thus boosting the model's generalization. However, previous methods suffered from limited input representation space introduced by linear transformations such as cutmix. To address such issue, a consistency regularization based on masked image modeling (MIM) called MIMSeg is proposed to achieve accurate segmentation with limited labeled images. First, MIM pixel-wise perception with ViT encoder-decoder lays the foundation for expanding the data representation space. Second, collaborating with weak data augmentations, two MIM-related strong data augmentations are developed to generate more challenging input views for consistent predictions. Precisely, weak data augmentations are employed to replicate input views from various perspectives while a controllable generative strong data augmentation called masked image reconstruction (MIR) is crafted to simulate multiple imaging diversity while preserving the original semantic information intact. In addition, a more severe strong data augmentation masked context perturbation (MCP) is designed to further generate more challenging input views and alleviate semantic deficiency via masked category prediction. Leveraging the MIM perception and two MIM-related strong data augmentations, the model is compelled to achieve consistency predictions across diverse input views from weak data augmentations, MIR and MCP. These components result in the generation of more stable pixel-level pseudo-labels and facilitate collaborative training between unlabeled and labeled images. Extensive experiments have shown that MIMSeg can achieve state-of-the-art performance in pixel-level prediction with very limited sample annotations.",Semantics,Data augmentation,Semantic segmentation,Data models,"Zhuang, Yin","Chen, Liang",,,Training,Predictive models,Imaging,Consistency regularization,masked image modeling (MIM),semisupervised semantic segmentation,,,,,,,,,,,,,,,,,,,
Row_932,"Liu, Xinran","Peng, Yuexing","Lu, Zili",Feature-Fusion Segmentation Network for Landslide Detection Using High-Resolution Remote Sensing Images and Digital Elevation Model Data,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,31,"Landslide is one of the most dangerous and frequently occurred natural disasters. The semantic segmentation technique is efficient for wide area landslide identification from high-resolution remote sensing images (HRSIs). However, considerable challenges exist because the effects of sediments, vegetation, and human activities over long periods of time make visually blurred old landslides very challenging to detect based upon HRSIs. Moreover, for terrain features like slopes, aspect and altitude variations cannot be sufficiently extracted from 2-D HRSIs but can be from digital elevation model (DEM) data. Then, a feature-fusion based semantic segmentation network (FFS-Net) is proposed, which can extract texture and shape features from 2-D HRSIs and terrain features from DEM data before fusing these two distinct types of features in a higher feature layer. To segment landslides from background, a multiscale channel attention module is purposely designed to balance the low-level fine information and high-level semantic features. In the decoder, transposed convolution layer replaces original mathematical bilinear interpolation to better restore image resolution via learnable convolutional kernels, and both dropout and batch normalization (BN) are introduced to prevent over-fitting and accelerate the network convergence. Experimental results are presented to validate that the proposed FFS-Net can greatly improve the segmentation accuracy of visually blurred old landslides. Compared to U-Net and DeepLabV3+, FFS-Net can improve the mean intersection over union (mIoU) metric from 0.508 and 0.624 to 0.67, the F1 metric from 0.254 and 0.516 to 0.596, and the pixel accuracy (PA) metric from 0.874 and 0.906 to 0.92, respectively. For the detection of visually distinct landslides, FFS-NET also offers comparable detection performance, and the segmentation is improved for visually distinct landslides with similar color and texture to surroundings.",Feature extraction,Terrain factors,Semantics,Shape,"Li, Wei","Yu, Junchuan","Ge, Daqing","Xiang, Wei",Image color analysis,Data mining,Optical sensors,Digital elevation model (DEM),feature fusion,high-resolution remote sensing image (HRSI),landslide detection,semantic segmentation,,,,,,Siamese network,,,,,,,,,,,
Row_933,"Fu, Junjie","Yi, Xiaomei","Wang, Guoying",Research on Ground Object Classification Method of High Resolution Remote-Sensing Images Based on Improved DeeplabV3+,SENSORS,OCT 2022,11,"Ground-object classification using remote-sensing images of high resolution is widely used in land planning, ecological monitoring, and resource protection. Traditional image segmentation technology has poor effect on complex scenes in high-resolution remote-sensing images. In the field of deep learning, some deep neural networks are being applied to high-resolution remote-sensing image segmentation. The DeeplabV3+ network is a deep neural network based on encoder-decoder architecture, which is commonly used to segment images with high precision. However, the segmentation accuracy of high-resolution remote-sensing images is poor, the number of network parameters is large, and the cost of training network is high. Therefore, this paper improves the DeeplabV3+ network. Firstly, MobileNetV2 network was used as the backbone feature-extraction network, and an attention-mechanism module was added after the feature-extraction module and the ASPP module to introduce focal loss balance. Our design has the following advantages: it enhances the ability of network to extract image features; it reduces network training costs; and it achieves better semantic segmentation accuracy. Experiments on high-resolution remote-sensing image datasets show that the mIou of the proposed method on WHDLD datasets is 64.76%, 4.24% higher than traditional DeeplabV3+ network mIou, and the mIou on CCF BDCI datasets is 64.58%. This is 5.35% higher than traditional DeeplabV3+ network mIou and outperforms traditional DeeplabV3+, U-NET, PSP-NET and MACU-net networks.",high-resolution remote-sensing images,semantic segmentation,object classification,,"Mo, Lufeng","Wu, Peng","Kapula, Kasanda Ernest",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_934,"Chong, Qianpeng","Xu, Jindong",,A DUAL-BRANCH AWARENESS NETWORK FOR SMALL OBJECT SEGMENTATION IN LARGE-SCALE REMOTE SENSING SCENES,,2023,0,"The more detailed and accurate earth observation has been made driven by the advancement of satellites and sensors optical photography technology, which poses both a challenge and an opportunity to small object segmentation task. However, the inherent difficulty and inadequate consideration still make small object segmentation task inevitably encounter a performance gain bottleneck. In this paper, we consider the longstanding but underestimated challenges in this task and give a point-to-point solution to response them. Specifically, we introduce a discriminative structure, i.e., a dual-branch awareness network for small object segmentation, named DASNet. In this structure, we propose the small object activation branch and the fuzzy refinement branch to avoid the negative influence of redundant background and ensure the small object segmentation accuracy, respectively. These two branches work collaboratively to mimic the process of human visual perception on small object. Finally, we propose a hierarchical unbiased loss to eliminate the bias against small objects in the regression process. Extensive experiments demonstrated that DASNet is competitive against some advanced methods for small object segmentation.",dual-branch,remote sensing,semantic segmentation,small object,,,,,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_935,"Huang, Wei","Shi, Yilei","Xiong, Zhitong",AdaptMatch: Adaptive Matching for Semisupervised Binary Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,10,"There are various binary semantic segmentation tasks in remote sensing (RS) that aim to extract the foreground areas of interest, such as buildings and roads, from the background in satellite images. In particular, semisupervised learning (SSL), which can use limited labeled data to guide a large amount of unlabeled data for model training, can significantly promote the fast applications of these tasks in practice. However, due to the predominance of the background in RS images, the foreground only accounts for a small proportion of the pixels. It poses a challenge: models are biased toward the majority class of the background, leading to poor performance on the minority class of the foreground. To address this issue, this article proposes a novel and effective SSL framework, adaptive matching (AdaptMatch), for RS binary segmentation. AdaptMatch calculates individual and adaptive thresholds of the foreground and background based on their convergence difficulty in an online manner at the training stage; the adaptive thresholds are then used to select the high-confidence pseudo-labeled data of the two classes for model self-training in turn. Extensive experiments are conducted on two widely studied RS binary segmentation tasks, building footprint extraction and road extraction, to demonstrate the effectiveness and generalizability of the proposed method. The results show that the proposed AdaptMatch achieves superior performance compared with some state-of-the-art semisupervised methods in RS binary segmentation tasks. The codes will be publicly available at https://github.com/zhu-xlab/AdaptMatch.",Adaptive threshold,binary segmentation,building footprint extraction,remote sensing (RS),"Zhu, Xiao Xiang",,,,road extraction,semisupervised learning (SSL),,,,,,,,,,,,,,,,,,,,,,,
Row_936,"Wang, Di","Zhang, Jing","Du, Bo",An Empirical Study of Remote Sensing Pretraining,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,108,"Deep learning has largely reshaped remote sensing (RS) research for aerial image understanding and made a great success. Nevertheless, most of the existing deep models are initialized with the ImageNet pretrained weights since natural images inevitably present a large domain gap relative to aerial images, probably limiting the fine-tuning performance on downstream aerial scene tasks. This issue motivates us to conduct an empirical study of RS pretraining (RSP) on aerial images. To this end, we train different networks from scratch with the help of the largest RS scene recognition dataset up to now-MillionAID-to obtain a series of RS pretrained backbones, including both convolutional neural networks (CNNs) and vision transformers, such as Swin and ViTAE, which have shown promising performance on computer vision tasks. Then, we investigate the impact of RSP on representative downstream tasks, including scene recognition, semantic segmentation, object detection, and change detection using these CNN and vision transformer backbones. Empirical study shows that RSP can help deliver distinctive performances in scene recognition tasks and in perceiving RS-related semantics, such as ""Bridge"" and ""Airplane."" We also find that, although RSP mitigates the data discrepancies of traditional ImageNet pretraining on RS images, it may still suffer from task discrepancies, where downstream tasks require different representations from scene recognition tasks. These findings call for further research efforts on both large-scale pretraining datasets and effective pretraining methods. The codes and pretrained models will be released at https://github.com/ViTAETransformer/ViTAE-Transformer-Remote-Sensing.",Classification,convolutional neural network (CNN),detection,remote sensing (RS) pretraining (RSP),"Xia, Gui-Song","Tao, Dacheng",,,semantic segmentation,vision transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_937,"Abdollahi, Abolfazl","Pradhan, Biswajeet","Sharma, Gaurav",Improving Road Semantic Segmentation Using Generative Adversarial Network,IEEE ACCESS,2021,45,"Road network extraction from remotely sensed imagery has become a powerful tool for updating geospatial databases, owing to the success of convolutional neural network (CNN) based deep learning semantic segmentation techniques combined with the high-resolution imagery that modern remote sensing provides. However, most CNN approaches cannot obtain high precision segmentation maps with rich details when processing high-resolution remote sensing imagery. In this study, we propose a generative adversarial network (GAN)-based deep learning approach for road segmentation from high-resolution aerial imagery. In the generative part of the presented GAN approach, we use a modified UNet model (MUNet) to obtain a high-resolution segmentation map of the road network. In combination with simple pre-processing comprising edge-preserving filtering, the proposed approach offers a significant improvement in road network segmentation compared with prior approaches. In experiments conducted on the Massachusetts road image dataset, the proposed approach achieves 91.54% precision and 92.92% recall, which correspond to a Mathews correlation coefficient (MCC) of 91.13%, a Mean intersection over union (MIOU) of 87.43% and a F1-score of 92.20%. Comparisons demonstrate that the proposed GAN framework outperforms prior CNN-based approaches and is particularly effective in preserving edge information.",Roads,Image segmentation,Generative adversarial networks,Semantics,"Maulud, Khairul Nizam Abdul","Alamri, Abdullah",,,Remote sensing,Generators,Feature extraction,GAN,road segmentation,remote sensing,deep learning,U-Net,,,,,,,,,,,,,,,,,
Row_938,"Tang, Bochuan","Tuerxun, Palidan","Qi, Ranran",AMFFNet: attention-guided multi-level feature fusion network for land cover classification of remote sensing images,JOURNAL OF APPLIED REMOTE SENSING,APR 1 2023,1,"In the field of remote sensing, the classification of land cover is a pivotal and challenging issue. Standard models fail to capture global and semantic information in remote sensing images despite the fact that a convolutional neural network provides robust support for semantic segmentation. In addition, owing to disparities in semantic levels and spatial resolution, the simple fusion of low-level and high-level features may diminish the efficiency. To address these deficiencies, an attention-guided multi-level feature fusion network (AMFFNet) is proposed in this study. The proposed AMFFNet approach is designed as an encoder-decoder network with the inclusion of a multi-level feature fusion module (MFF) and a dual attention map module (DAM). A DAM models the semantic association of features from a spatial and channel perspective, and an MFF bridges the semantic and resolution gaps between high-level and low-level features. Furthermore, we propose a residual-based boundary refinement upsample module to further optimize the object boundaries. The experimental results indicate that the proposed strategy can considerably enhance the accuracy of land cover classification, achieving a mean intersection over union of 90.39% on the LandCover.ai dataset and 63.14% on the Gaofen Image Dataset with 15 categories (GID-15).",remote sensing,land cover classification,semantic segmentation,multi-level feature fusion,"Yang, Guangqi","Qian, Yurong",,,attention mechanism,,,,,,,,,,,,,,,,,,,,,,,,
Row_939,"Zhao, Wenyu","Xia, Min","Weng, Liguo",SPNet: Dual-Branch Network with Spatial Supplementary Information for Building and Water Segmentation of Remote Sensing Images,REMOTE SENSING,SEP 2024,1,"Semantic segmentation is primarily employed to generate accurate prediction labels for each pixel of the input image, and then classify the images according to the generated labels. Semantic segmentation of building and water in remote sensing images helps us to conduct reasonable land planning for a city. However, many current mature networks face challenges in simultaneously attending to both contextual and spatial information when performing semantic segmentation on remote sensing imagery. This often leads to misclassifications and omissions. Therefore, this paper proposes a Dual-Branch Network with Spatial Supplementary Information (SPNet) to address the aforementioned issues. We introduce a Context-aware Spatial Feature-Extractor Unit (CSF) to extract contextual and spatial information, followed by the Feature-Interaction Module (FIM) to supplement contextual semantic information with spatial details. Additionally, incorporating the Goal-Oriented Attention Mechanism helps in handling noise. Finally, to obtain more detailed branches, a Multichannel Deep Feature-Extraction Module (MFM) is introduced to extract features from shallow-level network layers. This branch guides the fusion of low-level semantic information with high-level semantic information. Experiments were conducted on building and water datasets, respectively. The results indicate that the segmentation accuracy of the model proposed in this paper surpasses that of other existing mature models. On the building dataset, the mIoU reaches 87.57, while on the water dataset, the mIoU achieves 96.8, which means that the model introduced in this paper demonstrates strong generalization capabilities.",semantic segmentation,building and water,spatial information,dual-branch network,"Hu, Kai","Lin, Haifeng","Zhang, Youke","Liu, Ziheng",,,,,,,,,,,,,,,,,,,,,,,,,
Row_940,"Sevak, Jay S.","Kapadia, Aerika D.","Chavda, Jaiminkumar B.",Survey on Semantic Image Segmentation Techniques,,2017,25,"Semantic image segmentation is a vast area of interest for computer vision and machine learning researchers. Many vision applications need accurate and efficient image segmentation and segment classification mechanisms for assessing the visual contents and perform the real-time decision making. The application area includes remote sensing, autonomous driving, indoor navigation, video surveillance and virtual or augmented reality systems etc. The segmentation and classification of objects generate the specific performance parameters for various applications which require detailed domain analysis. There are broad range of applications where remote sensing image scene classification play an important role and has been receiving remarkable attention. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This survey paper provides a review of different traditional methods of image segmentation and classification. By comparing these methods with semantic image segmentation using deep learning it is assumed to show the far better result.",Semantic segmentation,segmentation pipeline,unsupervised segmentation,random decision forest,"Shah, Arpita","Rahevar, Mrugendrasinh",,,deep learning,feature and preprocessing method,,,,,,,,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT SUSTAINABLE SYSTEMS (ICISS 2017),,,,,,,,,,,,,,,
Row_941,"Wang, He","Zhang, Mengmeng","Li, Wei",Unbalanced Class Learning Network With Scale-Adaptive Perception for Complicated Scene in Remote Sensing Images Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,3,"The semantic segmentation of wide-field remote sensing images (RSIs) plays a significant role in many fields. However, due to the complexity of the content of RSIs, the dataset often has an uneven distribution of land type between different classes and large gaps in the scales of different objects. This often creates great problems for fine segmentation. To solve the issues, an unbalanced class learning network with scale-adaptive perception (UCSANet) is proposed, which can adaptively cope with multiscale objects and unbalanced classes. The design can be inserted in any convolution network easily and can enrich features without increasing too many parameters. The network groups feature and use atrous convolutions with different dilated rates on different groups to extract multiscale features while separable convolutions reduce the amount of network parameters. Then, the fusion of features between different scales is achieved through the self-attention mechanism. Furthermore, a weight map is designed to adaptively combine the predictions of two segmentation heads with cross-entropy loss and Lovasz-Softmax loss, respectively, which enable the network to focus on learning low-frequency classes without affecting high-frequency classes. Experimental results on GF-6 MSI datasets demonstrate that the proposed UCSANet performs significantly better than others and achieves multiclass segmentation more accurately.",Feature extraction,Convolution,Semantic segmentation,Remote sensing,"Gao, Yunhao","Gui, Yuanyuan","Zhang, Yuxiang",,Data mining,Head,Kernel,Deep learning,scale-adaptive,semantic segmentation,unbalanced data,,,,,,,,,,,,,,,,,,
Row_942,"Wang, Gaihua","Zhai, Qianyu","Lin, Jinheng",Multi-scale network for remote sensing segmentation,IET IMAGE PROCESSING,MAY 2022,3,"The semantic segmentation of remote sensing images is a critical and challenging task. How to easily and reliably segment useful information from vast remote sensing images is a significant issue. Many methods based on convolutional neural networks have been widely explored to obtain more accurate segmentation from remote sensing images. However, due to the uniqueness of remote sensing images, such as the dramatic changes in the scale of the target object, the results are not satisfactory. To solve the problem, a special network is designed: (1) Create a new backbone network. Compared with ResNet50, the proposed method extracts features of varying sizes more effectively. (2) Reduce spatial information loss. Building a hybrid location module to compensate for the position loss caused by the down-sampling operation. (3) Models with high discriminant ability. In order to improve the discrimination ability of the model, a novel auxiliary loss function is designed to constrain the distance between inter-class and intra-class. The proposed algorithm is tested on remote sensing datasets (e.g., NWPU-45, DLRSD, and WHDLD). The experimental results show that this method obtains the best results and achieves state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_943,"Zheng, Zixian","Zhang, Xueliang","Xiao, Pengfeng",Integrating Gate and Attention Modules for High-Resolution Image Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,20,"Semantic segmentation of high-resolution (HR) remote sensing images achieved great progress by utilizing deep convolutional neural networks (DCNNs) in recent years. However, the decrease of resolution in the feature map of DCNNs brings about the loss of spatial information and thus leads to the blurring of object boundary and misclassification of small objects. In addition, the class imbalance and the high diversity of geographic objects in HR images exacerbate the performance. To deal with the above problems, we proposed an end-to-end DCNN network named GAMNet to balance the contradiction between global semantic information and local details. An integration of attention and gate module (GAM) is specially designed to simultaneously realize multiscale feature extraction and boundary recovery. The integration module can be inserted in an encoder-decoder network with skip connection. Meanwhile, a composite loss function is designed to achieve deep supervision of GAM by adding an auxiliary loss, which can help improve the effectiveness of the integration module. The performance of GAMNet is quantitatively evaluated on the ISPRS 2-D semantic labeling datasets and achieves state-of-the-art performance in comparison with other representative methods.",Semantics,Image segmentation,Feature extraction,Decoding,"Li, Zhenshi",,,,Remote sensing,Spatial resolution,Logic gates,Attention module (AM),gate module (GM),high-resolution (HR) remote sensing imagery,semantic segmentation,,,,,,,,,,,,,,,,,,
Row_944,"Wang, Jiahao","Zhao, Junhao","Sun, Hong",Satellite Remote Sensing Identification of Discolored Standing Trees for Pine Wilt Disease Based on Semi-Supervised Deep Learning,REMOTE SENSING,DEC 2022,15,"Pine wilt disease (PWD) is the most dangerous biohazard of pine species and poses a serious threat to forest resources. Coupling satellite remote sensing technology and deep learning technology for the accurate monitoring of PWD is an important tool for the efficient prevention and control of PWD. We used Gaofen-2 remote sensing images to construct a dataset of discolored standing tree samples of PWD and selected three semantic segmentation models-DeepLabv3+, HRNet, and DANet-for training and to compare their performance. To build a GAN-based semi-supervised semantic segmentation model for semi-supervised learning training, the best model was chosen as the generator of generative adversarial networks (GANs). The model was then optimized for structural adjustment and hyperparameter adjustment. Aimed at the characteristics of Gaofen-2 images and discolored standing trees with PWD, this paper adopts three strategies-swelling prediction, raster vectorization, and forest floor mask extraction-to optimize the image identification process and results and conducts an application demonstration study in Nanping city, Fujian Province. The results show that among the three semantic segmentation models, HRNet was the optimal conventional semantic segmentation model for identifying discolored standing trees of PWD based on Gaofen-2 images and that its MIoU value was 68.36%. Additionally, the GAN-based semi-supervised semantic segmentation model GAN_HRNet_Semi improved the MIoU value by 3.10%, and its recognition segmentation accuracy was better than the traditional semantic segmentation model. The recall rate of PWD discolored standing tree monitoring in the demonstration area reached 80.09%. The combination of semi-supervised semantic segmentation technology and high-resolution satellite remote sensing technology provides new technical methods for the accurate wide-scale monitoring, prevention, and control of PWD.",pine wilt disease (PWD),semi-supervised,semantic segmentation,satellite remote sensing,"Lu, Xiao","Huang, Jixia","Wang, Shaohua","Fang, Guofei",accurate monitoring,,,,,,,,,,,,,,,,,,,,,,,,
Row_945,"Liu, Ruizhong","Luo, Tingzhang","Huang, Shaoguang",CrossMatch: Cross-View Matching for Semi-Supervised Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Recently, weak-to-strong consistency-based methods have yielded a remarkable performance for remote sensing image segmentation. However, they are designed within a single view, which encounters the problems of unreliable pseudo-label supervision and insufficient ability to capture informative features for the segmentation of complex remote sensing data. In this article, we propose a cross-view weak-to-strong consistency-based method, which aggregates rich information from two irrelevant views. We employ two subnets for the two views to generate view-specific features while encouraging them to yield the same prediction. Within each view, we enhance the perturbation space of data at the image level and feature level for a more robust representation. To leverage the information from unlabeled data, we propose a cross-view weak-to-strong consistency scheme, which employs the pseudo-label of the weakly augmented data in one view to supervise the model training in another view, facilitating an effective information exchange across views. To avoid identical information extraction from the two views, we propose a cross-view contrastive loss to maximize the dissimilarity of the feature representations across views, which ensures that the learned complementary information from one view provides additional helpful information for the model training in another view. Finally, we propose a cross-view discrepancy-based supervised constraint by imposing a larger weight on the areas that exist discrepant predictions across views in the cross-entropy loss, allowing the model to focus more on the hard-to-classify regions of the images. Extensive experimental results on several benchmark datasets demonstrate that our method outperforms the state-of-the-art.",Remote sensing,Perturbation methods,Semantic segmentation,Data models,"Wu, Yuwei","Jiang, Zhen","Zhang, Hongyan",,Training,Predictive models,Feature extraction,Sensors,Reliability,Deep learning,image segmentation,remote sensing,,,,,,semi-supervised learning (SSL),,,,,,,,,,,
Row_946,"Gu, Xingjian","Yu, Supeng","Huang, Fen",Consistency Self-Training Semi-Supervised Method for Road Extraction from Remote Sensing Images,REMOTE SENSING,NOV 2024,0,"Road extraction techniques based on remote sensing image have significantly advanced. Currently, fully supervised road segmentation neural networks based on remote sensing images require a significant number of densely labeled road samples, limiting their applicability in large-scale scenarios. Consequently, semi-supervised methods that utilize fewer labeled data have gained increasing attention. However, the imbalance between a small quantity of labeled data and a large volume of unlabeled data leads to local detail errors and overall cognitive mistakes in semi-supervised road extraction. To address this challenge, this paper proposes a novel consistency self-training semi-supervised method (CSSnet), which effectively learns from a limited number of labeled data samples and a large amount of unlabeled data. This method integrates self-training semi-supervised segmentation with semi-supervised classification. The semi-supervised segmentation component relies on an enhanced generative adversarial network for semantic segmentation, which significantly reduces local detail errors. The semi-supervised classification component relies on an upgraded mean-teacher network to handle overall cognitive errors. Our method exhibits excellent performance with a modest amount of labeled data. This study was validated on three separate road datasets comprising high-resolution remote sensing satellite images and UAV photographs. Experimental findings showed that our method consistently outperformed state-of-the-art semi-supervised methods and several classic fully supervised methods.",semi-supervised,semantic segmentation,generative adversarial network,road extraction,"Ren, Shougang","Fan, Chengcheng",,,remote sensing image,,,,,,,,,,,,,,,,,,,,,,,,
Row_947,"Li, Yazhou","Cheng, Zhiyou","Wang, Chuanjian",RCCT-ASPPNet: Dual-Encoder Remote Image Segmentation Based on Transformer and ASPP,REMOTE SENSING,JAN 2023,18,"Remote image semantic segmentation technology is one of the core research elements in the field of computer vision and has a wide range of applications in production life. Most remote image semantic segmentation methods are based on CNN. Recently, Transformer provided a view of long-distance dependencies in images. In this paper, we propose RCCT-ASPPNet, which includes the dual-encoder structure of Residual Multiscale Channel Cross-Fusion with Transformer (RCCT) and Atrous Spatial Pyramid Pooling (ASPP). RCCT uses Transformer to cross fuse global multiscale semantic information; the residual structure is then used to connect the inputs and outputs. ASPP based on CNN extracts contextual information of high-level semantics from different perspectives and uses Convolutional Block Attention Module (CBAM) to extract spatial and channel information, which will further improve the model segmentation ability. The experimental results show that the mIoU of our method is 94.14% and 61.30% on the datasets Farmland and AeroScapes, respectively, and that the mPA is 97.12% and 84.36%, respectively, both outperforming DeepLabV3+ and UCTransNet.",remote image,deep learning,semantic segmentation,CNN,"Zhao, Jinling","Huang, Linsheng",,,multiscale feature fusion,Transformer,,,,,,,,,,,,,,,,,,,,,,,
Row_948,"Chen, Bo","Zhang, Jiahao","Zhou, Jianbang",Semantic image segmentation network based on deep learning,,2020,1,"Semantic segmentation is one of the basic themes in computer vision. Its purpose is to assign semantic tags to each pixel of an image, which has been applied in many fields such as medical field, intelligent transportation and remote sensing image. In this paper, we use deep learning to solve the task of remote sensing semantic image segmentation. We propose an algorithm for semantic segmentation of the Attention Seg-Net network combined with SegNet and attention gate. Our proposed network can better segment vegetation, buildings, water bodies and roads in the test set of remote sensing images.",Semantic Segmentation,Attention Seg-Net,Deep Learning,Attention gate,"Chen, Zhong","Yang, Tian","Zhang, Yanna",,,,,,,,,,,MIPPR 2019: AUTOMATIC TARGET RECOGNITION AND NAVIGATION,,,,,,,,,,,,,,,
Row_949,"Xia, Liegang","Liu, Ruiyan","Su, Yishao",Crop field extraction from high resolution remote sensing images based on semantic edges and spatial structure map,GEOCARTO INTERNATIONAL,JAN 1 2024,2,"Crop field boundary extraction is crucial to remote sensing images attained to support agricultural production and planning. In recent years, deep convolutional neural networks (CNNs) have gained significant attention for edge detection tasks. Moreover, transformers have shown superior feature extraction and classification capabilities compared to CNNs due to their self-attention mechanism. We proposed a novel structure that combines full edge extraction with CNNs and enhances connectivity with transformers, consisting of three stages: a) preprocessing the training data; b) training the semantic edge and spatial structure graph models; and c) vectorizing the fusion of semantic edge and spatial structure graph outputs. To cater specifically to high-resolution remote sensing image crop-field boundary extraction, we developed a CNN model called Densification D-LinkNet. Its full-scale skip connections and edge-guided module adapted well to different crop-field boundary features. Additionally, we employed a spatial graph structure generator (Relationformer) based on object detection that directly outputs the structural graph of the crop field boundary. This method relies on good connectivity to repair fragmented edges that may appear in semantic edge detection. Through multiple experiments and comparisons with other edge-detection methods, such as BDCN, DexiNed, PidiNet, and EDTER, we demonstrated that our proposed method can achieve at least 9.77% improvement in boundary intersection over union (IoU) and 2.07% improvement in polygon IoU on two customized datasets. These results indicate the effectiveness and robustness of our approach.",High-resolution remote sensing images,image segmentation,semantic edge detection,transformer,"Mi, Shulin","Yang, Dezhi","Chen, Jun","Shen, Zhanfeng",,,,,,,,,,,,,,,,,,,,,,,,,
Row_950,"Yan, Zhiyuan","Li, Junxi","Li, Xuexue",RingMo-SAM: A Foundation Model for Segment Anything in Multimodal Remote-Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,24,"The proposal of the segment anything model (SAM) has created a new paradigm for the deep-learning-based semantic segmentation field and has shown amazing generalization performance. However, we find it may fail or perform poorly on multimodal remote-sensing scenarios, especially synthetic aperture radar (SAR) images. Besides, SAM does not provide category information for objects. In this article, we propose a foundation model for multimodal remote-sensing image segmentation called RingMo-SAM, which can not only segment anything in optical and SAR remote-sensing data, but also identify object categories. First, a large-scale dataset containing millions of segmentation instances is constructed by collecting multiple open-source datasets in this field to train the model. Then, by constructing an instance-type and terrain-type category-decoupling mask decoder (CDMDecoder), the categorywise segmentation of various objects is achieved. In addition, a prompt encoder embedded with the characteristics of multimodal remote-sensing data is designed. It not only supports multibox prompts to improve the segmentation accuracy of multiobjects in complicated remote-sensing scenes, but also supports SAR characteristics prompts to improve the segmentation performance on SAR images. Extensive experimental results on several datasets including iSAID, ISPRS Vaihingen, ISPRS Potsdam, AIR-PolSAR-Seg, and so on have demonstrated the effectiveness of our method.",Remote sensing,Task analysis,Semantic segmentation,Feature extraction,"Zhou, Ruixue","Zhang, Wenkai","Feng, Yingchao","Diao, Wenhui",Radar polarimetry,Training,Adaptation models,Multimodal remote-sensing images,prompt learning,segment anything model (SAM),semantic segmentation,,"Fu, Kun",,"Sun, Xian",,,,,,,,,,,,,,
Row_951,"Zhang, Zhen","Huang, Xin","Li, Jiayi",DWin-HRFormer: A High-Resolution Transformer Model With Directional Windows for Semantic Segmentation of Urban Construction Land,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,17,"In this article, a deep neural network for semantic segmentation of high-resolution remote sensing images is proposed for urban construction land classification. The network follows a high-resolution network (HRNet) architecture. Specifically, a directional self-attention on the paths of different resolutions is proposed, aiming to correct the directional bias caused by the attention of strip windows during the model learning, while also reducing the computational complexity, and allowing the model to improve both the accuracy and the speed. At the end of the network, a distributed alignment module with spatial information is constructed to train additional learnable parameters, to adjust the biased decision boundaries through a two-stage learning strategy, and alleviate the problem of accuracy degradation due to the unbalanced training data. We tested the proposed method and compared it with the current state-of-the-art (SOTA) semantic segmentation methods on the Luojia-fine-grained land cover (FGLC) dataset and the Wuhan Dense Labeling Dataset (WHDLD), and the proposed one obtained the best performance. We also verified the effectiveness of each component of the network through ablation experiments.",Transformers,Semantic segmentation,Computational modeling,Task analysis,,,,,Windows,Monitoring,Remote sensing,Deep learning,remote sensing imagery,semantic segmentation,transformer,urban construction land,,,,,,,,,,,,,,,,,
Row_952,"Pang, Shuai","Gao, Lianxue",,Multihead attention mechanism guided ConvLSTM for pixel-level segmentation of ocean remote sensing images,MULTIMEDIA TOOLS AND APPLICATIONS,JUL 2022,5,"Semantic segmentation of ocean remote sensing images classifies each pixel in the image according to the ocean background and island type, and is an important research direction in the field of remote sensing image processing. Due to large differences in the scale of islands in ocean remote sensing images and the complexity of island boundaries, it is difficult to accurately extract features of ocean remote sensing images, which makes it difficult to accurately segment ocean remote sensing images. Convolutional neural networks have gradually become the mainstream algorithm in the field of image processing due to their autonomous hierarchical extraction of image features. In this paper, the MAGC-Net neural network model, which is based on the multihead attention mechanism and ConvLSTM, is used to segment ocean remote sensing images to improve the accuracy of semantic segmentation. First, shallow features are obtained via multiscale convolution, and multiple weights are assigned to features by the multihead attention mechanism (global, local, maximum). Then, the semantic relationship between the features is described through the integrated ConvLSTM module, and deep features are generated. Finally, deep features are filtered through residual blocks, reducing redundant features and improving segmentation accuracy. Experimental results with the NWPU-RESISC45 dataset demonstrate the effectiveness and robustness of the proposed algorithm.",Ocean remote sensing,Deep learning,Multihead attention mechanism,Multiscale convolutional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_953,"Niu, Mengjia","Zhang, Yongjun","Yang, Gang",Semantic segmentation for remote sensing images via dense feature extraction and companion loss neural network,INTERNATIONAL JOURNAL OF REMOTE SENSING,NOV 17 2021,2,"Semantic segmentation models with good performance are crucial for the practical application of high-resolution remote-sensing images (RSI). Compared with nature images, in most cases the RSI dataset has the problem of unbalanced sample distribution between classes and unbalanced target size ratio. Using semantic segmentation to pixel-wise classify and identification RSI can solve this problem to some extent. At present, most semantic segmentation models based on mainstream networks solve these problems from the object scale and super-pixel perspective, whereas the accuracy still needs to be improved. To enhance the quality of the predicted feature maps, the Dense-Inception Net (D-INet) model is proposed based on the idea of DenseNet feature reuse and combined with the attention mechanism, which enables the network to maintain depth while widening the width to obtain more advanced semantic information. The connection of contextual information is strengthened by connecting RFB multi-scale modules at the shallow level, and shallow features with more spatial features are extracted for feature fusion with the decoder. To further lift the segmentation accuracy, a companion loss is introduced in the encoder, and the model is trained to have better segmentation performance for small sample objects by adaptively adjusting the loss coefficients. Experimental results show that the proposed method significantly increases the accuracy and mean Intersection Over Union (mIOU) scores.",,,,,"Wang, Zewei","Liu, Junwen","Cui, Zhongwei",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_954,"Pang, Shiyan","Lan, Jingjing","Zuo, Zhiqi",SFGT-CD: Semantic Feature-Guided Building Change Detection From Bitemporal Remote-Sensing Images With Transformers,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,6,"High-resolution remote-sensing-image change detection is widely used in urban dynamic monitoring, geographic information updating, natural disaster monitoring, illegal building investigation, and land resource surveys. Common change-detection algorithms are mainly implemented in a fully supervised manner that relies on a large number of high-quality samples. Compared with a building change-detection dataset, a building semantic-segmentation dataset is easier to accumulate and obtain. Making full use of this semantic information in the design of a building change-detection network can effectively reduce the sample size required to train a change-detection model. In view of this, a semantic feature-guided Siamese change-detection framework is devised in this letter. The framework effectively exploits the prior information of building semantic features and uses the popular transformer structure to improve the change analysis module. The results of extensive experiments on two public datasets show that the framework is more accurate than the other state-of-the-art change detection algorithms and can effectively reduce the dependence of data on change detection samples in the model training process.",Feature extraction,Semantics,Transformers,Convolutional neural networks,"Chen, Jia",,,,Training,Remote sensing,Decoding,Change detection,high-resolution optical remote-sensing images,prior semantic information,transformers,,,,,,,,,,,,,,,,,,
Row_955,"Yin, Hao","Zhang, Chengming","Han, Yingjuan",Improved semantic segmentation method using edge features for winter wheat spatial distribution extraction from Gaofen-2 images,JOURNAL OF APPLIED REMOTE SENSING,MAY 25 2021,3,"In the final feature map obtained using a convolutional neural network for remote sensing image segmentation, there are great differences between the feature values of the pixels near the edge of the block and those inside the block; ensuring consistency between these feature values is the key to improving the accuracy of segmentation results. The proposed model uses an edge feature branch and a semantic feature branch called the edge assistant feature network (EFNet). The EFNET model consists of one semantic branch, one edge branch, one shared decoder, and one classifier. The semantic branch extracts semantic features from remote sensing images, whereas the edge branch extracts edge features from remote sensing images and edge images. In addition, the two branches extract five-level features through five sets of feature extraction units. The shared decoder sets up five levels of shared decoding units, which are used to further integrate edge features and deep semantic features. This strategy can reduce the feature differences between the edge pixels and the inner pixels of the object, obtaining a per-pixel feature vector with high inter-class differentiation and intra-class consistency. Softmax is used as the classifier to generate the final segmentation result. We selected a representative winter wheat region in China (Feicheng City) as the study area and established a dataset for experiments. The comparison experiment included three original models and two models modified by adding edge features: SegNet, UNet, and ERFNet, and edge-UNet and edge-ERFNet, respectively. EFNet's recall (91.01%), intersection over union (81.39%), and F1-Score (91.68%) were superior to those of the other methods. The results clearly show that EFNET improves the accuracy of winter wheat extraction from remote sensing images. This is an important basis not only for crop monitoring, yield estimation, and disaster assessment but also for calculating land carrying capacity and analyzing the comprehensive production capacity of agricultural resources. (C) The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License.",convolutional neural network,remote sensing,semantic segmentation,winter wheat,"Qian, Yonglan","Xu, Tao","Zhang, Ziyun","Kong, Ailing",edge feature,Gaofen-2,,,,,,,,,,,,,,,,,,,,,,,
Row_956,"Yang, Junliang","Chen, Guorong","Huang, Jiaming",GLE-net: global-local information enhancement for semantic segmentation of remote sensing images,SCIENTIFIC REPORTS,OCT 25 2024,0,"Remote sensing (RS) images contain a wealth of information with expansive potential for applications in image segmentation. However, Convolutional Neural Networks (CNN) face challenges in fully harnessing the global contextual information. Leveraging the formidable capabilities of global information modeling with Swin-Transformer, a novel RS images segmentation model with CNN (GLE-Net) was introduced. This integration gives rise to a revamped encoder structure. The subbranch initiates the process by extracting features at varying scales within the RS images using the Multiscale Feature Fusion Module (MFM), acquiring rich semantic information, discerning localized finer features, and adeptly handling occlusions. Subsequently, Feature Compression Module (FCM) is introduced in main branch to downsize the feature map, effectively reducing information loss while preserving finer details, enhancing segmentation accuracy for smaller targets. Finally, we integrate local features and global features through Spatial Information Enhancement Module (SIEM) for comprehensive feature modeling, augmenting the segmentation capabilities of model. We performed experiments on public datasets provided by ISPRS, yielding notably remarkable experimental outcomes. This underscores the substantial potential of our model in the realm of RS image segmentation within the context of scientific research.",Remote sening,Multiscale feature,Swin-transformer convolutional neural networks,,"Ma, Denglong","Liu, Jingcheng","Zhu, Huazheng",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_957,"Yang, Peiqi","Wang, Mingjun","Yuan, Hao",Using contour loss constraining residual attention U-net on optical remote sensing interpretation,VISUAL COMPUTER,SEP 2023,5,"Using deep learning in remote sensing interpretation could reduce a lot of human and material costs. Semantic segmentation is the main method for this task. It can automatically outline the objects and it has recently achieved great success in remote sensing images. However, in the appliance of remote sensing interpretation, the accuracy of contour largely determines the evaluation of remote sensing interpretation. Though the current loss functions reflect the segmentation performance, they could not guide the model to optimize itself toward a more precise contour. This paper proposed an exactly defined contour loss (CL) for remote sensing interpretation with Residual Attention U-Net (RA U-Net) as the main framework. The RA U-Net uses the residual attention module as the skip connection layer. It enhances the judgment of U-Net. In CL, image processing methods are used to extract the contours of the foreground. And elements-sum and elements-subtract operations are used to transfer the contour information to a matrix of the same size as label images. Then, these matrices would be the weights for CE. By assigning different weights for different elements in different regions, this function will guide the model to reach a balance between accurate segmentation results and precise contours. The experiment on open datasets shows a good performance. The proposed model was also trained on the Construction Disturbance Dataset collected from Jiang Xi Province, China. The dataset was labeled manually. The evaluation enhanced a lot on the Construction Disturbance Dataset and the IoU on two datasets increased 1% to 2% when using CL as the loss function. This paper also compared the proposed method with other state-of-the-art methods and the results showed extensive effectiveness.",Remote sensing,Image interpretation,Loss function,Semantic segmentation,"He, Ci","Cong, Li",,,U-Net,Residual attention mechanism,,,,,,,,,,,,,,,,,,,,,,,
Row_958,"Canedo, Daniel","Fonte, Joao","Dias, Rita",Automated Detection of Hillforts in Remote Sensing Imagery With Deep Multimodal Segmentation,ARCHAEOLOGICAL PROSPECTION,SEP 2024,0,"Recent advancements in remote sensing and artificial intelligence can potentially revolutionize the automated detection of archaeological sites. However, the challenging task of interpreting remote sensing imagery combined with the intricate shapes of archaeological sites can hinder the performance of computer vision systems. This work presents a computer vision system trained for efficient hillfort detection in remote sensing imagery. Equipped with an adapted multimodal semantic segmentation model, the system integrates LiDAR-derived LRM images and aerial orthoimages for feature fusion, generating a binary mask pinpointing detected hillforts. Post-processing includes margin and area filters to remove edge inferences and smaller anomalies. The resulting inferences are subjected to hard positive and negative mining, where expert archaeologists classify them to populate the training data with new samples for retraining the segmentation model. As the computer vision system is far more likely to encounter background images during its search, the training data are intentionally biased towards negative examples. This approach aims to reduce the number of false positives, typically seen when applying machine learning solutions to remote sensing imagery. Northwest Iberia experiments witnessed a drastic reduction in false positives, from 5678 to 40 after a single hard positive and negative mining iteration, yielding a 99.3% reduction, with a resulting F-1 score of 66%. In England experiments, the system achieved a 59% F1 score when fine-tuned and deployed countrywide. Its scalability to diverse archaeological sites is demonstrated by successfully detecting hillforts and other types of enclosures despite their typical complex and varied shapes. Future work will explore archaeological predictive modelling to identify regions with higher archaeological potential to focus the search, addressing processing time challenges.",computer vision,hillforts,LiDAR,multimodal semantic segmentation,"do Pereiro, Tiago","Goncalves-Seco, Luis","Vazquez, Marta","Georgieva, Petia",orthoimagery,transformer,,,,,,,"Neves, Antonio J. R.",,,,,,,,,,,,,,,,
Row_959,"Bi, Hanbo","Feng, Yingchao","Yan, Zhiyuan",Not Just Learning From Others but Relying on Yourself: A New Perspective on Few-Shot Segmentation in Remote Sensing,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,8,"Few-shot segmentation (FSS) is proposed to segment unknown class targets with just a few annotated samples. Most current FSS methods follow the paradigm of mining the semantics from the support images to guide the query image segmentation. However, such a pattern of ""learning from others"" struggles to handle the extreme intraclass variation, preventing FSS from being directly generalized to remote sensing scenes. To bridge the gap of intraclass variance, we develop a dual-mining network named DMNet for cross-image mining and self-mining, meaning that it no longer focuses solely on support images but pays more attention to the query image itself. Specifically, we propose a class-public region mining (CPRM) module to effectively suppress irrelevant feature pollution by capturing the common semantics between the support-query image pair. The class-specific region mining (CSRM) module is then proposed to continuously mine the class-specific semantics of the query image itself in a ""filtering"" and ""purifying"" manner. In addition, to prevent the coexistence of multiple classes in remote sensing scenes from exacerbating the collapse of FSS generalization, we also propose a new known-class metasuppressor (KMS) module to suppress the activation of known-class objects in the sample. Extensive experiments on the iSAID and LoveDA remote sensing datasets have demonstrated that our method sets the state of the art with a minimum number of model parameters. Significantly, our model with the backbone of Resnet-50 achieves the mean Intersection over Union (mIoU) of 49.58% and 51.34% on iSAID under 1- and 5-shot settings, outperforming the state-of-the-art method by 1.8% and 1.12%, respectively. The code is publicly available at https://github.com/HanboBizl/DMNet/.",Few-shot learning,few-shot segmentation (FSS),prototype learning,remote sensing,"Mao, Yongqiang","Diao, Wenhui","Wang, Hongqi","Sun, Xian",semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,,
Row_960,"Yang, Liangzhe","Zi, Wenjie","Chen, Hao",DRE-Net: A Dynamic Radius-Encoding Neural Network with an Incremental Training Strategy for Interactive Segmentation of Remote Sensing Images,REMOTE SENSING,FEB 2023,5,"Semantic segmentation of remote sensing (RS) images, which is a fundamental research topic, classifies each pixel in an image. It plays an essential role in many downstream RS areas, such as land-cover mapping, road extraction, traffic monitoring, and so on. Recently, although deep-learning-based methods have shown their dominance in automatic semantic segmentation of RS imagery, the performance of these existing methods has relied heavily on large amounts of high-quality training data, which are usually hard to obtain in practice. Moreover, human-in-the-loop semantic segmentation of RS imagery cannot be completely replaced by automatic segmentation models, since automatic models are prone to error in some complex scenarios. To address these issues, in this paper, we propose an improved, smart, and interactive segmentation model, DRE-Net, for RS images. The proposed model facilitates humans' performance of segmentation by simply clicking a mouse. Firstly, a dynamic radius-encoding (DRE) algorithm is designed to distinguish the purpose of each click, such as a click for the selection of a segmentation outline or for fine-tuning. Secondly, we propose an incremental training strategy to cause the proposed model not only to converge quickly, but also to obtain refined segmentation results. Finally, we conducted comprehensive experiments on the Potsdam and Vaihingen datasets and achieved 9.75% and 7.03% improvements in NoC95 compared to the state-of-the-art results, respectively. In addition, our DRE-Net can improve the convergence and generalization of a network with a fast inference speed.",interactive segmentation,dynamic radius encoding,incremental learning,remote sensing,"Peng, Shuang",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_961,"Pang, Kai","Weng, Liguo","Zhang, Yonghong",SGBNet: An Ultra Light-weight Network for Real-time Semantic Segmentation of Land Cover,INTERNATIONAL JOURNAL OF REMOTE SENSING,AUG 18 2022,19,"Designing a lightweight and robust real-time land cover segmentation algorithm is an important task for land resource applications. In recent years, with the development of edge computing and the increasing resolution of remote sensing images, the huge amount of calculations and parameters have restricted the efficiency of real-time semantic segmentation. Therefore, the emergence of lightweight CNN (convolutional neural network) has accelerated the development of real-time semantic segmentation of land cover. However, nowadays, the time and space span of aerial images is getting larger and larger, resulting in the loss of details and blurred edges of lightweight CNN. Therefore, the existing lightweight CNN model has low segmentation accuracy and poor generalization ability in real-time land cover segmentation tasks. In order to solve the problem of lightweight network in this respect, this paper proposes a Semantics Guided Bottleneck Network (SGBNet) to balance accuracy and reasoning speed. First, a basic unit and the overall network structure are redesigned to increase the overall reasoning efficiency of the model. The model can efficiently extract spatial details and contextual semantic information. Then, the model optimizes the lightweight network and realizes the extraction of details and contextual semantic information. Finally, a lightweight attention mechanism is used to restore high-resolution pixel-level features. The results of comparative experiments show that the method in this paper has a higher segmentation accuracy than existing models while achieving lightweight.",Land cover,lightweight,real-time semantic segmentation,remote sensing image,"Liu, Jia","Lin, Haifeng","Xia, Min",,deep learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_962,"Yang, Qinchen","Liu, Man","Zhang, Zhitao",Mapping Plastic Mulched Farmland for High Resolution Images of Unmanned Aerial Vehicle Using Deep Semantic Segmentation,REMOTE SENSING,SEP 1 2019,30,"With increasing consumption, plastic mulch benefits agriculture by promoting crop quality and yield, but the environmental and soil pollution is becoming increasingly serious. Therefore, research on the monitoring of plastic mulched farmland (PMF) has received increasing attention. Plastic mulched farmland in unmanned aerial vehicle (UAV) remote images due to the high resolution, shows a prominent spatial pattern, which brings difficulties to the task of monitoring PMF. In this paper, through a comparison between two deep semantic segmentation methods, SegNet and fully convolutional networks (FCN), and a traditional classification method, Support Vector Machine (SVM), we propose an end-to-end deep-learning method aimed at accurately recognizing PMF for UAV remote sensing images from Hetao Irrigation District, Inner Mongolia, China. After experiments with single-band, three-band and six-band image data, we found that deep semantic segmentation models built via single-band data which only use the texture pattern of PMF can identify it well; for example, SegNet reaching the highest accuracy of 88.68% in a 900 nm band. Furthermore, with three visual bands and six-band data (3 visible bands and 3 near-infrared bands), deep semantic segmentation models combining the texture and spectral features further improve the accuracy of PMF identification, whereas six-band data obtains an optimal performance for FCN and SegNet. In addition, deep semantic segmentation methods, FCN and SegNet, due to their strong feature extraction capability and direct pixel classification, clearly outperform the traditional SVM method in precision and speed. Among three classification methods, SegNet model built on three-band and six-band data obtains the optimal average accuracy of 89.62% and 90.6%, respectively. Therefore, the proposed deep semantic segmentation model, when tested against the traditional classification method, provides a promising path for mapping PMF in UAV remote sensing images.",plastic mulched farmland,fully convolutional networks,unmanned aerial vehicle remote sensing image,deep semantic segmentation,"Yang, Shuqin","Ning, Jifeng","Han, Wenting",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_963,"Bona, Daniel Sande","Murni, Aniati","Mursanto, Petrus",Semantic Segmentation And Segmentation Refinement Using Machine Learning Case Study: Water Turbidity Segmentation,,2019,1,"Classical methods for image segmentation such as pixel thresholding, clustering, region growing, maximum likelihood have been used regularly and relied on for a long time. However, these classical methods have limitations, particularly on images where there are many overlapping pixel values between features, which is common in remote sensing images. The advent of machine learning, in particular, deep learning in computer vision and image analysis, has gained interest in the remote sensing field. Current deep learning architecture has been able to achieve high accuracy for image recognition, object detection, and segmentation. This study performed image segmentation on the coastal area with high water turbidity using Landsat-8 images. Currently, the standard tool to derive water turbidity data from Landsat-8 images is the level-2 plugin of SEADAS software. However, due to its rigorous processing method, the processing time using SEADAS Level-2 Plugin is quite long; for example, processing one Landsat-8 image took around 8 hours. As a consequence, the amount of time needed to process multiple images is increasing. Deep learning has advantages once the model trained, the inference or prediction process is quite fast. Therefore it has the potential to be used as a complementary tool to predict and segment high turbidity areas, because in deep learning. In this study, we implemented U-Net architecture with ResNet connection and used Generative-Adversarial Network (GAN) to refined segmentation results.",Image segmentation,remote sensing,deep learning,machine learning,,,,,CNN,GAN,,,,,,,,2019 IEEE INTERNATIONAL CONFERENCE ON AEROSPACE ELECTRONICS AND REMOTE SENSING TECHNOLOGY (ICARES 2019),,,,,,,,,,,,,,,
Row_964,"Huang, Bohao","Collins, Leslie M.","Bradbury, Kyle",DEEP CONVOLUTIONAL SEGMENTATION OF REMOTE SENSING IMAGERY: A SIMPLE AND EFFICIENT ALTERNATIVE TO STITCHING OUTPUT LABELS,,2018,9,"In this work we consider the application of convolutional neural networks (CNNs) for the semantic segmentation of remote sensing imagery (e.g., aerial color or hyperspectral imagery). In segmentation the goal is to provide a dense pixel-wise labeling of the input imagery. However, remote sensing imagery is usually stored in the form of very large images, called ""tiles"", which are too large to be segmented directly using most CNNs and their associated hardware. During label inference (i.e., obtaining labels for a new large tile) smaller sub-images, called ""patches"", are extracted uniformly over a tile and the resulting label maps are ""stitched"" (or concatenated) to create a tile-sized label map. This approach suffers from computational inefficiency and risks of discontinuities at the boundaries between the output of individual patches. In this work we propose a simple alternative approach in which the input size of the CNN is dramatically increased only during label inference. We evaluate the performance of the proposed approach against a standard stitching approach using two popular segmentation CNN models on the INRIA building labeling dataset. The results suggest that the proposed approach substantially reduces label inference time, while also yielding modest overall label accuracy increases. This approach also contributed to our winning entry (overall performance) in the INRIA building labeling competition.",semantic segmentation,convolutional neural networks,deep learning,aerial imagery,"Malof, Jordan M.",,,,building detection,,,,,,,,,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_965,"Castillo-Navarro, J.","Audebert, N.","Boulch, A.",What Data are needed for Semantic Segmentation in Earth Observation?,,2019,2,"This paper explores different aspects of semantic segmentation of remote sensing data using deep neural networks. Learning with deep neural networks was revolutionized by the creation of ImageNet. Remote sensing benefited of these new techniques, however Earth Observation (EO) datasets remain small in comparison. In this work, we investigate how we can progress towards the ImageNet of remote sensing. In particular, two questions are addressed in this paper. First, how robust are existing supervised learning strategies with respect to data volume? Second, which properties are expected from a large-scale EO dataset? The main contributions of this work are: (i) a strong robustness analysis of existing supervised learning strategies with respect to remote sensing data, (ii) the introduction of a new, large-scale dataset named MiniFrance.",Deep Learning,Supervised Learning,Semantic Segmentation,Land Use/Land Cover Mapping,"Le Saux, B.","Lefevre, S.",,,,,,,,,,,,2019 JOINT URBAN REMOTE SENSING EVENT (JURSE),,,,,,,,,,,,,,,
Row_966,"Wang, Chunshan","Zhu, Penglei","Yang, Shuo",A Semantic Segmentation Method for Winter Wheat in North China Based on Improved HRNet,AGRONOMY-BASEL,NOV 2024,0,"Winter wheat is one of the major crops for global food security. Accurate statistics of its planting area play a crucial role in agricultural policy formulation and resource management. However, the existing semantic segmentation methods for remote sensing images are subjected to limitations in dealing with noise, ambiguity, and intra-class heterogeneity, posing a negative impact on the segmentation performance of the spatial distribution and area of winter wheat fields in practical applications. In response to the above challenges, we proposed an improved HRNet-based semantic segmentation model in this paper. First, this model incorporates a semantic domain module (SDM), which improves the model's precision of pixel-level semantic parsing and reduces the interference from noise through multi-confidence scale class representation. Second, a nested attention module (NAM) is embedded, which enhances the model's capability of recognizing correct correlations in pixel classes. The experimental results show that the proposed model achieved a mean intersection over union (mIoU) of 80.51%, a precision of 88.64%, a recall of 89.14%, an overall accuracy (OA) of 90.12%, and an F1-score of 88.89% on the testing set. Compared to traditional methods, our model demonstrated better segmentation performance in winter wheat semantic segmentation tasks. The achievements of this study not only provide an effective tool and technical support for accurately measuring the area of winter wheat fields, but also have important practical value and profound strategic significance for optimizing agricultural resource allocation and achieving precision agriculture.",remote sensing image,winter wheat,semantic segmentation,HRNet,"Zhang, Lijie",,,,semantic domain optimization,nested attention optimization,,,,,,,,,,,,,,,,,,,,,,,
Row_967,"Zhang, Xiaolu","Wang, Zhaoshun","Wei, Anlei",Multiscale Cascaded Network for the Semantic Segmentation of High-Resolution Remote Sensing Images,CANADIAN JOURNAL OF REMOTE SENSING,JAN 2 2023,0,"As remote sensing images have complex backgrounds and varying object sizes, their semantic segmentation is challenging. This study proposes a multiscale cascaded network (MSCNet) for semantic segmentation. The resolutions employed with respect to the input remote sensing images are 1, 1/2, and 1/4, which represent high, medium, and low resolutions. First, 3 backbone networks extract features with different resolutions. Then, using a multiscale attention network, the fused features are input into the dense atrous spatial pyramid pooling network to obtain multiscale information. The proposed MSCNet introduces multiscale feature extraction and attention mechanism modules suitable for remote sensing land-cover classification. Experiments are performed using the Deepglobe, Vaihingen, and Potsdam datasets; the results are compared with those of the existing classical semantic segmentation networks. The findings indicate that the mean intersection over union (mIoU) of the MSCNet is 4.73% higher than that of DeepLabv3+ with the Deepglobe datasets. For the Vaihingen datasets, the mIoU of the MSCNet is 15.3%, and 6.4% higher than those of a segmented network (SegNet), and DeepLabv3+, respectively. For the Potsdam datasets, the mIoU of the MSCNet is higher than those of a fully convolutional network, Res-U-Net, SegNet, and DeepLabv3+ by 11.18%, 5.89%, 4.78%, and 3.03%, respectively.Comme les images de teledetection ont des arriere-plans complexes et des tailles d'objets variables, leur segmentation semantique est difficile. Cette etude propose un reseau multi-echelle en cascade (MSCNet) pour la segmentation semantique. Les resolutions utilisees par rapport aux images de teledetection d'entree sont 1, 1/2, et 1/4, representant les resolutions haute, moyenne et basse. Tout d'abord, trois reseaux federateurs extraient les caracteristiques avec des resolutions differentes. Ensuite, a l'aide d'un reseau d'attention multi-echelle, les caracteristiques fusionnees sont entrees dans le reseau de mise en commun des pyramides spatiales denses et a trous pour obtenir des informations multi-echelles. Le MSCNet propose introduit des modules multi-echelles d'extraction de caracteristiques et de mecanismes d'attention adaptes a la classification de la couverture terrestre par teledetection. Les experiences sont realisees a l'aide des ensembles de donnees Deepglobe, Vaihingen et Potsdam. Les resultats sont compares a ceux des reseaux de segmentation semantique classique existants. Les resultats indiquent que l'intersection moyenne sur l'union (mIoU) du MSCNet est superieure par 4,73% a celle de DeepLabv3+ avec les ensembles de donnees Deepglobe. Pour les jeux de donnees Vaihingen, le mIoU du MSCNet est superieur par 15,3% a celui d'un reseau segmente (SegNet) et par 6,4% a celui de DeepLabv3+. Pour les donnees de Potsdam, le mIoU du MSCNet est superieur a ceux du reseau entierement convolutif, de Res-U-Net, de SegNet et de DeepLabv3+ par 11,18%, 5,89%, 4,78%, et 3,03%, respectivement.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_968,"Hu, Qiongqiong","Wu, Yuechao","Li, Ying",Semi-supervised semantic labeling of remote sensing images with improved image-level selection retraining,ALEXANDRIA ENGINEERING JOURNAL,MAY 2024,3,"In recent years, image semantic segmentation technology has developed rapidly, but image annotation usually requires a significant amount of human and financial resources, especially for remote sensing image annotation, which can be expensive and sometimes even unaffordable. To address this issue, this paper integrates the idea of curriculum learning into the self-training method and screens reliable pseudo-labels through computing imagelevel confidence, significantly reducing the confirmation error problem. Furthermore, the semi-supervised model in this paper combines implicit semantic enhancement with strong data augmentation, which can reduce the coupling between the teacher model and the student model's prediction distribution and enhance the model's robustness. Finally, the proposed semi-supervised method is experimentally verified using the ISPRS competition dataset and compared with existing state -of -the -art (SOTA) methods. Experimental results show that the proposed semi-supervised segmentation method achieves higher segmentation accuracy compared to self-training methods. Moreover, despite not using iterative training to simplify the training process, the proposed method still yields satisfactory segmentation results.",Deep convolutional neural networks,Remote sensing images,Semantic labeling,Semi-supervised learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_969,"Park, Seula","Song, Ahram",,Shoreline Change Analysis with Deep Learning Semantic Segmentation Using Remote Sensing and GIS Data,KSCE JOURNAL OF CIVIL ENGINEERING,FEB 2024,3,"Shoreline management is essential for navigation, coastal resource management, and coastal planning and development. Shoreline change detection is vital for shoreline monitoring; however, traditional methods used for such detection are laborious and have limited accuracy. An approach that integrates remote sensing imagery and geographic information systems (GISs) is proposed herein to simultaneously identify shoreline changes and perform grid-level visualization for updating shoreline data. The integrated approach uses deep learning-based segmentation networks and water indexes to accurately classify land and sea in remote sensing images. Transfer learning was used to address the issue of insufficient data, wherein weights trained on a large open dataset were applied to the target area. The segmentation results were compared with existing shoreline GIS data to identify the areas experiencing shoreline changes. Grid-level visualization enhanced the identification of regions requiring flexible data updates and investigation efficiency by focusing on specific areas. The proposed approach accurately detected shoreline changes, albeit with some errors of commission, predominantly in regions featuring intricate shorelines and small clusters of islands. The proposed approach offers efficient solutions for shoreline change detection, with potential applications in coastal management, environmental science, urban planning, and coastal hazard assessment.",Shoreline,Change analysis,Remote sensing,GIS,,,,,Attention U-net,Water index,,,,,,,,,,,,,,,,,,,,,,,
Row_970,"Nguyen, Gia-Vuong","Huynh-The, Thien",,Enhancing Aerial Semantic Segmentation With Feature Aggregation Network for DeepLabV3+,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,0,"As a cutting-edge deep encoder-decoder architecture, DeepLabV3+ has been realized as a cutting-edge solution for image segmentation, especially aerial semantic segmentation in remote sensing applications. This is because of an atrous spatial pyramid pooling (ASPP) block deployed in its encoder with multiple atrous convolutional layers to enrich diversified feature extraction and learning efficiency. However, the DeepLabV3+ encoder-decoder architecture has some limitations, including the lack of information during the upsampling process and some inappropriate customizations that cause incorrect segmentation. To address these shortcomings, we introduce an efficient architecture with a novel feature aggregation network (FAN), which facilitates the extraction of features across multiple scales and stages. Concurrently, we apply some adaptive upgrades to the ASPP block, involving a new set of dilation factors that are adept at accommodating low-resolution inputs. Through simulations, we demonstrate the effectiveness and generalizability of our improved model by evaluating it with different backbones and dilation rates. In addition, compared with recent deep segmentation models, our improved model is superior in terms of mean $F1$ score (mF1) by at least 1.32%-6.34% and 0.98%-5.61% on the UAVid and Vaihingen datasets, respectively.",Feature extraction,Fans,Convolution,Decoding,,,,,Semantic segmentation,Accuracy,Semantics,Aerial image semantic segmentation,computer vision,deep learning (DL),remote sensing,,,,,,,,,,,,,,,,,,
Row_971,"Moradkhani, Kaveh","Fathi, Abdolhossein",,Segmentation of waterbodies in remote sensing images using deep stacked ensemble model,APPLIED SOFT COMPUTING,JUL 2022,8,"Identifying surface water resources is considered as one of the principal applications of remote sensing image analysis that plays a crucial role in controlling optimal use of these resources, and preventing floods and crises such as drought. Traditional machine learning methods for extracting waterbodies require complex spectral analysis and selection of features based on previous knowledge. Although applying deep learning-based approaches, which has been considered in recent years, has eliminated the necessity of extracting manual features, they require too many training data and computational resources to achieve high performance. Consequently, each presented deep architecture can detect some of the existing patterns in the predefined conditions. This paper trains and optimizes three robust deep architectures, presented in various fields, using surface water data, and combines their results to achieve a robust model for detecting surface water. To this end, a deep hybrid architecture called ""deep stacked ensemble model "" is employed on the outputs of three independent deep sub-models and extracts the final segmentation mask of the water areas more accurately. We evaluated our proposed model on a water body detection dataset provided by artificial intelligence crowd landsat (AIcrowd(1) LNDST) challenge. The proposed technique improves the semantic segmentation performance and surpasses state-of-the-art results. (C) 2022 Elsevier B.V. All rights reserved.",Waterbody segmentation,Remote sensing image analysis,Deep stacked ensemble model,U-net,,,,,Deep learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_972,"Chen, Wei","Wang, Qingpeng","Wang, Dongliang",A lightweight and scalable greenhouse mapping method based on remote sensing imagery,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,DEC 2023,5,"Seeking a low-cost, high-efficiency greenhouse mapping technology has immense significance. While greenhouse extraction methods using deep learning have been proposed, the challenge of extracting dense small objects remains an unresolved problem. The inherent downscaling strategy in general-purpose semantic segmentation (SS) models renders them unsuitable for such tasks. In contrast, the dramatically increasing computational complexity associated with this problem may result in an unaffordable cost for consumer-level applications. To address the aforementioned challenges, this study presents a novel greenhouse mapping model based on remote sensing (RS) images, which not only exhibits high precision and robust generalization capabilities but also offers significant lightweight advantages. To meet broader needs, we also provide corresponding customizable and scalable rules that allow for a trade-off between accuracy and speed. To evaluate the performance of our model, we select several representative works to conduct benchmark experiments on a self-annotated dataset. The results demonstrate that our method can provide more powerful visual representations for greenhouse segmentation with minimal cost. Compared to the control group, the proposed method achieves an mIoU improvement of 1.116 %-10.77 % using only 3.282 M parameters, while maintaining a considerable inference speed. Code will be available at: https://github.com/W-qp/EGENet.git",Greenhouses,Deep learning,Semantic segmentation,Remote sensing imagery,"Xu, Yameng","He, Yingxuan","Yang, Lan","Tang, Hongzhao",Low-cost,,,,,,,,,,,,,,,,,,,,,,,,
Row_973,"Liu, Zhiheng","Chen, Xuemei","Zhou, Suiping",DUPnet: Water Body Segmentation with Dense Block and Multi-Scale Spatial Pyramid Pooling for Remote Sensing Images,REMOTE SENSING,NOV 2022,10,"Water body segmentation is an important tool for the hydrological monitoring of the Earth. With the rapid development of convolutional neural networks, semantic segmentation techniques have been used on remote sensing images to extract water bodies. However, some difficulties need to be overcome to achieve good results in water body segmentation, such as complex background, huge scale, water connectivity, and rough edges. In this study, a water body segmentation model (DUPnet) with dense connectivity and multi-scale pyramidal pools is proposed to rapidly and accurately extract water bodies from Gaofen satellite and Landsat 8 OLI (Operational Land Imager) images. The proposed method includes three parts: (1) a multi-scale spatial pyramid pooling module (MSPP) is introduced to combine shallow and deep features for small water bodies and to compensate for the feature loss caused by the sampling process; (2) dense blocks are used to extract more spatial features to DUPnet's backbone, increasing feature propagation and reuse; (3) a regression loss function is proposed to train the network to deal with the unbalanced dataset caused by small water bodies. The experimental results show that the F1, MIoU, and FWIoU of DUPnet on the 2020 Gaofen dataset are 97.67%, 88.17%, and 93.52%, respectively, and on the Landsat River dataset, they are 96.52%, 84.72%, 91.77%, respectively.",encoder-decoder,multi-scale spatial pyramid pooling,dense connection,regression loss,"Yu, Hang","Guo, Jianhua","Liu, Yanming",,remote sensing,water body semantic segmentation,,,,,,,,,,,,,,,,,,,,,,,
Row_974,"Nan, Guojun","Li, Haorui","Du, Haibo",A Semantic Segmentation Method Based on AS-Unet plus plus for Power Remote Sensing of Images,SENSORS,JAN 2024,4,"In order to achieve the automatic planning of power transmission lines, a key step is to precisely recognize the feature information of remote sensing images. Considering that the feature information has different depths and the feature distribution is not uniform, a semantic segmentation method based on a new AS-Unet++ is proposed in this paper. First, the atrous spatial pyramid pooling (ASPP) and the squeeze-and-excitation (SE) module are added to traditional Unet, such that the sensing field can be expanded and the important features can be enhanced, which is called AS-Unet. Second, an AS-Unet++ structure is built by using different layers of AS-Unet, such that the feature extraction parts of each layer of AS-Unet are stacked together. Compared with Unet, the proposed AS-Unet++ automatically learns features at different depths and determines a depth with optimal performance. Once the optimal number of network layers is determined, the excess layers can be pruned, which will greatly reduce the number of trained parameters. The experimental results show that the overall recognition accuracy of AS-Unet++ is significantly improved compared to Unet.",semantic segmentation,Unet,atrous spatial pyramid pooling,squeeze-and-excitation module,"Liu, Zhuo","Wang, Min","Xu, Shuiqing",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_975,"Zhang, Shichao","Wang, Changying","Li, Jinhua",MF-Dfnet: a deep learning method for pixel-wise classification of very high-resolution remote sensing images,INTERNATIONAL JOURNAL OF REMOTE SENSING,JAN 2 2022,3,"Semantic segmentation of high-resolution remote sensing images is very important. However, the targets in the high-resolution optical satellite images are always various in size, which lead to multiscale problems resulting in difficulty of locating and identifying the target. High-resolution remote sensing is more complex than natural phenomena; this leads to false alarms due to a greater intraclass inconsistency. Thus, the pixel-wise classification of high-resolution remote sensing images becomes challenging. Aiming at the above problems, we propose a multiscale feature and discriminative feature network (MF-DFNet). We introduce the hierarchical-split block (HSB) and the residual receptive field block module (RRFBM) to extract multiscale information to address multiscale problems. We also introduce a foreground-scene relation module to enhance the discrimination of features and deal with the false alarm phenomenon. In addition, the channel attention block (CAB) is introduced to select more discriminative features. We use two publicly available remote sensing image datasets (Vaihingen and Massachusetts building) for the experiments in this paper. Compared to current advanced models, our results show that MF-DFNet achieves state-of-the-art performance and can effectively improve the integrity and correctness of semantic segmentation in high-resolution remote sensing images.",semantic segmentation,deep learning,remote sensing images,hierarchical-split block,"Sui, Yi",,,,channel attention block,residual receptive field block module,foreground-scene relation module,,,,,,,,,,,,,,,,,,,,,,
Row_976,"Huang, Yifei","Feng, Zideng","Yang, Junli",LE-BEIT: A LOCAL-ENHANCED SELF-SUPERVISED TRANSFORMER FOR SEMANTIC SEGMENTATION OF HIGH RESOLUTION REMOTE SENSING IMAGES,,2022,1,"Semantic segmentation for remote sensing images (RSI) has been a thriving research topic for a long time. Existing supervised learning methods usually require a huge amount of labeled data. Meanwhile, large size, variation in object scales, and intricate details in RSI make it essential to capture both long-range context and local information. To address these problems, we propose Le-BEIT, a self-supervised Transformer with an improved positional encoding Local-Enhanced Positional Encoding (LePE). Self-supervised learning relieves the demanding requirement of a large amount of labeled data. The self-attention mechanism in Transformer has remarkable capability in capturing long-range context. Meanwhile, we use LePE as a substitution for Relative Positional Encoding (RPE) to represent local information more effectively. Moreover, considering the domain difference between natural images and RSI, instead of ImageNet-22K, we pre-train Le-BEIT on a very small high-resolution RSI dataset-GID. To investigate the influence of pre-training dataset size on segmentation accuracy, we furtherly conduct experiments on a larger pre-training dataset called GID-DOTA, which is 1/100 of ImageNet-22K, and have observed considerable accuracy improvements. The result of our method, which relies on a much smaller pretrained dataset, achieves competitive accuracy compared to the counterpart on ImageNet-22K.",Remote Sensing,Self-supervised Learning,Transformer,,"Wang, Bin","Wang, Jiaying","Xian, Zhenglin",,,,,,,,,,,"2022 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",,,,,,,,,,,,,,,
Row_977,"Jia, Xinqi","Song, Xiaoyong","Rao, Lei",DEUFormer: High-precision semantic segmentation for urban remote sensing images,IET COMPUTER VISION,DEC 2024,0,"Urban remote sensing image semantic segmentation has a wide range of applications, such as urban planning, resource exploration, intelligent transportation, and other scenarios. Although UNetFormer performs well by introducing the self-attention mechanism of Transformer, it still faces challenges arising from relatively low segmentation accuracy and significant edge segmentation errors. To this end, this paper proposes DEUFormer by employing a special weighted sum method to fuse the features of the encoder and the decoder, thus capturing both local details and global context information. Moreover, an Enhanced Feature Refinement Head is designed to finely re-weight features on the channel dimension and narrow the semantic gap between shallow and deep features, thereby enhancing multi-scale feature extraction. Additionally, an Edge-Guided Context Module is introduced to enhance edge areas through effective edge detection, which can improve edge information extraction. Experimental results show that DEUFormer achieves an average Mean Intersection over Union (mIoU) of 53.8% on the LoveDA dataset and 69.1% on the UAVid dataset. Notably, the mIoU of buildings in the LoveDA dataset is 5.0% higher than that of UNetFormer. The proposed model outperforms methods such as UNetFormer on multiple datasets, which demonstrates its effectiveness.",computer vision,convolutional neural nets,,,"Fan, Guangyu","Cheng, Songlin","Chen, Niansheng",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_978,"Deng, Guohui","Wu, Zhaocong","Xu, Miaozhong",Crisscross-Global Vision Transformers Model for Very High Resolution Aerial Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,5,"Semantic segmentation is a key means for understanding very high resolution (VHR) aerial imagery. With the explosive development of deep learning, deep learning methods are being applied to the segmentation of VHR images, with convolutional neural networks (CNNs) as the basic framework; however, owing to the highly complex details present in VHR images and the high spatial dependence of geographical objects, CNN-based methods are inadequate. This is because the inherent locality of CNNs limits the size of the receptive field, thus limiting the ability to obtain long-range context information. To solve this problem, in this article, we propose a transformer-based novel deep learning model called crisscross-global vision transformers (CGVTs). CGVT exploits the transformer's inherent ability to obtain long-range context information to solve the restricted receptive field problem. Specifically, we redesign the self-attention (SA) mechanism in the transformer and call it crisscross-global attention. It consists of two parts: a crisscross transformer encoder block (CC-TEB) and a global squeeze transformer encoder block (GS-TEB). CC-TEB overcomes the limitation of the traditional SA design (specifically, difficulty applying it to VHR aerial image segmentation) and further increases the local feature representation ability of the model. GS-TEB increases the global feature representation ability of the model. The results of experiments conducted on the popular ISPRS Vaihingen, IEEE GRSS data fusion contest Zeebrugge, and LoveDA semantic segmentation challenge datasets verify the effectiveness and superiority of our proposed method. Specifically, it achieved state-of-the-art performance on both Zeebrugge and LoveDA datasets and is currently ranked second in the Vaihingen dataset.",Aerial imagery,remote sensing,semantic segmentation,transformers,"Wang, Chengjun","Wang, Zhiye","Lu, Zhongyuan",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_979,"Hu, Haiyang","Yang, Linnan","Chen, Jiaojiao",The remote sensing image segmentation of land cover based on multi-scale attention features,,2023,0,"Segmentation of land cover in remote sensing images is a task that involves interpreting remote sensing data using machine vision. Satisfying segmentation results in agriculture and forestry regions can guide land resource management, natural environment protection, urban construction, and the distribution of agricultural products. However, the performance of the widely used deep learning segmentation model on high-resolution remote sensing segmentation datasets in agriculture and forestry regions needs to be improved. To solve the problems of poor accuracy and loss of context information in remote sensing image semantic segmentation, this paper proposes an improved semantic segmentation network architecture. The model utilizes multi-scale feature extraction, deploys a multi-layer attention feature fusion module and an up-sampling fusion module to capture high-quality multi-scale context information, correctly handle scale changes, and help narrow the semantic gap between different levels. Finally, the proposed MLP decoder refers to the dynamic up-sampling operator to aggregate the information at different levels to achieve pixel segmentation. To verify the effectiveness of our proposed model, the researchers conducted experiments on two land cover segmentation datasets. The training process specifically designs data augmentation strategies for remote sensing segmentation tasks to enhance the model's generalization ability. The final model achieved an mIoU (mean Intersection over Union) of 65.05% on the self-built rural land cover datasets, surpassing the benchmark network UPerNet by 5.92%. On the LoveDA dataset, our model achieved state-of-the-art performance with an mIoU of 53.39%, demonstrating its versatility.",land cover,remote sensing image,neural network,attention mechanism,"Luo, Shuang",,,,multi-scale feature,,,,,,,,,"2023 IEEE 35TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI",,,,,,,,,,,,,,,
Row_980,"Saxena, Nidhi","Babu, N. Kishore","Raman, Balasubramanian",Semantic Segmentation of Multispectral Images using Res-Seg-net Model,,2020,12,"Semantic segmentation is pixel-wise labeling of the image. Recently deep convolutional neural network (DCNN) providing progressive results in semantic segmentation. However, in remote sensing multispectral imagery very limited work has been done due to lack of training dataset. In this paper, a Res-eg-net model is proposed for the semantic segmentation which is motivated by the existing Resnet and Segnet models. This model consists of encoder-decoder parts in which residual mapping is followed. For validation and testing of the proposed model, the RIT-18 dataset of multispectral imagery is used. The comparison results of the experiment on a multispectral imagery dataset have demonstrated the effectiveness of the proposed model.",Convolutional neural network,Multispectral images,Semantic Segmentation,,,,,,,,,,,,,,,2020 IEEE 14TH INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING (ICSC 2020),,,,,,,,,,,,,,,
Row_981,"Dong, Shan","Zhuang, Yin","Chen, He",Full Semantic Constructed Network for Urban Use Classification From Very High-Resolution Optical Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,0,"Recently, semantic segmentation technology has been a research hotspot in optical remote sensing urban use classification. However, because of coupled semantic relations in very high-resolution and complex urban scenes, a more effective semantic description for pixelwise urban use interpretation has become a challenge. Then, aiming to set up a more effective semantic description, the effective receptive field (ERF) is analyzed in general convolutional neural networks. The unreasonable ERF distribution in the stacked convolutional layers of the encoder would lead to a large amound of small ERFs and fewer not large enough ERFs that form a naive semantic description in decoder. Therefore, in this article, a novel full semantic constructed network (FSCNet) is proposed to improve the naive semantic description and set up an effective semantic description. First, to avoid noise from shallow feature layers, a residual refinement convolution is designed to optimize the full-scale skip connections based on the U-shaped encoder-decoder. Second, an interscale fusion module is newly designed for multiscale feature fusion, which can generate three initial semantic modalities that are prepared for redefining the full semantic description. Third, a multiscale local context spatial attention module and boundary supervision are designed for an initial shallow semantic modality to capture the pure boundary information, and then, pyramid spatial pooling is employed for an initial deep semantic modality to further enlarge the ERF and obtain more abstract global information. Next, a self-calibration convolution combined with the atrous spatial pyramid pooling is designed to rectify and enrich an initial middle semantic modality, which can improve the naive semantic description and bridge the semantic gap between the redefined shallow and deep semantic modalities to advance the full semantic feature fusion. Finally, extensive experiments are carried out on three benchmarks (e.g., ISPRS Vaihingen, Potsdam, and DLRSD), and comparative results show that the proposed FSCNet can get remarkable performance compared to state-of-the-art (SOTA) methods. Besides, the code is available at https://github.com/DorisCV/FSCNet.",Semantics,Location awareness,Feature extraction,Convolution,"Zhang, Tong","Li, Lianlin","Long, Teng",,Remote sensing,Optical sensors,Decoding,Full semantic description,optical remote sensing,urban use classification,very high resolution (VHR),,,,,,,,,,,,,,,,,,
Row_982,"Fan, Runyu","Li, Fengpeng","Han, Wei",Fine-Scale Urban Informal Settlements Mapping by Fusing Remote Sensing Images and Building Data via a Transformer-Based Multimodal Fusion Network,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,25,"Urban informal settlements (UISs) are high-density population settlements with low standards of living and supply. UIS semantic segmentation, which identifies pixels corresponding to informal settlements in remote sensing images, is crucial to the estimation of poor communities, urban management, resource allocation, and future planning, particularly in megacities. However, most studies on informal settlement mapping are either based on parcels (image classification) or pixels (semantic segmentation). Few studies utilize object information to improve UIS mapping. Since informal settlements are formed by buildings (objects), utilizing object information can improve UIS semantic segmentation. Furthermore, current UIS mapping studies mainly focus on using single-modality remote sensing images, and there is a lack of related research on using multimodal data. Due to the spatial heterogeneity of informal settlements, using only a single modality of remote sensing image features limits the effectiveness and accuracy of informal settlements semantic segmentation. Aiming at achieving fine-scale UIS mapping results, this article proposes a UIS semantic segmentation method, namely UisNet, that utilizes a transformer-based block to receive multimodal data, including high-spatial-resolution remote sensing images (parcel- and pixel-level) and building polygon data (object-level) to identify UIS. The experiments were conducted in Shenzhen City, and they confirmed the superior performance of UisNet, which achieved an overall accuracy (OA) of 94.80% and a mean intersection over union (mIoU) of 85.51% in the testing set of the manually labeled UIS semantic segmentation dataset (UIS-Shenzhen dataset) and outperformed the best models on semantic segmentation tasks. Besides, we add a set of experiments on a public dataset [gaofen image dataset (GID) dataset] and compare our method with the current state-of-the-art semantic segmentation methods. Experiments show that the proposed UisNet improves mIoU by 1.64% to 7.58% compared to other methods. This work will be available at https://github.com/RunyuFan/.",Semantics,Image segmentation,Buildings,Remote sensing,"Yan, Jining","Li, Jun","Wang, Lizhe",,Task analysis,Urban areas,Transformers,Deep learning,multimodal,remote sensing,semantic segmentation,urban informal settlements (UISs),,,,,,,,,,,,,,,,,
Row_983,"Song, Pengfei","Li, Jinjiang","An, Zhiyong",CTMFNet: CNN and Transformer Multiscale Fusion Network of Remote Sensing Urban Scene Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,38,"Semantic segmentation of remotely sensed urban scene images is widely demanded in areas such as land cover mapping, urban change detection, and environmental protection. With the development of deep learning, methods based on convolutional neural networks (CNNs) have been dominant due to their powerful ability to represent hierarchical feature information. However, the limitations of the convolution operation itself limit the network's ability to extract global contextual information. With the successful use of transformer in computer vision in recent years, transformer has shown great potential for modeling global contextual information. However, transformer is not sufficiently capable of capturing local detailed information. In this article, to explore the potential of the joint CNN and transformer mechanism for semantic segmentation of remotely sensed urban scenes, we propose a CNN and transformer multiscale fusion network (CTMFNet) based on encoding-decoding for urban scene understanding. To couple local-global context information more efficiently, we designed a dual backbone attention fusion module (DAFM) to couple the local and global context information of the dual-branch encoder. In addition, to bridge the semantic gap between scales, we built a multi-layer dense connectivity network (MDCN) as our decoder. The MDCN enables the full flow of semantic information between multiple scales to be fused with each other through upsampling and residual connectivity. We conducted extensive subjective and objective comparison experiments and ablation experiments on both the International Society of Photogrammetry and Remote Sensing (ISPRS) Vaihingen and ISPRS Potsdam datasets. Numerous experimental results have proven the superiority of our method compared to currently popular methods.",Feature extraction,Transformers,Semantics,Remote sensing,"Fan, Hui","Fan, Linwei",,,Decoding,Semantic segmentation,Convolutional neural networks,Convolutional neural network (CNN),multiscale fusion,semantic segmentation,transformer,,,,,,,,,,,,,,,,,,
Row_984,"Li, Yuxia","Peng, Bo","He, Lei",Road Segmentation of Unmanned Aerial Vehicle Remote Sensing Images Using Adversarial Network With Multiscale Context Aggregation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,JUL 2019,40,"Semantic segmentation using adversarial networks has proved to be effective in image processing fields. However, two problems need to be solved in the field of the road segmentation of unmanned aerial vehicle (UAV) remote sensing images. One is the occupied proportion of road area in UAV remote sensing images; the other is that the constant size of convolutional kernel cannot deal with multiscale feature very well. To solve these two problems, this paper proposed a road segmentation model that combined the adversarial networks with multiscale context aggregation. First, the output feature-maps of three scales (0.5n, 1n, 2n) were obtained, based on an end-to-end training from image segmentation network. Second, after the convolution and deconvolution operations, the processed images were unified to the size scale of original images. Third, with the pixel-by-pixel addition method, the three scales of image feature (0.5n, 1n, 2n) were merged together, then inputted into the discriminative network. Finally, the errors were obtained and propagated backwards compared with the label, and then the parameters of a generative network and a discriminative network could be updated. Further, the segmented results were compared with those from normal adversarial networks, Linknet and D-linknet, and were developed with the morphological operation. The research results show that the proposed model can improve the precision of road segmentation from UAV images with multiscale context aggregation and the regularization property of adversarial networks.",Adversarial network,image processing,multiscale context aggregation,road segmentation,"Fan, Kunlong","Tong, Ling",,,unmanned aerial vehicle (UAV) image,,,,,,,,,,,,,,,,,,,,,,,,
Row_985,"Liu, Wei","Wang, He","Qiao, Yicheng",DLAFNET: A DIRECT FUSION METHOD OF 2D AERIAL IMAGE AND 3D LIDAR POINT CLOUD FOR SEMANTIC SEGMENTATION,,2023,1,"Semantic segmentation of high-resolution remote sensing images (RSIs) is developing rapidly. Multispectral images can provide rich spectral information for semantic segmentation, while 3D LiDAR point cloud data can provide depth information. Thus, semantic segmentation accuracy could be improved by fusing multispectral images and 3D LiDAR point cloud. In this paper, we propose a method titled Direct LiDAR-Aerial Fusion Network (DLAFNet) which directly uses RSIs and LiDAR point cloud for semantic segmentation tasks. In particular, owing to the fact that sparse features extracted from the KPConv branch are not as essential as features from RSIs, we design LiDAR Assisted Attention Module (L-AAM). Our experiments on the modified GRSS18 dataset prove that our method is proper and can obtain the best results by comparing with its components and other methods.",Semantic segmentation,data fusion,,,"Liang, Bin","Yang, Junli","Zhang, Haopeng",,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_986,"Niu, Binglin",,,Semantic Segmentation of Remote Sensing Image Based on Convolutional Neural Network and Mask Generation,MATHEMATICAL PROBLEMS IN ENGINEERING,JUN 2 2021,9,"High-resolution remote sensing images usually contain complex semantic information and confusing targets, so their semantic segmentation is an important and challenging task. To resolve the problem of inadequate utilization of multilayer features by existing methods, a semantic segmentation method for remote sensing images based on convolutional neural network and mask generation is proposed. In this method, the boundary box is used as the initial foreground segmentation profile, and the edge information of the foreground object is obtained by using the multilayer feature of the convolutional neural network. In order to obtain the rough object segmentation mask, the general shape and position of the foreground object are estimated by using the high-level features in the process of layer-by-layer iteration. Then, based on the obtained rough mask, the mask is updated layer by layer using the neural network characteristics to obtain a more accurate mask. In order to solve the difficulty of deep neural network training and the problem of degeneration after convergence, a framework based on residual learning was adopted, which can simplify the training of those very deep networks and improve the accuracy of the network. For comparison with other advanced algorithms, the proposed algorithm was tested on the Potsdam and Vaihingen datasets. Experimental results show that, compared with other algorithms, the algorithm in this article can effectively improve the overall precision of semantic segmentation of high-resolution remote sensing images and shorten the overall training time and segmentation time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_987,"Zheng, Chen","Zhang, Yun","Wang, Leiguang",Multilayer semantic segmentation of remote-sensing imagery using a hybrid object-based Markov random field model,INTERNATIONAL JOURNAL OF REMOTE SENSING,DEC 2016,5,"High spatial resolution (HR) remote-sensing image usually contains hierarchical semantic information. Many supervised methods have been developed to interpret this information through data training. In this article, without data training, a hybrid object-based Markov random field (HOMRF) model is proposed for multi-layer semantic segmentation of remote-sensing images. In this method, label fields of different semantic layers are defined on the same region adjacency graph (RAG) of a given image, and a hybrid framework is suggested to capture and utilize the interactions within and between semantic layers by label fields. Namely a new transition probability matrix is introduced into the energy functions of label fields for describing the semantic context between layers, and the multilevel logistic model is employed to describe the interactions within the same layer. A principled probabilistic inference is developed to determine the optimal solution of the proposed method by iteratively updating each label field until convergence. The computational complexity of the proposed model is O(knt), where k is the number of classes in all of the layers, n is the number of sites in the probability graph of the MRF model, and t is the number of iterations. Experimental results from various remote-sensing images demonstrate that the proposed method can produce higher segmentation accuracy than state-of-the-art MRF-based methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_988,Zhu Ying,Zhao Ming,,Registration of Laser Point Cloud and Optical Image in Urban Area Based on Semantic Segmentation,ACTA PHOTONICA SINICA,JAN 2021,4,"The collaborative application of point cloud data and optical remote sensing image has been widely concerned in the field of remote sensing. In order to accurately register two kind of data and better integrate their advantages, an automatic registration method of point cloud and optical remote sensing image in urban scene is proposed. Firstly, the depth image is generated from point cloud data, that is, 3D data is converted into 2D image. Secondly, the Unet model is used to train the depth image and the optical remote sensing image respectively and get building segmentations. Thirdly, the minimum circumscribed rectangles of buildings are constructed based on the contour set of building segmentation, and the length-width ratio of rectangle is taken as the constraint condition to find Corresponding Points (CPs). Then, we use the similar triangle principle to find CPs of the rectangle' s center point. Finally , the coordinate of the CPs are substituted into the transformation model to calculate the model parameters, thus the registration is achieved. The experimental results show that the proposed method can achieve better registration effect when it is difficult to match with traditional point feature method, and it is resistant to image translation, rotation and scaling.",Remote sensing,Registration,Semantic segmentation,Point cloud,,,,,Optical,,,,,,,,,,,,,,,,,,,,,,,,
Row_989,"Wang, Jin","Ding, Ning","He, Guangjun",A boundary enhancement loss function for semantic segmentation of land cover,INTERNATIONAL JOURNAL OF REMOTE SENSING,JUN 18 2023,1,"With the rapid development of UAV remote sensing, satellite remote sensing and computer vision, the semantic segmentation of remote sensing images has also developed rapidly and is widely used in research on land utilization classification, ecology, urban planning and other problems. Large differences in spatial and temporal scales, different image resolutions, insufficient model robustness to the data domain, and blurred object boundaries are the main problems for existing semantic segmentation models based on deep learning. This paper studies the problem of blurred target boundaries after semantic segmentation and propose a boundary enhancement loss function that highlights the importance of target edges. Compared to other models used in investigating higher boundary accuracy, the proposed model can be trained without boundary-labelled data, and no additional inference time is consumed. This loss function is applied to some other deep learning networks as a plug-and-play module on two different datasets and show that the IoU has an improvement of 2-5% with better clear boundary and continuity, which is more prominent on buildings and roads.",Semantic segmentation,boundary enhancement loss,land utilization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row_990,"Qian, Xiaoliang","Li, Chao","Wang, Wei",Semantic segmentation guided pseudo label mining and instance re-detection for weakly supervised object detection in remote sensing images,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,MAY 2023,28,"Weakly supervised object detection (WSOD) in remote sensing images (RSIs) has good practical value because it only requires the image-level annotations. The existing methods usually have two problems. The first problem is that many methods mine the pseudo ground truth (PGT) instances solely depending on the class confidence score (CCS), however, the reliability of CCS is not enough because of the inter-class similarity and intra-class diversity in RSIs, consequently, the reliability of corresponding PGT instances is limited, in addition, the most discriminative part with high CCS rather than the whole object is easily selected as the PGT instance. The second problem is that the object localization solely relies on the candidate proposals generated by the selective search or edge boxes algorithm, however, the localization accuracy of the candidate proposals is not enough because of the cluttered background in RSIs. To address the first problem, a semantic segmentation guided pseudo label mining (SGPLM) module is proposed, which uses a novel metric named class-specific object confidence score (COCS) to mine high-quality PGT instances. The COCS is made up of the CCS and class-specific object overlap score (COOS) which is calculated through the weakly supervised semantic segmentation. The mined PGT instances are more robust and incline to cover the whole object by combining the COOS. To handle the second problem, an instance re-detection (IR) module is proposed for improving the localization accuracy of the WSOD model, in which an enhanced PGT instance generation strategy is designed to obtain the enhanced PGT instances on the basis of the candidate proposals, and the enhanced PGT instances are used to train the instance re-classification and re-localization branches which are jointly utilized to infer the final results. The ablation studies validate the effectiveness of the SGPLM and IR modules. The comprehensive comparisons with other advanced methods show that the performance of the proposed method is state-of-the-art on two RSI datasets.",Weakly supervised object detection (WSOD),Remote sensing images (RSIs),Semantic segmentation guided pseudo label,mining (SGPLM),"Yao, Xiwen","Cheng, Gong",,,Instance re-detection (IR),,,,,,,,,,,,,,,,,,,,,,,,
Row_991,"Zhuang, Zhenrong","Shi, Wenzao","Sun, Wenting",Multi-class remote sensing change detection based on model fusion,INTERNATIONAL JOURNAL OF REMOTE SENSING,FEB 1 2023,3,"Change detection in remote sensing images has an important impact in various application fields. In recent years, great progress has been made in the change detection methods of multiple types of ground objects, but there are still limited recognition capabilities of the extracted features, resulting in unclear boundaries, and the accuracy rate needs to be improved. To address these issues, we use a high-resolution network (HRNet) to generate high-resolution representations and add new data augmentation methods to improve its accuracy. Secondly, we introduce the model of Transformer structure -- CSWin and HRNet to fuse to improve the performance and effect of the model. In order to enhance the model's ability to perceive ground objects at different scales, a feature fusion network suitable for multi-class semantic segmentation is designed, named A-FPN. This feature fusion network is introduced between the CSWin backbone network and the semantic segmentation network. The experimental results show that the fusion method greatly improves the accuracy to 89.31% on the SECOND dataset, significantly reduces false detections, and recognizes the edges of objects more clearly. And achieved good results in the three evaluation indicators of precision, recall, and F1-score.",Deep learning,change detection,semantic segmentation,remote sensing image,"Wen, Pengyu","Wang, Lei","Yang, Weiqi","Li, Tian",model integration,,,,,,,,,,,,,,,,,,,,,,,,
Row_992,"Doi, Kento","Iwasaki, Akira",,THE EFFECT OF FOCAL LOSS IN SEMANTIC SEGMENTATION OF HIGH RESOLUTION AERIAL IMAGE,,2018,20,"The semantic segmentation of High Resolution Remote Sensing (HRRS) images is the fundamental research area of the earth observation. Convolutional Neural Network(CNN), which has achieved superior performance in computer vision task, is also useful for semantic segmentation of HRRS images. In this work, focal loss is used instead of cross-entropy loss in training of CNN to handle the imbalance in training data. To evaluate the effect of focal loss, we train SegNet and FCN with focal loss and con firm improvement in accuracy in ISPRS 2D Semantic Labeling Contest dataset, especially when gamma is 0.5 in SegNet.",deep learning,semantic segmentation,CNN,focal loss,,,,,,,,,,,,,,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,,,,,,,,,,,,,,,
Row_993,"Zhao, Sijie","Chen, Hao","Zhang, Xueliang",RS-Mamba for Large Remote Sensing Image Dense Prediction,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,4,"Context modeling is critical for remote sensing image dense prediction tasks. Nowadays, the growing size of very-high-resolution (VHR) remote sensing images poses challenges in effectively modeling context. While transformer-based models possess global modeling capabilities, they encounter computational challenges when applied to large VHR images due to their quadratic complexity. The conventional practice of cropping large images into smaller patches results in a notable loss of contextual information. To address these issues, we propose the remote sensing Mamba (RSM) for dense prediction tasks in large VHR remote sensing images. RSM is specifically designed to capture the global context of remote sensing images with linear complexity, facilitating the effective processing of large VHR images. Considering that the land covers in remote sensing images are distributed in arbitrary spatial directions due to characteristics of remote sensing over-head imaging, the RSM incorporates an omnidirectional selective scan module (OSSM) to globally model the context of images in multiple directions, capturing large spatial features from various directions. We designed simple yet effective models based on RSM, achieving state-of-the-art performance on dense prediction tasks in VHR remote sensing images without fancy training strategies. Extensive experiments on semantic segmentation (SS) and change detection (CD) tasks across various land covers demonstrate the effectiveness of the proposed RSM. Leveraging the linear complexity and global modeling capabilities, RSM achieves better efficiency and accuracy than transformer-based models on large remote sensing images. Interestingly, we also demonstrated that our model generally performs better with a larger image size on dense prediction tasks.",Remote sensing,Task analysis,Context modeling,Transformers,"Xiao, Pengfeng","Bai, Lei","Ouyang, Wanli",,Feature extraction,Predictive models,Complexity theory,Change detection (CD),deep learning,dense prediction,large remote sensing images,semantic segmentation (SS),,,,,,state space model (SSM),very high resolution (VHR),,,,,,,,,,
Row_994,"Zhang, Tony","Dick, Robert P.",,SPATIAL-FREQUENCY NETWORK FOR SEGMENTATION OF REMOTE SENSING IMAGES,,2023,0,We describe a deep learning system for satellite image segmentation. Our CNN model embeds contextual feature dependencies in both spatial and frequency domains. Its Spatial Weighting Module uses a multi-scale pooling layer to represent correlations at longer length scales in the spatial domain. Its Frequency Weighting Module uses frequency-domain information to better discriminate between object classes. Experimental results on the Potsdam dataset demonstrate that our model has a 1.9% higher average F1 accuracy than previous methods.,Remote sensing segmentation,spatial,frequency,,,,,,,,,,,,,,,"2023 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",,,,,,,,,,,,,,,
Row_995,"Sun, Shuting","Mu, Lin","Wang, Lizhe",Semantic Segmentation for Buildings of Large Intra-Class Variation in Remote Sensing Images with O-GAN,REMOTE SENSING,FEB 2021,25,"Remote sensing building extraction is of great importance to many applications, such as urban planning and economic status assessment. Deep learning with deep network structures and back-propagation optimization can automatically learn features of targets in high-resolution remote sensing images. However, it is also obvious that the generalizability of deep networks is almost entirely dependent on the quality and quantity of the labels. Therefore, building extraction performances will be greatly affected if there is a large intra-class variation among samples of one class target. To solve the problem, a subdivision method for reducing intra-class differences is proposed to enhance semantic segmentation. We proposed that backgrounds and targets be separately generated by two orthogonal generative adversarial networks (O-GAN). The two O-GANs are connected by adding the new loss function to their discriminators. To better extract building features, drawing on the idea of fine-grained image classification, feature vectors for a target are obtained through an intermediate convolution layer of O-GAN with selective convolutional descriptor aggregation (SCDA). Subsequently, feature vectors are clustered into new, different subdivisions to train semantic segmentation networks. In the prediction stages, the subdivisions will be merged into one class. Experiments were conducted with remote sensing images of the Tibet area, where there are both tall buildings and herdsmen's tents. The results indicate that, compared with direct semantic segmentation, the proposed subdivision method can make an improvement on accuracy of about 4%. Besides, statistics and visualizing building features validated the rationality of features and subdivisions.",building extraction,GF-2,orthogonal generative adversarial networks,subdivision,"Liu, Peng","Liu, Xiaolei","Zhang, Yuwei",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_996,"Liu, Guoying","Zhou, Hongyu",,Semantic Segmentation Based on Multi-stage Region-level Clustering,,2013,1,"In the field of remote-sensed image segmentation, it is very important to obtain semantic results. However, in high resolution remote sensed images, different complex patterns always have components with the same spectrum, which makes it rather difficult to extract such patterns only through traditional clustering methods. In this paper, a novel multi-stage region-level clustering method is proposed to solve this problem. Firstly, the initial oversegmentation is obtained by using the Mean Shift algorithm, based on which a region adjacent graph (RAG) is built; Then, FCM is employed to get the spectral-based segmentation result; After that, the context clues for each region is calculated according to the label and size of neighboring regions, followed by the second FCM clustering on each set of regions with the same label to distinct regions with the same spectrum but belongs to different objects; Rearranging all of these clustering results to form the finial processing unit, this algorithm goes a step further to calculate more accurate context clues, and use the third FCM to obtain the final segmentation result. Experiments on the high resolution remote-sensed images have shown the superiority to the competitions.",image segmentation,region adjacent graph,fuzzy c-means clustering,remote sensing,,,,,,,,,,,,,,"MIPPR 2013: MULTISPECTRAL IMAGE ACQUISITION, PROCESSING, AND ANALYSIS",,,,,,,,,,,,,,,
Row_997,"Li, Xiaolong","Li, Yuyin","Ai, Jinquan",Semantic segmentation of UAV remote sensing images based on edge feature fusing and multi-level upsampling integrated with Deeplabv3+,PLOS ONE,JAN 20 2023,13,"Deeplabv3+ currently is the most representative semantic segmentation model. However, Deeplabv3+ tends to ignore targets of small size and usually fails to identify precise segmentation boundaries in the UAV remote sensing image segmentation task. To handle these problems, this paper proposes a semantic segmentation algorithm of UAV remote sensing images based on edge feature fusing and multi-level upsampling integrated with Deeplabv3+ (EMNet). EMNet uses MobileNetV2 as its backbone and adds an edge detection branch in the encoder to provide edge information for semantic segmentation. In the decoder, a multi-level upsampling method is designed to retain high-level semantic information (e.g., the target's location and boundary information). The experimental results show that the mIoU and mPA of EMNet improved over Deeplabv3+ by 7.11% and 6.93% on the dataset UAVid, and by 0.52% and 0.22% on the dataset ISPRS Vaihingen.",,,,,"Shu, Zhaohan","Xia, Jing","Xia, Yuanping",,,,,,,,,,,,,,,,,,,,,,,,,,
Row_998,"Li, Rui","Zheng, Shunyi","Zhang, Ce",ABCNet: Attentive bilateral contextual network for efficient semantic segmentation of Fine-Resolution remotely sensed imagery,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,NOV 2021,163,"Semantic segmentation of remotely sensed imagery plays a critical role in many real-world applications, such as environmental change monitoring, precision agriculture, environmental protection, and economic assessment. Following rapid developments in sensor technologies, vast numbers of fine-resolution satellite and airborne remote sensing images are now available, for which semantic segmentation is potentially a valuable method. However, because of the rich complexity and heterogeneity of information provided with an ever-increasing spatial resolution, state-of-the-art deep learning algorithms commonly adopt complex network structures for segmentation, which often result in significant computational demand. Particularly, the frequently-used fully convolutional network (FCN) relies heavily on fine-grained spatial detail (fine spatial resolution) and contextual information (large receptive fields), both imposing high computational costs. This impedes the practical utility of FCN for real-world applications, especially those requiring real-time data processing. In this paper, we propose a novel Attentive Bilateral Contextual Network (ABCNet), a lightweight convolutional neural network (CNN) with a spatial path and a contextual path. Extensive experiments, including a comprehensive ablation study, demonstrate that ABCNet has strong discrimination capability with competitive accuracy compared with stateof-the-art benchmark methods while achieving significantly increased computational efficiency. Specifically, the proposed ABCNet achieves a 91.3% overall accuracy (OA) on the Potsdam test dataset and outperforms all lightweight benchmark methods significantly. The code is freely available at https;//github.com./lironui/ABCNet.",Semantic Segmentation,Attention Mechanism,Bilateral Architecture,Convolutional Neural Network,"Duan, Chenxi","Wang, Libo","Atkinson, Peter M.",,Deep Learning,,,,,,,,,,,,,,,,,,,,,,,,
Row_999,"Yang, Zenan","Niu, Haipeng","Wang, Xiaoxuan",An unsupervised semantic segmentation method that combines the ImSE-Net model with SLICm superpixel optimization,INTERNATIONAL JOURNAL OF DIGITAL EARTH,DEC 31 2024,1,"In the field of remote sensing, using a large amount of labeled image data to supervise the training of fully convolutional networks for the semantic segmentation of images is expensive. However, using a small amount of labeled data can lead to reduced network performance. This paper proposes an unsupervised semantic segmentation method that combines the ImSE-Net model with SLICm superpixel optimization. First, the ImSE-Net model is used to extract semantic features from the image to obtain rough semantic segmentation results. Then, the SLICm superpixel segmentation algorithm is used to segment the input image into superpixel images. Finally, an unsupervised semantic segmentation model (UGLS) is used to combine high-level abstract semantic features with detailed information on superpixels to obtain edge-optimized semantic segmentation results. Experimental results show that compared with other semantic segmentation algorithms, our method more effectively handles unbalanced areas, such as object boundaries, and achieves better segmentation results, with higher semantic consistency.",High-spatial-resolution remote sensing images,semantic segmentation,ImSE-Net model,SLICm superpixel optimization model,"Huang, Liang","Yang, Kui",,,unsupervised semantic segmentation model,,,,,,,,,,,,,,,,,,,,,,,,
Row_1000,"Veljanovski, Tatjana","Kanjir, Ursa","Ostir, Kristof",OBJECT-BASED IMAGE ANALYSIS OF REMOTE SENSING DATA,GEODETSKI VESTNIK,DEC 2011,7,"Remote sensing has developed various methods and technologies for con tactless and cost-effective mapping of large area land cover/land use maps and other thematic maps. The key factor for the availability and reliability of these maps for use in Earth sciences is the development of effective procedures for satellite data analysis and classification. The most appropriate approach for classifying low and medium resolution satellite images (pixel size is coarser than, or at best similar to, the size of geographical objects) is pixel-based classification in which an individual pixel is classified into the closest class based on its spectral similarityWith increasing spatial resolution, pixel-based classification methods became less effective, since the relationship between the pixel size and the dimension of the observed objects on the Earth's surface has changed significantly Therefore object-oriented classification has become increasingly popular over the past decade. This combines segmentation (which is a fundamental phase of the approach) and contextual classification. Segmentation divides the image into homogeneous pixel groups (segments), which are during the semantic classification process - arranged into classes based on their spectral, geometric, textural and other features during. The intent of this paper is to present the theoretical argumentation and methodology of object-based image analysis of remote sensing data, provide all overview of the field and point out certain restrictions as regards the current operational solutions.",remote sensing,object-based image analysis,segmentation,object-based classification,,,,,semantic classification,,,,,,,,,,,,,,,,,,,,,,,,
