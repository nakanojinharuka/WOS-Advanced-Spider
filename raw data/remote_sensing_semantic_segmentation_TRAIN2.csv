,author1,author2,author3,author4,author5,title,journal/book,publish time,citation,abstract,keyword1,keyword2,keyword3,keyword4,keyword5,keyword6,keyword7,keyword8,keyword9,keyword10,keyword11,author6,author7,author8,author9,author10,keyword12,keyword13,keyword14,conference,keyword15
Row_801,"Chen, Guangchen","Shi, Benjie","Zhang, Yinhui","He, Zifen","Zhang, Pengcheng",CGSNet: Cross-consistency guiding semi-supervised semantic segmentation network for remote sensing of plateau lake,JOURNAL OF NETWORK AND COMPUTER APPLICATIONS,OCT 2024,0,"Analyzing the geographical information for the Plateau Lake region with remote sensing images (RSI) is an emerging technology to monitor the changes of the ecological environment. To alleviate the requirement of abundant labels for supervised RSI segmentation, the Cross-consistency Guiding Semi-supervised Learning (SSL) Semantic Segmentation Network is proposed, and it can perform high-quality multi-category semantic segmentation for complex remote sensing scenes with limited quantity of labeled images. Firstly, based on the SSL semantic segmentation framework, through the cross-consistency method training a teacher model with less annotated images and plentiful unannotated images, then generating higher-quality pseudo labels to guide the learning process of the student model. Secondly, dense conditional random field and mask hole repair are used to patch and fill the flaw areas of pseudo-labels based on the pixel features of position, color, and texture, further improving the granularity and reliability of the student model training dataset. Additionally, to improve the accuracy of the model, we designed a strong data augmentation (SDA) method based on a stochastic cascaded strategy, which connects multiple augmentation techniques in random order and probability cascade to generate new training samples. It mimics a variety of image transformations and noise conditions that occur in the real world to enhance the robustness in complex scenarios. To validate the effectiveness of CGSNet in complex remote sensing scenes, extended experiments are conducted on the self-built plateau lake RSI dataset and two public multi-category RSI datasets. The experiment results demonstrate that, compared with other state-of-the-art SSL methods, the proposed CGSNet achieves the highest 77.47% mIoU and 87.06% F1 scores with a limited quantity of annotated data.",Environment change monitoring,Plateau lake remote sensing,Cross-consistency guiding,Semi-supervised learning,Dense conditional random field,,,,,,,,,,,,,,,,
Row_802,"Qiu, Luyi","Yu, Dayu","Zhang, Xiaofeng","Zhang, Chenxiao",,Efficient Remote-Sensing Segmentation With Generative Adversarial Transformer,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,0,"Most deep-learning methods that achieve high segmentation accuracy require deep network architectures that are too heavy and complex to run on embedded devices with limited storage and memory space. To address this issue, this letter proposes an efficient generative adversarial transformer (GATrans) for achieving high-precision semantic segmentation while maintaining an extremely efficient size. The framework utilizes a global transformer network (GTNet) as the generator, efficiently extracting multilevel features through residual connections. GTNet employs global transformer blocks with progressively linear computational complexity to reassign global features based on a learnable similarity function. To focus on object- and pixel-level information, the GATrans optimizes the objective function by combining structural similarity losses. We validate the effectiveness of our approach through extensive experiments on the Vaihingen dataset, achieving an average $F1$ score of 90.17% and an overall accuracy (OA) of 91.92%. Codes are available at https://github.com/qiuluyi/GATrans.",Transformers,Generators,Merging,Mathematical models,Training,Remote sensing,Feature extraction,Generative-adversarial strategy,global transformer network (GTNet),remote sensing,semantic segmentation,,,,,,,,,,
Row_803,"Hong, Danfeng","Zhang, Bing","Li, Hao","Li, Yuxuan","Yao, Jing",Cross-city matters: A multimodal remote sensing benchmark dataset for cross-city semantic segmentation using high-resolution domain adaptation networks,REMOTE SENSING OF ENVIRONMENT,DEC 15 2023,255,"Artificial intelligence (AI) approaches nowadays have gained remarkable success in single-modality-dominated remote sensing (RS) applications, especially with an emphasis on individual urban environments (e.g., single cities or regions). Yet these AI models tend to meet the performance bottleneck in the case studies across cities or regions, due to the lack of diverse RS information and cutting-edge solutions with high generalization ability. To this end, we build a new set of multimodal remote sensing benchmark datasets (including hyperspectral, mul-tispectral, SAR) for the study purpose of the cross-city semantic segmentation task (called C2Seg dataset), which consists of two cross-city scenes, i.e., Berlin-Augsburg (in Germany) and Beijing-Wuhan (in China). Beyond the single city, we propose a high-resolution domain adaptation network, HighDAN for short, to promote the AI model's generalization ability from the multi-city environments. HighDAN is capable of retaining the spatially topological structure of the studied urban scene well in a parallel high-to-low resolution fusion fashion but also closing the gap derived from enormous differences of RS image representations between different cities by means of adversarial learning. In addition, the Dice loss is considered in HighDAN to alleviate the class imbalance issue caused by factors across cities. Extensive experiments conducted on the C2Seg dataset show the superiority of our HighDAN in terms of segmentation performance and generalization ability, compared to state-of-the-art com-petitors. The C2Seg dataset and the semantic segmentation toolbox (involving the proposed HighDAN) will be available publicly at https://github.com/danfenghong/RSE_Cross-city.",Cross-city,Deep learning,Dice loss,Domain adaptation,High-resolution network,Land cover,Multimodal benchmark datasets,Remote sensing,Segmentation,,,"Li, Chenyu","Werner, Martin","Chanussot, Jocelyn","Zipf, Alexander","Zhu, Xiao Xiang",,,,,
Row_804,"Liu, Wenshu","Cui, Nan","Guo, Luo","Du, Shihong","Wang, Weiyin",DESformer: A Dual-Branch Encoding Strategy for Semantic Segmentation of Very-High-Resolution Remote Sensing Images Based on Feature Interaction and Multiscale Context Fusion,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Global contextual information is crucial for the semantic segmentation of remote sensing (RS) images. However, the majority of current approaches depend on convolutional neural networks (CNNs). Due to the local receptive fields inherent in convolutional operations, these networks typically capture image features within limited areas and struggle to comprehend broader contextual information in the images. In this study, a dual-branch encoding approach, DESformer, is proposed, integrating transformers with CNN, to effectively capture global multiscale context information and enhance edge feature extraction. In addition, DESformer incorporates a feature interaction module (FIM) to combine local features with global representations extracted by transformers and CNN, respectively, across different resolutions. This approach enhances the capability to capture local features in RS images and improves the understanding of extensive spatial relationships. Subsequently, we employ a novel top-down approach for global supervision of the traditional feature pyramid multilevel visual integration (MVI) module, by harnessing the clear visual center information obtained from the deepest internal features. To successfully concentrate on important information and preserve sensitivity to features at various scales, the preceding shallow features are muted. In addition, FIAB-Loss, a loss function is introduced, combining a focal loss with IOU and active boundary loss (ABL). This composite loss function strengthens the model's focus on challenging-to-distinguish categories. Extensive experiments conducted on three datasets, including the semantic segmentation of lakes in the Tibetan Plateau and the ISPRS's Vaihingen benchmark, validate the efficacy of the proposed method. The experimental results indicate that the network exhibits exceptional performance in processing VHR images and accurately extracting edge features.",Feature extraction,Convolutional neural networks,Transformers,Remote sensing,Semantic segmentation,Visualization,Semantics,Deep learning,dual-branch coding structure,remote sensing (RS),semantic segmentation,,,,,,,,,,
Row_805,"Wu, Xinjia","Zhang, Jing","Li, Wensheng","Li, Jiafeng","Zhuo, Li",Spatial-specific Transformer with involution for semantic segmentation of high-resolution remote sensing images,INTERNATIONAL JOURNAL OF REMOTE SENSING,FEB 16 2023,2,"High-resolution remote sensing images (HR-RSIs) have a strong dependency between geospatial objects and background. Considering the complex spatial structure and multiscale objects in HR-RSIs, how to fully mine spatial information directly determines the quality of semantic segmentation. In this paper, we focus on the Spatial-specific Transformer with involution for semantic segmentation of HR-RSIs. First, we integrate the spatial-specific involution branch with self-attention branch to form a Spatial-specific Transformer backbone to produce multilevel features with global and spatial information without additional parameters. Then, we introduce multiscale feature representation with large window attention into Swin Transformer to capture multiscale contextual information. Finally, we add a geospatial feature supplement branch in the semantic segmentation decoder to mitigate the loss of semantic information caused by down-sampling multiscale features of geospatial objects. Experimental results demonstrate that our method can achieve a competitive semantic segmentation performance of 87.61% and 80.08% mIoU on Potsdam and Vaihingen datasets, respectively.",,,,,,,,,,,,"Zhang, Jie",,,,,,,,,
Row_806,"Zhang, Cheng","Jiang, Wanshou","Zhao, Qing",,,Semantic Segmentation of Aerial Imagery via Split-Attention Networks with Disentangled Nonlocal and Edge Supervision,REMOTE SENSING,MAR 2021,15,"In this work, we propose a new deep convolution neural network (DCNN) architecture for semantic segmentation of aerial imagery. Taking advantage of recent research, we use split-attention networks (ResNeSt) as the backbone for high-quality feature expression. Additionally, a disentangled nonlocal (DNL) block is integrated into our pipeline to express the inter-pixel long-distance dependence and highlight the edge pixels simultaneously. Moreover, the depth-wise separable convolution and atrous spatial pyramid pooling (ASPP) modules are combined to extract and fuse multiscale contextual features. Finally, an auxiliary edge detection task is designed to provide edge constraints for semantic segmentation. Evaluation of algorithms is conducted on two benchmarks provided by the International Society for Photogrammetry and Remote Sensing (ISPRS). Extensive experiments demonstrate the effectiveness of each module of our architecture. Precision evaluation based on the Potsdam benchmark shows that the proposed DCNN achieves competitive performance over the state-of-the-art methods.",semantic segmentation,ResNeSt,edge constrains,disentangled non-local,depth-wise separable ASPP,remote sensing,aerial image,,,,,,,,,,,,,,
Row_807,"Wang, Falin","Ji, Jian","Wang, Yuan",,,DSViT: Dynamically Scalable Vision Transformer for Remote Sensing Image Segmentation and Classification,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,5,"The relationship between the foreground target and the background of remote sensing image is very complex. The vision task of remote sensing image faces the problems of complex targets and unbalanced categories. These problems make the modeling method have further improvement space. Therefore, this article proposes a dynamically scalable attention model that combines convolutional features and Transformer features. It can dynamically select the model depth according to the size of the input image, which alleviates the problem of insufficient global information extraction of the single convolution model and the computational overhead limitation of the pure Transformer model. We validated the model on two public remote sensing image classifications and two remote sensing image segmentation datasets. The accuracy and mean pixel accuracy (mPA) of the method in this article reached 96.16% and 93.44%, respectively, on the university of california (UC) Merced classification dataset. Compared with some recent work, the method has a net improvement of 5.0% and 4.82% over the pyramid vision transformer (PVT) model. On the Potsdam segmentation dataset, the accuracy and F1 of the transformer and CNN hybrid neural network (TCHNN) model are 91.5% and 92.86%, respectively. The performance of the method has improved 0.64% and 1.0%, and the other two datasets have also achieved the best results.",Transformers,Remote sensing,Feature extraction,Computational modeling,Convolutional neural networks,Convolution,Task analysis,CNN,classification,remote sensing image,semantic segmentation,,,,,,transformer,,,,
Row_808,"Han, Zheng","Fu, Bangjie","Fang, Zhenxiong","Li, Yange","Li, Jiaying",Dynahead-YOLO-Otsu: an efficient DCNN-based landslide semantic segmentation method using remote sensing images,GEOMATICS NATURAL HAZARDS & RISK,DEC 31 2024,2,"Recent advancements in deep convolutional neural networks (DCNNs) have significantly improved landslides identification using remote sensing images. Pixel-wise semantic segmentation (PSS) and object-oriented detection (OOD) are two dominant approaches, wherein PSS are better as providing detailed delineation of landslide shapes. However, PSS are limited by the difficulty in labelling training data and low segmentation speed compared to OOD. In this paper, we propose an efficient DCNN-based landslide semantic segmentation method, the so-called Dynahead-YOLO-Otsu, to perform a PSS based on the OOD results. This is achieved by locating potential landslide regions in advance using the ODD-based Dynahead-YOLO model, which enhances the capacity for detecting landslides with variable proportions and complex background in the images. The preliminary results are then processed using the Otsu binarization algorithm to cluster pixels belonging to landslides from the images of potential regions for semantic segmentation. To validate the performance, we tested the proposed method using an open-source dataset containing 950 landslide images. We compared the results with three up-to-date DCNN-and PSS- based approaches, namely DeepLab v3+, PSPnet, and Unet. Results demonstrate that the proposed method achieves comparable Recall (71.80%) and F1 scores (75.80%), with an average improvement of 22% and 16% in Precision and IoU, respectively.",Landslide,semantic segmentation,deep convolutional neural networks,Dynahead-YOLO model,Otsu binarization method,,,,,,,"Jiang, Nan","Chen, Guangqi",,,,,,,,
Row_809,"Guo, Ningbo","Jiang, Mingyong","Hu, Xiaoyu","Su, Zhijuan","Zhang, Weibin",NPSFF-Net: Enhanced Building Segmentation in Remote Sensing Images via Novel Pseudo-Siamese Feature Fusion,REMOTE SENSING,SEP 2024,0,"Building segmentation has extensive research value and application prospects in high-resolution remote sensing image (HRSI) processing. However, complex architectural contexts, varied building morphologies, and non-building occlusions make building segmentation challenging. Compared with traditional methods, deep learning-based methods present certain advantages in terms of accuracy and intelligence. At present, the most popular option is to first apply a single neural network to encode an HRSI, then perform a decoding process through up-sampling or using a transposed convolution operation, and then finally obtain the segmented building image with the help of a loss function. Although effective, this approach not only tends to lead to a loss of detail information, but also fails to fully utilize the contextual features. As an alternative, we propose a novel network called NPSFF-Net. First, using an improved pseudo-Siamese network composed of ResNet-34 and ResNet-50, two sets of deep semantic features of buildings are extracted with the support of transfer learning, and four encoded features at different scales are obtained after fusion. Then, information from the deepest encoded feature is enriched using a feature enhancement module, and the resolutions are recovered via the operations of skip connections and transposed convolutions. Finally, the discriminative features of buildings are obtained using the designed feature fusion algorithm, and the optimal segmentation model is obtained by fitting a cross-entropy loss function. Our method obtained intersection-over-union values of 89.45% for the Aerial Imagery Dataset, 71.88% for the Massachusetts Buildings Dataset, and 68.72% for the Satellite Dataset I.",remote sensing images,building segmentation,skip connection,,,,,,,,,"Li, Ruibo","Luo, Jiancheng",,,,,,,,
Row_810,"Cui, Mengtian","Li, Kai","Chen, Jianying","Yu, Wei",,CM-Unet: A Novel Remote Sensing Image Segmentation Method Based on Improved U-Net,IEEE ACCESS,2023,2,"Semantic segmentation is an active research area for high-resolution (HR) remote sensing image processing. Most existing algorithms are better at segmenting different features. However, for complex scenes, many algorithms have insufficient segmentation accuracy. In this study, we propose a new method CM-Unet based on the U-Net framework to address the problems of holes, omissions, and fuzzy edge segmentation. First, we add the channel attention mechanism in the encoding network and the residual module to transmit information. Second, a multi-feature fusion mechanism is proposed in the decoding network, and an improved sub-pixel convolution method replaces the traditional upsampling operation. We conducted simulation experiments on the Potsdam, Vaihingen and GID datasets. The experimental results show that the proposed CM-Unet required segmentation time is approximately 62 ms/piece, the MIoU is 90.4% and the floating point operations (FLOPs) is 20.95 MFLOPs. Compared with U-Net, CM-Unet only increased the total number of parameters and floating point operations slightly, but achieved the best segmentation effect compared with the other models. CM-Unet can segment remote sensing images efficiently and accurately owing to its lower time consumption and space requirements; the precision of the segmentation results is better than other methods.",Remote sensing image,channel attention,multi-feature fusion,improved sub-pixel convolution,semantic segmentation,,,,,,,,,,,,,,,,
Row_811,"Zhang, Jie","Shao, Mingwen","Wan, Yecong","Meng, Lingzhuang","Cao, Xiangyong",Boundary-Aware Spatial and Frequency Dual-Domain Transformer for Remote Sensing Urban Images Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"Semantic segmentation of remote sensing (RS) images refers to labeling each pixel with a class to identify objects or land cover types. Existing mainstream spatial-domain semantic segmentation methods are mainly categorized into convolutional neural network (CNN)-based and vision transformer (ViT)-based approaches. The former excels at capturing local features, while the latter is adept at extracting global features. Several recent approaches consider combining CNN and ViT to efficiently capture local and global features. However, these approaches still struggle to capture complete features of the RS images, resulting in inaccurate segmentation. To address this issue, we introduce the fast Fourier transform (FFT), which transforms images into the frequency domain for feature extraction, acquiring the image-size receptive field that can complement spatial-domain methods. Based on this, we propose a boundary-aware spatial and frequency dual-domain transformer, termed dual-domain transformer. Specifically, our dual-domain transformer incorporates a dual-domain mixer (DualM), where the spatial-domain branch combines depthwise convolution and the attention mechanism to extract local and global features effectively, while the frequency-domain branch uses FFT to extract image-size features. The two branches complement each other, enabling a more comprehensive feature extraction of RS images. Meanwhile, a boundary-guided training strategy utilizing a boundary-aware module (BAM) is devised to constrain the model extract and predict boundary detail texture, which is an auxiliary task. In addition, the decoder incorporates a scale-feature fusion module (SFM) for adaptive information fusion between the encoder and decoder. Comprehensive experiments on the Zeebrugge and ISPRS datasets, including Vaihingen and Potsdam, showcase that the dual-domain transformer significantly outperforms state-of-the-art (SOTA) methods.",Feature extraction,Frequency-domain analysis,Transformers,Semantic segmentation,Semantics,Fast Fourier transforms,Training,Fast Fourier transform (FFT),frequency domain representation,remote sensing (RS) image,semantic segmentation,"Wang, Shuigen",,,,,,,,,
Row_812,Yuan Wei,Xu Wenbo,Zhou Tian,,,A loss function of road segmentation in remote sensing image by deep learning,CHINESE SPACE SCIENCE AND TECHNOLOGY,AUG 25 2021,5,"Traditional road segmentation based on spectral features or morphological algorithms has some disadvantages such as low precision and difficulty in determining the threshold value, and the existing methods in deep learning do not consider the characteristics of roads, only using general methods to segment roads. A deep learning loss function named morphological loss function with road unique trait was proposed. Firstly, the connectivity algorithm was used to divide the prediction results into several separated connected regions, and the ratios of the region area to the circumscribed circle area was calculated respectively. Then, the average value of regions was taken as the morphological loss function of this batch of training data. Finally, the morphological loss function was summed with the cross entropy loss function according to a certain proportion to obtain the final loss function. Through the comparative experiment on open remote sensing dataset, MIoU, ACC and F1-Score were all improved by the addition of morphological loss function. According to the prediction image, the predicted road was more continuous when morphological loss function was added. The morphological loss function proposed is an effective method to improve the accuracy of road segmentation in remote sensing.",morphology,remote sensing image,road segmentation,semantic segmentation,deep learning,,,,,,,,,,,,,,,,
Row_813,"Li, Wenyuan","Chen, Keyan","Chen, Hao","Shi, Zhenwei",,Geographical Knowledge-Driven Representation Learning for Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,53,"The proliferation of remote sensing satellites has resulted in a massive amount of remote sensing images. However, due to human and material resource constraints, the vast majority of remote sensing images remain unlabeled. As a result, it cannot be applied to currently available deep learning methods. To fully utilize the remaining unlabeled images, we propose a Geographical Knowledge-driven Representation (GeoKR) learning method for remote sensing images, improving network performance and reduce the demand for annotated data. The global land cover products and geographical location associated with each remote sensing image are regarded as geographical knowledge to provide supervision for representation learning and network pretraining. An efficient pretraining framework is proposed to eliminate the supervision noises caused by imaging times and resolutions difference between remote sensing images and geographical knowledge. A large-scale pretraining dataset Levir-KR is constructed to support network pretraining. It contains 1,431,950 remote sensing images from Gaofen series satellites with various resolutions. Experimental results demonstrate that our proposed method outperforms ImageNet pretraining and self-supervised representation learning methods and significantly reduces the burden of data annotation on downstream tasks, such as scene classification, semantic segmentation, object detection, and cloud/snow detection. It demonstrates that our proposed method can be used as a novel paradigm for pretraining neural networks. Codes will be available on https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.",Remote sensing,Task analysis,Satellites,Sensors,Semantics,Annotations,Training,Cloud,snow detection,object detection,remote sensing images,,,,,,representation learning,scene classification,semantic segmentation,,
Row_814,"He, Bingnan","Wu, Dongyang","Wang, Li","Xu, Sheng",,FA-HRNet: A New Fusion Attention Approach for Vegetation Semantic Segmentation and Analysis,REMOTE SENSING,NOV 2024,0,"Semantic segmentation of vegetation in aerial remote sensing images is a critical aspect of vegetation mapping. Accurate vegetation segmentation effectively informs real-world production and construction activities. However, the presence of species heterogeneity, seasonal variations, and feature disparities within remote sensing images poses significant challenges for vision tasks. Traditional machine learning-based methods often struggle to capture deep-level features for the segmentation. This work proposes a novel deep learning network named FA-HRNet that leverages the fusion of attention mechanism and a multi-branch network structure for vegetation detection and segmentation. Quantitative analysis from multiple datasets reveals that our method outperforms existing approaches, with improvements in MIoU and PA by 2.17% and 4.85%, respectively, compared with the baseline network. Our approach exhibits significant advantages over the other methods regarding cross-region and cross-scale capabilities, providing a reliable vegetation coverage ratio for ecological analysis.",computer vision,aerial remote sensing images,vision models,semantic segmentation,vegetation coverage,panoptic segmentation,,,,,,,,,,,,,,,
Row_815,"Zheng, Chen","Pan, Xinxin","Chen, Xiaohui","Yang, Xiaohui","Xin, Xin",An Object-Based Markov Random Field Model with Anisotropic Penalty for Semantic Segmentation of High Spatial Resolution Remote Sensing Imagery,REMOTE SENSING,DEC 1 2019,3,"The Markov random field model (MRF) has attracted a lot of attention in the field of remote sensing semantic segmentation. But, most MRF-based methods fail to capture the various interactions between different land classes by using the isotropic potential function. In order to solve such a problem, this paper proposed a new generalized probability inference with an anisotropic penalty for the object-based MRF model (OMRF-AP) that can distinguish the differences in the interactions between any two land classes. Specifically, an anisotropic penalty matrix was first developed to describe the relationships between different classes. Then, an expected value of the penalty information (EVPI) was developed in this inference criterion to integrate the anisotropic class-interaction information and the posteriori distribution information of the OMRF model. Finally, by iteratively updating the EVPI terms of different classes, segmentation results could be achieved when the iteration converged. Experiments of texture images and different remote sensing images demonstrated that our method could show a better performance than other state-of-the-art MRF-based methods, and a post-processing scheme of the OMRF-AP model was also discussed in the experiments.",semantic segmentation,object-based Markov random field,anisotropic penalty matrix,,,,,,,,,"Su, Limin",,,,,,,,,
Row_816,"Zulfiqar, Annus","Ghaffar, Muhammad M.","Shahzad, Muhammad","Weis, Christian","Malik, Muhammad, I",AI-ForestWatch: semantic segmentation based end-to-end framework for forest estimation and change detection using multi-spectral remote sensing imagery,JOURNAL OF APPLIED REMOTE SENSING,MAY 31 2021,15,"Forest change detection is crucial for sustainable forest management. The changes in the forest area due to deforestation (such as wild fires or logging due to development activities) or afforestation alter the total forest area. Additionally, it impacts the available stock for commercial purposes, climate change due to carbon emissions, and biodiversity of the forest habitat estimations, which are essential for disaster management and policy making. In recent years, foresters have relied on hand-crafted features or bi-temporal change detection methods to detect change in the remote sensing imagery to estimate the forest area. Due to manual processing steps, these methods are fragile and prone to errors and can generate inaccurate (i.e., under or over) segmentation results. In contrast to traditional methods, we present AI-ForestWatch, an end to end framework for forest estimation and change analysis. The proposed approach uses deep convolution neural network-based semantic segmentation to process multi-spectral spaceborne images to quantitatively monitor the forest cover change patterns by automatically extracting features from the dataset. Our analysis is completely data driven and has been performed using extended (with vegetation indices) Landsat-8 multi-spectral imagery from 2014 to 2020. As a case study, we estimated the forest area in 15 districts of Pakistan and generated forest change maps from 2014 to 2020, where major afforestation activity is carried out during this period. Our critical analysis shows an improvement of forest cover in 14 out of 15 districts. The AI-ForestWatch framework along with the associated dataset will be made public upon publication so that it can be adapted by other countries or regions. (C) The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License.",deep neural networks,semantic segmentation,multi-spectral remote sensing,multi-temporal forest change detection,,,,,,,,"Shafait, Faisal","Wehn, Norbert",,,,,,,,
Row_817,"Zhao, Yingying","Zheng, Guizhou","Xu, Zhangyan","Qiu, Zhonghang","Chen, Zhixing",Multiscale Feature Weighted-Aggregating and Boundary Enhancement Network for Semantic Segmentation of High-Resolution Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2022,4,"High-resolution remote sensing images (HRRSIs) play an important role in large area and real-time earth observation tasks. However, HRRSIs typically comprise heterogeneous objects of various sizes and complex boundary lines, which pose challenges to HRRSI segmentation. Despite the fact that deep convolutional neural networks dramatically boosted the accuracy, several limitations exist in standard models. Existing methods, mainly concatenate multiscale information to extract the various sizes of objects. However, these methods ignore differentiating information, making it difficult to take advantage of them and completely extract small objects. In addition, there have remained some difficulties in extracting boundary information with positions of uncertainty in previous works. In this article, we propose a novel multiscale feature weighted-aggregating and boundary enhancement network (MFBE-Net) for the segmentation of HRRSIs. ResNet-50, possessing a strong ability to extract features, is employed as the backbone. To fully utilize the information that was extracted, we propose a multiscale feature weighted-aggregating module, which aims to weight-integrate deep features, shallow features, and global information. The boundary enhancement module is designed to solve the blurry boundary information problems and locate its positions. Coordinate attention is also applied in the framework to coherently label size-varied ground objects from different categories and reduce information redundancy. Meanwhile, a mixed loss function is used to supervise the network training process. Finally, MFBE-Net was verified on two public HRRSI datasets, and the experimental results show that the proposed framework outperformed other existing mainstream deep learning methods and could further improve the accuracy of HRRSI segmentation.",Semantics,Image segmentation,Feature extraction,Data mining,Deep learning,Remote sensing,Convolutional neural networks,Boundary enhancement,deep learning,feature weighted-aggregating,high-resolution remote sensing images (HRRSIs),,,,,,semantic segmentation,,,,
Row_818,"Pastorino, Martina","Moser, Gabriele","Serpico, Sebastiano B.","Zerubia, Josiane",,CRFNet: A Deep Convolutional Network to Learn the Potentials of a CRF for the Semantic Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"This article presents a method for the automatic learning of the potentials of a stochastic model, in particular a conditional random field (CRF), in a non-parametric fashion. The proposed model is based on a neural architecture, in order to leverage the modeling capabilities of deep learning (DL) approaches to directly learn semantic and spatial information from the input data. Specifically, the methodology is based on fully convolutional networks and fully connected neural networks. The idea is to access the multiscale information intrinsically extracted in the intermediate layers of a fully convolutional network through the integration of fully connected neural networks at different scales, while favoring the interpretability of the hidden layers as posterior probabilities. The potentials of the CRF are learned through an additional convolutional layer, whose kernel models the local spatial information considered. The loss function is computed as a linear combination of cross-entropy losses, accounting for the multiscale and the spatial information. To evaluate the capabilities of the proposed approach for the semantic segmentation of remote sensing images, the experimental validation was conducted with the ISPRS 2-D semantic labeling challenge Vaihingen and Potsdam datasets and with the IEEE GRSS data fusion contest Zeebruges dataset. As the ground truths of these benchmark datasets are spatially exhaustive, they have been modified to approximate the spatially sparse ground truths common in real remote sensing applications. The results are significant, as the proposed approach obtains higher average classification accuracies than recent state-of-the-art techniques considered in this article. The code is available at https://github.com/Ayana-Inria/CRFNet-RS.",Convolutional neural networks,Remote sensing,Semantic segmentation,Semantics,Task analysis,Computer architecture,Conditional random fields,Conditional random fields (CRFs),convolutional neural network (CNN),fully convolutional network (FCN),remote sensing,,,,,,semantic segmentation,,,,
Row_819,"Hua, Xia","Wang, Xinqing","Rui, Ting","Shao, Faming","Wang, Dong",Cascaded panoptic segmentation method for high resolution remote sensing image,APPLIED SOFT COMPUTING,SEP 2021,17,"Great progress has been made for remote sensing image segmentation with the development of Deep Convolutional Neural Networks. However, Multiple convolutions significantly reduce the resolution and lead to the loss of many key information, the prediction accuracy of pixel categories is reduced. And DCNN accumulate context information on a large receptive field, which leads to blurred boundary segmentation of objects. This paper proposes a cascaded panoptic segmentation network to target the aforementioned problems. Firstly, a shared feature pyramid network backbone and a new hybrid task cascade framework are designed, which share the features and integrate the complementary features of different tasks in different stages, which can extract rich context information. Then, a functional module is designed to learn the mask quality of predicted instances in Mask R-CNN to calibrate the inconsistency between mask quality and mask score, thus to deal with the scale change of the object. Finally, a new Visual-saliency ranking module is designed to overcome the mutual occlusion problem between the prediction results, and strengthen robustness to illumination. The experimental results prove that our method still has significant advantages even compared with the most advanced methods, and ablation experiments also verify the effectiveness of our designed strategies. (C) 2021 Elsevier B.V. All rights reserved.",Remote sensing images,Panoptic segmentation,Instance segmentation,Semantic segmentation,Deep Convolutional Neural Networks,,,,,,,,,,,,,,,,
Row_820,"de Carvalho, Osmar L. F.","de Carvalho Junior, Osmar A.","de Albuquerque, Anesmar O.","Luiz, Argelica S.","Santana, Nickolas C.",BEYOND THE VISIBLE PIXELS USING SEMANTIC AMODAL SEGMENTATION IN REMOTE SENSING IMAGES,,2022,2,"2D representations of 3D scenes generate occlusions among different targets. Understanding targets by only seeing parts of them is referred to as an amodal perception, which is still unexplored in remote sensing. Thus, we propose integrating this concept using peculiarities of remote sensing Nadir images to classify non-visible targets at a pixel level. Nadir images present a hierarchical order of occlusions, allowing us to separate different layers. We developed a dataset with 600 images and three classes (roads, vehicles, and trees) with independent labelling for each class. Any semantic segmentation model is suitable for this task, but we explored the U-net architecture with three backbones (Efficient-net-B7, ResNet-101, and ResNeXt-101). The evaluation considered the IoU metric, providing 80% for the best model (Efficient-net-B7). Future studies aim to extend this approach by introducing competing classes among each layer and increasing the number of samples and categories.",occlusion,deep learning,aerial image,,,,,,,,,"Borges, Dibio L.",,,,,,,,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),
Row_821,"Yang, Le","Chen, Yiming","Song, Shiji","Li, Fan","Huang, Gao",Deep Siamese Networks Based Change Detection with Remote Sensing Images,REMOTE SENSING,SEP 2021,39,"Although considerable success has been achieved in change detection on optical remote sensing images, accurate detection of specific changes is still challenging. Due to the diversity and complexity of the ground surface changes and the increasing demand for detecting changes that require high-level semantics, we have to resort to deep learning techniques to extract the intrinsic representations of changed areas. However, one key problem for developing deep learning metho for detecting specific change areas is the limitation of annotated data. In this paper, we collect a change detection dataset with 862 labeled image pairs, where the urban construction-related changes are labeled. Further, we propose a supervised change detection method based on a deep siamese semantic segmentation network to handle the proposed data effectively. The novelty of the method is that the proposed siamese network treats the change detection problem as a binary semantic segmentation task and learns to extract features from the image pairs directly. The siamese architecture as well as the elaborately designed semantic segmentation networks significantly improve the performance on change detection tasks. Experimental results demonstrate the promising performance of the proposed network compared to existing approaches.",change detection,remote sensing,semantic segmentation,deep neural network,,,,,,,,,,,,,,,,,
Row_822,"Guo, Yuxuan","Wang, Zhe",,,,HEIGHT ESTIMATION BASED ON SEMANTIC SEGMENTATION,,2023,1,"Buildings are crucial for urban development, and building extraction and height estimation are two essential components in the process of architectural reconstruction. Nowadays, the use of instance segmentation methods to extract buildings from remote sensing imagery has become quite mature. However, accurately estimating the heights of complex buildings from images remains highly challenging. To address the task of building height estimation, this paper attempts to use semantic segmentation methods. Firstly, the height label quantification is simplified into a semantic segmentation task. Then, the segmentation results from different models are integrated through voting to improve the segmentation accuracy of buildings. Finally, the average heights predicted by different models are obtained based on the positions containing building regions to obtain the final height estimation. In the DFC2023 competition, our team ranked first in the height estimation task, sixth in the building extraction task, and third in the overall ranking.",Height Estimation,Instance segmentation,Building Extraction,,,,,,,,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,
Row_823,"Liu, Fang","Liu, Keming","Liu, Jia","Yang, Jingxiang","Tang, Xu",Content-Guided and Class-Oriented Learning for VHR Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,2,"With the flourishing of remote sensing (RS) platform techniques, very high-resolution (VHR) images have become more and more popular in recent years, which benefit the task of semantic segmentation but bring new challenges as well. Small objects, such as cars and trees, only occupy a few pixels in VHR images and are usually hard to segment. Moreover, the overlap problem about similar ground objects, such as low vegetation and trees, always results in underperformance. In this article, a content-guided and class-oriented network (CGCO-Net) for VHR image semantic segmentation is proposed to tackle this problem. Specifically, an adaptive content-guided fusion (ACGF) module with deformable convolution is introduced to capture long-distance dependencies and spatial aggregation effectively. With the guidance of the high-level features, the semantic content knowledge is gradually aggregated into low-level features and the details of the original features could be preserved. In addition, a multiscale channel alignment module is introduced into the encoder-decoder structure to further extract the long-range context information and reduce the calculation consumption. In order to improve the ability of pixel-level classification, a class-oriented representation learning (CORL) way is designed with transformer blocks by class embedding and deep supervision, which gradually enhance the discrimination and benefit the final segmentation. Furthermore, a weighted loss function and a threshold optimization strategy are employed to alleviate the sample imbalance problem. Tested on three public datasets and compared with several state-of-the-art methods, the proposed CGCO-net achieves good performance in both qualitative and quantitative analysis.",Transformers,Semantics,Semantic segmentation,Feature extraction,Remote sensing,Convolution,Aggregates,Class-oriented,content-guided,remote sensing (RS),semantic segmentation,"Xiao, Liang",,,,,very high-resolution (VHR) image,,,,
Row_824,"Li, Ziyao","Wang, Rui","Zhang, Wen","Hu, Fengmin","Meng, Lingkui",Multiscale Features Supported DeepLabV3 Optimization Scheme for Accurate Water Semantic Segmentation,IEEE ACCESS,2019,53,"In the task of using deep learning semantic segmentation model to extract water from high-resolution remote sensing images, multiscale feature sensing and extraction have become critical factors that affect the accuracy of image classification tasks. A single-scale training mode will cause one-sided extraction results, which can lead to reverse errors and imprecise detail expression. Therefore, fusing multiscale features for pixel-level classification is the key to achieving accurate image segmentation. Based on this concept, this paper proposes a deep learning scheme to achieve fine extraction of image water bodies. The process includes multiscale feature perception splitting of images, a restructured deep learning network model, multiscale joint prediction, and postprocessing optimization performed by a fully connected conditional random field (CRF). According to the scale space concept of remote sensing, we apply hierarchical multiscale splitting processing to images. Then, we improve the structure of the image semantic segmentation model DeepLabV3, an advanced image semantic segmentation model, and adjust the feature output layer of the model to multiscale features after weighted fusion. At the back end of the deep learning model, the water boundary details are optimized with the fully connected CRF. The proposed multiscale training method is well adapted to feature extraction for the different scale images in the model. In the multiscale output fusion, assigning different weights to the output features of each scale controls the influence of the various scale features on the water extraction results. We carried out a large number of water extraction experiments on GF1 remote sensing images. The results show that the method significantly improves the accuracy of water extraction and demonstrates the effectiveness of the method.",Feature extraction,Image segmentation,Semantics,Remote sensing,Data mining,Deep learning,Indexes,Remote sensing,deep learning,semantic segmentation,water information extraction,,,,,,multi-scales,DeepLabV 3+,,,
Row_825,"Yu, Jie","Cai, Yang","Lyu, Xin","Xu, Zhennan","Wang, Xinyuan",Boundary-Guided Semantic Context Network for Water Body Extraction from Remote Sensing Images,REMOTE SENSING,SEP 2023,2,"Automatically extracting water bodies is a significant task in interpreting remote sensing images (RSIs). Convolutional neural networks (CNNs) have exhibited excellent performance in processing RSIs, which have been widely used for fine-grained extraction of water bodies. However, it is difficult for the extraction accuracy of CNNs to satisfy the requirements in practice due to the limited receptive field and the gradually reduced spatial size during the encoder stage. In complicated scenarios, in particular, the existing methods perform even worse. To address this problem, a novel boundary-guided semantic context network (BGSNet) is proposed to accurately extract water bodies via leveraging boundary features to guide the integration of semantic context. Firstly, a boundary refinement (BR) module is proposed to preserve sufficient boundary distributions from shallow layer features. In addition, abstract semantic information of deep layers is also captured by a semantic context fusion (SCF) module. Based on the results obtained from the aforementioned modules, a boundary-guided semantic context (BGS) module is devised to aggregate semantic context information along the boundaries, thereby enhancing intra-class consistency of water bodies. Extensive experiments were conducted on the Qinghai-Tibet Plateau Lake (QTPL) and the Land-cOVEr Domain Adaptive semantic segmentation (LoveDA) datasets. The results demonstrate that the proposed BGSNet outperforms the mainstream approaches in terms of OA, MIoU, F1-score, and kappa. Specifically, BGSNet achieves an OA of 98.97% on the QTPL dataset and 95.70% on the LoveDA dataset. Additionally, an ablation study was conducted to validate the efficacy of the proposed modules.",remote sensing images,water body extraction,convolutional neural networks,boundary-guided semantic context,,,,,,,,"Fang, Yiwei","Jiang, Wenxuan","Li, Xin",,,,,,,
Row_826,"Huang, Haitao","Li, Baopu","Zhang, Yuchen","Chen, Tao","Wang, Bin",Joint Distribution Adaptive-Alignment for Cross-Domain Segmentation of High-Resolution Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,2,"Although existing unsupervised domain adaptation (UDA) methods have successfully applied to semantic segmentation tasks for high-resolution remote sensing (HRS) images, they still have some limitations that need to be addressed: 1) they mainly focus on aligning the marginal distributions while ignoring the interdomain differences in the conditional distributions, which may be suboptimal because they assume that the boundaries of category decision are identical across domains; and 2) they depend on self-supervised learning for easy-to-hard alignment, which may result in model learning erroneous knowledge from the pseudo labels. To address the above limitations, we propose a joint distribution adaptive-alignment framework (JDAF) to eliminate the distribution difference between the source and target domains, which is mainly composed of a marginal distribution alignment (MDA) module, a conditional distribution alignment (CDA) module, and an improved easy-to-hard adaptation strategy. The MDA module is used to narrow local semantic and global spatial differences between domains and first advance, and then, the CDA module that includes a category-invariant feature alignment (CFA) block and a dataset-level context aggregation (DCA) block is presented and designed, which can dynamically update and align the feature representations that are invariant to category change and adaptively incorporate dataset-level context into the features of source domain to enhance the pixel-level representation. An uncertainty-adaptive learning (UAL) method is, moreover, proposed to improve the easy-to-hard adaptation strategy by enabling the model to learn accurate knowledge from the pseudo labels, which can boost the adaptive performance of the whole JDAF. Comprehensive experiments with four cross-domain tasks on two benchmark datasets of aerospace HRS images demonstrate that the proposed JDAF achieves significant performance gains compared to the state-of-the-art cross-domain semantic segmentation methods. Our code is available at: https://github.com/maple-hx/JDAF.",Dataset-level context aggregation (DCA),high-resolution remote sensing (HRS) images,joint distribution alignment,semantic segmentation,uncertainty-adaptive learning (UAL),unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,
Row_827,"Salgueiro, Luis","Marcello, Javier","Vilaplana, Veronica",,,SEG-ESRGAN: A Multi-Task Network for Super-Resolution and Semantic Segmentation of Remote Sensing Images,REMOTE SENSING,NOV 2022,9,"The production of highly accurate land cover maps is one of the primary challenges in remote sensing, which depends on the spatial resolution of the input images. Sometimes, high-resolution imagery is not available or is too expensive to cover large areas or to perform multitemporal analysis. In this context, we propose a multi-task network to take advantage of the freely available Sentinel-2 imagery to produce a super-resolution image, with a scaling factor of 5, and the corresponding high-resolution land cover map. Our proposal, named SEG-ESRGAN, consists of two branches: the super-resolution branch, that produces Sentinel-2 multispectral images at 2 m resolution, and an encoder-decoder architecture for the semantic segmentation branch, that generates the enhanced land cover map. From the super-resolution branch, several skip connections are retrieved and concatenated with features from the different stages of the encoder part of the segmentation branch, promoting the flow of meaningful information to boost the accuracy in the segmentation task. Our model is trained with a multi-loss approach using a novel dataset to train and test the super-resolution stage, which is developed from Sentinel-2 and WorldView-2 image pairs. In addition, we generated a dataset with ground-truth labels for the segmentation task. To assess the super-resolution improvement, the PSNR, SSIM, ERGAS, and SAM metrics were considered, while to measure the classification performance, we used the IoU, confusion matrix and the F1-score. Experimental results demonstrate that the SEG-ESRGAN model outperforms different full segmentation and dual network models (U-Net, DeepLabV3+, HRNet and Dual_DeepLab), allowing the generation of high-resolution land cover maps in challenging scenarios using Sentinel-2 10 m bands.",multi-task network,super-resolution,semantic segmentation,Sentinel-2,WorldView-2,,,,,,,,,,,,,,,,
Row_828,"Zhou, Ruixue","Yuan, Zhiqiang","Rong, Xuee","Ma, Weicong","Sun, Xian",Weakly Supervised Semantic Segmentation in Aerial Imagery via Cross-Image Semantic Mining,REMOTE SENSING,FEB 2023,6,"Weakly Supervised Semantic Segmentation (WSSS) with only image-level labels reduces the annotation burden and has been rapidly developed in recent years. However, current mainstream methods only employ a single image's information to localize the target and do not account for the relationships across images. When faced with Remote Sensing (RS) images, limited to complex backgrounds and multiple categories, it is challenging to locate and differentiate between the categories of targets. As opposed to previous methods that mostly focused on single-image information, we propose CISM, a novel cross-image semantic mining WSSS framework. CISM explores cross-image semantics in multi-category RS scenes for the first time with two novel loss functions: the Common Semantic Mining (CSM) loss and the Non-common Semantic Contrastive (NSC) loss. In particular, prototype vectors and the Prototype Interactive Enhancement (PIE) module were employed to capture semantic similarity and differences across images. To overcome category confusions and closely related background interferences, we integrated the Single-Label Secondary Classification (SLSC) task and the corresponding single-label loss into our framework. Furthermore, a Multi-Category Sample Generation (MCSG) strategy was devised to balance the distribution of samples among various categories and drastically increase the diversity of images. The above designs facilitated the generation of more accurate and higher-granularity Class Activation Maps (CAMs) for each category of targets. Our approach is superior to the RS dataset based on extensive experiments and is the first WSSS framework to explore cross-image semantics in multi-category RS scenes and obtain cutting-edge state-of-the-art results on the iSAID dataset by only using image-level labels. Experiments on the PASCAL VOC2012 dataset also demonstrated the effectiveness and competitiveness of the algorithm, which pushes the mean Intersection-Over-Union (mIoU) to 67.3% and 68.5% on the validation and test sets of PASCAL VOC2012, respectively.",weakly supervised semantic segmentation,remote sensing images,image-level labels,,,,,,,,,"Fu, Kun","Zhang, Wenkai",,,,,,,,
Row_829,"Zhang, Lili","Xu, Mengqi","Wang, Gaoxu","Shi, Rui","Xu, Yi",SiameseNet Based Fine-Grained Semantic Change Detection for High Resolution Remote Sensing Images,REMOTE SENSING,DEC 2023,0,"Change detection in high resolution (HR) remote sensing images faces more challenges than in low resolution images because of the variations of land features, which prompts this research on faster and more accurate change detection methods. We propose a pixel-level semantic change detection method to solve the fine-grained semantic change detection for HR remote sensing image pairs, which takes one lightweight semantic segmentation network (LightNet), using the parameter-sharing SiameseNet, as the architecture to carry out pixel-level semantic segmentations for the dual-temporal image pairs and achieve pixel-level change detection based directly on semantic comparison. LightNet consists of four long-short branches, each including lightweight dilated residual blocks and an information enhancement module. The feature information is transmitted, fused, and enhanced among the four branches, where two large-scale feature maps are fused and then enhanced via the channel information enhancement module. The two small-scale feature maps are fused and then enhanced via a spatial information enhancement module, and the four upsampling feature maps are finally concatenated to form the input of the Softmax. We used high resolution remote sensing images of Lake Erhai in Yunnan Province in China, collected by GF-2, to make one dataset with a fine-grained semantic label and a dual-temporal image-pair label to train our model, and the experiments demonstrate the superiority of our method and the accuracy of LightNet; the pixel-level semantic change detection methods are up to 89% and 86%, respectively.",change detection,dual-temporal remote sensing images,information enhancement,Siamese network,,,,,,,,"Yan, Ruijie",,,,,,,,,
Row_830,"Wang, Yiqin",,,,,Remote Sensing Image Semantic Segmentation Algorithm Based on Improved ENet Network,SCIENTIFIC PROGRAMMING,OCT 4 2021,9,"A remote sensing image semantic segmentation algorithm based on improved ENet network is proposed to improve the accuracy of segmentation. First, dilated convolution and decomposition convolution are introduced in the coding stage. They are used in conjunction with ordinary convolution to increase the receptive field of the model. Each convolution output contains a larger range of image information. Second, in the decoding stage, the image information of different scales is obtained through the upsampling operation and then through the compression, excitation, and reweighting operations of the Squeeze and Excitation (SE) module. The weight of each feature channel is recalibrated to improve the accuracy of the network. Finally, the Softmax activation function and the Argmax function are used to obtain the final segmentation result. Experiments show that our algorithm can significantly improve the accuracy of remote sensing image semantic segmentation.",,,,,,,,,,,,,,,,,,,,,
Row_831,"Wang, Biao","Jiang, Zhenghao","Ma, Weichun","Xu, Xiao","Zhang, Peng",Dual-Dimension Feature Interaction for Semantic Change Detection in Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"Remote sensing semantic change detection (SCD) involves extracting information about changes in land cover/land use (LCLU) within the same area at different times. This issue is of crucial significance in many Earth observation tasks, such as precise urban planning and natural resource management. However, the current methods primarily focus on spatial feature extraction, lacking awareness of temporal features. Consequently, there are challenges in extracting change features, making distinguishing intraclass and interclass differences difficult. This also contributes to pseudochange, posing challenges for SCD tasks. To overcome the limitations of existing methods, we present a dual-dimension feature interaction network (DFINet) for SCD. First, to enhance the assessment and perceptual abilities related to intraclass and interclass differences, we introduce a temporal difference feature enhancement (TDFE) module. This module comprehensively captures features from the temporal dimension. Then, to address the interrelation between multitemporal and multilevel features, we investigate the feature selection interaction (FSIA) and interaction attention modules (IAM), which enable multidimensional deep fusion and interaction of change features. This enhances the capacity for information transfer and integration among the features within multitemporal remote sensing images (RSIs). The experimental results demonstrate that, compared to existing methods, the proposed architecture achieves a significant improvement in accuracy. Additionally, the design enhancements added to DFINet boost the practicality of remote sensing SCD, underscoring its substantial research value.",Feature extraction,Semantics,Task analysis,Remote sensing,Sensors,Semantic segmentation,Convolution,Dual-dimension,interclass,intraclass,remote sensing images (RSIs),"Wu, Yanlan","Yang, Hui",,,,semantic change detection (SCD),,,,
Row_832,"Liu, Wei","Su, Fulin","Jin, Xinfei","Li, Hongxu","Qin, Rongjun",Bispace Domain Adaptation Network for Remotely Sensed Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,22,"Supervised learning for semantic segmentation has achieved impressive success in remote sensing, while this normally has a high demand on pixel-level ground truth from the testing images (target domain). Labeling data for semantic segmentation is labor-intensive and time-consuming. To reduce the workload of manual labeling, domain adaptation (DA) utilizes preexisting labeled images from other sources (source domain) to classify the images in the target domain. In this article, we propose a bispace alignment network for DA named BSANet. BSANet is designed to have a dual-branch structure which is able to extract features in the image domain and the wavelet domain simultaneously. To minimize the discrepancy between the source and target domains, we propose a bispace adversarial learning strategy. Specifically, BSANet employs two discriminators in different spaces, one aligning the source and target feature distributions, and the other helping the classification outputs render reasonable spatial layouts. The proposed method shows the ability to train an end-to-end network for semantic segmentation without using any label in the target domain. Extensive experiments and ablation studies are conducted in cross-city scenarios. Comparative experiments with several state-of-the-art DA methods show that our method achieves the best performance.",Semantics,Feature extraction,Image segmentation,Wavelet domain,Generators,Training,Loss measurement,Adversarial learning,domain adaptation (DA),semantic segmentation,transfer learning (TL),,,,,,wavelet transform,,,,
Row_833,"Cui, Liangyi","Jing, Xin","Wang, Yu","Huan, Yixuan","Xu, Yang",Improved Swin Transformer-Based Semantic Segmentation of Postearthquake Dense Buildings in Urban Areas Using Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,42,"Timely acquiring the earthquake-induced damage of buildings is crucial for emergency assessment and post-disaster rescue. Optical remote sensing is a typical method for obtaining seismic data due to its wide coverage and fast response speed. Convolutional neural networks (CNNs) are widely applied for remote sensing image recognition. However, insufficient extraction and expression ability of global correlations between local image patches limit the performance of dense building segmentation. This paper proposes an improved Swin Transformer to segment dense urban buildings from remote sensing images with complex backgrounds. The original Swin Transformer is used as a backbone of the encoder, and a convolutional block attention module is employed in the linear embedding and patch merging stages to focus on significant features. Hierarchical feature maps are then fused to strengthen the feature extraction process and fed into the UPerNet (as the decoder) to obtain the final segmentation map. Collapsed and non-collapsed buildings are labeled from remote sensing images of the Yushu and Beichuan earthquakes. Data augmentations of horizontal and vertical flipping, brightness adjustment, uniform fogging, and non-uniform fogging are performed to simulate actual situations. The effectiveness and superiority of the proposed method over the original Swin Transformer and several mature CNN-based segmentation models are validated by ablation experiments and comparative studies. The results show that the mean intersection-over-union of the improved Swin Transformer reaches 88.53%, achieving an improvement of 1.3% compared to the original model. The stability, robustness, and generalization ability of dense building recognition under complex weather disturbances are also validated.",Attention mechanism,complex weather disturbances,dense seismic building segmentation,feature fusion,improved Swin Transformer,remote sensing images,,,,,,"Zhang, Qiangqiang",,,,,,,,,
Row_834,"Wang, Fang","Piao, Shihao","Xie, Jindong",,,CSE-HRNet: A Context and Semantic Enhanced High-Resolution Network for Semantic Segmentation of Aerial Imagery,IEEE ACCESS,2020,14,"Semantic segmentation of high-resolution aerial images is a concerning issue of remote sensing applications. To address the issues of intra-class heterogeneity and inter-class homogeneity, a novel end-to-end semantic segmentation network, namely Context and Semantic Enhanced High-Resolution Network (CSE-HRNet), is proposed in this paper. Two procedures are considered comprehensively, which are multi-scale contextual feature extractor and multi-level semantic feature producer. Nested Dilated Residual Block (NDRB) is designed firstly, which could enhance the representational power of multi-scale contexts and tackle the issue of intra-class heterogeneity. The pyramidal feature hierarchy is introduced secondly, by which multi-level feature fusions could be utilized to enlarge inter-class semantic differences. Experimental results verify that, based on the Potsdam and Vaihingen benchmarks, the proposed CSE-HRNet can achieve competitive performance compared with other state-of-the-art methods.",Semantic segmentation,image analysis,machine learning,remote sensing image,,,,,,,,,,,,,,,,,
Row_835,"Shuangpeng, Zheng","Ta, Fang","Hong, Huo",,,Farmland Recognition of High Resolution Multispectral Remote Sensing Imagery using Deep Learning Semantic Segmentation Method,,2019,5,"Farmland mapping is an important step for estimating grain yields. However extraction of farmland from multispectral remote sensing images (RSIs) is still a challenging work, as farmland is located on not only plains but also mountains, which displays divergent and confusing characteristics in RSIs. To solve the problem of lacking the multispectral remote sensing image dataset for pretraining, we extend Deep Feature Aggregation Net (DFANet) with fewer network parameters, a semantic segmentation network, to automatically map farmland from 3-band to multispectral images in a pixel-wise strategy. In this network, we first utilize more information aggregation. The fully-connected attention module is then replaced by a proposed convolution attention module. Finally, a new proposed decoder is used to recover the details of the feature map. Experimental results show that the model with multispectral RSIs outperforms the baselines.",Deep Learing,Remote Sensing,Farmland Recognition,High-resolution Multispectral Image,Agriculture,,,,,,,,,,,,,,,PROCEEDINGS OF 2019 INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE (PRAI 2019),
Row_836,"Yu, Minmin","Qin, Fen",,,,Research on the Applicability of Transformer Model in Remote-Sensing Image Segmentation,APPLIED SCIENCES-BASEL,FEB 2023,6,"Transformer models have achieved great results in the field of computer vision over the past 2 years, drawing attention from within the field of remote sensing. However, there are still relatively few studies on this model in the field of remote sensing. Which method is more suitable for remote-sensing segmentation? In particular, how do different transformer models perform in the face of high-spatial resolution and the multispectral resolution of remote-sensing images? To explore these questions, this paper presents a comprehensive comparative analysis of three mainstream transformer models, including the segmentation transformer (SETRnet), SwinUnet, and TransUnet, by evaluating three aspects: a visual analysis of feature-segmentation results, accuracy, and training time. The experimental results show that the transformer structure has obvious advantages for the feature-extraction ability of large-scale remote-sensing data sets and ground objects, but the segmentation performance of different transfer structures in different scales of remote-sensing data sets is also very different. SwinUnet exhibits better global semantic interaction and pixel-level segmentation prediction on the large-scale Potsdam data set, and the SwinUnet model has the highest accuracy metrics for KAPPA, MIoU, and OA in the Potsdam data set, at 76.47%, 63.62%, and 85.01%, respectively. TransUnet has better segmentation results in the small-scale Vaihingen data set, and the three accuracy metrics of KAPPA, MIoU, and OA are the highest, at 80.54%, 56.25%, and 85.55%, respectively. TransUnet is better able to handle the edges and details of feature segmentation thanks to the network structure together built by its transformer and convolutional neural networks (CNNs). Therefore, TransUnet segmentation accuracy is higher when using a small-scale Vaihingen data set. Compared with SwinUnet and TransUnet, the segmentation performance of SETRnet in different scales of remote-sensing data sets is not ideal, so SETRnet is not suitable for the research task of remote-sensing image segmentation. In addition, this paper discusses the reasons for the performance differences between transformer models and discusses the differences between transformer models and CNN. This study further promotes the application of transformer models in remote-sensing image segmentation, improves the understanding of transformer models, and helps relevant researchers to select a more appropriate transformer model or model improvement method for remote-sensing image segmentation.",transformer,multihead attention,remote-sensing image segmentation,deep learning,SwinUner,TransUnet,SETRnet,visual classification,,,,,,,,,,,,,
Row_837,"Yang, Yunsong","Yuan, Genji","Li, Jinjiang",,,Multielement-Feature-Based Hierarchical Context Integration Network for Remote Sensing Image Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,1,"In the current remote sensing segmentation tasks, we identify issues of insufficient accuracy in segmenting objects and types with similar colors, along with a lack of adequate smoothness and coherence in edge segmentation. To address these challenges, we propose a network framework called the multielement-feature-based hierarchical context integration network (MHCINet). This framework achieves deep integration of global information, local information, multiscale information, and edge information. First, we introduce an Edge and Levels Grouped Aggregator to fuse shallow features, deep features, and edge information, enhancing foreground saliency. Finally, to better identify instances with similar colors during the feature reconstruction stage, we design a constant multivariate feature integrator to fully exploit multiscale information and global context, thereby improving the segmentation model's performance. Comprehensive experimental results on the Vaihingen and Potsdam datasets demonstrate that MHCINet outperforms existing state-of-the-art methods, achieving mean intersection over union of 84.8% and 87.6% on the Vaihingen and Potsdam datasets, respectively.",Remote sensing,Image edge detection,Image color analysis,Semantics,Semantic segmentation,Feature extraction,Task analysis,Edge fusion,multiscale fusion,remote sensing,semantic segmentation,,,,,,transformer,,,,
Row_838,"Yang, Wanying","Cheng, Yali","Xu, Wenbo",,,Remote Sensing Semantic Change Detection Model for Improving Objects Completeness,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2025,0,"Semantic change detection (SCD) extends beyond binary change detection by not only discerning the locations of change areas, but also offering the alterations in land-cover/land-use types. This refined change information is concernful for various applications. While deep learning methods have made significant progress in SCD, accurately capturing the integrity of targets remains challenging in intricate scenarios. Therefore, this article proposes a deformable multiscale composite transformer network (DMCTNet). This network effectively models relevant semantic information and spatio-temporal dependencies. DMCTNet leverages a variant vision foundation models encoder to learn specific knowledge, facilitating effective visual representation in remote sensing images. A multiscale feature aggregator module is developed to discern both the ""what"" and ""where"" of changes by integrating features across different scales. Subsequently, a masked decoder through queries to convey rich semantic change information, guided by conspicuous change potential locations to decode. A substantial volume of experimental results consistently demonstrate that this model achieves more accurate and reliable results in change areas, improving the intersection over the union of change by 2.65% and 1.67% on the SECOND and Landsat-SCD datasets, respectively. Code will be made available.",Semantics,Transformers,Feature extraction,Tuning,Correlation,Visualization,Decoding,Accuracy,Remote sensing,Computational modeling,Remote sensing (RS),,,,,,semantic change detection (SCD),semantic segmentation,visual foundation models (VFMs),,
Row_839,"Xiong, Wei","Cai, Mi","Lv, Yafei","Pei, Jiazheng",,FA-Net: feature attention network for semantic segmentation of ship port,GEOCARTO INTERNATIONAL,MAR 19 2022,1,"Accurate understanding of the scene of ship ports is important in a broad range of military and civilian applications, such as maritime management, maritime safety, fisheries management, maritime situational awareness (MSA), and ocean traffic surveillance. Semantic segmentation, which implements pixel-level classification, can achieve an exhaustive analysis of ship ports. However, in the earlier time, the state-of-the-art methods of semantic segmentation were mostly based on the study of natural images. Subsequently, semantic segmentation has been gradually widely used in remote sensing, but still few of them has focused on the parsing of ship ports in remote sensing. In order to realize a detailed analysis of ship ports, a novel framework (called Feature Attention Network) is proposed for the accurate segmentation of multiple targets in a ship port in this paper. In this framework, a multi-label classification auxiliary network is first designed to solve the problem of confused multiple prediction for one target by capturing more global context information. Then, an attention model is introduced to solve the problem of error segmentation between similar targets with different labels. Finally, a feature aggregation model is presented to obtain more contextual information. In addition, we construct a data set for the semantic segmentation of ship ports (called HRSC2016-SP) by labeling the HRSC2016 data set to evaluate our proposed framework. Our approach has achieved a state-of-the-art result (82.16% mIoU) on the test set of HRSC2016-SP.",FA-Net,semantic segmentation,ship port,remote sensing,HRSC2016-SP,,,,,,,,,,,,,,,,
Row_840,"Wang, Hongzhen","Wang, Ying","Zhang, Qian","Xiang, Shiming","Pan, Chunhong",Gated Convolutional Neural Network for Semantic Segmentation in High-Resolution Images,REMOTE SENSING,MAY 2017,168,"Semantic segmentation is a fundamental task in remote sensing image processing. The large appearance variations of ground objects make this task quite challenging. Recently, deep convolutional neural networks (DCNNs) have shown outstanding performance in this task. A common strategy of these methods (e.g., SegNet) for performance improvement is to combine the feature maps learned at different DCNN layers. However, such a combination is usually implemented via feature map summation or concatenation, indicating that the features are considered indiscriminately. In fact, features at different positions contribute differently to the final performance. It is advantageous to automatically select adaptive features when merging different-layer feature maps. To achieve this goal, we propose a gated convolutional neural network to fulfill this task. Specifically, we explore the relationship between the information entropy of the feature maps and the label-error map, and then a gate mechanism is embedded to integrate the feature maps more effectively. The gate is implemented by the entropy maps, which are generated to assign adaptive weights to different feature maps as their relative importance. Generally, the entropy maps, i.e., the gates, guide the network to focus on the highly-uncertain pixels, where detailed information from lower layers is required to improve the separability of these pixels. The selected features are finally combined to feed into the classifier layer, which predicts the semantic label of each pixel. The proposed method achieves competitive segmentation accuracy on the public ISPRS 2D Semantic Labeling benchmark, which is challenging for segmentation by only using the RGB images.",semantic segmentation,CNN,deep learning,ISPRS,remote sensing,gate,,,,,,,,,,,,,,,
Row_841,"Nalinipriya, G.","Lydia, E. Laxmi","Alshenaifi, Reem","Kavuri, Radhika","Ishak, Mohamad Khairi",A Two-Tiered Bidirectional Atrous Spatial Pyramid Pooling-Based Semantic Segmentation Model for Landslide Classification Using Remote Sensing Images,IEEE ACCESS,2024,0,"Effective landslide representation from great spatial resolution images is significant in numerous applications. Many research works and techniques have been advertised. Still, these methods are very challenging to relate in real time since they depend on remotely sensing landslides from a solitary sensor with an exact spatial resolution. Precisely identifying landslides over a vast region with intricate background entities is difficult. Machine Learning (ML) and Deep Learning (DL) have attained extraordinary performance in classifying images utilizing remotely sensed images from numerous platforms. Moreover, techniques built within DL architectures tend to implement encoder-decoder network structures, where constant convolutions effortlessly strain out numerous landslide features. This study develops a Bidirectional Atrous Spatial Pyramid Pooling-Based Semantic Segmentation and Classification Model (BASPP-SSCM) technique for landslide Remote Sensing Images. The main goal of the BASPP-SSCM technique is to segment and classify the landslide areas. In the preprocessing stage, the BASPP-SSCM model employs an adaptive Wiener filtering (AWF) technique to eliminate the noise. Next, for the semantic segmentation method, the BASPP-SSCM technique utilizes the DeepLabV3 method with the backbone of the ConvNeXtLarge model for determining the landslide region. Furthermore, the CapsNet model is utilized for the feature extraction process. Besides, the Rigdelet neural network (RNN) technique is employed for the landslide classification process. At last, the pelican optimization algorithm (POA) methodology is implemented to fine-tune the parameters involved in the RNN model. A wide range of investigations is performed to highlight the superiority of the BASPP-SSCM method using a benchmark dataset. The performance validation of the BASPP-SSCM method underscored a superior accuracy value of 98.23% of other existing approaches.",Terrain factors,Feature extraction,Remote sensing,Semantic segmentation,Adaptation models,Accuracy,Image recognition,Computational modeling,Spatial resolution,Reliability,landslide remote sensing images,,,,,,pelican optimization algorithm,feature extraction,atrous spatial pyramid pooling,,
Row_842,"El Rai, Marwa Chendeb","Aburaed, Nour","Al-Saad, Mina","Al-Ahmad, Hussain","Al Mansoori, Saeed",Integrating Deep Learning with Active Contour Models in Remote Sensing Image Segmentation,,2020,1,"Semantic image segmentation using deep learning is a crucial step in remote sensing and image processing. It has been exploited in oil spill identification in this work. Remote sensing Synthetic Aperture Radar (SAR) images have been used to identify oil spills due to their capability to cover wide scenery irrespective of the weather and illumination conditions. Oil spills can be seen by radar sensors as black spots. Nonetheless, the discrimination between the oil spills and looks-alike is challenging in the case of semantic segmentation at pixel level. To overcome this problem, the active contour without edges models take into account the length of boundaries, the areas inside and outside the region of interest to be integrated in the deep learning image segmentation model. For this purpose, a loss function, which includes the area and the length of object, is back propagated into the semantic segmentation architecture to optimize the deep learning process. The method is evaluated on a publicly available oil spill dataset. The experiments show that the proposed approach outperforms other state-of-the-art methods in terms of Intersection over Union (IoU).",Oil Spill detection,Synthetic Aperture Radar,Semantic Segmentation,Deep Learning,Active Contour Models,,,,,,,"Marshall, Stephen",,,,,,,,"2020 27TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS, CIRCUITS AND SYSTEMS (ICECS)",
Row_843,"Li, Xin","Xu, Feng","Xia, Runliang","Li, Tao","Chen, Ziqi",Encoding Contextual Information by Interlacing Transformer and Convolution for Remote Sensing Imagery Semantic Segmentation,REMOTE SENSING,AUG 2022,23,"Contextual information plays a pivotal role in the semantic segmentation of remote sensing imagery (RSI) due to the imbalanced distributions and ubiquitous intra-class variants. The emergence of the transformer intrigues the revolution of vision tasks with its impressive scalability in establishing long-range dependencies. However, the local patterns, such as inherent structures and spatial details, are broken with the tokenization of the transformer. Therefore, the ICTNet is devised to confront the deficiencies mentioned above. Principally, ICTNet inherits the encoder-decoder architecture. First of all, Swin Transformer blocks (STBs) and convolution blocks (CBs) are deployed and interlaced, accompanied by encoded feature aggregation modules (EFAs) in the encoder stage. This design allows the network to learn the local patterns and distant dependencies and their interactions simultaneously. Moreover, multiple DUpsamplings (DUPs) followed by decoded feature aggregation modules (DFAs) form the decoder of ICTNet. Specifically, the transformation and upsampling loss are shrunken while recovering features. Together with the devised encoder and decoder, the well-rounded context is captured and contributes to the inference most. Extensive experiments are conducted on the ISPRS Vaihingen, Potsdam and DeepGlobe benchmarks. Quantitative and qualitative evaluations exhibit the competitive performance of ICTNet compared to mainstream and state-of-the-art methods. Additionally, the ablation study of DFA and DUP is implemented to validate the effects.",semantic segmentation,Swin Transformer,local patterns and distant dependencies,feature aggregation,well-rounded context,,,,,,,"Wang, Xinyuan","Xu, Zhennan","Lyu, Xin",,,,,,,
Row_844,"Jiang, Jie","Lyu, Chengjin","Liu, Siying","He, Yongqiang","Hao, Xuetao",RWSNet: a semantic segmentation network based on SegNet combined with random walk for remote sensing,INTERNATIONAL JOURNAL OF REMOTE SENSING,JAN 17 2020,36,"Semantic segmentation methods based on deep learning considerably improve the segmentation performance of remote sensing images. However, with the extensive application of high-resolution remote sensing images, additional details introduce considerable interference to the learning process for classification, thereby diminishing the accuracy of segmentation and resulting in blurry object boundaries. To address this problem, this study designed Random-Walk-SegNet (RWSNet), a semantic segmentation network based on SegNet combined with random walk. First, SegNet is used as the basic architecture with the sliding window strategy that optimizes the network output to improve the continuity and smoothness of segmentation. Second, seed regions of the random walk are selected in accordance with the classification output of SegNet. Third, the weights of the undirected graph edge are determined by fusing the gradient of the original image and probability map of SegNet. Finally, random walk is implemented on the entire image, thus reducing edge blur and realizing high-performance semantic segmentation of remote sensing images. In comparison with mainstream and other improved methods, the proposed network has lower complexity but better performance, and the algorithm is state-of-the-art and robust.",,,,,,,,,,,,,,,,,,,,,
Row_845,"Chen, Kaiqiang","Fu, Kun","Yan, Menglong","Gao, Xin","Sun, Xian",Semantic Segmentation of Aerial Images With Shuffling Convolutional Neural Networks,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,FEB 2018,71,"Semantic segmentation of aerial images refers to assigning one land cover category to each pixel. This is a challenging task due to the great differences in the appearances of ground objects. Many attempts have been made during the past decades. In recent years, convolutional neural networks (CNNs) have been introduced in the remote sensing field, and various solutions have been proposed to realize dense semantic labeling with CNNs. In this letter, we propose shuffling CNNs to realize semantic segmentation of aerial images in a periodic shuffling manner. This approach is a supplement to current methods for semantic segmentation of aerial images. We propose a naive version and a deeper version of this method, and both are adept at detecting small objects. Additionally, we propose a method called field-of-view (FoV) enhancement that can enhance the predictions. This method can be applied to various networks, and our experiments verify its effectiveness. The final results are further improved through an ensemble method that averages the score maps generated by the models at different checkpoints of the same network. We evaluate our models using the ISPRS Vaihingen and Potsdam data sets, and we acquire promising results using these two data sets.",Aerial images,convolutional neural networks (CNNs),deep learning,remote sensing,semantic segmentation,,,,,,,"Wei, Xin",,,,,,,,,
Row_846,"Grau, Marc","Lontke, Alexander","Jiang, Xuemei","Scheibenreif, Linus",,SELF SUPERVISED LEARNING IN REMOTE SENSING: QUANTIFYING APPROACHES EFFECTIVENESS ACROSS DOWNSTREAM TASKS,,2023,0,"In the remote sensing field, vast amounts of data are available. However, labeling such data is expensive. Self-supervised learning makes it possible to leverage unlabeled data for the training of deep neural network models. This work focuses on the effectiveness of self-supervised pretext tasks for different supervised downstream tasks. Therefore, we compare generative, contrastive, and generative-contrastive pretext tasks across classification and semantic segmentation downstream tasks. Our results show that the contrastive setup is beneficial for remote sensing image classification, whereas the generative-contrastive setup shows the best results for the semantic segmentation downstream task. Therefore, our work indicates that the choice of self-supervised pretext task is an important consideration to optimize downstream task performance.",Remote Sensing,Deep Learning,Self-Supervised Learning,Classification,Segmentation,,,,,,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,
Row_847,"Chen, Suting","Wu, Chaoqun","Mukherjee, Mithun","Zheng, Yujie",,HA-MPPNet: Height Aware-Multi Path Parallel Network for High Spatial Resolution Remote Sensing Image Semantic Seg-Mentation,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,OCT 2021,2,"Semantic segmentation of remote sensing images (RSI) plays a significant role in urban management and land cover classification. Due to the richer spatial information in the RSI, existing convolutional neural network (CNN)-based methods cannot segment images accurately and lose some edge information of objects. In addition, recent studies have shown that leveraging additional 3D geometric data with 2D appearance is beneficial to distinguish the pixels' category. However, most of them require height maps as additional inputs, which severely limits their applications. To alleviate the above issues, we propose a height aware-multi path parallel network (HA-MPPNet). Our proposed MPPNet first obtains multi-level semantic features while maintaining the spatial resolution in each path for preserving detailed image information. Afterward, gated high-low level feature fusion is utilized to complement the lack of low-level semantics. Then, we designed the height feature decode branch to learn the height features under the supervision of digital surface model (DSM) images and used the learned embeddings to improve semantic context by height feature guide propagation. Note that our module does not need a DSM image as additional input after training and is end-to-end. Our method outperformed other state-of-the-art methods for semantic segmentation on publicly available remote sensing image datasets.",remote sensing image,semantic segmentation,high spatial resolution,gated feature fusion,digital surface model (DSM),height features,,,,,,,,,,,,,,,
Row_848,"Bello, Inuwa Mamuda","Zhang, Ke","Su, Yu","Wang, Jingyu","Aslam, Muhammad Azeem",Densely multiscale framework for segmentation of high resolution remote sensing imagery,COMPUTERS & GEOSCIENCES,OCT 2022,6,"Semantic segmentation has gained research attention in recent times, especially within the remote sensing community. The deep neural network has proven to be the most effective approach for segmentation applications due to its automatic feature extraction capability. Research results indicate that the multiscale segmentation frameworks are more suitable for high-level feature extraction, especially from complex remote sensing images. However, most existing multiscale frameworks are either complex or highly parameterized, making them inefficient for real-time remote sensing applications. In this work, we propose an accurate and highly efficient densely multiscale segmentation network specifically for real-time segmentation of remotely sensed imagery. We significantly improve the representation capability of the network by embedding its structure with the dense connection, which allows gradient to flow with ease through the network. The proposed network with few trainable parameters performed significantly on two publicly available challenging datasets, making it suitable for deployment on resource-constrained devices for real-time remote sensing applications.",Segmentation,Dense convolution,Multiscale,Neural network,,,,,,,,,,,,,,,,,
Row_849,"Xu, Yizhe","Yan, Liangliang","Jiang, Jie",,,EI-HCR: An Efficient End-to-End Hybrid Consistency Regularization Algorithm for Semisupervised Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,3,"Recently, remote sensing image (RSI) semantic segmentation technology has advanced greatly, with the fully supervised process achieving particularly strong performance. However, the technology depends heavily on dataset labels, leading to high annotation costs. To alleviate this problem, we propose a novel efficient end-to-end hybrid consistency regularization algorithm (EI-HCR) for the semisupervised semantic segmentation of RSI, wherein only a few labeled images and a large number of unlabeled images are effectively used. First, we devise data perturbation (DP) consistency regularization (CR), which includes a data mix-up method to combine unlabeled and labeled images. Then, we employ teacher and student networks to conduct model perturbation (MP) CR. Both segmentation results are regarded as pseudo-labels for each other. In the end, the semisupervised loss is composed of DP and MP consistency loss and supervises network training along with the fully supervised loss. More importantly, we first combine the characteristics of knowledge distillation to make the student network more lightweight, efficiently reducing the model inference time. Experimental results demonstrate the effectiveness of EI-HCR on the ISPRS Vaihingen and Massachusetts Buildings datasets. With only 5% of the labeled images, EI-HCR can achieve the same accuracy as the fully supervised training with 50% of the labeled images, and the number of student model parameters is only 9.64 M, indicating the method's great advantages over other algorithms.",Consistency regularization (CR),knowledge distillation,remote sensing image (RSI),semantic segmentation,semisupervised learning,,,,,,,,,,,,,,,,
Row_850,"Wei, Yao","Zhang, Kai","Ji, Shunping",,,Simultaneous Road Surface and Centerline Extraction From Large-Scale Remote Sensing Images Using CNN-Based Segmentation and Tracing,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,DEC 2020,96,"Accurate and up-to-date road maps are of great importance in a wide range of applications. Unfortunately, automatic road extraction from high-resolution remote sensing images remains challenging due to the occlusion of trees and buildings, discriminability of roads, and complex backgrounds. To address these problems, especially road connectivity and completeness, in this article, we introduce a novel deep learning-based multistage framework to accurately extract the road surface and road centerline simultaneously. Our framework consists of three steps: boosting segmentation, multiple starting points tracing, and fusion. The initial road surface segmentation is achieved with a fully convolutional network (FCN), after which another lighter FCN is applied several times to boost the accuracy and connectivity of the initial segmentation. In the multiple starting points tracing step, the starting points are automatically generated by extracting the road intersections of the segmentation results, which then are utilized to track consecutive and complete road networks through an iterative search strategy embedded in a convolutional neural network (CNN). The fusion step aggregates the semantic and topological information of road networks by combining the segmentation and tracing results to produce the final and refined road segmentation and centerline maps. We evaluated our method utilizing three data sets covering various road situations in more than 40 cities around the world. The results demonstrate the superior performance of our proposed framework. Specifically, our method's performance exceeded the other methods by 7% and 40% for the connectivity indicator for road surface segmentation and for the completeness indicator for centerline extraction, respectively.",Roads,Image segmentation,Remote sensing,Boosting,Feature extraction,Surface topography,Semantics,Convolutional neural network (CNN),remote sensing images,road extraction,segmentation,,,,,,tracing,,,,
Row_851,"Ji, Wei","Fang, Zhou","Feng, Decai","Ge, Xizhi",,Semantic segmentation of Arctic Sea ice in summer from remote sensing satellite images based on BAU-NET,JOURNAL OF APPLIED REMOTE SENSING,OCT 2022,1,"To effectively solve the accurate identification of gray ice, melt ponds water, floe, brash ice, and thin ice in the melting state of the Arctic Sea ice during summer, we propose adding a batch normalization layer and adaptive moment estimation optimizer of a U-NET (BAU-NET) method for Arctic Sea ice semantic segmentation in summer from remote sensing satellite optical images. The U-NET network structure is optimized to 18 convolution layers, and a batch normalization layer and a nonlinear activation function rectified linear unit are added behind each convolution layer. Then the hyper-parameters of the network structure are adjusted. The cross-entropy loss function based on SOFTMAX and L2 regularization are used in training the model, and the adaptive moment estimation optimizer is used for iterative training until the error convergence. The experimental results show that the accuracy, precision, recall, and F1 score of sea ice extraction results reaches more than 97.26%. Compared with the DeepLabv3 and U-Net methods, the sea ice prediction time efficiency is improved by 90.99s and 1.57s, respectively, and the accuracy is improved by 6.59% and 8.05%, respectively, which indicate that the sea ice prediction time efficiency and accuracy of the BAU-NET method are significantly improved. (c) 2022 Society of Photo-Optical Instrumentation Engineers (SPIE)",deep learning,batch normalization layer and adaptive moment estimation optimizer of U-NET,remote sensing satellite images,arctic sea ice in summer,semantic segmentation,,,,,,,,,,,,,,,,
Row_852,"Zhang, Liangji","Yang, Zaichun","Zhou, Guoxiong","Lu, Chao","Chen, Aibin",MDMASNet: A dual-task interactive semi-supervised remote sensing image segmentation method,SIGNAL PROCESSING,NOV 2023,6,"Remote sensing image (RSIs) segmentation is widely used in urban planning, natural disaster detection and many other fields. Compared with natural scene images, RSIs have higher resolution, complex imaging, and diverse object shapes and sizes, while semantic segmentation methods based on deep learning often require many data labels. In this paper, we propose a semi-supervised RSIs segmentation network with multi-scale deformable threshold feature extraction module and mixed attention (MDMANet). First, a pyramid ensemble structure is used, which incorporates deformable convolution and bole convolution, to extract features of objects with different shapes and sizes and reduce the influence of redundant features. Meanwhile, a mixed attention (MA) is proposed to aggregate long-range contextual relationships and fuse low-level features with high-level features. Second, an FCN-based full convolution discriminator task network is designed to help evaluate the feasibility of unlabeled image prediction results. We performed experimental validation on three datasets, and the results show that MDMANet segmentation provides more significant improvement in accuracy and better generalization than existing segmentation networks. & COPY; 2023 Published by Elsevier B.V.",Semi-supervised learning,GAN,Attention mechanism,Semantic segmentation,,,,,,,,"Ding, Yao","Wang, Yanfeng","Li, Liujun","Cai, Weiwei",,,,,,
Row_853,"Zhang, Yunfeng","Chi, Mingmin",,,,Mask-R-FCN: A Deep Fusion Network for Semantic Segmentation,IEEE ACCESS,2020,20,"Remote sensing image classification plays a significant role in urban applications, precision agriculture, water resource management. The task of classification in the field of remote sensing is to map raw images to semantic maps. Typically, fully convolutional network (FCN) is one of the most effective deep neural networks for semantic segmentation. However, small objects in remote sensing images can be easily overlooked and misclassified as the majority label, which is often the background of the image. Although many works have attempted to deal with this problem, making a trade-off between background semantics and edge details is still a problem. This is mainly because they are based on a single neural network model. To deal with this problem, a convolutional deep network with regions (R-CNN), which is highly effective for object detection is leveraged as a complementary component in our work. A learning-based and decision-level strategy is applied to fuse both semantic maps from a semantic model and an object detection model. The proposed network is referred to as Mask-R-FCN. Experimental results on real remote sensing images from the Zurich dataset, Gaofen Image Dataset (GID), and DataFountain2017 show that the proposed network can obtain higher accuracy than single deep neural networks and other machine learning algorithms. The proposed network achieved better average accuracies, which are approximately 2% higher than those of any other single deep neural networks on the Zurich, GID, and DataFoundation2017 datasets.",Deep fusion,deep semantic segmentation,fully convolutional network,object detection,remote sensing,,,,,,,,,,,,,,,,
Row_854,"Zhang, Xiangrong","Weng, Zhenhang","Zhu, Peng","Han, Xiao","Zhu, Jin",ESDINet: Efficient Shallow-Deep Interaction Network for Semantic Segmentation of High-Resolution Aerial Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"Semantic segmentation of high-resolution remote sensing images is essential in many fields. Nevertheless, in practical applications, constrained by limited computational resources and complex network structures, many advanced models on semantic segmentation often fail to show efficient performance, prompting research on lightweight models. For lightweight semantic segmentation models, the two-branch architecture has been shown to work well in speed and performance. However, such two-branch architectures usually do not utilize enough information for shallow structures to efficiently provide richer multiscale information for the two branches. The lightweight modules it uses are difficult to extract the global context information of the features effectively. Compared with the current advanced semantic segmentation models, lightweight models still have some differences in performance. In order to solve these problems, we propose a new lightweight dual-branch architecture efficient shallow-deep interaction network (ESDINet), which can quickly extract low-level spatial and high-level semantic information of images through the detail branch and semantic branch. Specifically, we have constructed an efficient double-branch structure with shallow and deep different interactions to achieve multiscale information interaction. At the same time, we optimize the semantic branch and propose a new linear attention block to effectively improve the global perception of the semantic branch. We performed extensive experiments and the results show that our model achieves a good balance between segmentation accuracy and inference speed. In particular, ESDINet achieves 82.03% mean intersection over union (mIoU) on the Vaihingen test set, while the proposed model achieves an inference speed of 116 frames/s (FPS) for 512 chi 512 inputs on a single NVIDIA GTX 2080Ti GPU.",High-resolution remote sensing image,lightweight,multiscale,semantic segmentation,,,,,,,,"Jiao, Licheng",,,,,,,,,
Row_855,"Chen, Canyu","Zhu, Guobin","Chen, Xiliang",,,Wetland Scene Segmentation of Remote Sensing Images Based on Lie Group Feature and Graph Cut Model,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2025,0,"Given the increasingly severe destruction of wetlands in recent years, research and monitoring for wetland protection are urgently needed. However, wetland monitoring still faces significant challenges such as high manual costs and low image monitoring accuracy, primarily due to the complex composition of wetland terrain and diverse spectral characteristics, especially located in remote suburban areas. With their rich semantic and spectral features, studying wetlands as scenes in remote sensing images and segmenting them effectively can address these challenges. To address the issue of low segmentation accuracy on remote sensing image with GrabCut, this study proposes a remote sensing image segmentation extraction method, named SceneCut, based on feature extraction and multifeature joint graph cuts. The method consists of three parts: feature extraction, feature fusion, and multifeature joint graph cuts. In the first part, image features are calculated using a sliding window approach. In the second part, region covariance calculation and matrix eigenvalue decomposition are used to generate a multichannel feature image from the extracted feature vectors. Finally, in the third part, GrabCut is applied to the multichannel image to perform graph cuts, considering multiple features, and the segmentation result is mapped back to the original image to generate the segmented image. Experimental analysis and field exploration validation of wetland images conducted in this study demonstrate that compared to algorithms that only use RGB features for segmentation or those that do not consider the relationship between multiple features, SceneCut performs well in wetland scene extraction, especially in extracting boundaries with significant blending.",Wetlands,Feature extraction,Image segmentation,Remote sensing,Monitoring,Semantics,Accuracy,Object oriented modeling,Lie groups,Deep learning,Graph cut model,,,,,,lie group,remote sensing,scene segmentation,,wetland
Row_856,"Lian, Renbao","Huang, Liqin",,,,Weakly Supervised Road Segmentation in High-Resolution Remote Sensing Images Using Point Annotations,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,19,"Road segmentation methods based on deep neural networks have achieved great success in recent years, but creating accurate pixel-wise training labels is still a boring and expensive task, especially for large-scale high-resolution remote sensing images (HRSIs). Inspired by the stacked hourglass model for human joints detection, we propose a weakly supervised road segmentation method using point annotations in this article. First, we design a patch-based deep convolutional neural network (DCNN) model for road seeds and background points detection and train the model using point annotations. Then, in the process of road segmentation, the DCNN model detects a series of road and background points that are used to train a Support Vector Machine Classifier (SVC) for classifying each pixel into road or nonroad. According to the local geometry of road and the inaccurate classification of SVC, a multiscale and multidirection Gabor filter (MMGF) is put forward to estimate the road potential. Finally, the active contour model based on local binary fitting energy (LBF-Snake) is introduced to extract the road regions from the inhomogeneous road potential. Qualitative and quantitative comparisons show that our method achieves results close to the fully supervised semantic methods without considering the annotation cost and outperforms them given a fixed budget.",Roads,Image segmentation,Annotations,Semantics,Training,Remote sensing,Estimation,Active contour model,Gabor filter,point annotation,remote sensing images,,,,,,road segmentation,weakly supervised segmentation,,,
Row_857,"Huan, Hai","Liu, Yuan","Xie, Yaqin","Wang, Chao","Xu, Dongdong",MAENet: Multiple Attention Encoder-Decoder Network for Farmland Segmentation of Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2022,14,"With the rapid development of computer vision, semantic segmentation as an important part of the technology has made some achievements in different applications. However, in the farmland segmentation scenario of remote sensing images, the capability of common semantic segmentation methods in restoring the farmland edge and identifying narrow farmland ridges needs to be improved. Therefore, in this letter a semantic segmentation method-multiple attention encoder-decoder network (MAENet)-for farmland segmentation is proposed. The design of a dual-pooling efficient channel attention (DPECA) module and its embedment in the backbone to improve the efficiency of feature extraction is described; secondly, a dual-feature attention (DFA) module is proposed to extract contextual information of high-level features; finally, a global-guidance information upsample (GIU) module is added to the decoder to reduce the influence of redundant information on feature fusion. We use three self-made farmland image datasets representing UAV data to train MAENet and compare them with other methods. The results show that the performances of segmentation and generalization of MAENet are improved compared with other methods. The MIoU and Kappa coefficient in the farmland multi-classification test set can reach 93.74% and 96.74%.",Feature extraction,Image segmentation,Semantics,Data mining,Convolution,Remote sensing,Decoding,Attention module,farmland segmentation,feature fusion,pyramid,"Zhang, Yi",,,,,UAV images,,,,
Row_858,"Li, Shuo","Liu, Fang","Jiao, Licheng","Liu, Xu","Chen, Puhua",Mask-Guided Correlation Learning for Few-Shot Segmentation in Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,4,"Few-shot segmentation aims to segment specific objects in a query image based on a few densely annotated images and has been extensively studied in recent years. In remote sensing, image segmentation faces challenges such as less training data, large intraclass diversity, and low foreground-background contrast. In this work, we propose a novel few-shot segmentation method in remote sensing imagery based on mask-guided correlation learning (MGCL) to alleviate the above challenges. In our MGCL, a novel mask-guided feature enhancement (MGFE) module is proposed, which makes features have intramask consistency by leveraging oversegmented masks. In order to enhance the contrast between foreground and background, a novel foreground-background correlation (FBC) module is proposed, which enhances background correlation representation by learning foreground correlation and background correlation separately. Furthermore, a novel mask-guided correlation decoder (MGCD) module is proposed to guide the decoder to focus on the consistency within the mask, thereby learning how to segment complete objects and improving segmentation accuracy. Sufficient experiments on the iSAID-5(i )and DLRSD-5(i )datasets show that our MGCL outperforms all comparative methods. In particular, in the one-shot setting of the iSAID-5(i )dataset, we achieve an mIoU of 39.92 based on ResNet50, which is an improvement of 4.25 over the state-of-the-art (SOAT) method. The visualization of features before and after the MGFE module further concretely demonstrates the motivation and advantages of our MGCL. The code is available at https://github.com/LiShuo1001/MGCL.",Image segmentation,Correlation,Remote sensing,Feature extraction,Prototypes,Accuracy,Semantics,Correlation learning,few-shot learning,few-shot segmentation,segment anything,"Li, Lingling",,,,,semantic segmentation,,,,
Row_859,Wang Xi,Yu Ming,Ren Hong-e,,,Remote sensing image semantic segmentation combining UNET and FPN,CHINESE JOURNAL OF LIQUID CRYSTALS AND DISPLAYS,MAR 2021,10,"The traditional remote sensing image segmentation method is inefficient and the segmentation fineness is not enough in complex scenes. The UNET model is well-known for its good segmentation effect, but it does not perform well for the smaller objects contained in the image and the edge segmentation of larger objects. In order to solve this problem, a method combining UNET structure with FPN structure is proposed in this paper to improve the ability of UNET model to integrate multi-scale information. At the same time, the BLR loss function which can better capture the edge of the target edge is used to improve the segmentation effect of UNET model on the target boundary. The experimental results show that the method used in this paper effectively improves the accuracy of semantic segmentation and alleviates the problem of poor edge segmentation of small-scale targets and large-scale targets. The target edge segmentation can be more accurate to achieve better segmentation results.",deep learning,UNET,FPN,BLR,,,,,,,,,,,,,,,,,
Row_860,"Sun, Weiwei","Wang, Ruisheng",,,,Fully Convolutional Networks for Semantic Segmentation of Very High Resolution Remotely Sensed Images Combined With DSM,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,MAR 2018,225,"Recently, approaches based on fully convolutional networks (FCN) have achieved state-of-the-art performance in the semantic segmentation of very high resolution (VHR) remotely sensed images. One central issue in this method is the loss of detailed information due to downsampling operations in FCN. To solve this problem, we introduce the maximum fusion strategy that effectively combines semantic information from deep layers and detailed information from shallow layers. Furthermore, this letter develops a powerful backend to enhance the result of FCN by leveraging the digital surface model, which provides height information for VHR images. The proposed semantic segmentation scheme has achieved an overall accuracy of 90.6% on the ISPRS Vaihingen benchmark.",Fully convolutional networks (FCN),deep learning,semantic segmentation,remote sensing,very high resolution (VHR),,,,,,,,,,,,,,,,
Row_861,"Ma, Xianping","Zhang, Xiaokang","Pun, Man-On","Liu, Ming",,MSFNET: MULTI-STAGE FUSION NETWORK FOR SEMANTIC SEGMENTATION OF FINE-RESOLUTION REMOTE SENSING DATA,,2022,3,"This work proposes a Multi-Stage Fusion Network (MSFNet) for semantic segmentation of fine-resolution remote sensing data by exploiting a multi-stage transformer architecture. The proposed MSFNet fuses information of different scales and modalities using a multi-stage scheme based on cross-attention mechanism. More specifically, the proposed MSFNet is composed of two Multi-Level Transformers (ML-Trans), one Crossmodal Fusion Transformer (CFTrans) and one Global-Context Augmented Transformer (GCATrans). DMLTrans and CFTrans are designed to fuse features in different levels in each modality and high-level crossmodal abstract features, respectively, whereas GCATrans enhances the fusion feature of the main modal. Capitalizing on MSFNet, this work demonstrates the fusion of red-green-blue (RGB) remote sensing images and digital surface model (DSM) data. Extensive experiments on large-scale fine-resolution remote sensing data sets, namely the ISPRS Vaihingen, confirm the excellent performance of the proposed architecture as compared to conventional multimodal methods.",,,,,,,,,,,,,,,,,,,,2022 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2022),
Row_862,"Ma, Xianping","Zhang, Xiaokang","Wang, Zhiguo","Pun, Man-On",,Unsupervised Domain Adaptation Augmented by Mutually Boosted Attention for Semantic Segmentation of VHR Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,31,"This work investigates unsupervised domain adaptation (UDA)-based semantic segmentation of very high-resolution (VHR) remote sensing (RS) images from different domains. Most existing UDA methods resort to generative adversarial networks (GANs) to cope with the domain shift problem caused by the discrepancies across different domains. However, these GAN-based UDA methods directly align two domains in the appearance, latent, or output space based on convolutional neural networks (CNNs), making them ineffective in exploiting long-range dependencies across the high-level feature maps derived from different domains. Unfortunately, such high-level features play an essential role in characterizing RS images with complex content. To circumvent this obstacle, a mutually boosted attention transformer (MBATrans) is proposed to capture cross-domain dependencies of semantic feature representations in this work. Compared with conventional UDA methods, MBATrans can significantly reduce domain discrepancies by capturing transferable features using global attention. More specifically, MBATrans utilizes a novel mutually boosted attention (MBA) module to align cross-domain feature maps while enhancing domain-general features. Furthermore, a novel GAN-based network with improved discriminative capability is devised by integrating an additional discriminator to learn domain-specific features. Extensive experiments on two large-scale VHR RS datasets, namely, International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Vaihingen, confirm the superior performance of the proposed MBATrans-augmented GAN (MBATA-GAN) architecture. The source code in this work is available at https://github.com/sstary/SSRS.",Generative adversarial networks (GANs),mutually boosted attention (MBA),remote sensing (RS) image,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,
Row_863,"Guo, Rui","Liu, Jianbo","Li, Na","Liu, Shibin","Chen, Fu",Pixel-Wise Classification Method for High Resolution Remote Sensing Imagery Using Deep Neural Networks,ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION,MAR 2018,34,"Considering the classification of high spatial resolution remote sensing imagery, this paper presents a novel classification method for such imagery using deep neural networks. Deep learning methods, such as a fully convolutional network (FCN) model, achieve state-of-the-art performance in natural image semantic segmentation when provided with large-scale datasets and respective labels. To use data efficiently in the training stage, we first pre-segment training images and their labels into small patches as supplements of training data using graph-based segmentation and the selective search method. Subsequently, FCN with atrous convolution is used to perform pixel-wise classification. In the testing stage, post-processing with fully connected conditional random fields (CRFs) is used to refine results. Extensive experiments based on the Vaihingen dataset demonstrate that our method performs better than the reference state-of-the-art networks when applied to high-resolution remote sensing imagery classification.",high resolution imagery,remote sensing,convolution neural network,semantic segmentation,data augmentation,deep learning,,,,,,"Cheng, Bo","Duan, Jianbo","Li, Xinpeng","Ma, Caihong",,,,,,
Row_864,"He, Shuang","Lu, Xia","Gu, Jason","Tang, Haitong","Yu, Qin",RSI-Net: Two-Stream Deep Neural Network for Remote Sensing Images-Based Semantic Segmentation,IEEE ACCESS,2022,16,"For semantic segmentation of remote sensing images (RSI), trade-off between representation power and location accuracy is quite important. How to get the trade-off effectively is an open question, where current approaches of utilizing very deep models result in complex models with large memory consumption. In contrast to previous work that utilizes dilated convolutions or deep models, we propose a novel two-stream deep neural network for semantic segmentation of RSI (RSI-Net) to obtain improved performance through modeling and propagating spatial contextual structure effectively and a decoding scheme with image-level and graph-level combination. The first component explicitly models correlations between adjacent land covers and conduct flexible convolution on arbitrarily irregular image regions by using graph convolutional network, while densely connected atrous convolution network (DenseAtrousCNet) with multi-scale atrous convolution can expand the receptive fields and obtain image global information. Extensive experiments are implemented on the Vaihingen, Potsdam and Gaofen RSI datasets, where the comparison results demonstrate the superior performance of RSI-Net in terms of overall accuracy (91.83%, 93.31% and 93.67% on three datasets, respectively), F1 score (90.3%, 91.49% and 89.35% on three datasets, respectively) and kappa coefficient (89.46%, 90.46% and 90.37% on three datasets, respectively) when compared with six state-of-the-art RSI semantic segmentation methods.",Semantics,Image segmentation,Feature extraction,Decoding,Convolutional neural networks,Neural networks,Streaming media,Convolutional neural network (CNN),graph convolutional network (GCN),encoder,decoder,"Liu, Kaiyue","Ding, Haozhou","Chang, Chunqi","Wang, Nizhuan",,feature fusion,semantic segmentation,,,
Row_865,"Zhang, Tong","Zhuang, Yin","Wang, Guanqun","Dong, Shan","Chen, He",Multiscale Semantic Fusion-Guided Fractal Convolutional Object Detection Network for Optical Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,44,"Optical remote sensing object detection is a challenging task, because of the complex background interference, ambiguous appearances of tiny objects, densely arranged circumstances, and multiclass object with vaster scale variances and irregular aspect ratios. The performance of object detection is seriously restricted. Thus, in this article, inspired by the anchor-free object detection framework, and aiming to solve these difficulties to improve the optical remote sensing object detection performance, a powerful one-stage detector of multiscale semantic fusion-guided fractal convolution network (MSFC-Net) is proposed. First, facing these strong-coupled semantic relations in each complex scene, a compound semantic feature fusion (CSFF) way is designed for generating an effective semantic description, which is a benefit to pixel-wise object center point interpretation. In addition, it can be easily extended into a semantic segmentation task. Second, in view of accurate multiclass pixel-wise center point predictions based on an effective compound semantic description, a novel fractal convolution (FC) regression layer is designed, which adaptively achieves the regression of multiscale bounding boxes (bboxes) with irregular aspect ratio under no priori information. Third, related to the set up FC regression layer, a specific hybrid loss is designed to make the proposed MSFC-Net converge better. Finally, the extensive experiments on challenge data sets of large-scale dataset for object detection in aerial images (DOTA) and object detection in optical remote sensing images (DIOR) datasets are carried out, and comparisons indicate that the proposed MSFC-Net can perform the remarkable performance than other state-of-the-art one-stage detectors, as it can reach 80.26% mean average precision (mAP) and 79.33% mF1 on DOTA and 70.08% mAP and 73.45% mF1 on DIOR. Then, our work is available at https://github.com/ZhAnGToNG1/MSFC-Net.",Object detection,Detectors,Semantics,Remote sensing,Optical imaging,Optical detectors,Feature extraction,Anchor-free,compound semantic description,fractal convolution (FC),multiscale feature fusion,"Li, Lianlin",,,,,object detection,optical remote sensing,,,
Row_866,"Li, Xuemei","Wang, Xing","Ye, Huping","Qiu, Shi","Liao, Xiaohan",Multinetwork Algorithm for Coastal Line Segmentation in Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"The demarcation between the sea and the land, commonly referred to as the coastline, is of paramount importance for the dynamic monitoring of its alterations. This monitoring is essential for the effective utilization of marine resources and the conservation of the ecological environment. Addressing the challenges posed by the extensive expanse of coastal lines, which can complicate their acquisition and processing, this study utilizes remote sensing imagery to introduce an algorithm for coastal line segmentation. The algorithm integrates multiple networks to enhance its effectiveness. Innovations encompass the development of an extraction algorithm for coastal lines that are as follows. First, utilize an attention-guided conditional generative adversarial network (AC-GAN) model, which redefines the task of image segmentation by framing it as a style transformation problem. Second, a strategy for coastal line segmentation utilizes Dense Swin Transformer Unet (DSTUnet) to construct a densely structured model. This approach integrates Transformer to prioritize focal regions, thereby enhancing image and semantic interpretation. Third, a transfer learning framework is proposed to integrate multiple features, leveraging the strengths of different networks to achieve accurate segmentation of coastal lines. The study introduced two datasets, and the experimental results confirm that parallel network configurations and asymmetric weighting are superior in achieving optimal results, with an area overlap measure (AOM) score of 85%, outperforming the Unet by 5%.",Sea measurements,Image segmentation,Feature extraction,Remote sensing,Transformers,Training,Generators,Coastal line,generative adversarial network (GAN),remote sensing,segmentation,,,,,,transformer,Unet,,,
Row_867,"Ding, Lei","Guo, Haitao","Liu, Sicong","Mou, Lichao","Zhang, Jing",Bi-Temporal Semantic Reasoning for the Semantic Change Detection in HR Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,89,"Semantic change detection (SCD) extends the multiclass change detection (MCD) task to provide not only the change locations but also the detailed land-cover/land-use (LCLU) categories before and after the observation intervals. This fine-grained semantic change information is very useful in many applications. Recent studies indicate that the SCD can be modeled through a triple-branch convolutional neural network (CNN), which contains two temporal branches and a change branch. However, in this architecture, the communications between the temporal branches and the change branch are insufficient. To overcome the limitations in existing methods, we propose a novel CNN architecture for the SCD, where the semantic temporal features are merged in a deep CD unit. Furthermore, we elaborate on this architecture to reason the bi-temporal semantic correlations. The resulting bi-temporal semantic reasoning network (Bi-SRNet) contains two types of semantic reasoning blocks to reason both single-temporal and cross-temporal semantic correlations, as well as a novel loss function to improve the semantic consistency of change detection results. Experimental results on a benchmark dataset show that the proposed architecture obtains significant accuracy improvements over the existing approaches, while the added designs in the Bi-SRNet further improve the segmentation of both semantic categories and the changed areas. The codes in this article are accessible at https://github.com/ggsDing/Bi-SRNet.",Semantics,Convolutional neural networks,Task analysis,Feature extraction,Image segmentation,Data models,Correlation,Change detection (CD),convolutional neural network (CNN),remote sensing,semantic change detection (SCD),"Bruzzone, Lorenzo",,,,,semantic segmentation (SS),,,,
Row_868,"Cui, Wei","Feng, Zhanyun","Chen, Jiale","Xu, Xing","Tian, Yueling",Long-Tailed Effect Study in Remote Sensing Semantic Segmentation Based on Graph Kernel Principles,REMOTE SENSING,APR 2024,1,"The performance of semantic segmentation in remote sensing, based on deep learning models, depends on the training data. A commonly encountered issue is the imbalanced long-tailed distribution of data, where the head classes contain the majority of samples while the tail classes have fewer samples. When training with long-tailed data, the head classes dominate the training process, resulting in poorer performance in the tail classes. To address this issue, various strategies have been proposed, such as resampling, reweighting, and transfer learning. However, common resampling methods suffer from overfitting to the tail classes while underfitting the head classes, and reweighting methods are limited in the extreme imbalanced case. Additionally, transfer learning tends to transfer patterns learned from the head classes to the tail classes without rigorously validating its generalizability. These methods often lack additional information to assist in the recognition of tail class objects, thus limiting performance improvements and constraining generalization ability. To tackle the abovementioned issues, a graph neural network based on the graph kernel principle is proposed for the first time. By leveraging the graph kernel, structural information for tail class objects is obtained, serving as additional contextual information beyond basic visual features. This method partially compensates for the imbalance between tail and head class object information without compromising the recognition accuracy of head classes objects. The experimental results demonstrate that this study effectively addresses the poor recognition performance of small and rare targets, partially alleviates the issue of spectral confusion, and enhances the model's generalization ability.",graph neural network,long-tailed distribution,graph kernel,remote sensing,,,,,,,,"Zhao, Huilin","Wang, Chenglei",,,,,,,,
Row_869,"Zang, Ning","Cao, Yun","Wang, Yuebin","Huang, Bo","Zhang, Liqiang",Land-Use Mapping for High-Spatial Resolution Remote Sensing Image Via Deep Learning: A Review,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,39,"Land-use mapping (LUM) using high-spatial resolution remote sensing images (HSR-RSIs) is a challenging and crucial technology. However, due to the characteristics of HSR-RSIs, such as different image acquisition conditions and massive, detailed information, and performing LUM faces unique scientific challenges. With the emergence of new deep learning (DL) algorithms in recent years, methods to LUM with DL have achieved huge breakthroughs, which offer novel opportunities for the development of LUM for HSR-RSIs. This article aims to provide a thorough review of recent achievements in this field. Existing high spatial resolution datasets in the research of semantic segmentation and single-object segmentation are presented first. Next, we introduce several basic DL approaches that are frequently adopted for LUM. After reviewing DL-based LUM methods comprehensively, which highlights the contributions of researchers in the field of LUM for HSR-RSIs, we summarize these DL-based approaches based on two LUM criteria. Individually, the first one has supervised learning, semisupervised learning, or unsupervised learning, while another one is pixel-based or object-based. We then briefly review the fundamentals and the developments of the development of semantic segmentation and single-object segmentation. At last, quantitative results that experiment on the dataset of ISPRS Vaihingen and ISPRS Potsdam are given for several representative models such as fully convolutional network (FCN) and U-Net, following up with a comparison and discussion of the results.",Semantics,Meters,Buildings,Satellites,Remote sensing,Image segmentation,Spatial resolution,Deep learning (DL),high-spatial resolution remote sensing images (HSR-RSIs),land-use mapping (LUM),semantic segmentation,"Mathiopoulos, P. Takis",,,,,,,,,
Row_870,"Lu, Tingyu","Gao, Meixiang","Wang, Lei",,,Crop classification in high-resolution remote sensing images based on multi-scale feature fusion semantic segmentation model,FRONTIERS IN PLANT SCIENCE,AUG 1 2023,7,"The great success of deep learning in the field of computer vision provides a development opportunity for intelligent information extraction of remote sensing images. In the field of agriculture, a large number of deep convolutional neural networks have been applied to crop spatial distribution recognition. In this paper, crop mapping is defined as a semantic segmentation problem, and a multi-scale feature fusion semantic segmentation model MSSNet is proposed for crop recognition, aiming at the key problem that multi-scale neural networks can learn multiple features under different sensitivity fields to improve classification accuracy and fine-grained image classification. Firstly, the network uses multi-branch asymmetric convolution and dilated convolution. Each branch concatenates conventional convolution with convolution nuclei of different sizes with dilated convolution with different expansion coefficients. Then, the features extracted from each branch are spliced to achieve multi-scale feature fusion. Finally, a skip connection is used to combine low-level features from the shallow network with abstract features from the deep network to further enrich the semantic information. In the experiment of crop classification using Sentinel-2 remote sensing image, it was found that the method made full use of spectral and spatial characteristics of crop, achieved good recognition effect. The output crop classification mapping was better in plot segmentation and edge characterization of ground objects. This study can provide a good reference for high-precision crop mapping and field plot extraction, and at the same time, avoid excessive data acquisition and processing.",remote sensing,crop classification,deep learning,convolutional neural network,multi-scale feature,,,,,,,,,,,,,,,,
Row_871,"Wu, Hao","Dong, Hongli","Wang, Zhibao","Bai, Lu","Huo, Fengcai",SEMANTIC SEGMENTATION OF OIL WELL SITES USING SENTINEL-2 IMAGERY,,2023,1,"The number and geographical location of oil well sites can reflect the local oil production situation and there is a growing interest in automatically identifying oil well sites from remote sensing images. Traditionally, visual interpretation was employed to extract oil well sites locations from remotely sensing images. However, this approach is time-consuming and heavily dependent on domain experts. Advancements in remote sensing satellite technology and the widespread use of deep learning algorithms have enabled the automated extraction of oil well sites from remote sensing images. In this paper, we established the Northeast Petroleum University Oil Well Sites Dataset Version 1.0 (NEPU-OWS V1.0), and to evaluate its usability by comparing several different deep learning models based on semantic segmentation algorithms for optical remote sensing images. Experimental results show that current advanced deep learning models achieve high accuracy on this dataset, demonstrating great potential for remote sensing detection in oil well sites.",Sentinel 2,semantic segmentation,oil well sites,deep learning,,,,,,,,"Tao, Jinhua","Chen, Liangfu",,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,
Row_872,"He, Chen","Liu, Yalan","Wang, Dacheng","Liu, Shufu","Yu, Linjun",Automatic Extraction of Bare Soil Land from High-Resolution Remote Sensing Images Based on Semantic Segmentation with Deep Learning,REMOTE SENSING,MAR 2023,7,"Accurate monitoring of bare soil land (BSL) is an urgent need for environmental governance and optimal utilization of land resources. High-resolution imagery contains rich semantic information, which is beneficial for the recognition of objects on the ground. Simultaneously, it is susceptible to the impact of its background. We propose a semantic segmentation model, Deeplabv3+-M-CBAM, for extracting BSL. First, we replaced the Xception of Deeplabv3+ with MobileNetV2 as the backbone network to reduce the number of parameters. Second, to distinguish BSL from the background, we employed the convolutional block attention module (CBAM) via a combination of channel attention and spatial attention. For model training, we built a BSL dataset based on BJ-2 satellite images. The test result for the F1 of the model was 88.42%. Compared with Deeplabv3+, the classification accuracy improved by 8.52%, and the segmentation speed was 2.34 times faster. In addition, compared with the visual interpretation, the extraction speed improved by 11.5 times. In order to verify the transferable performance of the model, Jilin-1GXA images were used for the transfer test, and the extraction accuracies for F1, IoU, recall and precision were 86.07%, 87.88%, 87.00% and 95.80%, respectively. All of these experiments show that Deeplabv3+-M-CBAM achieved efficient and accurate extraction results and a well transferable performance for BSL. The methodology proposed in this study exhibits its application value for the refinement of environmental governance and the surveillance of land use.",bare soil land,high-resolution remote sensing imagery,semantic segmentation,deep learning,Deeplabv3+,CBAM,,,,,,"Ren, Yuhuan",,,,,,,,,
Row_873,"Hasan, Kazi Rakib","Tuli, Anamika Biswas","Khan, Md Al-Masrur","Kee, Seong-Hoon","Samad, Md Abdus",Deep-Learning-Based Semantic Segmentation for Remote Sensing: A Bibliometric Literature Review,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,3,"Deep learning (DL) has emerged as a powerful technique for a wide range of computer vision applications. Consequently, DL is also being adopted to process geospatial and remote sensing (RS) images. As these methods are sporadic over different studies, many review papers have also been published to gather the approaches and summarize the existing models in this field. However, a state-of-the-art review paper is still scarce in this field that will present a bibliometric analysis as well as a critical analysis of the recent works. Therefore, this article aims to spur the researchers with a bibliometric analysis to identify the current research trend. As a research sample, in total, 281 related papers were collected from the Web of Science source, and bibliometric analysis was accomplished using VOSviewer software. Among the collection of associated works from the database, 28 papers were selected according to the defined criteria for detailed analysis. Besides this, a few research questions were generated to extract necessary information from the literature for extracting the pros and cons of the selected works. DL techniques were applied in these works and achieved results. Furthermore, the papers were also categorized based on the addressed RS application domain.",Bibliometric analysis,deep learning,remote sensing (RS),segmentation,VOSviewer,,,,,,,"Nahid, Abdullah-Al",,,,,,,,,
Row_874,"Zhao, Junqi","Du, Dongsheng","Chen, Lifu","Liang, Xiujuan","Chen, Haoda",HA-Net for Bare Soil Extraction Using Optical Remote Sensing Images,REMOTE SENSING,AUG 2024,0,"Bare soil will cause soil erosion and contribute to air pollution through the generation of dust, making the timely and effective monitoring of bare soil an urgent requirement for environmental management. Although there have been some researches on bare soil extraction using high-resolution remote sensing images, great challenges still need to be solved, such as complex background interference and small-scale problems. In this regard, the Hybrid Attention Network (HA-Net) is proposed for automatic extraction of bare soil from high-resolution remote sensing images, which includes the encoder and the decoder. In the encoder, HA-Net initially utilizes BoTNet for primary feature extraction, producing four-level features. The extracted highest-level features are then input into the constructed Spatial Information Perception Module (SIPM) and the Channel Information Enhancement Module (CIEM) to emphasize the spatial and channel dimensions of bare soil information adequately. To improve the detection rate of small-scale bare soil areas, during the decoding stage, the Semantic Restructuring-based Upsampling Module (SRUM) is proposed, which utilizes the semantic information from input features and compensate for the loss of detailed information during downsampling in the encoder. An experiment is performed based on high-resolution remote sensing images from the China-Brazil Resources Satellite 04A. The results show that HA-Net obviously outperforms several excellent semantic segmentation networks in bare soil extraction. The average precision and IoU of HA-Net in two scenes can reach 90.9% and 80.9%, respectively, which demonstrates the excellent performance of HA-Net. It embodies the powerful ability of HA-Net for suppressing the interference from complex backgrounds and solving multiscale issues. Furthermore, it may also be used to perform excellent segmentation tasks for other targets from remote sensing images.",deep learning,semantic segmentation,remote sensing images,bare soil extraction,attention mechanism,,,,,,,"Jin, Yuchen",,,,,,,,,
Row_875,"Li, Zhenghong","Chen, Hao","Jing, Ning","Li, Jun",,RemainNet: Explore Road Extraction from Remote Sensing Image Using Mask Image Modeling,REMOTE SENSING,SEP 2023,10,"Road extraction from a remote sensing image is a research hotspot due to its broad range of applications. Despite recent advancements, achieving precise road extraction remains challenging. Since a road is thin and long, roadside objects and shadows cause occlusions, thus influencing the distinguishment of the road. Masked image modeling reconstructs masked areas from unmasked areas, which is similar to the process of inferring occluded roads from nonoccluded areas. Therefore, we believe that mask image modeling is beneficial for indicating occluded areas from other areas, thus alleviating the occlusion issue in remote sensing image road extraction. In this paper, we propose a remote sensing image road extraction network named RemainNet, which is based on mask image modeling. RemainNet consists of a backbone, image prediction module, and semantic prediction module. An image prediction module reconstructs a masked area RGB value from unmasked areas. Apart from reconstructing original remote sensing images, a semantic prediction module of RemainNet also extracts roads from masked images. Extensive experiments are carried out on the Massachusetts Roads dataset and DeepGlobe Road Extraction dataset; the proposed RemainNet improves 0.82-1.70% IoU compared with other state-of-the-art road extraction methods.",remote sensing,road extraction,semantic segmentation,masked image modeling,,,,,,,,,,,,,,,,,
Row_876,"Lin, Rui","Zhang, Ying","Zhu, Xue","Chen, Xueyun",,Local-Global Feature Capture and Boundary Information Refinement Swin Transformer Segmentor for Remote Sensing Images,IEEE ACCESS,2024,3,"Semantic segmentation of urban remote sensing images is a highly challenging task. Due to the complex background, occlusion overlap, and small-scale targets in urban remote sensing images, the semantic segmentation results suffer from deficiencies such as similar target confusion, blurred target boundaries, and small-scale target omission. To solve the above problems, a local-global feature capture and boundary information refinement Swin Transformer segmentor (LGBSwin) is proposed. First, the dual linear attention module (DLAM) utilizes spatial linear attention and channel linear attention mechanisms for strengthening global modeling capabilities to improve the segmentation ability of similar targets. Second, boundary-aware enhancement (BAE) adaptively mines the boundary semantic information through the effective integration of high-level and low-level features to alleviate blurred boundaries. Finally, feature refinement aggregation (FRA) establishes information relationships between different layers, reduces the loss of local information, and enhances local-global dependence, thus significantly improving the recognition ability of small targets. Experimental results demonstrate the effectiveness of LGBSwin, with an F1 of 91.02% on the ISPRS Vaihingen dataset and 93.35% on the ISPRS Potsdam dataset.",Boundary information,dual linear attention,feature capture,remote sensing images,semantic segmentation,,,,,,,,,,,,,,,,
Row_877,"Ismael, Sarmad F.","Kayabol, Koray","Aptoula, Erchan",,,Unsupervised Domain Adaptation for the Semantic Segmentation of Remote Sensing Images via One-Shot Image-to-Image Translation,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,7,"Domain adaptation is one of the prominent strategies for handling both the scarcity of pixel-level ground truth and the domain shift, that is widely encountered in large-scale land use/cover map calculation. Studies focusing on adversarial domain adaptation via re-styling source domain samples, commonly through generative adversarial networks (GANs), have reported varying levels of success, yet they suffer from semantic inconsistencies, visual corruptions, and often require a large number of target domain samples. In this letter, we propose a new lightweight unsupervised domain adaptation (UDA) method for the semantic segmentation of very high-resolution remote sensing images, based on an image-to-image translation (I2IT) approach, via an encoder-decoder strategy where latent content representations are mixed across domains, and a perceptual network module and loss function enforce visual semantic consistency. We show through cross-domain comparative experiments that it: 1) leads to semantically consistent images; 2) can operate with a single target domain sample (i.e., one-shot); and 3) at a fraction of the number of parameters required from the state-of-the-art methods, while still outperforming them. Code is available at github.com/Sarmadfismael/RSOS_I2I.",Image translation,one-shot learning,semantic segmentation,unsupervised domain adaptation (UDA),,,,,,,,,,,,,,,,,
Row_878,"Geng, Xinzhe","Lei, Tao","Chen, Qi","Su, Jian","He, Xi",GLOBAL EVOLUTION NEURAL NETWORK FOR SEGMENTATION OF REMOTE SENSING IMAGES,,2022,0,"The popular convolutional neural networks (CNNs) have been successfully used in very high-resolution remote sensing image semantic segmentation. However, these networks often suffer from performance limitations. First, although deeper networks usually provide better feature representation, they may cause parameter redundancy and the inefficient use of prior knowledge. Secondly, attention-based networks often only focus on weighting different features of a single sample but ignore the correlation of all samples in training set, thus leading to the loss of global information. To address above issues, we propose two simple yet effective global evolution strategies. The first is knowledge enhancement. This strategy can reactivate invalid convolutional kernels through convergence of different models and make full use of prior knowledge from the network to improve its feature representation. The second is a dict-attention module that greatly enhances the generalization of networks by learning and inferring the global relationship among different samples through the dictionary unit. As a result, a novel global evolution network (GENet) is designed based on knowledge enhancement and dict-attention for remote sensing image semantic segmentation. Experiments demonstrate that the proposed GENet is not only superior to popular networks in segmentation accuracy.",Deep learning,image segmentation,knowledge enhancement,attention mechanism,,,,,,,,"Wang, Qi","Nandi, Asoke K.",,,,,,,"2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP)",
Row_879,"Song, Zhishuang","Zhang, Zhitao","Yang, Shuqin","Ding, Dianyuan","Ning, Jifeng",Identifying sunflower lodging based on image fusion and deep semantic segmentation with UAV remote sensing imaging,COMPUTERS AND ELECTRONICS IN AGRICULTURE,DEC 2020,78,"Sunflower lodging is a common agricultural disorder taking place in the middle and late sunflower growth periods. This disorder reduces the sunflower seed yield, damages the seed quality, and hence usually causes great losses in both crop quantity and quality. Sunflower lodging is mainly caused by extreme and destructive weather events, which have been recently occurring more frequently. This is why it is highly crucial to develop methods for fast and accurate identification of sunflower lodging. In this work, an efficient method for sunflower lodging identification is proposed based on image fusion and deep semantic segmentation of remote sensing images obtained from an unmanned aerial vehicle (UAV). First, the resolution of low-resolution multispectral images was enhanced through matching their features with those of high-resolution visible-range images. Then, for effective lodging assessment, high-quality multispectral images with rich spectral information and high spatial resolution were obtained through fusing the visible-range images and the enhanced multispectral ones. Subsequently, in order to refine the identification outcomes, a variant of the segmentation network (SegNet) deep architecture was developed for semantic segmentation. This variant has skip connections, separable convolution, and a conditional random field. Experimental evaluation shows that the fusion-based approaches clearly outperform the no-fusion ones in terms of the lodging identification accuracy for all compared architectures including support vector machine (SVM), fully convolutional network (FCN), SegNet, and the proposed SegNet variant. Meanwhile, the deep semantic segmentation methods consistently outperform the classical SVM one with hand-crafted features. As well, the improved SegNet method outperformed all of the compared methods and achieved the best accuracies of 84.4% and 89.8% without and with image fusion, respectively, on one test. The corresponding accuracies on another test set were 76.6% and 83.3%, respectively. Moreover, the proposed method can also identify the sunflower lodging and non-lodging patterns and separate them from the background. These capabilities are highly beneficial for lodging hazard assessment and sunflower harvest survey. Overall, the proposed method effectively exploited UAV remote sensing image data with fusion and deep semantic segmentation modules in order to provide a useful reference for sunflower lodging assessment and mapping.",Sunflower lodging identification,Image fusion,Deep learning,Unmanned aerial vehicle remote sensing image,,,,,,,,,,,,,,,,,
Row_880,"Kemker, Ronald","Salvaggio, Carl","Kanan, Christopher",,,Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,NOV 2018,372,"Deep convolutional neural networks (DCNNs) have been used to achieve state-of-the-art performance on many computer vision tasks (e.g., object recognition, object detection, semantic segmentation) thanks to a large repository of annotated image data. Large labeled datasets for other sensor modalities, e.g., multispectral imagery (MSI), are not available due to the large cost and manpower required. In this paper, we adapt state-of-the-art DCNN frameworks in computer vision for semantic segmentation for MSI imagery. To overcome label scarcity for MSI data, we substitute real MSI for generated synthetic MSI in order to initialize a DCNN framework. We evaluate our network initialization scheme on the new RIT-18 dataset that we present in this paper. This dataset contains very-high resolution MSI collected by an unmanned aircraft system. The models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work.",Deep learning,Convolutional neural network,Semantic segmentation,Multispectral,Unmanned aerial system,Synthetic imagery,,,,,,,,,,,,,,,
Row_881,"Audebert, Nicolas","Le Saux, Bertrand","Lefevre, Sebastien",,,HOW USEFUL IS REGION-BASED CLASSIFICATION OF REMOTE SENSING IMAGES IN A DEEP LEARNING FRAMEWORK ?,,2016,27,"In this paper, we investigate the impact of segmentation algorithms as a preprocessing step for classification of remote sensing images in a deep learning framework. Especially, we address the issue of segmenting the image into regions to be classified using pre-trained deep neural networks as feature extractors for an SVM-based classifier. An efficient segmentation as a preprocessing step helps learning by adding a spatially-coherent structure to the data. Therefore, we compare algorithms producing superpixels with more traditional remote sensing segmentation algorithms and measure the variation in terms of classification accuracy. We establish that superpixel algorithms allow for a better classification accuracy as a homogenous and compact segmentation favors better generalization of the training samples.",Remote sensing,Segmentation algorithms,Image classification,Deep learning,Superpixels,,,,,,,,,,,,,,,2016 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),
Row_882,"Ma, Jiabao","Zhou, Wujie","Qian, Xiaohong","Yu, Lu",,Deep-Separation Guided Progressive Reconstruction Network for Semantic Segmentation of Remote Sensing Images,REMOTE SENSING,NOV 2022,5,"The success of deep learning and the segmentation of remote sensing images (RSIs) has improved semantic segmentation in recent years. However, existing RSI segmentation methods have two inherent problems: (1) detecting objects of various scales in RSIs of complex scenes is challenging, and (2) feature reconstruction for accurate segmentation is difficult. To solve these problems, we propose a deep-separation-guided progressive reconstruction network that achieves accurate RSI segmentation. First, we design a decoder comprising progressive reconstruction blocks capturing detailed features at various resolutions through multi-scale features obtained from various receptive fields to preserve accuracy during reconstruction. Subsequently, we propose a deep separation module that distinguishes various classes based on semantic features to use deep features to detect objects of different scales. Moreover, adjacent middle features are complemented during decoding to improve the segmentation performance. Extensive experimental results on two optical RSI datasets show that the proposed network outperforms 11 state-of-the-art methods.",digital surface model,multimodal,multi-scale supervision,feature separation,reconstruction refinement,,,,,,,,,,,,,,,,
Row_883,"Wu, Tong","Hu, Yuan","Peng, Ling","Chen, Ruonan",,Improved Anchor-Free Instance Segmentation for Building Extraction from High-Resolution Remote Sensing Images,REMOTE SENSING,SEP 2020,30,"Building extraction from high-resolution remote sensing images plays a vital part in urban planning, safety supervision, geographic databases updates, and some other applications. Several researches are devoted to using convolutional neural network (CNN) to extract buildings from high-resolution satellite/aerial images. There are two major methods, one is the CNN-based semantic segmentation methods, which can not distinguish different objects of the same category and may lead to edge connection. The other one is CNN-based instance segmentation methods, which rely heavily on pre-defined anchors, and result in the highly sensitive, high computation/storage cost and imbalance between positive and negative samples. Therefore, in this paper, we propose an improved anchor-free instance segmentation method based on CenterMask with spatial and channel attention-guided mechanisms and improved effective backbone network for accurate extraction of buildings in high-resolution remote sensing images. Then we analyze the influence of different parameters and network structure on the performance of the model, and compare the performance for building extraction of Mask R-CNN, Mask Scoring R-CNN, CenterMask, and the improved CenterMask in this paper. Experimental results show that our improved CenterMask method can successfully well-balanced performance in terms of speed and accuracy, which achieves state-of-the-art performance at real-time speed.",building extraction,improved anchor-free instance segmentation,high-resolution remote sensing images,deep learning,,,,,,,,,,,,,,,,,
Row_884,"Nie, Jie","Wang, Jingyu","Jing, Niantai","Zuo, Zijie","Chen, Shuguo",Bidirectional Layout-Semantic-Pixel Joint Decoupling and Embedding Network for Remote Sensing Colorization,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,1,"In recent years, there has been a growing demand for the colorization of remote sensing images due to their inherent limitations caused by remote sensors, such as hazy or noisy atmospheric conditions. These factors result in the captured images needing to be clarified. Compared to ordinary images, remote sensing images present unique challenges in color recovery due to their imbalanced spatial distribution of objects. In this article, we propose a novel bidirectional layout-semantic-pixel joint decoupling and embedding network (BDEnet) following the idea of human painting to generate highly saturated color images with strong spatial consistency and object salience. The proposed BDEnet model emulates the process of human painting through a step-by-step approach. It begins by determining the overall tone of a large macroscopic region and progressively refining the local color based on this initial assessment. Specifically, BDEnet incorporates finer-grained semantics and pixel color information into a colored layout that represents a wide range of continuous areas, thereby accomplishing the colorization task. The BDEnet model operates at three scales, namely the layout (macro), semantic (medium), and pixel (micro) scales. It comprises three key modules: the multiscale feature decoupling (MFD) module, the layout-semantic-pixel multigranularity learning (MGL) module, and the semantic-pixel embedding (SPE) module. MFD module effectively reduces redundant noise from the semantic and layout scales by employing scale decoupling. This process ensures the extraction of efficient features essential for MGL. In the MGL module, three branches with different scales are employed to achieve layout division, semantic segmentation, and pixel coloring. To address the issue of insufficient category label guidance in layouts, we propose a novel approach called similar semantic merging (SSM) using a weakly supervised scheme to accomplish layout division. Finally, the SPE module incorporates stable semantic and pixel information into the layout features. This integration results in the generation of color images that exhibit strong spatial consistency, emphasize object salience, and possess high color saturation.",Colorization,decoupling,multiscale,remote sensing,,,,,,,,"Liang, Xinyue",,,,,,,,,
Row_885,"Shen, Qian","Huang, Jiru","Wang, Min","Tao, Shikang","Yang, Rui",Semantic feature-constrained multitask siamese network for building change detection in high-spatial-resolution remote sensing imagery,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,JUL 2022,53,"In the field of remote sensing applications, semantic change detection (SCD) simultaneously identifies changed areas and their change types by jointly conducting bitemporal image classification and change detection. It facilitates change reasoning and provides more application value than binary change detection (BCD), which offers only a binary map of the changed/unchanged areas. In this study, we propose a multitask Siamese network, named the semantic feature-constrained change detection (SFCCD) network, for building change detection in bitemporal high-spatial-resolution (HSR) images. SFCCD conducts feature extraction, semantic segmentation and change detection simultaneously, where change detection and semantic segmentation are the main and auxiliary tasks, respectively. For the segmentation task, ResNet50 is used to conduct image feature extraction, and the extracted semantic features are provided to execute the change detection task via a series of jump connections. For the change detection task, a global channel attention (GCA) module and a multiscale feature fusion (MSFF) module are designed, where high-level features offer training guidance to the low-level feature maps, and multiscale features are fused with multiple convolutions that possess different receptive fields. In bitemporal HSR images with different view angles, high-rise buildings have different directional height displacements, which generally cause serious false alarms for common change detection methods. However, known public building change detection datasets often lack buildings with height displacement. We thus create the Nanjing Dataset (NJDS) and design the aforementioned network structures and modules to target this issue. Experiments for method validation and comparison are conducted on the NJDS and two additional public datasets, i.e., the WHU Building Dataset (WBDS) and Google Dataset (GDS). Ablation experiments on the NJDS show that the joint utilization of the GCA and MSFF modules performs better than several classic modules, including atrous spatial pyramid pooling (ASPP), efficient spatial pyramid (ESP), channel attention block (CAB) and global attention upsampling (GAU) modules, in dealing with building height displacement. Furthermore, SFCCD achieves higher accuracy in terms of the OA, recall, F1-score and mIoU measures than several state-of-the-art change detection methods, including deeply supervised image fusion network (DSIFN), the dual-task constrained deep Siamese convolutional network (DTCDSCN), and multitask U-Net (MTU-Net).",Multitask learning,Height displacement,High-spatial-resolution remote sensing,Semantic segmentation,Change detection,Siamese network,,,,,,"Zhang, Xin",,,,,,,,,
Row_886,"Ji, Xun","Tang, Longbin","Chen, Long","Hao, Li-Ying","Guo, Hui",Toward efficient and lightweight sea-land segmentation for remote sensing images,ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,SEP 2024,0,"Sea-land segmentation is of great significance for autonomous coastline monitoring, which is fundamental research in the remote sensing community. Due to the diverse contents and easily confused sea-land boundaries contained in remote sensing images, it is always challenging to achieve precise sea-land segmentation for complex scenarios. Although existing deep learning -based methods have exhibited promising performance, excessive computational load and insufficient use of hierarchical features remain unresolved. In this paper, we contribute to addressing the problems by developing an efficient and lightweight convolutional neural network (CNN) termed E -Net. On the one hand, the proposed network adopts a novel E -shaped architecture that reforms the conventional U-codec structure to make full use of hierarchical features at different depths, so that the sea-land segmentation effect can be significantly improved without excessive computational load. On the other hand, a contextual aggregation attention mechanism (CA2M) is designed to further facilitate efficient aggregation and transmission of contextual information, so that the fuzzy and irregular sea-land boundaries can be accurately distinguished. Extensive experiments reveal that our approach not only produces superior sea-land segmentation effect but also demonstrates promising computational efficiency. Specifically, the proposed E -Net achieves state-of-the-art sea-land segmentation performance with 92.78% and 93.62% mean Intersection over Union (mIoU) on the SLSD and HRSC2016 datasets, respectively, while the frames per second (FPS) reaches 108.032 with as low as 52.287G floating point operations per second (FLOPs).",Sea-land segmentation,Convolutional neural network (CNN),Remote sensing images,Deep learning,Attention mechanism,,,,,,,,,,,,,,,,
Row_887,"Zhu, Hongchun","Lu, Zhiwei","Zhang, Chao","Yang, Yanrui","Zhu, Guocan",Remote Sensing Classification of Offshore Seaweed Aquaculture Farms on Sample Dataset Amplification and Semantic Segmentation Model,REMOTE SENSING,SEP 2023,1,"Satellite remote sensing provides an effective technical means for the precise extraction of information on aquacultural areas, which is of great significance in realizing the scientific supervision of the aquaculture industry. Existing optical remote sensing methods for the extraction of aquacultural area information mostly focus on the use of image spatial features and research on classification methods of single aquaculture patterns. Accordingly, the comprehensive utilization of a combination of spectral information and deep learning automatic recognition technology in the feature expression and discriminant extraction of aquaculture areas needs to be further explored. In this study, using Sentinel-2 remote sensing images, a method for the accurate extraction of different algae aquaculture zones combined with spectral information and deep learning technology was proposed for the characteristics of small samples, multidimensions, and complex water components in marine aquacultural areas. First, the feature expression ability of the aquaculture area target was enhanced through the calculation of the normalized difference aquaculture water index (NDAWI). Second, on this basis, the improved deep convolution generative adversarial network (DCGAN) algorithm was used to amplify the samples and create the NDAWI dataset. Finally, three semantic segmentation methods (UNet, DeepLabv3, and SegNet) were used to design models for classifying the algal aquaculture zones based on the sample amplified time series dataset and comprehensively compare the accuracy of the model classifications for achieving accurate extraction of different algal aquaculture information within the seawater aquaculture zones. The results show that the improved DCGAN amplification exhibited a better effect than the generative adversarial networks (GANs) and DCGAN under the indexes of structural similarity (SSIM) and peak signal-to-noise ratio (PSNR). The UNet classification model constructed on the basis of the improved DCGAN-amplified NDAWI dataset achieved better classification results (Lvshunkou: OA = 94.56%, kappa = 0.905; Jinzhou: OA = 94.68%, kappa = 0.913). The algorithmic model in this study provides a new method for the fine classification of marine aquaculture area information under small sample conditions.",Sentinel-2,normalized difference aquaculture water index (NDAWI),sample amplification,semantic segmentation,classification of aquaculture seas,,,,,,,"Zhang, Yining","Liu, Haiying",,,,,,,,
Row_888,Zhao Xinwei,Li Haichang,Wang Rui,Zheng Changwen,Shi Song,Semantic Segmentation of Remote Sensing Images via Stepwise-Refined Large-Kernel Deconvolutional Networks,,2019,0,"Deep CNN based semantic segmentation has been developed for several years and many models are proposed. However, most of them are designed for natural scene images such as PASCAL VOC, and cannot perform very well on remote sensing images, in which objects are much smaller and more densely distributed than those in natural scene images. In this paper, we demonstrate the importance of high-resolution feature maps and the problem of large dilated convolutional kernels in semantic segmentation of remote sensing images. Furthermore, we propose a Stepwise-Refined Large-Kernel Deconvolutional Network with a focus on small and densely-distributed objects such as houses and buildings, or long and narrow ones such as roads and rivers. Experiments on a public available ISPRS Vaihingen Challenge Dataset and our self-compiled Fujian Dataset show that our model outperforms the state-of-the-art models in semantic segmentation of remote sensing images.",,,,,,,,,,,,,,,,,,,,"2019 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, AUTOMATION AND CONTROL TECHNOLOGIES (AIACT 2019)",
Row_889,"Lu, Wanzhen","Liang, Longxue","Wu, Xiaosuo","Wang, Xiaoyu","Cai, Jiali",An Adaptive Multiscale Fusion Network Based on Regional Attention for Remote Sensing Images,IEEE ACCESS,2020,4,"With the widespread application of semantic segmentation in remote sensing images with high-resolution, how to improve the accuracy of segmentation becomes a research goal in the remote sensing field. An innovative Fully Convolutional Network (FCN) is proposed based on regional attention for improving the performance of the semantic segmentation framework for remote sensing images. The proposed network follows the encoder-decoder architecture of semantic segmentation and includes the following three strategies to improve segmentation accuracy. The enhanced GCN module is applied to capture the semantic features of remote sensing images. MGFM is proposed to capture different contexts by sampling at different densities. Furthermore, RAM is offered to assign large weights to high-value information in different regions of the feature map. Our method is assessed on two datasets: ISPRS Potsdam dataset and CCF dataset. The results indicate that our model with those strategies outperforms baseline models (DCED50) concerning F1, mean IoU and PA, 10.81%,19.11%, and 11.36% on the Potsdam dataset and 29.26%, 27.64% and 13.57% on the CCF dataset.",Convolution,Feature extraction,Remote sensing,Image segmentation,Semantics,Random access memory,Kernel,Remote sensing,fully convolutional networks,semantic segmentation,encoder-decoder architecture,,,,,,regional attention,Potsdam dataset,CCF dataset,,
Row_890,"Chen, Jing","Xia, Min","Wang, Dehao","Lin, Haifeng",,Double Branch Parallel Network for Segmentation of Buildings and Waters in Remote Sensing Images,REMOTE SENSING,MAR 2023,40,"The segmentation algorithm for buildings and waters is extremely important for the efficient planning and utilization of land resources. The temporal and space range of remote sensing pictures is growing. Due to the generic convolutional neural network's (CNN) insensitivity to the spatial position information in remote sensing images, certain location and edge details can be lost, leading to a low level of segmentation accuracy. This research suggests a double-branch parallel interactive network to address these issues, fully using the interactivity of global information in a Swin Transformer network, and integrating CNN to capture deeper information. Then, by building a cross-scale multi-level fusion module, the model can combine features gathered using convolutional neural networks with features derived using Swin Transformer, successfully extracting the semantic information of spatial information and context. Then, an up-sampling module for multi-scale fusion is suggested. It employs the output high-level feature information to direct the low-level feature information and recover the high-resolution pixel-level features. According to experimental results, the proposed networks maximizes the benefits of the two models and increases the precision of semantic segmentation of buildings and waters.",double branch,CNN,semantic segmentation,buildings and waters,deep learning,,,,,,,,,,,,,,,,
Row_891,"Wang, Sheng","Huang, Xiaohui","Han, Wei","Li, Jun","Zhang, Xiaohan",Lithological mapping of geological remote sensing via adversarial semi-supervised segmentation network,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,DEC 2023,11,"Geological remote sensing interpretation (GRSI), which aims to recognize multiple geological elements based on their characteristics on satellite remote sensing images, is vital in large-scale regional lithological mapping. However, due to the influence of long-term geological movements, the spatial distribution of geological elements (such as lithology, glaciers, and soils) on the image is often complex and highly fragmented. In addition, the characteristics of high inter-class similarity and severe homogenization make the annotation of geological element samples require significant cost and expert knowledge. These lead to insufficient interpretation accuracy and limited annotation samples in GRSI. To alleviate the dependence of labeled samples and promote the performance of GRSI, we propose an adversarial semi-supervised segmentation network with object-context and global-attention (AdvSemi-OCGNet) for the GRSI, which achieves effective segmentation results in the case of limited labeled samples. Under the architecture of adversarial learning, a proposed baseline network OCGNet and a full convolution discriminator (FCD) are integrated to conduct semi-supervised segmentation. OCGNet, as the generator, aims to confuse FCD by predicting probability maps. Then FCD selects trustworthy regions with high-confidence of unlabeled sample to generate pseudo-labels for semi-supervised segmentation of OCGNet. Iterative adversarial learning is employed to simulate the process of expert interpretation of geological elements through continuous discrimination and correction. Finally, a fully connected conditional random field eliminates holes and isolated areas of segmented results caused by misclassification. Two study areas are selected, which include various geological elements such as multiple lithologies, soils, rock glaciers, and surface water. Numerous experiments have revealed the superiority of the proposed model in GRSI with limited annotation samples.",Geological remote sensing,Lithological mapping,Semi-supervised semantic segmentation,Adversarial learning,,,,,,,,"Wang, Lizhe",,,,,,,,,
Row_892,"Liu, Chenfang","Sun, Yuli","Xu, Yanjie","Sun, Zhongzhen","Zhang, Xianghui",A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,1,"With the advent of the era of high-resolution remote sensing, semantic segmentation methods for solving pixel-level classification have been widely studied. Deep learning has significantly advanced deep feature extraction methods, becoming widely employed in remote sensing image analysis. Deep feature fusion methods are able to effectively combine features from different sources. Optical and synthetic aperture radar (SAR) images stand out as primary data sources in remote sensing, offering complementary and consistent information. Fusion of deep semantic features of optical and SAR images can alleviate the limitations of single-source images in application and improve semantic segmentation accuracy. Therefore, this article reviews the research on deep fusion of optical and SAR images in semantic segmentation tasks from four aspects. First, we provide a summary of challenges and research methods pertinent to semantic segmentation of remote sensing images. Then the challenges and urgent needs of deep feature fusion of optical and SAR images are analyzed, and current research is summarized from the perspective of structural design by studying various feature fusion strategies. In addition, the compilation and in-depth analysis of open-source optical and SAR datasets suitable for semantic segmentation are undertaken, serving as fundamental resources for future research endeavors. Finally, the article identifies the major challenges summarized from the literature review in this field, outlining expectations and potential future directions for researchers.",Optical sensors,Remote sensing,Optical imaging,Semantic segmentation,Radar polarimetry,Adaptive optics,Feature extraction,Deep feature fusion,optical images,review,semantic segmentation,"Lei, Lin","Kuang, Gangyao",,,,synthetic aperture radar (SAR) images,,,,
Row_893,"Tan, Xiaowei","Xiao, Zhifeng","Wan, Qiao","Shao, Weiping",,Scale Sensitive Neural Network for Road Segmentation in High-Resolution Remote Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,MAR 2021,34,"Road segmentation in remote sensing images has been widely used in many fields. Semantic segmentation, based on deep learning, has become a hot topic for road segmentation. With the deepening of convolutional neural network (CNN) structures, features in the convolution layer that has more semantic information become more important for road segmentation. However, the spatial resolution of the convolutional layer reduced as the CNN network deepens, which causes the extracted roads to lose some important location information. To solve this problem, this letter proposes a novel end-to-end road segmentation method to effectively utilize the different levels of convolutional layers to enhance the model's ability to precisely perceive road edges and shapes. The model includes an encoder and a decoder. The encoder encodes the image to obtain the features of different levels and scales. The decoder consists of two modules: scale fusion module and scale sensitive module. In the scale fusion module, features in pooling layers of different scales are fused to obtain a fusion feature. In a scale sensitive module, a weight tensor at the end of the network is learned to evaluate the importance of fusion features. This road segmentation network has been experimentally verified using public data sets, which greatly improves the road segmentation accuracy and achieves good performance.",Roads,Image segmentation,Semantics,Convolution,Spatial resolution,Decoding,Tensile stress,Convolutional neural network~(CNN),multiscale feature,remote sensing,road segmentation,,,,,,,,,,
Row_894,"Hou, Jianlong","Guo, Zhi","Feng, Yingchao","Wu, Youming","Diao, Wenhui",SPANet: Spatial Adaptive Convolution Based Content-Aware Network for Aerial Image Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,9,"Semantic segmentation of remote sensing images encounters four significant difficulties: 1) complex backgrounds, 2) large-scale differences, 3) numerous small objects, and 4) extreme foreground-background imbalance. However, the existing generic semantic segmentation models mainly focus on the modeling context information and rarely focus on these four issues. This article presents an enhanced remote sensing image semantic segmentation framework to solve these problems through the hierarchical atrous pyramid (HASP) module and spatial-adaptive convolution-based FPN decoder framework. On the one hand, HASP solved the problem of complex backgrounds and large-scale differences by further enlarging the receptive field of the network through the cascade of atrous convolution with various rates. On the other hand, spatial adaptive convolution is embedded in FPN decoder framework step by step to solve the problems of numerous small objects and extreme foreground-background imbalance. Besides, a boundary-based loss function is constructed to help the network optimize the relevant segmentation results. Extensive experiments over iSAID and ISPRS Vaihingen datasets reflect the superiority of the presented structure to conventional the state-of-the-art semantic segmentation approaches.",Remote sensing,Convolution,Semantic segmentation,Feature extraction,Decoding,Semantics,Deep learning,Attention module,remote sensing,semantic segmentation,spatial adaptive,,,,,,,,,,
Row_895,"Qi, Qingqing","Zhao, Jinghao","Lin, Lu","Zhang, Xiaoqing","Tian, Yajun",Combined multi-level context aggregation and attention mechanism method for photovoltaic panel extraction from high resolution remote sensing images,INTERNATIONAL JOURNAL OF REMOTE SENSING,JUN 2 2024,0,"In the context of global carbon emission reduction, solar photovoltaic (PV) technology is experiencing rapid development. Using high-resolution remote sensing images to accurately obtain PV information over a large region, including location and size, has the advantages of high statistical efficiency and timely data update for the PV energy management. Due to the intra-class diversity of PV panels and the intricate variability in their deployment environments, existing semantic segmentation methods often have problems such as under-segmentation and mis-segmentation. To alleviate these problems, this paper proposes an improved DeepLabv3+ semantic segmentation network to more accurately extract PV panels from high-resolution remote sensing images. With the aim of alleviating under-segmentation, a multi-level context aggregation module is developed. This module can enhance the model's ability to learn the characteristics of PV panels and their surrounding environment by aggregating rich contextual information from multi-scale and semantic levels. To alleviate the problem of mis-segmentation, a hybrid attention module is introduced. This module sequentially and adaptively adjusts the weight distribution in both the channel and spatial dimensions, thus enabling the model to focus more on the feature information and spatial positions of PV objects. Experiments conducted on a self-constructed Beijing PV segmentation dataset show that the method in this paper has advantages of completeness and accuracy in extracting PV panels compared to the baseline model and current mainstream semantic segmentation network. In addition, the results of experiments on extracting PV panels in real region show that our model also has good stability and generalization capability.",Photovoltaic panels,remote sensing images,semantic segmentation,context modelling,attention mechanism,,,,,,,,,,,,,,,,
Row_896,"Chaurasia, Kuldeep","Nandy, Rijul","Pawar, Omkar","Singh, Ravi Ranjan","Ahire, Meghana",Semantic segmentation of high-resolution satellite images using deep learning,EARTH SCIENCE INFORMATICS,DEC 2021,8,"The increasing common use of incidental unrectified satellite images have many applications for mapping of earth for coastal and ocean applications. Hazard assessment and natural resource management can also be done via this process. Remote sensing is being used extensively due to the increase in the number of satellites in space. It is also the future of optimization of GPS systems and the internet. To demonstrate the semantic segmentation process, this study presents proposed solutions along with their evaluation metrics adapted from fully connected neural networks such as UNet and PSPNet. UNet architecture based deep learning model has outperformed PSPNet based architecture with overall Mean-IOU score of 0.51 on the test set in the semantic segmentation. The overall accuracy of the model can further be improved by providing homogeneous features to train the model, balance classes and by incorporating more data set for semantic segmentation. The developed model can be useful to the authorities for smart city planning and landuse mapping.",Deep Learning,Semantic Segmentation,Feature extraction,UNet Architecture,Remote Sensing,,,,,,,,,,,,,,,,
Row_897,"Zhang, Tianjian","Xue, Zhaohui","Su, Hongjun",,,Deformable Transformer and Spectral U-Net for Large-Scale Hyperspectral Image Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"Remote sensing semantic segmentation tasks aim to automatically extract land cover types by accurately classifying each pixel. However, large-scale hyperspectral remote sensing images possess rich spectral information, complex and diverse spatial distributions, significant scale variations, and a wide variety of land cover types with detailed features, which pose significant challenges for segmentation tasks. To overcome these challenges, this study introduces a U-shaped semantic segmentation network that combines global spectral attention and deformable Transformer for segmenting large-scale hyperspectral remote sensing images. First, convolution and global spectral attention are utilized to emphasize features with the richest spectral information, effectively extracting spectral characteristics. Second, deformable self-attention is employed to capture global-local information, addressing the complex scale and distribution of objects. Finally, deformable cross-attention is used to aggregate deep and shallow features, enabling comprehensive semantic information mining. Experiments conducted on a large-scale hyperspectral remote sensing dataset (WHU-OHS) demonstrate that: first, in different cities including Changchun, Shanghai, Guangzhou, and Karamay, DTSU-Net achieved the highest performance in terms of mIoU compared to the baseline methods, reaching 56.19%, 37.89%, 52.90%, and 63.54%, with an average improvement of 7.57% to 34.13%, respectively; second, module ablation experiments confirm the effectiveness of our proposed modules, and deformable Transformer significantly reduces training costs compared to conventional Transformers; third, our approach achieves the highest mIoU of 57.22% across the entire dataset, with a balanced trade-off between accuracy and parameter efficiency, demonstrating an improvement of 1.65% to 56.58% compared to the baseline methods.",Feature extraction,Transformers,Data mining,Hyperspectral imaging,Semantics,Semantic segmentation,Convolution,Sensors,Land surface,Decoding,Deep learning,,,,,,hyperspectral remote sensing,large-scale,semantic segmentation,,transformer
Row_898,"Yu, Xizi","Li, Shuang","Zhang, Yu",,,Incorporating convolutional and transformer architectures to enhance semantic segmentation of fine-resolution urban images,EUROPEAN JOURNAL OF REMOTE SENSING,DEC 31 2024,1,"Though convolutional neural networks (CNN) exhibit promise in image semantic segmentation, they have limitations in capturing global context information, resulting in inaccuracies in segmenting small object features and object boundaries. This study introduces a hybrid network, ICTANet, which incorporate convolutional and Transformer architectures to improve the segmentation performance of fine-resolution remote sensing urban imagery. The ICTANet model is essentially a Transformer-based encoder-decoder structure. The dual-encoder architecture, which combines CNN and Swin Transformer modules, is designed to extract both global and local detail information. The feature information at various stages is collected by the Feature Extraction and Fusion modules (FEF), enabling multi-scale contextual information fusion. In addition, an Auxiliary Boundary Detection (ABD) module is introduced at the end of the decoder to enhance the model's ability to capture object boundary information. Numerous ablation experiments have been conducted to demonstrate the efficacy of various components within the network. The testing results have proven that the proposed model can achieve satisfactory performance on the ISPRS Vaihingen and Potsdam dataset, with overall accuracies of 91.9% and 92.0%, respectively. Simultaneously, the proposed model is also compared to the current state-of-the-art methods, exhibiting competitive performance, particularly in the segmentation of diminutive objects like cars and trees.",Image semantic segmentation,transformers,convolutional neural networks,feature extraction,remote sensing,,,,,,,,,,,,,,,,
Row_899,"Yu, Chih-Chang","Chen, Yuan-Di","Cheng, Hsu-Yung","Jiang, Chi-Lun",,Semantic Segmentation of Satellite Images for Landslide Detection Using Foreground-Aware and Multi-Scale Convolutional Attention Mechanism,SENSORS,OCT 2024,0,"Advancements in satellite and aerial imagery technology have made it easier to obtain high-resolution remote sensing images, leading to widespread research and applications in various fields. Remote sensing image semantic segmentation is a crucial task that provides semantic and localization information for target objects. In addition to the large-scale variation issues common in most semantic segmentation datasets, aerial images present unique challenges, including high background complexity and imbalanced foreground-background ratios. However, general semantic segmentation methods primarily address scale variations in natural scenes and often neglect the specific challenges in remote sensing images, such as inadequate foreground modeling. In this paper, we present a foreground-aware remote sensing semantic segmentation model. The model introduces a multi-scale convolutional attention mechanism and utilizes a feature pyramid network architecture to extract multi-scale features, addressing the multi-scale problem. Additionally, we introduce a Foreground-Scene Relation Module to mitigate false alarms. The model enhances the foreground features by modeling the relationship between the foreground and the scene. In the loss function, a Soft Focal Loss is employed to focus on foreground samples during training, alleviating the foreground-background imbalance issue. Experimental results indicate that our proposed method outperforms current state-of-the-art general semantic segmentation methods and transformer-based methods on the LS dataset benchmark.",remote sensing,semantic segmentation,convolutional attention mechanism,multi-scale features fusion,,,,,,,,,,,,,,,,,
Row_900,"Wang, Qixiong","Yin, Jihao","Jiang, Hongxiang","Feng, Jiaqi","Zhang, Guangyun",Disentangled Foreground-Semantic Adapter Network for Generalized Aerial Image Few-Shot Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Semantic segmentation of remote sensing imagery requires extensive annotated samples for training, facing challenges in adapting to novel classes with few annotations. Few-shot semantic segmentation (FS-Seg) employs a support-to-query paradigm, which encounters many practical constraints. Recently, generalized few-shot semantic segmentation (GFS-Seg) has been proposed to align with general semantic segmentation paradigms, enabling the segmentation of all classes (both base and novel classes) in the image. However, existing GFS-Seg methods struggle with a large intra-class variance of background, degradation on base classes, and overfitting on novel classes during fine-tuning in aerial imagery. To address the above issues, we propose the disentangled foreground-semantic adapter network (DFSA-Net) for generalized aerial image FS-Seg. Specifically, to reduce the interference from background features, DFSA-Net employs a foreground-semantic decoder (FSD) to decompose semantic segmentation into foreground aggregation and multiclass refinement. To mitigate the base classes degradation and novel class overfitting during fine-tuning, we propose disentangled low-rank adapter (DLA) for fine-tuning phase, designed to preserve the base parameters while ensuring efficient adaptation to novel classes. Finally, we introduce an inference ensemble strategy that merges base and novel decoder prediction to achieve final output. Experimental results on NWPU and iSAID datasets demonstrate the superiority of our DFSA-Net over other compared methods.",Semantic segmentation,Decoding,Semantics,Training,Prototypes,Benchmark testing,Interference,Adapter tuning,aerial imagery,deep learning,disentanglement representation,,,,,,few-shot semantic segmentation (FS-Seg),,,,
Row_901,"Zhang, Yonghong","Lu, Huanyu","Ma, Guangyi","Zhao, Huajun","Xie, Donglin",MU-Net: Embedding MixFormer into Unet to Extract Water Bodies from Remote Sensing Images,REMOTE SENSING,JUL 2023,10,"Water bodies extraction is important in water resource utilization and flood prevention and mitigation. Remote sensing images contain rich information, but due to the complex spatial background features and noise interference, problems such as inaccurate tributary extraction and inaccurate segmentation occur when extracting water bodies. Recently, using a convolutional neural network (CNN) to extract water bodies is gradually becoming popular. However, the local property of CNN limits the extraction of global information, while Transformer, using a self-attention mechanism, has great potential in modeling global information. This paper proposes the MU-Net, a hybrid MixFormer architecture, as a novel method for automatically extracting water bodies. First, the MixFormer block is embedded into Unet. The combination of CNN and MixFormer is used to model the local spatial detail information and global contextual information of the image to improve the ability of the network to capture semantic features of the water body. Then, the features generated by the encoder are refined by the attention mechanism module to suppress the interference of image background noise and non-water body features, which further improves the accuracy of water body extraction. The experiments show that our method has higher segmentation accuracy and robust performance compared with the mainstream CNN- and Transformer-based semantic segmentation networks. The proposed MU-Net achieves 90.25% and 76.52% IoU on the GID and LoveDA datasets, respectively. The experimental results also validate the potential of MixFormer in water extraction studies.",attention mechanism,convolutional neural network,MixFormer,remote sensing,semantic segmentation,Transformer,,,,,,"Geng, Sutong","Tian, Wei","Sian, Kenny Thiam Choy Lim Kam",,,,,,,
Row_902,"He, Mingyuan","Zhang, Jie","He, Yang","Zuo, Xinjie","Gao, Zebin",Annotated Dataset for Training Cloud Segmentation Neural Networks Using High-Resolution Satellite Remote Sensing Imagery,REMOTE SENSING,OCT 2024,1,"The integration of satellite data with deep learning has revolutionized various tasks in remote sensing, including classification, object detection, and semantic segmentation. Cloud segmentation in high-resolution satellite imagery is a critical application within this domain, yet progress in developing advanced algorithms for this task has been hindered by the scarcity of specialized datasets and annotation tools. This study addresses this challenge by introducing CloudLabel, a semi-automatic annotation technique leveraging region growing and morphological algorithms including flood fill, connected components, and guided filter. CloudLabel v1.0 streamlines the annotation process for high-resolution satellite images, thereby addressing the limitations of existing annotation platforms which are not specifically adapted to cloud segmentation, and enabling the efficient creation of high-quality cloud segmentation datasets. Notably, we have curated the Annotated Dataset for Training Cloud Segmentation (ADTCS) comprising 32,065 images (512 x 512) for cloud segmentation based on CloudLabel. The ADTCS dataset facilitates algorithmic advancement in cloud segmentation, characterized by uniform cloud coverage distribution and high image entropy (mainly 5-7). These features enable deep learning models to capture comprehensive cloud characteristics, enhancing recognition accuracy and reducing ground object misclassification. This contribution significantly advances remote sensing applications and cloud segmentation algorithms.",high-resolution remote sensing images,cloud segmentation,annotated dataset,CloudLabel annotation technique,ground truth label data,,,,,,,,,,,,,,,,
Row_903,"Han, Jing","Wang, Weiyu","Lin, Yuqi","Lyu, Xueqiang",,MRU-Net: A remote sensing image segmentation network for enhanced edge contour Detection,KSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS,DEC 31 2023,0,"Remote sensing image segmentation plays an important role in realizing intelligent city construction. The current mainstream segmentation networks effectively improve the segmentation effect of remote sensing images by deeply mining the rich texture and semantic features of images. But there are still some problems such as rough results of small target region segmentation and poor edge contour segmentation. To overcome these three challenges, we propose an improved semantic segmentation model, referred to as MRU-Net, which adopts the U-Net architecture as its backbone. Firstly, the convolutional layer is replaced by BasicBlock structure in U-Net network to extract features, then the activation function is replaced to reduce the computational load of model in the network. Secondly, a hybrid multi scale recognition module is added in the encoder to improve the accuracy of image segmentation of small targets and edge parts. Finally, test on Massachusetts Buildings Dataset and WHU Dataset the experimental results show that compared with the original network the ACC,mIoU and F1 value are improved, and the imposed network shows good robustness and portability in different datasets.",Convolutional neural network,Image processing,Hybrid multiscale identification module,Micro residual structure,Remote sensing image segmentation,,,,,,,,,,,,,,,,
Row_904,"Zhu, Yuqian","Chen, Weitao","He, Wenxi","Wang, Ruizhen","Li, Xianju",CUG_MISDataset: A Remote Sensing Instance Segmentation Dataset for Improved Wide-Area High-Precision Mining Land Occupation Recognition,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,0,"The effective and rapid acquisition of wide-area mine occupation information is crucial for ecological geo-environmental protection and sustainable development. Remote sensing instance segmentation technology based on deep learning is a promising solution. However, there are two significant challenges including insufficient training datasets and unsuitable segmentation models. To overcome these issues, this study provides a large-scale remote sensing instance segmentation dataset for mining land occupation (CUG_MISDataset). The CUG_MISDataset comprises 1426 image blocks and more than 3000 instances, covering all 150 types of mines found in China's Hubei province. It features multiple mine types, various land occupations, and complex instance scales. First, this study compares the performance of seven mainstream remote sensing instance segmentation models using the proposed CUG_MISDataset. The results show that all seven models achieve high segmentation accuracy. It indicates that the constructed CUG_MISDataset is robust and can serve as a valuable benchmark for remote sensing instance segmentation of mining areas. Second, aiming at the difficulty of large scale variation in this dataset, we propose a multiscale dilation feature pyramid network (MSD-FPN), which introduces a dynamic weight allocation mechanism to give more weight to important semantic information, while convolution with different dilation rates is used in the module to enhance the expression of mines' multiscale features. The proposed MSD-FPN can achieve a 2.0% average precision improvement on the CUG_MISDataset.",Deep learning,instance segmentation,mining area,mining land occupation,remote sensing,,,,,,,"Wang, Lizhe",,,,,,,,,
Row_905,"Lan, Lingxiang","Wu, Dong","Chi, Mingmin",,,Multi-temporal Change Detection based on Deep Semantic Segmentation Networks,,2019,3,"The task of change detection (CD) can be considered as a pixel-level classification problem and can be fulfilled by a semantic segmentation task. In this paper, we propose an end-to-end change detection framework based on a deep semantic segmentation network, in particular, fully convolutional networks in terms of the difference image or the concatenated one of multi-temporal images. Experimental results illustrate that our framework can obtain a state-of-the-art result on the real-world high resolution CD dataset.",Remote sensing,change detection,deep learning,semantic segmentation,,,,,,,,,,,,,,,,2019 10TH INTERNATIONAL WORKSHOP ON THE ANALYSIS OF MULTITEMPORAL REMOTE SENSING IMAGES (MULTITEMP),
Row_906,"Shamsolmoali, Pourya","Zareapoor, Masoumeh","Wang, Ruili","Zhou, Huiyu","Yang, Jie",A Novel Deep Structure U-Net for Sea-Land Segmentation in Remote Sensing Images,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,SEP 2019,91,"Sea-land segmentation is an important process for many key applications in remote sensing. Proper operative sea-land segmentation for remote sensing images remains a challenging issue due to complex and diverse transition between sea and land. Although several convolutional neural networks (CNNs) have been developed for sea-land segmentation, the performance of these CNNs is far from the expected target. This paper presents a novel deep neural network structure for pixel-wise sea-land segmentation, a residual Dense U-Net (RDU-Net), in complex and high-density remote sensing images. RDU-Net is a combination of both downsampling and upsampling paths to achieve satisfactory results. In each downsampling and upsampling path, in addition to the convolution layers, several densely connected residual network blocks are proposed to systematically aggregate multiscale contextual information. Each dense network block contains multilevel convolution layers, short-range connections, and an identity mapping connection, which facilitates features reuse in the network and makes full use of the hierarchical features from the original images. These proposed blocks have a certain number of connections that are designed with shorter distance backpropagation between the layers and can significantly improve segmentation results while minimizing computational costs. We have performed extensive experiments on two real datasets, Google-Earth and ISPRS, and compared the proposed RDU-Net against several variations of dense networks. The experimental results show that RDU-Net outperforms the other state-of-the-art approaches on the sea-land segmentation tasks.",Deep neural network (DNN),dense network (DenseNet),remote sensing images,sea-land segmentation,U-Net,,,,,,,,,,,,,,,,
Row_907,"Wang, Pan","Zhao, Hengqian","Yang, Zihan","Jin, Qian","Wu, Yanhua",Fast Tailings Pond Mapping Exploiting Large Scene Remote Sensing Images by Coupling Scene Classification and Sematic Segmentation Models,REMOTE SENSING,JAN 2023,6,"In the process of extracting tailings ponds from large scene remote sensing images, semantic segmentation models usually perform calculations on all small-size remote sensing images segmented by the sliding window method. However, some of these small-size remote sensing images do not have tailings ponds, and their calculations not only affect the model accuracy, but also affect the model speed. For this problem, we proposed a fast tailings pond extraction method (Scene-Classification-Sematic-Segmentation, SC-SS) that couples scene classification and semantic segmentation models. The method can map tailings ponds rapidly and accurately in large scene remote sensing images. There were two parts in the method: a scene classification model, and a semantic segmentation model. Among them, the scene classification model adopted the lightweight network MobileNetv2. With the help of this network, the scenes containing tailings ponds can be quickly screened out from the large scene remote sensing images, and the interference of scenes without tailings ponds can be reduced. The semantic segmentation model used the U-Net model to finely segment objects from the tailings pond scenes. In addition, the encoder of the U-Net model was replaced by the VGG16 network with stronger feature extraction ability, which improves the model's accuracy. In this paper, the Google Earth images of Luanping County were used to create the tailings pond scene classification dataset and tailings pond semantic segmentation dataset, and based on these datasets, the training and testing of models were completed. According to the experimental results, the extraction accuracy (Intersection Over Union, IOU) of the SC-SS model was 93.48%. The extraction accuracy of IOU was 15.12% higher than the U-Net model, while the extraction time was shortened by 35.72%. This research is of great importance to the remote sensing dynamic observation of tailings ponds on a large scale.",tailings ponds,large scene remote sensing images,deep learning,fast mapping,,,,,,,,"Xia, Pengjiu","Meng, Lingxuan",,,,,,,,
Row_908,"Long, Jiang","Li, Mengmeng","Wang, Xiaoqin",,,Integrating Spatial Details With Long-Range Contexts for Semantic Segmentation of Very High-Resolution Remote-Sensing Images,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2023,13,"This letter presents a cross-learning network (i.e., CLCFormer) integrating fine-grained spatial details within long-range global contexts based upon convolutional neural networks (CNNs) and transformer, for semantic segmentation of very high-resolution (VHR) remote-sensing images. More specifically, CLCFormer comprises two parallel encoders, derived from the CNN and transformer, and a CNN decoder. The encoders are backboned on SwinV2 and EfficientNet-B3, from which the extracted semantic features are aggregated at multiple levels using a bilateral feature fusion module (BiFFM). First, we used attention gate (ATG) modules to enhance feature representation, improving segmentation results for objects with various shapes and sizes. Second, we used an attention residual (ATR) module to refine spatial features's learning, alleviating boundary blurring of occluded objects. Finally, we developed a new strategy, called auxiliary supervise strategy (ASS), for model optimization to further improve segmentation performance. Our method was tested on the WHU, Inria, and Potsdam datasets, and compared with CNN-based and transformer-based methods. Results showed that our method achieved state-of-the-art performance on the WHU building dataset (92.31% IoU), Inria building dataset (83.71% IoU), and Potsdam dataset (80.27% MIoU). We concluded that CLCFormer is a flexible, robust, and effective method for the semantic segmentation of VHR images. The codes of the proposed model are available at https://github.com/long123524/CLCFormer.",Feature extraction,Transformers,Semantics,Convolutional neural networks,Convolution,Buildings,Tiles,Auxiliary supervise,CLCFormer,convolutional neural networks (CNNs),semantic segmentation,,,,,,transformer,very high-resolution (VHR) images,,,
Row_909,"Wang, Yiwen","Lyn, Ye","Cao, Yanpeng","Yang, Michael Ying",,DEEP LEARNING FOR SEMANTIC SEGMENTATION OF UAV VIDEOS,,2019,4,"As one of the key problems in both remote sensing and computer vision, video semantic segmentation has been attracting increasing amounts of attention. Using video segmentation technique for Unmanned Aerial Vehicle (UAV) data processing is also a popular application. Previous methods extended single image segmentation approaches to multiple frames. The temporal dependencies are ignored in these methods. This paper proposes a novel segmentation method to solve this problem. Combining the fully convolutional networks (FCN) and the Convolution Long Short Term Memory (Conv-LSTM) together, we segment the sequence of the video frames instead of segmenting each individual frame separately. FCN serves as the frame-based segmentation method. Conv-LSTM makes use of the temporal information between consecutive frames. Experimental results show the superiority of this method especially in some classes compared to the single image segmentation model using video dataset from UAV.",FCN,Conv-LSTM,video semantic segmentation,UAV,,,,,,,,,,,,,,,,2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019),
Row_910,"Tang, Kai","Xu, Fei","Chen, Xuehong","Dong, Qi","Yuan, Yuheng",The ClearSCD model: Comprehensively leveraging semantics and change relationships for semantic change detection in high spatial resolution remote sensing imagery,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,MAY 2024,7,"The Earth has been undergoing continuous anthropogenic and natural change. High spatial resolution (HSR) remote sensing imagery provides a unique opportunity to accurately reveal these changes on a planetary scale. Semantic change detection (SCD) with HSR imagery has become a common technique for tracking the evolution of land surface types at a semantic level. However, existing SCD methods rarely model the dependency between semantics and changes, resulting in suboptimal accuracy in detecting complicated surface changes. To address this limitation, we propose ClearSCD, a multi-task learning model that leverages the mutual gain relationship between semantics and change through three innovative modules. The first module interprets semantic features at different times into posterior probabilities for surface types to detect binary change information; the second module learns the correlation between surface types over time and the binary change information; a semantic augmented contrastive learning module is used as the third module to improve the performance of the other two modules. We tested ClearSCD's performance against state-of-the-art methods on benchmark datasets and a real- world scenario (named LsSCD dataset), showing that ClearSCD outperformed the alternatives on mIoUscmetrics sc metrics by 1.23% to 19.34%. Furthermore, ablation experiments demonstrated the unique contribution of the three innovative modules to performance improvement. The high computational efficiency and robust performance over diverse landscapes demonstrate that ClearSCD is an operational tool for detecting detailed land surface changes from HSR imagery. Code and LsSCD dataset available at https://github.com/tangkai-RS/ClearSCD.",Remote sensing,Change detection,Semantic change detection,Semantic segmentation,Deep learning,Multi-task learning,,,,,,"Chen, Jin",,,,,,,,,
Row_911,"Zhang, Di","Zhao, Jiaqi","Chen, Jingyang","Zhou, Yong","Shi, Boyu",Edge-aware and spectral-spatial information aggregation network for multispectral image semantic segmentation,ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,SEP 2022,13,"Semantic segmentation is a fundamental task in the field of remote sensing image intelligent interpretation and computer vision. Multispectral remote sensing images have attracted more and more researchers' attention because they can accurately describe different types of reflection spectra. However, inaccurate multispectral feature description leads to edge semantic ambiguity and misclassification of small objects. In this article, we propose a novel network named edge-aware and spectral-spatial information aggregation net (ESSANet) to capture both high-level semantic features and low-level edge details for semantic segmentation of remote sensing images. Specifically, on the one hand, in order to improve the representation ability of discriminant features, we design a two-stream spectral-spatial feature extraction network via 3D hybrid convolution and multi-level aggregation network. On the other hand, in order to eliminate the effect of edge semantic ambiguity, we develop a siamese edge-aware structure and multi-stage edge loss function. Experimental results show that our method achieved 3.5% and 4.09% mean intersection over union (mIoU) score improvements and 2.59% and 3.32% Kappa score improvements compared with the competitive baseline algorithm on the SEN12MS and US3D datasets, respectively. In addition, the method proposed in this paper also achieves a better trade-off between speed and accuracy.",Spectral&ndash,spatial information,Edge-aware,Remote sensing,Multispectral semantic segmentation,,,,,,,"Yao, Rui",,,,,,,,,
Row_912,"Wang, Guoying","Chen, Jiahao","Mo, Lufeng","Wu, Peng","Yi, Xiaomei",Border-Enhanced Triple Attention Mechanism for High-Resolution Remote Sensing Images and Application to Land Cover Classification,REMOTE SENSING,AUG 2024,2,"With the continuous development and popularization of remote sensing technology, remote sensing images have been widely used in the field of land cover classification. Since remote sensing images have complex spatial structure and texture features, it is becoming a challenging problem to accurately categorize them. Land cover classification has practical application value in various fields, such as environmental monitoring and protection, urban and rural planning and management, and climate change research. In recent years, remote sensing image classification methods based on deep learning have been rapidly developed, in which semantic segmentation technology has become one of the mainstream methods for land cover classification using remote sensing image. Traditional semantic segmentation algorithms tend to ignore the edge information, resulting in poor classification of the edge part in land cover classification, and there are numerous attention mechanisms to make improvements for these problems. In this paper, a triple attention mechanism, BETAM (Border-Enhanced Triple Attention Mechanism), for edge feature enhancement of high-resolution remote sensing images is proposed. Furthermore, a new model on the basis of the semantic segmentation network model DeeplabV3+ is also introduced, which is called DeepBETAM. The triple attention mechanism BETAM is able to capture feature dependencies in three dimensions: position, space, and channel, respectively. Through feature importance weighting, modeling of spatial relationships, and adaptive learning capabilities, the model BETAM pays more attention to edge features, thus improving the accuracy of edge detection. A remote sensing image dataset SMCD (Subject Meticulous Categorization Dataset) is constructed to verify the robustness of the attention mechanism BETAM and the model DeepBETAM. Extensive experiments were conducted on the two self-built datasets FRSID and SMCD. Experimental results showed that the mean Intersection over Union (mIoU), mean Pixel Accuracy (mPA), and mean Recall (mRecall) of DeepBETAM are 63.64%, 71.27%, and 71.31%, respectively. These metrics are superior to DeeplabV3+, DeeplabV3+(SENet), DeeplabV3+(CBAM), DeeplabV3+(SAM), DeeplabV3+(ECANet), and DeeplabV3+(CAM), which are network models that incorporate different attention mechanisms. The reason is that BETAM has better edge segmentation results and segmentation accuracy. Meanwhile, on the basis of the self-built dataset, the four main classifications of buildings, cultivated land, water bodies and vegetation were subdivided and detected, and good experimental results were obtained, which verified the robustness of the attention mechanism BETAM and the model DeepBETAM. The method has broad application prospects and can provide favorable support for research and application in the field of surface classification.",semantic segmentation,remote sensing image,BETAM,triple attention mechanism,edge detection,,,,,,,,,,,,,,,,
Row_913,"Nunes de Castro, Heitor da Rocha","de Carvalho Junior, Osmar Abilio","Ferreira de Carvalho, Osmar Luiz","Trancoso Gomes, Roberto Arnaldo","Fontes Guimaraes, Renato",DETECTION OF KARST DEPRESSIONS IN BRAZIL USING DEEP SEMANTIC SEGMENTATION,,2023,0,"This research aims to investigate the use of semantic segmentation and Shuttle Radar Topography Mission (SRTM) data in detecting natural karst depressions developed on the carbonate rocks of the Neoproterozoic Bambui Group in Western Bahia, Brazil. The study area is a karst landscape containing depressions enclosed in limestone, many forming lakes. The methodology had the following steps: (a) visual interpretation of karst depressions from Sentinel-2 and OLI-Landsat 8 images; (b) generation of DEM-based sink depth plus nine morphometric attributes; (c) selection of 128x128-pixel samples for training (1600), validation (400), and testing (400) considering two channels (DEM and sink depth based on DEM) and eleven channels (the two previous ones and the morphometric attributes); and (d) semantic segmentation using U-Net architecture with EfficientNet-B7 backbone. The accuracy metrics were 98.26, 72.82, 79.50, 79.16, and 65.51 for OA, precision, recall, F-score, and IoU when considering SRTM plus morphometric attributes (11 channels).",Semantic segmentation,sparse annotation,iterative learning,remote sensing,,,,,,,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,
Row_914,"Ye, Wenhui","Lei, Weimin","Zhang, Wei","Yu, Tingting","Feng, Xiang",GFSCompNet: remote sensing image compression network based on global feature-assisted segmentation,MULTIMEDIA TOOLS AND APPLICATIONS,JAN 2024,1,"The proliferation of remote sensing image data in recent years has posed a pressing need for efficient compression techniques due to constrained transmission bandwidth. While lossless compression preserves image fidelity, it falls short of meeting real-time demands. Conversely, conventional lossy compression methods can attain high compression ratios for real-time applications, but often introduce issues like block artifacts, blurring, and distortions in the decompressed images. Hence, we propose the Global Feature-Assisted Segmentation Compression Network (GFSCompNet) as a solution for high compression ratio lossy compression. Initially, we design a segmentation network utilizing a dual-branch global feature-assisted segmentation approach to precisely detect small targets in remote sensing images. On the compression side of the network, we leverage an attention mechanism and code rate allocation technique to seamlessly merge the segmented small target information with the original image, thereby allocating a higher compression code rate to the small target region. Furthermore, a joint hyper-priority decoding and entropy coding estimation network is proposed to further remove the redundancy in the potential representation and improve the compression ratio. Experimental results conducted under conditions of high compression ratios and comparable bit rates demonstrate that our approach yields higher-quality reconstructed images compared to the JPEG algorithm and outperforms other deep learning-based image compression methods. Additionally, it effectively preserves small target information, thereby enhancing the interpretability of machine learning models.",Remote sensing image,Image compression,Semantic segmentation,Feature fusion,Separate hyperprior decoders,,,,,,,,,,,,,,,,
Row_915,"Shen, Zhengwei","Shang, Yongheng","Zhang, Xiaoyu","Yin, Jianwei","Han, Jun",CMMSNet:A Multi-modal Semantic Segmentation Network for Rooftop Extraction based on SAR and Optical Images,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,2024,0,"Distributed rooftop photovoltaic systems hold immense potential for renewable energy generation, and accurate extraction of building roofs from high-resolution remote sensing imagery is crucial for their development. While current semantic segmentation methods primarily rely on single-modality optical images, Synthetic Aperture Radar (SAR) offers complementary ground information that can significantly enhance segmentation accuracy. However, the modality disparities arising from different imaging mechanisms pose challenges in feature fusion between SAR and optical images, existing approaches rely on simplistic fusion methods to exploit the complementary information of each modality, ignoring the correlative information between the different modalities during feature extraction, this results in an insufficient integration of complementary information.To address these challenges, we introduce CMMSNet, a novel multi-modal fusion semantic segmentation network specifically designed for building roof extraction. The main architecture of CMMSNet is constituted by three three core modules: the feature extraction encoder module, the heterogeneous modality alignment module, and the modality fusion decoder module. Initially, dual independent pyramid-structured encoders are employed by CMMSNet to separately extract feature pyramids from SAR and optical images at various scales, this strategy is intended to capture multi-scale semantic contexts and address the issue of large spatial scale variations among different objects in remote sensing images. Furthermore, an Adaptive Feature Alignment Module (AFAM) is introduced, tasked with identifying correlative information between the two modalities from a spatial dimension and aligning the modal features accordingly, this process is crucial for facilitating cross-modal learning and in enhancing the feature representation of each modality. In addition, a Cross-Modal Multi-Scale Feature Fusion (CMMSFF) module is designed to effectively integrates multi-scale and multi-modal heterogeneous features from both modalities, this module employs a channel self-attention mechanism, which adaptively fuses discriminative features by applying weights to each modality and selectively discarding irrelevant components, thus enhancing the selection and fusion of key channels within the multimodal features set. This innovative approach allows us to harness the complementary information provided by SAR and optical images, enhancing the overall segmentation performance. A series of comprehensive experiments conducted on the DFC23 dataset demonstrate that our proposed CMMSNet outperforms other existing mainstream semantic segmentation methods in both stability and effectiveness, including both single-modal and multi-modal approaches. This achievement sets a new benchmark for the extraction of building rooftop through the use of multi-modal remote sensing images. Our findings highlight the importance of leveraging multi-modal data fusion in addressing real-world challenges in remote sensing image analysis and offer valuable insights for future research in this domain.",Building extraction,Multi-modal fusion,Remote sensing,semantic segmentation,,,,,,,,"Cai, Chao",,,,,,,,,
Row_916,"Lekavicius, Justinas","Gruzauskas, Valentas",,,,Data Augmentation with Generative Adversarial Network for Solar Panel Segmentation from Remote Sensing Images,ENERGIES,JUL 2024,2,"With the popularity of solar energy in the electricity market, demand rises for data such as precise locations of solar panels for efficient energy planning and management. However, these data are not easily accessible; information such as precise locations sometimes does not exist. Furthermore, existing datasets for training semantic segmentation models of photovoltaic (PV) installations are limited, and their annotation is time-consuming and labor-intensive. Therefore, for additional remote sensing (RS) data creation, the pix2pix generative adversarial network (GAN) is used, enriching the original resampled training data of varying ground sampling distances (GSDs) without compromising their integrity. Experiments with the DeepLabV3 model, ResNet-50 backbone, and pix2pix GAN architecture were conducted to discover the advantage of using GAN-based data augmentations for a more accurate RS imagery segmentation model. The result is a fine-tuned solar panel semantic segmentation model, trained using transfer learning and an optimal amount-60% of GAN-generated RS imagery for additional training data. The findings demonstrate the benefits of using GAN-generated images as additional training data, addressing the issue of limited datasets, and increasing IoU and F1 metrics by 2% and 1.46%, respectively, compared with classic augmentations.",deep learning,solar panels,semantic segmentation,data augmentation,generative adversarial network,remote sensing,transfer learning,,,,,,,,,,,,,,
Row_917,"Xiao, Tao","Liu, Yikun","Huang, Yuwen","Yang, Gongping",,MFRNet: A Multipath Feature Refinement Network for Semantic Segmentation in High-Resolution Remote Sensing Images,REMOTE SENSING LETTERS,DEC 2 2022,3,"Deep convolutional neural networks have made significant progress in the field of intelligent analysis of remote-sensing images. However, the semantic segmentation task in high-resolution remote-sensing (HRRS) images always faces the problem of large-scale variation and complex background samples, which causes difficulties in distinguishing confusable ground objects. In this letter, we propose a novel multipath feature refinement network (MFRNet) to alleviate the above problems. We design the feature refinement module (FRM) to fuse features at various scales, which helps to capture different levels of spatial information. It also alleviates the boundary ambiguity problem by enhancing the learning of features with boundary information. The multiscale feature attention module (MFAM) combines atrous convolution and non-local block to obtain larger receptive fields and long-range contextual information, while the feature fusion module (FFM) balances semantic and spatial information, further improving the embedding of locally discriminative features. Experimental results on ISPRS Potsdam and LoveDA datasets indicate that the proposed MFRNet outperforms other semantic segmentation methods and excels in the accuracy and consistency of object boundary segmentation.",,,,,,,,,,,,,,,,,,,,,
Row_918,"Guan, Renchu","Wang, Mingming","Bruzzone, Lorenzo","Zhao, Haishi","Yang, Chen",Lightweight Attention Network for Very High-Resolution Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,14,"Semantic segmentation is one of the most challenging tasks for very high-resolution (VHR) remote sensing applications. Deep convolutional neural networks (DCNNs) based on the attention mechanism have shown outstanding performance in VHR remote sensing images semantic segmentation. However, the existing attention-guided methods require the estimation of a large number of parameters that are affected by the limited number of available labeled samples that results in underperforming segmentation results. In this article, we propose a multistage feature fusion lightweight (MSFFL) model to greatly reduce the number of parameters and improve the accuracy of semantic segmentation. In this model, two parallel enhanced attention modules, i.e., the spatial attention module (SAM) and the channel attention module (CAM), are designed by introducing encoding position information. Then, a covariance calculation strategy is adopted to recalibrate the generated attention maps. The integration of enhanced attention modules into the proposed lightweight module results in an efficient lightweight attention network (LiANet). The performance of the proposed LiANet is assessed on two benchmark datasets. Experimental results demonstrate that LiANet can achieve promising performance with a small number of parameters.",Semantic segmentation,Remote sensing,Semantics,Feature extraction,Task analysis,Computational modeling,Covariance matrices,Covariance,lightweight,position information,remote sensing,,,,,,semantic segmentation,very high-resolution (VHR) images,,,
Row_919,"Huang, Xin","Wang, Wenrui","Li, Jiayi","Wang, Leiguang","Xie, Xing",A Stepwise Refining Image-Level Weakly Supervised Semantic Segmentation Method for Detecting Exposed Surface for Buildings (ESB) From Very High-Resolution Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,6,"Exposed surface for buildings (ESB), which refers to exposed surfaces with traces of building construction, often leads to urban dust. Accurate ESB detection is important for planning urban development and improving urban environment. Fine-grained monitoring of ESB typically needs massive high-quality pixel-level labels, which are demanding and expensive. In contrast, obtaining cost-efficient image-level labels is more promising. Most image-level weakly supervised methods can extract pixel-level pseudo labels using the class activation map (CAM) generated by the classification network. Subsequently, these labels are applied to train the semantic segmentation network. However, the CAM is easy to miss fine-grained information, which leads to label noise. Moreover, the downsampling in the segmentation networks will further loss the spatial information. Furthermore, the sparse distribution and irregular shape of ESB pose additional challenges. Given these problems, we propose a stepwise refining image-level weakly supervised semantic segmentation method (SRIWS): 1) we introduce a new data augmentation method called SRMix to oversample the classification dataset; 2) we propose a two-branch network with a superpixel pooling layer (SPNet) as the semantic segmentation network to capture both global semantic information and spatial details; and 3) to alleviate the impact of potential noise in the initial labels, we design the high-confidence sample filtering operation (HSF) during the SPNet training. The evaluation experiments for the SRIWS were performed on three datasets. The results confirm that our proposed SRIWS presents a superior performance in recognizing ESB compared with existing state-of-the-art methods. In addition, numerous ablation experimental results indicate the effectiveness and robustness of our SRIWS.",Exposed surface for buildings (ESB),image-level samples,very high-resolution remote sensing (VHR) images,weakly supervised semantic segmentation,,,,,,,,,,,,,,,,,
Row_920,"Lumban-Gaol, Y. A.","Rizaldy, A.","Murtiyoso, A.",,,COMPARISON OF DEEP LEARNING ARCHITECTURES FOR THE SEMANTIC SEGMENTATION OF SLUM AREAS FROM SATELLITE IMAGES,,2023,1,"The mapping of slum areas is an important task when considering the necessity for an inclusive, safe and resilient cities. While many methods exist in this regard, the use of machine learning and more specifically deep learning has gained traction in recent years. In this paper, we present a systematic comparison of existing deep learning architectures and backbones. The experiments in the paper investigate the question of which architecture and backbone combination and which configuration of dataset preparation is best for use in slum mapping. In another experiment we implemented the trained model to predict slums in existing open data. The experiments in the paper used public open data provided by Helber et al. (2018). Results show that FPN with vgg16 backbone showed the most potential in this particular application. The results of the semantic segmentation also shows promise, although the discrepancy in slum characteristic still hinders a proper generalization of its use.",deep learning,semantic segmentation,slums,remote sensing,,,,,,,,,,,,,,,,"GEOSPATIAL WEEK 2023, VOL. 48-1",
Row_921,"Song, A.",,,,,Semantic Segmentation of UAV image using Combined U-net and heterogeneous UAV imagery datasets,,2022,0,"Semantic segmentation of urban areas can provide useful information for analyzing and detecting changes in urban development. Recently, numerous remote sensing image datasets from various platforms have been acquired, and various semantic segmentation studies using them have been conducted. However, they do not contain many images because of their large data capacity and difficulty in constructing label data. Furthermore, it is difficult to use them simultaneously because each dataset has a different spatial resolution, shooting angle, and meaningful objects. In this study, two different UAV image datasets, such as UAVid semantic segmentation and semantic drone datasets, were used to train a combined U-net model to use heterogeneous remote sensing datasets for semantic segmentation tasks simultaneously. The UAVid dataset has a flight height of 50 m and 300 images with eight classes. However, the semantic drone dataset was acquired at an altitude of 5-30 m above the ground and contains 598 images with 20 classes. The combined U-net model is based on the U-net architecture, but it receives input from two different data sources. The experimental results showed that learning two datasets with a combined U-net improved semantic segmentation accuracy more than learning each data with a U-net. This study confirms the ability to train two different datasets acquired from different places and platforms simultaneously; thus, evaluating the applicability of semantic segmentation studies using heterogeneous remote sensing datasets.",Semantic segmentation,UAV,combined U-net,heterogeneous dataset,,,,,,,,,,,,,,,,REMOTE SENSING TECHNOLOGIES AND APPLICATIONS IN URBAN ENVIRONMENTS VII,
Row_922,"Zhang, Junping","Li, Tong","Lu, Xiaochen","Cheng, Zhen",,Semantic Classification of High-Resolution Remote-Sensing Images Based on Mid-level Features,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,JUN 2016,37,"With the resolution improvement of the remote-sensing images, more details are shown clearly. The challenge that comes along is how to boost the relatively low classification accuracy caused by using pixel-based image classification approaches and low-level visual structure. The low-level features (LLF) may not well describe the image due to the semantic gap between low-level visual features and high-level semantics of images. The bag-of-visual-words (BOV) model which generates mid-level features was proposed to bridge the two levels. However, it generally neglects the context information between local patches. In this paper, an object-oriented semantic classification algorithm that combines BOV with the optimal segmentation scale is presented. In this algorithm, BOV addresses the problem of the representation of mid-level for scenes, while the optimal segmentation scale intends to overcome the defect of conventional BOV in lacking of relationship between image patches and to give more thorough description. The object-based BOV is presented to construct mid-level representations for object description instead of LLF, and histogram intersection kernel (HIK) is introduced in support vector machine (SVM) for classification. The experiments conducted on three datasets testify the superiority of the proposed algorithm.",Bag-of-visual-words (BOV),mid-level representations,multiscale segmentation,object-oriented,semantic classification,,,,,,,,,,,,,,,,
Row_923,"Yu, Chuang","Hu, Zhuhua","Li, Ruoqing","Xia, Xin","Zhao, Yaochi",Segmentation and density statistics of mariculture cages from remote sensing images using mask R-CNN,INFORMATION PROCESSING IN AGRICULTURE,SEP 2022,11,"The normal growth of fishes is closely relevant to the density of mariculture. It is of great significance to accurately calculate the breeding area of specific sea area from satellite remote sensing images. However, there are no reports about cage segmentation and density detection based on remote sensing images so far. And the accurate segmentation of cages faces challenges from very large high-resolution images. Firstly, a new public mariculture cage data set is built. Secondly, the training set is augmented via sample variations to improve the robustness of the model. Then, for cage segmentation and density statistics, a new methodology based on Mask R-CNN is proposed. Using dividing and stitching technologies, the entire remote sensing test images of the cage can be accurately segmented. Finally, using the trained model, the object detection features and segmentation characteristics can be obtained at the same time. Considering only the area within the target detection frame, the proposed method can count the pixels in the segmented area, which can obtain accurate area and density while reducing time-consuming. Experimental results demonstrate that, compared with traditional contour extraction method and U-Net based scheme, the proposed scheme can significantly improve segmentation precision and model's robustness. The relative error of the actual area is only 1.3%.(c) 2021 China Agricultural University. Production and hosting by Elsevier B.V. on behalf of KeAi. This is an open access article under the CC BY-NC-ND license (http://creativecommons. org/licenses/by-nc-nd/4.0/).",Deep learning,Mask R-CNN,Image segmentation,Remote sensing,,,,,,,,"Fan, Xiang","Bai, Yong",,,,,,,,
Row_924,"Wang, Zhengxin","Zhao, Longlong","Meng, Jintao","Han, Yu","Li, Xiaoli",Deep Learning-Based Cloud Detection for Optical Remote Sensing Images: A Survey,REMOTE SENSING,DEC 2024,0,"In optical remote sensing images, the presence of clouds affects the completeness of the ground observation and further affects the accuracy and efficiency of remote sensing applications. Especially in quantitative analysis, the impact of cloud cover on the reliability of analysis results cannot be ignored. Therefore, high-precision cloud detection is an important step in the preprocessing of optical remote sensing images. In the past decade, with the continuous progress of artificial intelligence, algorithms based on deep learning have become one of the main methods for cloud detection. The rapid development of deep learning technology, especially the introduction of self-attention Transformer models, has greatly improved the accuracy of cloud detection tasks while achieving efficient processing of large-scale remote sensing images. This review provides a comprehensive overview of cloud detection algorithms based on deep learning from the perspective of semantic segmentation, and elaborates on the research progress, advantages, and limitations of different categories in this field. In addition, this paper introduces the publicly available datasets and accuracy evaluation indicators for cloud detection, compares the accuracy of mainstream deep learning models in cloud detection, and briefly summarizes the subsequent processing steps of cloud shadow detection and removal. Finally, this paper analyzes the current challenges faced by existing deep learning-based cloud detection algorithms and the future development direction of the field.",cloud detection,deep learning,semantic segmentation,optical satellite imagery,remote sensing,,,,,,,"Jiang, Ruixia","Chen, Jinsong","Li, Hongzhong",,,,,,,
Row_925,"Weng, Liguo","Pang, Kai","Xia, Min","Lin, Haifeng","Qian, Ming",Sgformer: A Local and Global Features Coupling Network for Semantic Segmentation of Land Cover,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2023,20,"With the introduction of Earth observation satellites, the classification technology through high-definition remote sensing images appeared. After decades of evolution, the land cover classification method in high-definition satellite maps has been gradually improved. Recently, high-definition remote sensing maps have been applied to land cover classification. Nowadays, classification methods using high-definition maps have these following problems. First, the traditional land cover classification methods cannot process the rich details in high-definition maps. Second, there are different acquisition conditions in the maps of different regions, which leads to distortion, deformation, and illumination blur of remote sensing images. Third, the existing methods are unable to provide a good generalization performance. To address these issues, a dual-branch parallel network structure is proposed, called Sgformer, to improve the performance of the transformer in the context of high-definition remote sensing maps. The network enhances perceptual learning with convolution operators that extract local features and a self-attention module that captures global representations. Local information and global representations with semantic divergence are fused through a feature coupling module. At last, a decoder is designed to maximize the preservation of local features and global representations and to better recover high-definition feature maps. The results of semantic segmentation experiments show that the methodology in this study has higher accuracy than the other methodologies.",Deep learning,land cover,neural network,remote sensing,semantic segmentation,,,,,,,"Zhu, Changjie",,,,,,,,,
Row_926,"He, Yawen","Jin, Feng","Li, Yongheng",,,Integrating semantic segmentation and edge detection for agricultural greenhouse extraction,JOURNAL OF APPLIED REMOTE SENSING,APR 1 2024,0,"Agricultural greenhouses have a negative impact on the ecological environment while bringing huge economic and social benefits. Therefore, it is of great significance to obtain greenhouse information in a timely and accurate manner. Due to the complex spectral characteristics and dense spatial distribution characteristics of greenhouses, although the extraction of greenhouses based on a single semantic segmentation model can extract the area with high precision, the segmentation process has a serious problem of boundary adhesion between greenhouses, which makes it difficult to accurately obtain the quantity of greenhouses. To address this, our study proposes a method for greenhouse extraction that integrates semantic segmentation and edge constraints, using high-spatial-resolution remote sensing images to accurately extract the area and quantity of greenhouses. This method employs an improved semantic segmentation model (AtDy-D-LinkNet) to extract the greenhouse area, which embeds a convolutional attention module into the D-LinkNet and adopts a dynamic upsampling strategy, achieving precise greenhouse extraction. Experiments demonstrate that the improved model increased the recall, precision, F1 score, and intersection over union by 1.68%, 2.27%, 1.93%, and 3.54%, respectively, compared to the original model. To address the significant edge adhesion issue in semantic segmentation and accurately extract the quantity of greenhouses, we developed an edge constraint approach. This approach uses an edge detection model to extract greenhouse boundaries, further constrains the greenhouse surfaces, separates adhered greenhouses, and outputs vector patches representing individual greenhouses, thereby achieving precise greenhouse quantity extraction. The experiments show that this method effectively combines the advantages of semantic segmentation and edge detection. It not only ensures the accuracy of greenhouse area extraction but also effectively solves the boundary adhesion issue, significantly improving quantity extraction accuracy, resulting in vector patches that align with the actual area, quantity, and spatial distribution of greenhouses. This can provide a data foundation for greenhouse management and planning in agriculture.",agricultural greenhouse,high spatial resolution remote sensing,semantic segmentation,edge detection,edge adhesion,,,,,,,,,,,,,,,,
Row_927,"Zhang, Mi","Hu, Xiangyun","Zhao, Like","Pang, Shiyan","Gong, Jinqi",Translation-aware semantic segmentation via conditional least-square generative adversarial networks,JOURNAL OF APPLIED REMOTE SENSING,DEC 23 2017,11,"Semantic segmentation has recently made rapid progress in the field of remote sensing and computer vision. However, many leading approaches cannot simultaneously translate label maps to possible source images with a limited number of training images. The core issue is insufficient adversarial information to interpret the inverse process and proper objective loss function to overcome the vanishing gradient problem. We propose the use of conditional least squares generative adversarial networks (CLS-GAN) to delineate visual objects and solve these problems. We trained the CLS-GAN network for semantic segmentation to discriminate dense prediction information either from training images or generative networks. We show that the optimal objective function of CLS-GAN is a special class of f-divergence and yields a generator that lies on the decision boundary of discriminator that reduces possible vanished gradient. We also demonstrate the effectiveness of the proposed architecture at translating images from label maps in the learning process. Experiments on a limited number of high resolution images, including close-range and remote sensing datasets, indicate that the proposed method leads to the improved semantic segmentation accuracy and can simultaneously generate high quality images from label maps. (C) 2017 Society of Photo-Optical Instrumentation Engineers (SPIE)",generative adversarial network,deep learning,semantic segmentation,divergence,,,,,,,,"Luo, Min",,,,,,,,,
Row_928,"Ye, Wenhui","Zhang, Wei","Lei, Weimin","Zhang, Wenchao","Chen, Xinyi",Remote sensing image instance segmentation network with transformer and multi-scale feature representation,EXPERT SYSTEMS WITH APPLICATIONS,DEC 30 2023,12,"The goal of remote sensing image (RSI) instance segmentation is to perform instance-level semantic parsing of its contents. Aside from classifying and locating regions of interest (RoI), it also requires assigning finer pixel -wise annotations to objects. However, RSI often suffers from cluttered backgrounds, variable object scales, and complex object edge contours, making the instance segmentation task more challenging. In this work, we analytically customize an instance segmentation model that is more suitable for RSI. Specifically, we propose three novel modules for a region-based instance segmentation framework, namely Channel-Spatial Attention Module (CSA), Multi-Scale Aware Module (MSA), and Semantic Relation Learning Module (SRL). Among them, feature calibration performed by CSA can alleviate the semantic gap between low-level features and high-level semantics in both channel and spatial dimensions. Inheriting the capabilities of both the convolutional neural network (CNN) and the Transformer, SRL can help the network integrate both neighborhood features and long-range dependencies for instance semantic prediction. The MSA module designs a cascaded residual structure with different receptive fields to model the scale variation of objects in RSI. Experimental results on challenging ISAID, NWPU VHR-10, SSDD, BITTC and HRSID datasets demonstrate the superiority of our method, achieving mask APs of 40.2%, 68.2%, 68.4%, 50.4% and 55.8% respectively. Code and pretrained models are available at https://github.com/Sherlock1018/RSIISN.",Remote sensing image,Instance segmentation,Region-based,Long-range dependencies,,,,,,,,"Wang, Yanwen",,,,,,,,,
Row_929,"Cai, Miaoxin","Chen, He","Zhang, Tong","Zhuang, Yin","Chen, Liang",Consistency Regularization Based on Masked Image Modeling for Semisupervised Remote Sensing Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2024,1,"Semisupervised semantic segmentation aims to effectively leverage both unlabeled and scare labeled images, reducing the reliance on labor-intensive pixel-level labeling for extensive training processes. The leading semisupervised learning method, consistency regularization, employs weak and strong data augmentations to diversify input representations. Ultimately the model is compelled to maintain consistent predictions across different input views, thus boosting the model's generalization. However, previous methods suffered from limited input representation space introduced by linear transformations such as cutmix. To address such issue, a consistency regularization based on masked image modeling (MIM) called MIMSeg is proposed to achieve accurate segmentation with limited labeled images. First, MIM pixel-wise perception with ViT encoder-decoder lays the foundation for expanding the data representation space. Second, collaborating with weak data augmentations, two MIM-related strong data augmentations are developed to generate more challenging input views for consistent predictions. Precisely, weak data augmentations are employed to replicate input views from various perspectives while a controllable generative strong data augmentation called masked image reconstruction (MIR) is crafted to simulate multiple imaging diversity while preserving the original semantic information intact. In addition, a more severe strong data augmentation masked context perturbation (MCP) is designed to further generate more challenging input views and alleviate semantic deficiency via masked category prediction. Leveraging the MIM perception and two MIM-related strong data augmentations, the model is compelled to achieve consistency predictions across diverse input views from weak data augmentations, MIR and MCP. These components result in the generation of more stable pixel-level pseudo-labels and facilitate collaborative training between unlabeled and labeled images. Extensive experiments have shown that MIMSeg can achieve state-of-the-art performance in pixel-level prediction with very limited sample annotations.",Semantics,Data augmentation,Semantic segmentation,Data models,Training,Predictive models,Imaging,Consistency regularization,masked image modeling (MIM),semisupervised semantic segmentation,,,,,,,,,,,
Row_930,"Liu, Xinran","Peng, Yuexing","Lu, Zili","Li, Wei","Yu, Junchuan",Feature-Fusion Segmentation Network for Landslide Detection Using High-Resolution Remote Sensing Images and Digital Elevation Model Data,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,31,"Landslide is one of the most dangerous and frequently occurred natural disasters. The semantic segmentation technique is efficient for wide area landslide identification from high-resolution remote sensing images (HRSIs). However, considerable challenges exist because the effects of sediments, vegetation, and human activities over long periods of time make visually blurred old landslides very challenging to detect based upon HRSIs. Moreover, for terrain features like slopes, aspect and altitude variations cannot be sufficiently extracted from 2-D HRSIs but can be from digital elevation model (DEM) data. Then, a feature-fusion based semantic segmentation network (FFS-Net) is proposed, which can extract texture and shape features from 2-D HRSIs and terrain features from DEM data before fusing these two distinct types of features in a higher feature layer. To segment landslides from background, a multiscale channel attention module is purposely designed to balance the low-level fine information and high-level semantic features. In the decoder, transposed convolution layer replaces original mathematical bilinear interpolation to better restore image resolution via learnable convolutional kernels, and both dropout and batch normalization (BN) are introduced to prevent over-fitting and accelerate the network convergence. Experimental results are presented to validate that the proposed FFS-Net can greatly improve the segmentation accuracy of visually blurred old landslides. Compared to U-Net and DeepLabV3+, FFS-Net can improve the mean intersection over union (mIoU) metric from 0.508 and 0.624 to 0.67, the F1 metric from 0.254 and 0.516 to 0.596, and the pixel accuracy (PA) metric from 0.874 and 0.906 to 0.92, respectively. For the detection of visually distinct landslides, FFS-NET also offers comparable detection performance, and the segmentation is improved for visually distinct landslides with similar color and texture to surroundings.",Feature extraction,Terrain factors,Semantics,Shape,Image color analysis,Data mining,Optical sensors,Digital elevation model (DEM),feature fusion,high-resolution remote sensing image (HRSI),landslide detection,"Ge, Daqing","Xiang, Wei",,,,semantic segmentation,Siamese network,,,
Row_931,"Fu, Junjie","Yi, Xiaomei","Wang, Guoying","Mo, Lufeng","Wu, Peng",Research on Ground Object Classification Method of High Resolution Remote-Sensing Images Based on Improved DeeplabV3+,SENSORS,OCT 2022,11,"Ground-object classification using remote-sensing images of high resolution is widely used in land planning, ecological monitoring, and resource protection. Traditional image segmentation technology has poor effect on complex scenes in high-resolution remote-sensing images. In the field of deep learning, some deep neural networks are being applied to high-resolution remote-sensing image segmentation. The DeeplabV3+ network is a deep neural network based on encoder-decoder architecture, which is commonly used to segment images with high precision. However, the segmentation accuracy of high-resolution remote-sensing images is poor, the number of network parameters is large, and the cost of training network is high. Therefore, this paper improves the DeeplabV3+ network. Firstly, MobileNetV2 network was used as the backbone feature-extraction network, and an attention-mechanism module was added after the feature-extraction module and the ASPP module to introduce focal loss balance. Our design has the following advantages: it enhances the ability of network to extract image features; it reduces network training costs; and it achieves better semantic segmentation accuracy. Experiments on high-resolution remote-sensing image datasets show that the mIou of the proposed method on WHDLD datasets is 64.76%, 4.24% higher than traditional DeeplabV3+ network mIou, and the mIou on CCF BDCI datasets is 64.58%. This is 5.35% higher than traditional DeeplabV3+ network mIou and outperforms traditional DeeplabV3+, U-NET, PSP-NET and MACU-net networks.",high-resolution remote-sensing images,semantic segmentation,object classification,,,,,,,,,"Kapula, Kasanda Ernest",,,,,,,,,
Row_932,"Chong, Qianpeng","Xu, Jindong",,,,A DUAL-BRANCH AWARENESS NETWORK FOR SMALL OBJECT SEGMENTATION IN LARGE-SCALE REMOTE SENSING SCENES,,2023,0,"The more detailed and accurate earth observation has been made driven by the advancement of satellites and sensors optical photography technology, which poses both a challenge and an opportunity to small object segmentation task. However, the inherent difficulty and inadequate consideration still make small object segmentation task inevitably encounter a performance gain bottleneck. In this paper, we consider the longstanding but underestimated challenges in this task and give a point-to-point solution to response them. Specifically, we introduce a discriminative structure, i.e., a dual-branch awareness network for small object segmentation, named DASNet. In this structure, we propose the small object activation branch and the fuzzy refinement branch to avoid the negative influence of redundant background and ensure the small object segmentation accuracy, respectively. These two branches work collaboratively to mimic the process of human visual perception on small object. Finally, we propose a hierarchical unbiased loss to eliminate the bias against small objects in the regression process. Extensive experiments demonstrated that DASNet is competitive against some advanced methods for small object segmentation.",dual-branch,remote sensing,semantic segmentation,small object,,,,,,,,,,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,
Row_933,"Huang, Wei","Shi, Yilei","Xiong, Zhitong","Zhu, Xiao Xiang",,AdaptMatch: Adaptive Matching for Semisupervised Binary Segmentation of Remote Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,10,"There are various binary semantic segmentation tasks in remote sensing (RS) that aim to extract the foreground areas of interest, such as buildings and roads, from the background in satellite images. In particular, semisupervised learning (SSL), which can use limited labeled data to guide a large amount of unlabeled data for model training, can significantly promote the fast applications of these tasks in practice. However, due to the predominance of the background in RS images, the foreground only accounts for a small proportion of the pixels. It poses a challenge: models are biased toward the majority class of the background, leading to poor performance on the minority class of the foreground. To address this issue, this article proposes a novel and effective SSL framework, adaptive matching (AdaptMatch), for RS binary segmentation. AdaptMatch calculates individual and adaptive thresholds of the foreground and background based on their convergence difficulty in an online manner at the training stage; the adaptive thresholds are then used to select the high-confidence pseudo-labeled data of the two classes for model self-training in turn. Extensive experiments are conducted on two widely studied RS binary segmentation tasks, building footprint extraction and road extraction, to demonstrate the effectiveness and generalizability of the proposed method. The results show that the proposed AdaptMatch achieves superior performance compared with some state-of-the-art semisupervised methods in RS binary segmentation tasks. The codes will be publicly available at https://github.com/zhu-xlab/AdaptMatch.",Adaptive threshold,binary segmentation,building footprint extraction,remote sensing (RS),road extraction,semisupervised learning (SSL),,,,,,,,,,,,,,,
Row_934,"Wang, Di","Zhang, Jing","Du, Bo","Xia, Gui-Song","Tao, Dacheng",An Empirical Study of Remote Sensing Pretraining,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,108,"Deep learning has largely reshaped remote sensing (RS) research for aerial image understanding and made a great success. Nevertheless, most of the existing deep models are initialized with the ImageNet pretrained weights since natural images inevitably present a large domain gap relative to aerial images, probably limiting the fine-tuning performance on downstream aerial scene tasks. This issue motivates us to conduct an empirical study of RS pretraining (RSP) on aerial images. To this end, we train different networks from scratch with the help of the largest RS scene recognition dataset up to now-MillionAID-to obtain a series of RS pretrained backbones, including both convolutional neural networks (CNNs) and vision transformers, such as Swin and ViTAE, which have shown promising performance on computer vision tasks. Then, we investigate the impact of RSP on representative downstream tasks, including scene recognition, semantic segmentation, object detection, and change detection using these CNN and vision transformer backbones. Empirical study shows that RSP can help deliver distinctive performances in scene recognition tasks and in perceiving RS-related semantics, such as ""Bridge"" and ""Airplane."" We also find that, although RSP mitigates the data discrepancies of traditional ImageNet pretraining on RS images, it may still suffer from task discrepancies, where downstream tasks require different representations from scene recognition tasks. These findings call for further research efforts on both large-scale pretraining datasets and effective pretraining methods. The codes and pretrained models will be released at https://github.com/ViTAETransformer/ViTAE-Transformer-Remote-Sensing.",Classification,convolutional neural network (CNN),detection,remote sensing (RS) pretraining (RSP),semantic segmentation,vision transformer,,,,,,,,,,,,,,,
Row_935,"Abdollahi, Abolfazl","Pradhan, Biswajeet","Sharma, Gaurav","Maulud, Khairul Nizam Abdul","Alamri, Abdullah",Improving Road Semantic Segmentation Using Generative Adversarial Network,IEEE ACCESS,2021,45,"Road network extraction from remotely sensed imagery has become a powerful tool for updating geospatial databases, owing to the success of convolutional neural network (CNN) based deep learning semantic segmentation techniques combined with the high-resolution imagery that modern remote sensing provides. However, most CNN approaches cannot obtain high precision segmentation maps with rich details when processing high-resolution remote sensing imagery. In this study, we propose a generative adversarial network (GAN)-based deep learning approach for road segmentation from high-resolution aerial imagery. In the generative part of the presented GAN approach, we use a modified UNet model (MUNet) to obtain a high-resolution segmentation map of the road network. In combination with simple pre-processing comprising edge-preserving filtering, the proposed approach offers a significant improvement in road network segmentation compared with prior approaches. In experiments conducted on the Massachusetts road image dataset, the proposed approach achieves 91.54% precision and 92.92% recall, which correspond to a Mathews correlation coefficient (MCC) of 91.13%, a Mean intersection over union (MIOU) of 87.43% and a F1-score of 92.20%. Comparisons demonstrate that the proposed GAN framework outperforms prior CNN-based approaches and is particularly effective in preserving edge information.",Roads,Image segmentation,Generative adversarial networks,Semantics,Remote sensing,Generators,Feature extraction,GAN,road segmentation,remote sensing,deep learning,,,,,,U-Net,,,,
Row_936,"Tang, Bochuan","Tuerxun, Palidan","Qi, Ranran","Yang, Guangqi","Qian, Yurong",AMFFNet: attention-guided multi-level feature fusion network for land cover classification of remote sensing images,JOURNAL OF APPLIED REMOTE SENSING,APR 1 2023,1,"In the field of remote sensing, the classification of land cover is a pivotal and challenging issue. Standard models fail to capture global and semantic information in remote sensing images despite the fact that a convolutional neural network provides robust support for semantic segmentation. In addition, owing to disparities in semantic levels and spatial resolution, the simple fusion of low-level and high-level features may diminish the efficiency. To address these deficiencies, an attention-guided multi-level feature fusion network (AMFFNet) is proposed in this study. The proposed AMFFNet approach is designed as an encoder-decoder network with the inclusion of a multi-level feature fusion module (MFF) and a dual attention map module (DAM). A DAM models the semantic association of features from a spatial and channel perspective, and an MFF bridges the semantic and resolution gaps between high-level and low-level features. Furthermore, we propose a residual-based boundary refinement upsample module to further optimize the object boundaries. The experimental results indicate that the proposed strategy can considerably enhance the accuracy of land cover classification, achieving a mean intersection over union of 90.39% on the LandCover.ai dataset and 63.14% on the Gaofen Image Dataset with 15 categories (GID-15).",remote sensing,land cover classification,semantic segmentation,multi-level feature fusion,attention mechanism,,,,,,,,,,,,,,,,
Row_937,"Zhao, Wenyu","Xia, Min","Weng, Liguo","Hu, Kai","Lin, Haifeng",SPNet: Dual-Branch Network with Spatial Supplementary Information for Building and Water Segmentation of Remote Sensing Images,REMOTE SENSING,SEP 2024,1,"Semantic segmentation is primarily employed to generate accurate prediction labels for each pixel of the input image, and then classify the images according to the generated labels. Semantic segmentation of building and water in remote sensing images helps us to conduct reasonable land planning for a city. However, many current mature networks face challenges in simultaneously attending to both contextual and spatial information when performing semantic segmentation on remote sensing imagery. This often leads to misclassifications and omissions. Therefore, this paper proposes a Dual-Branch Network with Spatial Supplementary Information (SPNet) to address the aforementioned issues. We introduce a Context-aware Spatial Feature-Extractor Unit (CSF) to extract contextual and spatial information, followed by the Feature-Interaction Module (FIM) to supplement contextual semantic information with spatial details. Additionally, incorporating the Goal-Oriented Attention Mechanism helps in handling noise. Finally, to obtain more detailed branches, a Multichannel Deep Feature-Extraction Module (MFM) is introduced to extract features from shallow-level network layers. This branch guides the fusion of low-level semantic information with high-level semantic information. Experiments were conducted on building and water datasets, respectively. The results indicate that the segmentation accuracy of the model proposed in this paper surpasses that of other existing mature models. On the building dataset, the mIoU reaches 87.57, while on the water dataset, the mIoU achieves 96.8, which means that the model introduced in this paper demonstrates strong generalization capabilities.",semantic segmentation,building and water,spatial information,dual-branch network,,,,,,,,"Zhang, Youke","Liu, Ziheng",,,,,,,,
Row_938,"Sevak, Jay S.","Kapadia, Aerika D.","Chavda, Jaiminkumar B.","Shah, Arpita","Rahevar, Mrugendrasinh",Survey on Semantic Image Segmentation Techniques,,2017,25,"Semantic image segmentation is a vast area of interest for computer vision and machine learning researchers. Many vision applications need accurate and efficient image segmentation and segment classification mechanisms for assessing the visual contents and perform the real-time decision making. The application area includes remote sensing, autonomous driving, indoor navigation, video surveillance and virtual or augmented reality systems etc. The segmentation and classification of objects generate the specific performance parameters for various applications which require detailed domain analysis. There are broad range of applications where remote sensing image scene classification play an important role and has been receiving remarkable attention. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This survey paper provides a review of different traditional methods of image segmentation and classification. By comparing these methods with semantic image segmentation using deep learning it is assumed to show the far better result.",Semantic segmentation,segmentation pipeline,unsupervised segmentation,random decision forest,deep learning,feature and preprocessing method,,,,,,,,,,,,,,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT SUSTAINABLE SYSTEMS (ICISS 2017),
Row_939,"Wang, He","Zhang, Mengmeng","Li, Wei","Gao, Yunhao","Gui, Yuanyuan",Unbalanced Class Learning Network With Scale-Adaptive Perception for Complicated Scene in Remote Sensing Images Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,3,"The semantic segmentation of wide-field remote sensing images (RSIs) plays a significant role in many fields. However, due to the complexity of the content of RSIs, the dataset often has an uneven distribution of land type between different classes and large gaps in the scales of different objects. This often creates great problems for fine segmentation. To solve the issues, an unbalanced class learning network with scale-adaptive perception (UCSANet) is proposed, which can adaptively cope with multiscale objects and unbalanced classes. The design can be inserted in any convolution network easily and can enrich features without increasing too many parameters. The network groups feature and use atrous convolutions with different dilated rates on different groups to extract multiscale features while separable convolutions reduce the amount of network parameters. Then, the fusion of features between different scales is achieved through the self-attention mechanism. Furthermore, a weight map is designed to adaptively combine the predictions of two segmentation heads with cross-entropy loss and Lovasz-Softmax loss, respectively, which enable the network to focus on learning low-frequency classes without affecting high-frequency classes. Experimental results on GF-6 MSI datasets demonstrate that the proposed UCSANet performs significantly better than others and achieves multiclass segmentation more accurately.",Feature extraction,Convolution,Semantic segmentation,Remote sensing,Data mining,Head,Kernel,Deep learning,scale-adaptive,semantic segmentation,unbalanced data,"Zhang, Yuxiang",,,,,,,,,
Row_940,"Wang, Gaihua","Zhai, Qianyu","Lin, Jinheng",,,Multi-scale network for remote sensing segmentation,IET IMAGE PROCESSING,MAY 2022,3,"The semantic segmentation of remote sensing images is a critical and challenging task. How to easily and reliably segment useful information from vast remote sensing images is a significant issue. Many methods based on convolutional neural networks have been widely explored to obtain more accurate segmentation from remote sensing images. However, due to the uniqueness of remote sensing images, such as the dramatic changes in the scale of the target object, the results are not satisfactory. To solve the problem, a special network is designed: (1) Create a new backbone network. Compared with ResNet50, the proposed method extracts features of varying sizes more effectively. (2) Reduce spatial information loss. Building a hybrid location module to compensate for the position loss caused by the down-sampling operation. (3) Models with high discriminant ability. In order to improve the discrimination ability of the model, a novel auxiliary loss function is designed to constrain the distance between inter-class and intra-class. The proposed algorithm is tested on remote sensing datasets (e.g., NWPU-45, DLRSD, and WHDLD). The experimental results show that this method obtains the best results and achieves state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,
Row_941,"Zheng, Zixian","Zhang, Xueliang","Xiao, Pengfeng","Li, Zhenshi",,Integrating Gate and Attention Modules for High-Resolution Image Semantic Segmentation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,2021,20,"Semantic segmentation of high-resolution (HR) remote sensing images achieved great progress by utilizing deep convolutional neural networks (DCNNs) in recent years. However, the decrease of resolution in the feature map of DCNNs brings about the loss of spatial information and thus leads to the blurring of object boundary and misclassification of small objects. In addition, the class imbalance and the high diversity of geographic objects in HR images exacerbate the performance. To deal with the above problems, we proposed an end-to-end DCNN network named GAMNet to balance the contradiction between global semantic information and local details. An integration of attention and gate module (GAM) is specially designed to simultaneously realize multiscale feature extraction and boundary recovery. The integration module can be inserted in an encoder-decoder network with skip connection. Meanwhile, a composite loss function is designed to achieve deep supervision of GAM by adding an auxiliary loss, which can help improve the effectiveness of the integration module. The performance of GAMNet is quantitatively evaluated on the ISPRS 2-D semantic labeling datasets and achieves state-of-the-art performance in comparison with other representative methods.",Semantics,Image segmentation,Feature extraction,Decoding,Remote sensing,Spatial resolution,Logic gates,Attention module (AM),gate module (GM),high-resolution (HR) remote sensing imagery,semantic segmentation,,,,,,,,,,
Row_942,"Wang, Jiahao","Zhao, Junhao","Sun, Hong","Lu, Xiao","Huang, Jixia",Satellite Remote Sensing Identification of Discolored Standing Trees for Pine Wilt Disease Based on Semi-Supervised Deep Learning,REMOTE SENSING,DEC 2022,15,"Pine wilt disease (PWD) is the most dangerous biohazard of pine species and poses a serious threat to forest resources. Coupling satellite remote sensing technology and deep learning technology for the accurate monitoring of PWD is an important tool for the efficient prevention and control of PWD. We used Gaofen-2 remote sensing images to construct a dataset of discolored standing tree samples of PWD and selected three semantic segmentation models-DeepLabv3+, HRNet, and DANet-for training and to compare their performance. To build a GAN-based semi-supervised semantic segmentation model for semi-supervised learning training, the best model was chosen as the generator of generative adversarial networks (GANs). The model was then optimized for structural adjustment and hyperparameter adjustment. Aimed at the characteristics of Gaofen-2 images and discolored standing trees with PWD, this paper adopts three strategies-swelling prediction, raster vectorization, and forest floor mask extraction-to optimize the image identification process and results and conducts an application demonstration study in Nanping city, Fujian Province. The results show that among the three semantic segmentation models, HRNet was the optimal conventional semantic segmentation model for identifying discolored standing trees of PWD based on Gaofen-2 images and that its MIoU value was 68.36%. Additionally, the GAN-based semi-supervised semantic segmentation model GAN_HRNet_Semi improved the MIoU value by 3.10%, and its recognition segmentation accuracy was better than the traditional semantic segmentation model. The recall rate of PWD discolored standing tree monitoring in the demonstration area reached 80.09%. The combination of semi-supervised semantic segmentation technology and high-resolution satellite remote sensing technology provides new technical methods for the accurate wide-scale monitoring, prevention, and control of PWD.",pine wilt disease (PWD),semi-supervised,semantic segmentation,satellite remote sensing,accurate monitoring,,,,,,,"Wang, Shaohua","Fang, Guofei",,,,,,,,
Row_943,"Liu, Ruizhong","Luo, Tingzhang","Huang, Shaoguang","Wu, Yuwei","Jiang, Zhen",CrossMatch: Cross-View Matching for Semi-Supervised Remote Sensing Image Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,0,"Recently, weak-to-strong consistency-based methods have yielded a remarkable performance for remote sensing image segmentation. However, they are designed within a single view, which encounters the problems of unreliable pseudo-label supervision and insufficient ability to capture informative features for the segmentation of complex remote sensing data. In this article, we propose a cross-view weak-to-strong consistency-based method, which aggregates rich information from two irrelevant views. We employ two subnets for the two views to generate view-specific features while encouraging them to yield the same prediction. Within each view, we enhance the perturbation space of data at the image level and feature level for a more robust representation. To leverage the information from unlabeled data, we propose a cross-view weak-to-strong consistency scheme, which employs the pseudo-label of the weakly augmented data in one view to supervise the model training in another view, facilitating an effective information exchange across views. To avoid identical information extraction from the two views, we propose a cross-view contrastive loss to maximize the dissimilarity of the feature representations across views, which ensures that the learned complementary information from one view provides additional helpful information for the model training in another view. Finally, we propose a cross-view discrepancy-based supervised constraint by imposing a larger weight on the areas that exist discrepant predictions across views in the cross-entropy loss, allowing the model to focus more on the hard-to-classify regions of the images. Extensive experimental results on several benchmark datasets demonstrate that our method outperforms the state-of-the-art.",Remote sensing,Perturbation methods,Semantic segmentation,Data models,Training,Predictive models,Feature extraction,Sensors,Reliability,Deep learning,image segmentation,"Zhang, Hongyan",,,,,remote sensing,semi-supervised learning (SSL),,,
Row_944,"Gu, Xingjian","Yu, Supeng","Huang, Fen","Ren, Shougang","Fan, Chengcheng",Consistency Self-Training Semi-Supervised Method for Road Extraction from Remote Sensing Images,REMOTE SENSING,NOV 2024,0,"Road extraction techniques based on remote sensing image have significantly advanced. Currently, fully supervised road segmentation neural networks based on remote sensing images require a significant number of densely labeled road samples, limiting their applicability in large-scale scenarios. Consequently, semi-supervised methods that utilize fewer labeled data have gained increasing attention. However, the imbalance between a small quantity of labeled data and a large volume of unlabeled data leads to local detail errors and overall cognitive mistakes in semi-supervised road extraction. To address this challenge, this paper proposes a novel consistency self-training semi-supervised method (CSSnet), which effectively learns from a limited number of labeled data samples and a large amount of unlabeled data. This method integrates self-training semi-supervised segmentation with semi-supervised classification. The semi-supervised segmentation component relies on an enhanced generative adversarial network for semantic segmentation, which significantly reduces local detail errors. The semi-supervised classification component relies on an upgraded mean-teacher network to handle overall cognitive errors. Our method exhibits excellent performance with a modest amount of labeled data. This study was validated on three separate road datasets comprising high-resolution remote sensing satellite images and UAV photographs. Experimental findings showed that our method consistently outperformed state-of-the-art semi-supervised methods and several classic fully supervised methods.",semi-supervised,semantic segmentation,generative adversarial network,road extraction,remote sensing image,,,,,,,,,,,,,,,,
Row_945,"Li, Yazhou","Cheng, Zhiyou","Wang, Chuanjian","Zhao, Jinling","Huang, Linsheng",RCCT-ASPPNet: Dual-Encoder Remote Image Segmentation Based on Transformer and ASPP,REMOTE SENSING,JAN 2023,18,"Remote image semantic segmentation technology is one of the core research elements in the field of computer vision and has a wide range of applications in production life. Most remote image semantic segmentation methods are based on CNN. Recently, Transformer provided a view of long-distance dependencies in images. In this paper, we propose RCCT-ASPPNet, which includes the dual-encoder structure of Residual Multiscale Channel Cross-Fusion with Transformer (RCCT) and Atrous Spatial Pyramid Pooling (ASPP). RCCT uses Transformer to cross fuse global multiscale semantic information; the residual structure is then used to connect the inputs and outputs. ASPP based on CNN extracts contextual information of high-level semantics from different perspectives and uses Convolutional Block Attention Module (CBAM) to extract spatial and channel information, which will further improve the model segmentation ability. The experimental results show that the mIoU of our method is 94.14% and 61.30% on the datasets Farmland and AeroScapes, respectively, and that the mPA is 97.12% and 84.36%, respectively, both outperforming DeepLabV3+ and UCTransNet.",remote image,deep learning,semantic segmentation,CNN,multiscale feature fusion,Transformer,,,,,,,,,,,,,,,
Row_946,"Chen, Bo","Zhang, Jiahao","Zhou, Jianbang","Chen, Zhong","Yang, Tian",Semantic image segmentation network based on deep learning,,2020,1,"Semantic segmentation is one of the basic themes in computer vision. Its purpose is to assign semantic tags to each pixel of an image, which has been applied in many fields such as medical field, intelligent transportation and remote sensing image. In this paper, we use deep learning to solve the task of remote sensing semantic image segmentation. We propose an algorithm for semantic segmentation of the Attention Seg-Net network combined with SegNet and attention gate. Our proposed network can better segment vegetation, buildings, water bodies and roads in the test set of remote sensing images.",Semantic Segmentation,Attention Seg-Net,Deep Learning,Attention gate,,,,,,,,"Zhang, Yanna",,,,,,,,MIPPR 2019: AUTOMATIC TARGET RECOGNITION AND NAVIGATION,
Row_947,"Xia, Liegang","Liu, Ruiyan","Su, Yishao","Mi, Shulin","Yang, Dezhi",Crop field extraction from high resolution remote sensing images based on semantic edges and spatial structure map,GEOCARTO INTERNATIONAL,JAN 1 2024,2,"Crop field boundary extraction is crucial to remote sensing images attained to support agricultural production and planning. In recent years, deep convolutional neural networks (CNNs) have gained significant attention for edge detection tasks. Moreover, transformers have shown superior feature extraction and classification capabilities compared to CNNs due to their self-attention mechanism. We proposed a novel structure that combines full edge extraction with CNNs and enhances connectivity with transformers, consisting of three stages: a) preprocessing the training data; b) training the semantic edge and spatial structure graph models; and c) vectorizing the fusion of semantic edge and spatial structure graph outputs. To cater specifically to high-resolution remote sensing image crop-field boundary extraction, we developed a CNN model called Densification D-LinkNet. Its full-scale skip connections and edge-guided module adapted well to different crop-field boundary features. Additionally, we employed a spatial graph structure generator (Relationformer) based on object detection that directly outputs the structural graph of the crop field boundary. This method relies on good connectivity to repair fragmented edges that may appear in semantic edge detection. Through multiple experiments and comparisons with other edge-detection methods, such as BDCN, DexiNed, PidiNet, and EDTER, we demonstrated that our proposed method can achieve at least 9.77% improvement in boundary intersection over union (IoU) and 2.07% improvement in polygon IoU on two customized datasets. These results indicate the effectiveness and robustness of our approach.",High-resolution remote sensing images,image segmentation,semantic edge detection,transformer,,,,,,,,"Chen, Jun","Shen, Zhanfeng",,,,,,,,
Row_948,"Yan, Zhiyuan","Li, Junxi","Li, Xuexue","Zhou, Ruixue","Zhang, Wenkai",RingMo-SAM: A Foundation Model for Segment Anything in Multimodal Remote-Sensing Images,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,24,"The proposal of the segment anything model (SAM) has created a new paradigm for the deep-learning-based semantic segmentation field and has shown amazing generalization performance. However, we find it may fail or perform poorly on multimodal remote-sensing scenarios, especially synthetic aperture radar (SAR) images. Besides, SAM does not provide category information for objects. In this article, we propose a foundation model for multimodal remote-sensing image segmentation called RingMo-SAM, which can not only segment anything in optical and SAR remote-sensing data, but also identify object categories. First, a large-scale dataset containing millions of segmentation instances is constructed by collecting multiple open-source datasets in this field to train the model. Then, by constructing an instance-type and terrain-type category-decoupling mask decoder (CDMDecoder), the categorywise segmentation of various objects is achieved. In addition, a prompt encoder embedded with the characteristics of multimodal remote-sensing data is designed. It not only supports multibox prompts to improve the segmentation accuracy of multiobjects in complicated remote-sensing scenes, but also supports SAR characteristics prompts to improve the segmentation performance on SAR images. Extensive experimental results on several datasets including iSAID, ISPRS Vaihingen, ISPRS Potsdam, AIR-PolSAR-Seg, and so on have demonstrated the effectiveness of our method.",Remote sensing,Task analysis,Semantic segmentation,Feature extraction,Radar polarimetry,Training,Adaptation models,Multimodal remote-sensing images,prompt learning,segment anything model (SAM),semantic segmentation,"Feng, Yingchao","Diao, Wenhui","Fu, Kun","Sun, Xian",,,,,,
Row_949,"Zhang, Zhen","Huang, Xin","Li, Jiayi",,,DWin-HRFormer: A High-Resolution Transformer Model With Directional Windows for Semantic Segmentation of Urban Construction Land,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,17,"In this article, a deep neural network for semantic segmentation of high-resolution remote sensing images is proposed for urban construction land classification. The network follows a high-resolution network (HRNet) architecture. Specifically, a directional self-attention on the paths of different resolutions is proposed, aiming to correct the directional bias caused by the attention of strip windows during the model learning, while also reducing the computational complexity, and allowing the model to improve both the accuracy and the speed. At the end of the network, a distributed alignment module with spatial information is constructed to train additional learnable parameters, to adjust the biased decision boundaries through a two-stage learning strategy, and alleviate the problem of accuracy degradation due to the unbalanced training data. We tested the proposed method and compared it with the current state-of-the-art (SOTA) semantic segmentation methods on the Luojia-fine-grained land cover (FGLC) dataset and the Wuhan Dense Labeling Dataset (WHDLD), and the proposed one obtained the best performance. We also verified the effectiveness of each component of the network through ablation experiments.",Transformers,Semantic segmentation,Computational modeling,Task analysis,Windows,Monitoring,Remote sensing,Deep learning,remote sensing imagery,semantic segmentation,transformer,,,,,,urban construction land,,,,
Row_950,"Pang, Shuai","Gao, Lianxue",,,,Multihead attention mechanism guided ConvLSTM for pixel-level segmentation of ocean remote sensing images,MULTIMEDIA TOOLS AND APPLICATIONS,JUL 2022,5,"Semantic segmentation of ocean remote sensing images classifies each pixel in the image according to the ocean background and island type, and is an important research direction in the field of remote sensing image processing. Due to large differences in the scale of islands in ocean remote sensing images and the complexity of island boundaries, it is difficult to accurately extract features of ocean remote sensing images, which makes it difficult to accurately segment ocean remote sensing images. Convolutional neural networks have gradually become the mainstream algorithm in the field of image processing due to their autonomous hierarchical extraction of image features. In this paper, the MAGC-Net neural network model, which is based on the multihead attention mechanism and ConvLSTM, is used to segment ocean remote sensing images to improve the accuracy of semantic segmentation. First, shallow features are obtained via multiscale convolution, and multiple weights are assigned to features by the multihead attention mechanism (global, local, maximum). Then, the semantic relationship between the features is described through the integrated ConvLSTM module, and deep features are generated. Finally, deep features are filtered through residual blocks, reducing redundant features and improving segmentation accuracy. Experimental results with the NWPU-RESISC45 dataset demonstrate the effectiveness and robustness of the proposed algorithm.",Ocean remote sensing,Deep learning,Multihead attention mechanism,Multiscale convolutional,,,,,,,,,,,,,,,,,
Row_951,"Niu, Mengjia","Zhang, Yongjun","Yang, Gang","Wang, Zewei","Liu, Junwen",Semantic segmentation for remote sensing images via dense feature extraction and companion loss neural network,INTERNATIONAL JOURNAL OF REMOTE SENSING,NOV 17 2021,2,"Semantic segmentation models with good performance are crucial for the practical application of high-resolution remote-sensing images (RSI). Compared with nature images, in most cases the RSI dataset has the problem of unbalanced sample distribution between classes and unbalanced target size ratio. Using semantic segmentation to pixel-wise classify and identification RSI can solve this problem to some extent. At present, most semantic segmentation models based on mainstream networks solve these problems from the object scale and super-pixel perspective, whereas the accuracy still needs to be improved. To enhance the quality of the predicted feature maps, the Dense-Inception Net (D-INet) model is proposed based on the idea of DenseNet feature reuse and combined with the attention mechanism, which enables the network to maintain depth while widening the width to obtain more advanced semantic information. The connection of contextual information is strengthened by connecting RFB multi-scale modules at the shallow level, and shallow features with more spatial features are extracted for feature fusion with the decoder. To further lift the segmentation accuracy, a companion loss is introduced in the encoder, and the model is trained to have better segmentation performance for small sample objects by adaptively adjusting the loss coefficients. Experimental results show that the proposed method significantly increases the accuracy and mean Intersection Over Union (mIOU) scores.",,,,,,,,,,,,"Cui, Zhongwei",,,,,,,,,
Row_952,"Pang, Shiyan","Lan, Jingjing","Zuo, Zhiqi","Chen, Jia",,SFGT-CD: Semantic Feature-Guided Building Change Detection From Bitemporal Remote-Sensing Images With Transformers,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,6,"High-resolution remote-sensing-image change detection is widely used in urban dynamic monitoring, geographic information updating, natural disaster monitoring, illegal building investigation, and land resource surveys. Common change-detection algorithms are mainly implemented in a fully supervised manner that relies on a large number of high-quality samples. Compared with a building change-detection dataset, a building semantic-segmentation dataset is easier to accumulate and obtain. Making full use of this semantic information in the design of a building change-detection network can effectively reduce the sample size required to train a change-detection model. In view of this, a semantic feature-guided Siamese change-detection framework is devised in this letter. The framework effectively exploits the prior information of building semantic features and uses the popular transformer structure to improve the change analysis module. The results of extensive experiments on two public datasets show that the framework is more accurate than the other state-of-the-art change detection algorithms and can effectively reduce the dependence of data on change detection samples in the model training process.",Feature extraction,Semantics,Transformers,Convolutional neural networks,Training,Remote sensing,Decoding,Change detection,high-resolution optical remote-sensing images,prior semantic information,transformers,,,,,,,,,,
Row_953,"Yin, Hao","Zhang, Chengming","Han, Yingjuan","Qian, Yonglan","Xu, Tao",Improved semantic segmentation method using edge features for winter wheat spatial distribution extraction from Gaofen-2 images,JOURNAL OF APPLIED REMOTE SENSING,MAY 25 2021,3,"In the final feature map obtained using a convolutional neural network for remote sensing image segmentation, there are great differences between the feature values of the pixels near the edge of the block and those inside the block; ensuring consistency between these feature values is the key to improving the accuracy of segmentation results. The proposed model uses an edge feature branch and a semantic feature branch called the edge assistant feature network (EFNet). The EFNET model consists of one semantic branch, one edge branch, one shared decoder, and one classifier. The semantic branch extracts semantic features from remote sensing images, whereas the edge branch extracts edge features from remote sensing images and edge images. In addition, the two branches extract five-level features through five sets of feature extraction units. The shared decoder sets up five levels of shared decoding units, which are used to further integrate edge features and deep semantic features. This strategy can reduce the feature differences between the edge pixels and the inner pixels of the object, obtaining a per-pixel feature vector with high inter-class differentiation and intra-class consistency. Softmax is used as the classifier to generate the final segmentation result. We selected a representative winter wheat region in China (Feicheng City) as the study area and established a dataset for experiments. The comparison experiment included three original models and two models modified by adding edge features: SegNet, UNet, and ERFNet, and edge-UNet and edge-ERFNet, respectively. EFNet's recall (91.01%), intersection over union (81.39%), and F1-Score (91.68%) were superior to those of the other methods. The results clearly show that EFNET improves the accuracy of winter wheat extraction from remote sensing images. This is an important basis not only for crop monitoring, yield estimation, and disaster assessment but also for calculating land carrying capacity and analyzing the comprehensive production capacity of agricultural resources. (C) The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License.",convolutional neural network,remote sensing,semantic segmentation,winter wheat,edge feature,Gaofen-2,,,,,,"Zhang, Ziyun","Kong, Ailing",,,,,,,,
Row_954,"Yang, Junliang","Chen, Guorong","Huang, Jiaming","Ma, Denglong","Liu, Jingcheng",GLE-net: global-local information enhancement for semantic segmentation of remote sensing images,SCIENTIFIC REPORTS,OCT 25 2024,0,"Remote sensing (RS) images contain a wealth of information with expansive potential for applications in image segmentation. However, Convolutional Neural Networks (CNN) face challenges in fully harnessing the global contextual information. Leveraging the formidable capabilities of global information modeling with Swin-Transformer, a novel RS images segmentation model with CNN (GLE-Net) was introduced. This integration gives rise to a revamped encoder structure. The subbranch initiates the process by extracting features at varying scales within the RS images using the Multiscale Feature Fusion Module (MFM), acquiring rich semantic information, discerning localized finer features, and adeptly handling occlusions. Subsequently, Feature Compression Module (FCM) is introduced in main branch to downsize the feature map, effectively reducing information loss while preserving finer details, enhancing segmentation accuracy for smaller targets. Finally, we integrate local features and global features through Spatial Information Enhancement Module (SIEM) for comprehensive feature modeling, augmenting the segmentation capabilities of model. We performed experiments on public datasets provided by ISPRS, yielding notably remarkable experimental outcomes. This underscores the substantial potential of our model in the realm of RS image segmentation within the context of scientific research.",Remote sening,Multiscale feature,Swin-transformer convolutional neural networks,,,,,,,,,"Zhu, Huazheng",,,,,,,,,
Row_955,"Yang, Peiqi","Wang, Mingjun","Yuan, Hao","He, Ci","Cong, Li",Using contour loss constraining residual attention U-net on optical remote sensing interpretation,VISUAL COMPUTER,SEP 2023,5,"Using deep learning in remote sensing interpretation could reduce a lot of human and material costs. Semantic segmentation is the main method for this task. It can automatically outline the objects and it has recently achieved great success in remote sensing images. However, in the appliance of remote sensing interpretation, the accuracy of contour largely determines the evaluation of remote sensing interpretation. Though the current loss functions reflect the segmentation performance, they could not guide the model to optimize itself toward a more precise contour. This paper proposed an exactly defined contour loss (CL) for remote sensing interpretation with Residual Attention U-Net (RA U-Net) as the main framework. The RA U-Net uses the residual attention module as the skip connection layer. It enhances the judgment of U-Net. In CL, image processing methods are used to extract the contours of the foreground. And elements-sum and elements-subtract operations are used to transfer the contour information to a matrix of the same size as label images. Then, these matrices would be the weights for CE. By assigning different weights for different elements in different regions, this function will guide the model to reach a balance between accurate segmentation results and precise contours. The experiment on open datasets shows a good performance. The proposed model was also trained on the Construction Disturbance Dataset collected from Jiang Xi Province, China. The dataset was labeled manually. The evaluation enhanced a lot on the Construction Disturbance Dataset and the IoU on two datasets increased 1% to 2% when using CL as the loss function. This paper also compared the proposed method with other state-of-the-art methods and the results showed extensive effectiveness.",Remote sensing,Image interpretation,Loss function,Semantic segmentation,U-Net,Residual attention mechanism,,,,,,,,,,,,,,,
Row_956,"Canedo, Daniel","Fonte, Joao","Dias, Rita","do Pereiro, Tiago","Goncalves-Seco, Luis",Automated Detection of Hillforts in Remote Sensing Imagery With Deep Multimodal Segmentation,ARCHAEOLOGICAL PROSPECTION,SEP 2024,0,"Recent advancements in remote sensing and artificial intelligence can potentially revolutionize the automated detection of archaeological sites. However, the challenging task of interpreting remote sensing imagery combined with the intricate shapes of archaeological sites can hinder the performance of computer vision systems. This work presents a computer vision system trained for efficient hillfort detection in remote sensing imagery. Equipped with an adapted multimodal semantic segmentation model, the system integrates LiDAR-derived LRM images and aerial orthoimages for feature fusion, generating a binary mask pinpointing detected hillforts. Post-processing includes margin and area filters to remove edge inferences and smaller anomalies. The resulting inferences are subjected to hard positive and negative mining, where expert archaeologists classify them to populate the training data with new samples for retraining the segmentation model. As the computer vision system is far more likely to encounter background images during its search, the training data are intentionally biased towards negative examples. This approach aims to reduce the number of false positives, typically seen when applying machine learning solutions to remote sensing imagery. Northwest Iberia experiments witnessed a drastic reduction in false positives, from 5678 to 40 after a single hard positive and negative mining iteration, yielding a 99.3% reduction, with a resulting F-1 score of 66%. In England experiments, the system achieved a 59% F1 score when fine-tuned and deployed countrywide. Its scalability to diverse archaeological sites is demonstrated by successfully detecting hillforts and other types of enclosures despite their typical complex and varied shapes. Future work will explore archaeological predictive modelling to identify regions with higher archaeological potential to focus the search, addressing processing time challenges.",computer vision,hillforts,LiDAR,multimodal semantic segmentation,orthoimagery,transformer,,,,,,"Vazquez, Marta","Georgieva, Petia","Neves, Antonio J. R.",,,,,,,
Row_957,"Bi, Hanbo","Feng, Yingchao","Yan, Zhiyuan","Mao, Yongqiang","Diao, Wenhui",Not Just Learning From Others but Relying on Yourself: A New Perspective on Few-Shot Segmentation in Remote Sensing,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,8,"Few-shot segmentation (FSS) is proposed to segment unknown class targets with just a few annotated samples. Most current FSS methods follow the paradigm of mining the semantics from the support images to guide the query image segmentation. However, such a pattern of ""learning from others"" struggles to handle the extreme intraclass variation, preventing FSS from being directly generalized to remote sensing scenes. To bridge the gap of intraclass variance, we develop a dual-mining network named DMNet for cross-image mining and self-mining, meaning that it no longer focuses solely on support images but pays more attention to the query image itself. Specifically, we propose a class-public region mining (CPRM) module to effectively suppress irrelevant feature pollution by capturing the common semantics between the support-query image pair. The class-specific region mining (CSRM) module is then proposed to continuously mine the class-specific semantics of the query image itself in a ""filtering"" and ""purifying"" manner. In addition, to prevent the coexistence of multiple classes in remote sensing scenes from exacerbating the collapse of FSS generalization, we also propose a new known-class metasuppressor (KMS) module to suppress the activation of known-class objects in the sample. Extensive experiments on the iSAID and LoveDA remote sensing datasets have demonstrated that our method sets the state of the art with a minimum number of model parameters. Significantly, our model with the backbone of Resnet-50 achieves the mean Intersection over Union (mIoU) of 49.58% and 51.34% on iSAID under 1- and 5-shot settings, outperforming the state-of-the-art method by 1.8% and 1.12%, respectively. The code is publicly available at https://github.com/HanboBizl/DMNet/.",Few-shot learning,few-shot segmentation (FSS),prototype learning,remote sensing,semantic segmentation,,,,,,,"Wang, Hongqi","Sun, Xian",,,,,,,,
Row_958,"Yang, Liangzhe","Zi, Wenjie","Chen, Hao","Peng, Shuang",,DRE-Net: A Dynamic Radius-Encoding Neural Network with an Incremental Training Strategy for Interactive Segmentation of Remote Sensing Images,REMOTE SENSING,FEB 2023,5,"Semantic segmentation of remote sensing (RS) images, which is a fundamental research topic, classifies each pixel in an image. It plays an essential role in many downstream RS areas, such as land-cover mapping, road extraction, traffic monitoring, and so on. Recently, although deep-learning-based methods have shown their dominance in automatic semantic segmentation of RS imagery, the performance of these existing methods has relied heavily on large amounts of high-quality training data, which are usually hard to obtain in practice. Moreover, human-in-the-loop semantic segmentation of RS imagery cannot be completely replaced by automatic segmentation models, since automatic models are prone to error in some complex scenarios. To address these issues, in this paper, we propose an improved, smart, and interactive segmentation model, DRE-Net, for RS images. The proposed model facilitates humans' performance of segmentation by simply clicking a mouse. Firstly, a dynamic radius-encoding (DRE) algorithm is designed to distinguish the purpose of each click, such as a click for the selection of a segmentation outline or for fine-tuning. Secondly, we propose an incremental training strategy to cause the proposed model not only to converge quickly, but also to obtain refined segmentation results. Finally, we conducted comprehensive experiments on the Potsdam and Vaihingen datasets and achieved 9.75% and 7.03% improvements in NoC95 compared to the state-of-the-art results, respectively. In addition, our DRE-Net can improve the convergence and generalization of a network with a fast inference speed.",interactive segmentation,dynamic radius encoding,incremental learning,remote sensing,,,,,,,,,,,,,,,,,
Row_959,"Pang, Kai","Weng, Liguo","Zhang, Yonghong","Liu, Jia","Lin, Haifeng",SGBNet: An Ultra Light-weight Network for Real-time Semantic Segmentation of Land Cover,INTERNATIONAL JOURNAL OF REMOTE SENSING,AUG 18 2022,19,"Designing a lightweight and robust real-time land cover segmentation algorithm is an important task for land resource applications. In recent years, with the development of edge computing and the increasing resolution of remote sensing images, the huge amount of calculations and parameters have restricted the efficiency of real-time semantic segmentation. Therefore, the emergence of lightweight CNN (convolutional neural network) has accelerated the development of real-time semantic segmentation of land cover. However, nowadays, the time and space span of aerial images is getting larger and larger, resulting in the loss of details and blurred edges of lightweight CNN. Therefore, the existing lightweight CNN model has low segmentation accuracy and poor generalization ability in real-time land cover segmentation tasks. In order to solve the problem of lightweight network in this respect, this paper proposes a Semantics Guided Bottleneck Network (SGBNet) to balance accuracy and reasoning speed. First, a basic unit and the overall network structure are redesigned to increase the overall reasoning efficiency of the model. The model can efficiently extract spatial details and contextual semantic information. Then, the model optimizes the lightweight network and realizes the extraction of details and contextual semantic information. Finally, a lightweight attention mechanism is used to restore high-resolution pixel-level features. The results of comparative experiments show that the method in this paper has a higher segmentation accuracy than existing models while achieving lightweight.",Land cover,lightweight,real-time semantic segmentation,remote sensing image,deep learning,,,,,,,"Xia, Min",,,,,,,,,
Row_960,"Yang, Qinchen","Liu, Man","Zhang, Zhitao","Yang, Shuqin","Ning, Jifeng",Mapping Plastic Mulched Farmland for High Resolution Images of Unmanned Aerial Vehicle Using Deep Semantic Segmentation,REMOTE SENSING,SEP 1 2019,30,"With increasing consumption, plastic mulch benefits agriculture by promoting crop quality and yield, but the environmental and soil pollution is becoming increasingly serious. Therefore, research on the monitoring of plastic mulched farmland (PMF) has received increasing attention. Plastic mulched farmland in unmanned aerial vehicle (UAV) remote images due to the high resolution, shows a prominent spatial pattern, which brings difficulties to the task of monitoring PMF. In this paper, through a comparison between two deep semantic segmentation methods, SegNet and fully convolutional networks (FCN), and a traditional classification method, Support Vector Machine (SVM), we propose an end-to-end deep-learning method aimed at accurately recognizing PMF for UAV remote sensing images from Hetao Irrigation District, Inner Mongolia, China. After experiments with single-band, three-band and six-band image data, we found that deep semantic segmentation models built via single-band data which only use the texture pattern of PMF can identify it well; for example, SegNet reaching the highest accuracy of 88.68% in a 900 nm band. Furthermore, with three visual bands and six-band data (3 visible bands and 3 near-infrared bands), deep semantic segmentation models combining the texture and spectral features further improve the accuracy of PMF identification, whereas six-band data obtains an optimal performance for FCN and SegNet. In addition, deep semantic segmentation methods, FCN and SegNet, due to their strong feature extraction capability and direct pixel classification, clearly outperform the traditional SVM method in precision and speed. Among three classification methods, SegNet model built on three-band and six-band data obtains the optimal average accuracy of 89.62% and 90.6%, respectively. Therefore, the proposed deep semantic segmentation model, when tested against the traditional classification method, provides a promising path for mapping PMF in UAV remote sensing images.",plastic mulched farmland,fully convolutional networks,unmanned aerial vehicle remote sensing image,deep semantic segmentation,,,,,,,,"Han, Wenting",,,,,,,,,
Row_961,"Bona, Daniel Sande","Murni, Aniati","Mursanto, Petrus",,,Semantic Segmentation And Segmentation Refinement Using Machine Learning Case Study: Water Turbidity Segmentation,,2019,1,"Classical methods for image segmentation such as pixel thresholding, clustering, region growing, maximum likelihood have been used regularly and relied on for a long time. However, these classical methods have limitations, particularly on images where there are many overlapping pixel values between features, which is common in remote sensing images. The advent of machine learning, in particular, deep learning in computer vision and image analysis, has gained interest in the remote sensing field. Current deep learning architecture has been able to achieve high accuracy for image recognition, object detection, and segmentation. This study performed image segmentation on the coastal area with high water turbidity using Landsat-8 images. Currently, the standard tool to derive water turbidity data from Landsat-8 images is the level-2 plugin of SEADAS software. However, due to its rigorous processing method, the processing time using SEADAS Level-2 Plugin is quite long; for example, processing one Landsat-8 image took around 8 hours. As a consequence, the amount of time needed to process multiple images is increasing. Deep learning has advantages once the model trained, the inference or prediction process is quite fast. Therefore it has the potential to be used as a complementary tool to predict and segment high turbidity areas, because in deep learning. In this study, we implemented U-Net architecture with ResNet connection and used Generative-Adversarial Network (GAN) to refined segmentation results.",Image segmentation,remote sensing,deep learning,machine learning,CNN,GAN,,,,,,,,,,,,,,2019 IEEE INTERNATIONAL CONFERENCE ON AEROSPACE ELECTRONICS AND REMOTE SENSING TECHNOLOGY (ICARES 2019),
Row_962,"Huang, Bohao","Collins, Leslie M.","Bradbury, Kyle","Malof, Jordan M.",,DEEP CONVOLUTIONAL SEGMENTATION OF REMOTE SENSING IMAGERY: A SIMPLE AND EFFICIENT ALTERNATIVE TO STITCHING OUTPUT LABELS,,2018,9,"In this work we consider the application of convolutional neural networks (CNNs) for the semantic segmentation of remote sensing imagery (e.g., aerial color or hyperspectral imagery). In segmentation the goal is to provide a dense pixel-wise labeling of the input imagery. However, remote sensing imagery is usually stored in the form of very large images, called ""tiles"", which are too large to be segmented directly using most CNNs and their associated hardware. During label inference (i.e., obtaining labels for a new large tile) smaller sub-images, called ""patches"", are extracted uniformly over a tile and the resulting label maps are ""stitched"" (or concatenated) to create a tile-sized label map. This approach suffers from computational inefficiency and risks of discontinuities at the boundaries between the output of individual patches. In this work we propose a simple alternative approach in which the input size of the CNN is dramatically increased only during label inference. We evaluate the performance of the proposed approach against a standard stitching approach using two popular segmentation CNN models on the INRIA building labeling dataset. The results suggest that the proposed approach substantially reduces label inference time, while also yielding modest overall label accuracy increases. This approach also contributed to our winning entry (overall performance) in the INRIA building labeling competition.",semantic segmentation,convolutional neural networks,deep learning,aerial imagery,building detection,,,,,,,,,,,,,,,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,
Row_963,"Castillo-Navarro, J.","Audebert, N.","Boulch, A.","Le Saux, B.","Lefevre, S.",What Data are needed for Semantic Segmentation in Earth Observation?,,2019,2,"This paper explores different aspects of semantic segmentation of remote sensing data using deep neural networks. Learning with deep neural networks was revolutionized by the creation of ImageNet. Remote sensing benefited of these new techniques, however Earth Observation (EO) datasets remain small in comparison. In this work, we investigate how we can progress towards the ImageNet of remote sensing. In particular, two questions are addressed in this paper. First, how robust are existing supervised learning strategies with respect to data volume? Second, which properties are expected from a large-scale EO dataset? The main contributions of this work are: (i) a strong robustness analysis of existing supervised learning strategies with respect to remote sensing data, (ii) the introduction of a new, large-scale dataset named MiniFrance.",Deep Learning,Supervised Learning,Semantic Segmentation,Land Use/Land Cover Mapping,,,,,,,,,,,,,,,,2019 JOINT URBAN REMOTE SENSING EVENT (JURSE),
Row_964,"Wang, Chunshan","Zhu, Penglei","Yang, Shuo","Zhang, Lijie",,A Semantic Segmentation Method for Winter Wheat in North China Based on Improved HRNet,AGRONOMY-BASEL,NOV 2024,0,"Winter wheat is one of the major crops for global food security. Accurate statistics of its planting area play a crucial role in agricultural policy formulation and resource management. However, the existing semantic segmentation methods for remote sensing images are subjected to limitations in dealing with noise, ambiguity, and intra-class heterogeneity, posing a negative impact on the segmentation performance of the spatial distribution and area of winter wheat fields in practical applications. In response to the above challenges, we proposed an improved HRNet-based semantic segmentation model in this paper. First, this model incorporates a semantic domain module (SDM), which improves the model's precision of pixel-level semantic parsing and reduces the interference from noise through multi-confidence scale class representation. Second, a nested attention module (NAM) is embedded, which enhances the model's capability of recognizing correct correlations in pixel classes. The experimental results show that the proposed model achieved a mean intersection over union (mIoU) of 80.51%, a precision of 88.64%, a recall of 89.14%, an overall accuracy (OA) of 90.12%, and an F1-score of 88.89% on the testing set. Compared to traditional methods, our model demonstrated better segmentation performance in winter wheat semantic segmentation tasks. The achievements of this study not only provide an effective tool and technical support for accurately measuring the area of winter wheat fields, but also have important practical value and profound strategic significance for optimizing agricultural resource allocation and achieving precision agriculture.",remote sensing image,winter wheat,semantic segmentation,HRNet,semantic domain optimization,nested attention optimization,,,,,,,,,,,,,,,
Row_965,"Zhang, Xiaolu","Wang, Zhaoshun","Wei, Anlei",,,Multiscale Cascaded Network for the Semantic Segmentation of High-Resolution Remote Sensing Images,CANADIAN JOURNAL OF REMOTE SENSING,JAN 2 2023,0,"As remote sensing images have complex backgrounds and varying object sizes, their semantic segmentation is challenging. This study proposes a multiscale cascaded network (MSCNet) for semantic segmentation. The resolutions employed with respect to the input remote sensing images are 1, 1/2, and 1/4, which represent high, medium, and low resolutions. First, 3 backbone networks extract features with different resolutions. Then, using a multiscale attention network, the fused features are input into the dense atrous spatial pyramid pooling network to obtain multiscale information. The proposed MSCNet introduces multiscale feature extraction and attention mechanism modules suitable for remote sensing land-cover classification. Experiments are performed using the Deepglobe, Vaihingen, and Potsdam datasets; the results are compared with those of the existing classical semantic segmentation networks. The findings indicate that the mean intersection over union (mIoU) of the MSCNet is 4.73% higher than that of DeepLabv3+ with the Deepglobe datasets. For the Vaihingen datasets, the mIoU of the MSCNet is 15.3%, and 6.4% higher than those of a segmented network (SegNet), and DeepLabv3+, respectively. For the Potsdam datasets, the mIoU of the MSCNet is higher than those of a fully convolutional network, Res-U-Net, SegNet, and DeepLabv3+ by 11.18%, 5.89%, 4.78%, and 3.03%, respectively.Comme les images de teledetection ont des arriere-plans complexes et des tailles d'objets variables, leur segmentation semantique est difficile. Cette etude propose un reseau multi-echelle en cascade (MSCNet) pour la segmentation semantique. Les resolutions utilisees par rapport aux images de teledetection d'entree sont 1, 1/2, et 1/4, representant les resolutions haute, moyenne et basse. Tout d'abord, trois reseaux federateurs extraient les caracteristiques avec des resolutions differentes. Ensuite, a l'aide d'un reseau d'attention multi-echelle, les caracteristiques fusionnees sont entrees dans le reseau de mise en commun des pyramides spatiales denses et a trous pour obtenir des informations multi-echelles. Le MSCNet propose introduit des modules multi-echelles d'extraction de caracteristiques et de mecanismes d'attention adaptes a la classification de la couverture terrestre par teledetection. Les experiences sont realisees a l'aide des ensembles de donnees Deepglobe, Vaihingen et Potsdam. Les resultats sont compares a ceux des reseaux de segmentation semantique classique existants. Les resultats indiquent que l'intersection moyenne sur l'union (mIoU) du MSCNet est superieure par 4,73% a celle de DeepLabv3+ avec les ensembles de donnees Deepglobe. Pour les jeux de donnees Vaihingen, le mIoU du MSCNet est superieur par 15,3% a celui d'un reseau segmente (SegNet) et par 6,4% a celui de DeepLabv3+. Pour les donnees de Potsdam, le mIoU du MSCNet est superieur a ceux du reseau entierement convolutif, de Res-U-Net, de SegNet et de DeepLabv3+ par 11,18%, 5,89%, 4,78%, et 3,03%, respectivement.",,,,,,,,,,,,,,,,,,,,,
Row_966,"Hu, Qiongqiong","Wu, Yuechao","Li, Ying",,,Semi-supervised semantic labeling of remote sensing images with improved image-level selection retraining,ALEXANDRIA ENGINEERING JOURNAL,MAY 2024,3,"In recent years, image semantic segmentation technology has developed rapidly, but image annotation usually requires a significant amount of human and financial resources, especially for remote sensing image annotation, which can be expensive and sometimes even unaffordable. To address this issue, this paper integrates the idea of curriculum learning into the self-training method and screens reliable pseudo-labels through computing imagelevel confidence, significantly reducing the confirmation error problem. Furthermore, the semi-supervised model in this paper combines implicit semantic enhancement with strong data augmentation, which can reduce the coupling between the teacher model and the student model's prediction distribution and enhance the model's robustness. Finally, the proposed semi-supervised method is experimentally verified using the ISPRS competition dataset and compared with existing state -of -the -art (SOTA) methods. Experimental results show that the proposed semi-supervised segmentation method achieves higher segmentation accuracy compared to self-training methods. Moreover, despite not using iterative training to simplify the training process, the proposed method still yields satisfactory segmentation results.",Deep convolutional neural networks,Remote sensing images,Semantic labeling,Semi-supervised learning,,,,,,,,,,,,,,,,,
Row_967,"Park, S","Song, AH",,,,Shoreline Change Analysis with Deep Learning Semantic Segmentation Using Remote Sensing and GIS Data,KSCE JOURNAL OF CIVIL ENGINEERING,FEB 2024,3,"Shoreline management is essential for navigation, coastal resource management, and coastal planning and development. Shoreline change detection is vital for shoreline monitoring; however, traditional methods used for such detection are laborious and have limited accuracy. An approach that integrates remote sensing imagery and geographic information systems (GISs) is proposed herein to simultaneously identify shoreline changes and perform grid-level visualization for updating shoreline data. The integrated approach uses deep learning-based segmentation networks and water indexes to accurately classify land and sea in remote sensing images. Transfer learning was used to address the issue of insufficient data, wherein weights trained on a large open dataset were applied to the target area. The segmentation results were compared with existing shoreline GIS data to identify the areas experiencing shoreline changes. Grid-level visualization enhanced the identification of regions requiring flexible data updates and investigation efficiency by focusing on specific areas. The proposed approach accurately detected shoreline changes, albeit with some errors of commission, predominantly in regions featuring intricate shorelines and small clusters of islands. The proposed approach offers efficient solutions for shoreline change detection, with potential applications in coastal management, environmental science, urban planning, and coastal hazard assessment.",,,,,,,,,,,,,,,,,,,,,
Row_968,"Nguyen, Gia-Vuong","Huynh-The, Thien",,,,Enhancing Aerial Semantic Segmentation With Feature Aggregation Network for DeepLabV3+,IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,2024,0,"As a cutting-edge deep encoder-decoder architecture, DeepLabV3+ has been realized as a cutting-edge solution for image segmentation, especially aerial semantic segmentation in remote sensing applications. This is because of an atrous spatial pyramid pooling (ASPP) block deployed in its encoder with multiple atrous convolutional layers to enrich diversified feature extraction and learning efficiency. However, the DeepLabV3+ encoder-decoder architecture has some limitations, including the lack of information during the upsampling process and some inappropriate customizations that cause incorrect segmentation. To address these shortcomings, we introduce an efficient architecture with a novel feature aggregation network (FAN), which facilitates the extraction of features across multiple scales and stages. Concurrently, we apply some adaptive upgrades to the ASPP block, involving a new set of dilation factors that are adept at accommodating low-resolution inputs. Through simulations, we demonstrate the effectiveness and generalizability of our improved model by evaluating it with different backbones and dilation rates. In addition, compared with recent deep segmentation models, our improved model is superior in terms of mean $F1$ score (mF1) by at least 1.32%-6.34% and 0.98%-5.61% on the UAVid and Vaihingen datasets, respectively.",Feature extraction,Fans,Convolution,Decoding,Semantic segmentation,Accuracy,Semantics,Aerial image semantic segmentation,computer vision,deep learning (DL),remote sensing,,,,,,,,,,
Row_969,"Moradkhani, Kaveh","Fathi, Abdolhossein",,,,Segmentation of waterbodies in remote sensing images using deep stacked ensemble model,APPLIED SOFT COMPUTING,JUL 2022,8,"Identifying surface water resources is considered as one of the principal applications of remote sensing image analysis that plays a crucial role in controlling optimal use of these resources, and preventing floods and crises such as drought. Traditional machine learning methods for extracting waterbodies require complex spectral analysis and selection of features based on previous knowledge. Although applying deep learning-based approaches, which has been considered in recent years, has eliminated the necessity of extracting manual features, they require too many training data and computational resources to achieve high performance. Consequently, each presented deep architecture can detect some of the existing patterns in the predefined conditions. This paper trains and optimizes three robust deep architectures, presented in various fields, using surface water data, and combines their results to achieve a robust model for detecting surface water. To this end, a deep hybrid architecture called ""deep stacked ensemble model "" is employed on the outputs of three independent deep sub-models and extracts the final segmentation mask of the water areas more accurately. We evaluated our proposed model on a water body detection dataset provided by artificial intelligence crowd landsat (AIcrowd(1) LNDST) challenge. The proposed technique improves the semantic segmentation performance and surpasses state-of-the-art results. (C) 2022 Elsevier B.V. All rights reserved.",Waterbody segmentation,Remote sensing image analysis,Deep stacked ensemble model,U-net,Deep learning,,,,,,,,,,,,,,,,
Row_970,"Chen, Wei","Wang, Qingpeng","Wang, Dongliang","Xu, Yameng","He, Yingxuan",A lightweight and scalable greenhouse mapping method based on remote sensing imagery,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,DEC 2023,5,"Seeking a low-cost, high-efficiency greenhouse mapping technology has immense significance. While greenhouse extraction methods using deep learning have been proposed, the challenge of extracting dense small objects remains an unresolved problem. The inherent downscaling strategy in general-purpose semantic segmentation (SS) models renders them unsuitable for such tasks. In contrast, the dramatically increasing computational complexity associated with this problem may result in an unaffordable cost for consumer-level applications. To address the aforementioned challenges, this study presents a novel greenhouse mapping model based on remote sensing (RS) images, which not only exhibits high precision and robust generalization capabilities but also offers significant lightweight advantages. To meet broader needs, we also provide corresponding customizable and scalable rules that allow for a trade-off between accuracy and speed. To evaluate the performance of our model, we select several representative works to conduct benchmark experiments on a self-annotated dataset. The results demonstrate that our method can provide more powerful visual representations for greenhouse segmentation with minimal cost. Compared to the control group, the proposed method achieves an mIoU improvement of 1.116 %-10.77 % using only 3.282 M parameters, while maintaining a considerable inference speed. Code will be available at: https://github.com/W-qp/EGENet.git",Greenhouses,Deep learning,Semantic segmentation,Remote sensing imagery,Low-cost,,,,,,,"Yang, Lan","Tang, Hongzhao",,,,,,,,
Row_971,"Liu, Zhiheng","Chen, Xuemei","Zhou, Suiping","Yu, Hang","Guo, Jianhua",DUPnet: Water Body Segmentation with Dense Block and Multi-Scale Spatial Pyramid Pooling for Remote Sensing Images,REMOTE SENSING,NOV 2022,10,"Water body segmentation is an important tool for the hydrological monitoring of the Earth. With the rapid development of convolutional neural networks, semantic segmentation techniques have been used on remote sensing images to extract water bodies. However, some difficulties need to be overcome to achieve good results in water body segmentation, such as complex background, huge scale, water connectivity, and rough edges. In this study, a water body segmentation model (DUPnet) with dense connectivity and multi-scale pyramidal pools is proposed to rapidly and accurately extract water bodies from Gaofen satellite and Landsat 8 OLI (Operational Land Imager) images. The proposed method includes three parts: (1) a multi-scale spatial pyramid pooling module (MSPP) is introduced to combine shallow and deep features for small water bodies and to compensate for the feature loss caused by the sampling process; (2) dense blocks are used to extract more spatial features to DUPnet's backbone, increasing feature propagation and reuse; (3) a regression loss function is proposed to train the network to deal with the unbalanced dataset caused by small water bodies. The experimental results show that the F1, MIoU, and FWIoU of DUPnet on the 2020 Gaofen dataset are 97.67%, 88.17%, and 93.52%, respectively, and on the Landsat River dataset, they are 96.52%, 84.72%, 91.77%, respectively.",encoder-decoder,multi-scale spatial pyramid pooling,dense connection,regression loss,remote sensing,water body semantic segmentation,,,,,,"Liu, Yanming",,,,,,,,,
Row_972,"Nan, Guojun","Li, Haorui","Du, Haibo","Liu, Zhuo","Wang, Min",A Semantic Segmentation Method Based on AS-Unet plus plus for Power Remote Sensing of Images,SENSORS,JAN 2024,4,"In order to achieve the automatic planning of power transmission lines, a key step is to precisely recognize the feature information of remote sensing images. Considering that the feature information has different depths and the feature distribution is not uniform, a semantic segmentation method based on a new AS-Unet++ is proposed in this paper. First, the atrous spatial pyramid pooling (ASPP) and the squeeze-and-excitation (SE) module are added to traditional Unet, such that the sensing field can be expanded and the important features can be enhanced, which is called AS-Unet. Second, an AS-Unet++ structure is built by using different layers of AS-Unet, such that the feature extraction parts of each layer of AS-Unet are stacked together. Compared with Unet, the proposed AS-Unet++ automatically learns features at different depths and determines a depth with optimal performance. Once the optimal number of network layers is determined, the excess layers can be pruned, which will greatly reduce the number of trained parameters. The experimental results show that the overall recognition accuracy of AS-Unet++ is significantly improved compared to Unet.",semantic segmentation,Unet,atrous spatial pyramid pooling,squeeze-and-excitation module,,,,,,,,"Xu, Shuiqing",,,,,,,,,
Row_973,"Zhang, Shichao","Wang, Changying","Li, Jinhua","Sui, Yi",,MF-Dfnet: a deep learning method for pixel-wise classification of very high-resolution remote sensing images,INTERNATIONAL JOURNAL OF REMOTE SENSING,JAN 2 2022,3,"Semantic segmentation of high-resolution remote sensing images is very important. However, the targets in the high-resolution optical satellite images are always various in size, which lead to multiscale problems resulting in difficulty of locating and identifying the target. High-resolution remote sensing is more complex than natural phenomena; this leads to false alarms due to a greater intraclass inconsistency. Thus, the pixel-wise classification of high-resolution remote sensing images becomes challenging. Aiming at the above problems, we propose a multiscale feature and discriminative feature network (MF-DFNet). We introduce the hierarchical-split block (HSB) and the residual receptive field block module (RRFBM) to extract multiscale information to address multiscale problems. We also introduce a foreground-scene relation module to enhance the discrimination of features and deal with the false alarm phenomenon. In addition, the channel attention block (CAB) is introduced to select more discriminative features. We use two publicly available remote sensing image datasets (Vaihingen and Massachusetts building) for the experiments in this paper. Compared to current advanced models, our results show that MF-DFNet achieves state-of-the-art performance and can effectively improve the integrity and correctness of semantic segmentation in high-resolution remote sensing images.",semantic segmentation,deep learning,remote sensing images,hierarchical-split block,channel attention block,residual receptive field block module,foreground-scene relation module,,,,,,,,,,,,,,
Row_974,"Huang, Yifei","Feng, Zideng","Yang, Junli","Wang, Bin","Wang, Jiaying",LE-BEIT: A LOCAL-ENHANCED SELF-SUPERVISED TRANSFORMER FOR SEMANTIC SEGMENTATION OF HIGH RESOLUTION REMOTE SENSING IMAGES,,2022,1,"Semantic segmentation for remote sensing images (RSI) has been a thriving research topic for a long time. Existing supervised learning methods usually require a huge amount of labeled data. Meanwhile, large size, variation in object scales, and intricate details in RSI make it essential to capture both long-range context and local information. To address these problems, we propose Le-BEIT, a self-supervised Transformer with an improved positional encoding Local-Enhanced Positional Encoding (LePE). Self-supervised learning relieves the demanding requirement of a large amount of labeled data. The self-attention mechanism in Transformer has remarkable capability in capturing long-range context. Meanwhile, we use LePE as a substitution for Relative Positional Encoding (RPE) to represent local information more effectively. Moreover, considering the domain difference between natural images and RSI, instead of ImageNet-22K, we pre-train Le-BEIT on a very small high-resolution RSI dataset-GID. To investigate the influence of pre-training dataset size on segmentation accuracy, we furtherly conduct experiments on a larger pre-training dataset called GID-DOTA, which is 1/100 of ImageNet-22K, and have observed considerable accuracy improvements. The result of our method, which relies on a much smaller pretrained dataset, achieves competitive accuracy compared to the counterpart on ImageNet-22K.",Remote Sensing,Self-supervised Learning,Transformer,,,,,,,,,"Xian, Zhenglin",,,,,,,,"2022 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",
Row_975,"Jia, Xinqi","Song, Xiaoyong","Rao, Lei","Fan, Guangyu","Cheng, Songlin",DEUFormer: High-precision semantic segmentation for urban remote sensing images,IET COMPUTER VISION,DEC 2024,0,"Urban remote sensing image semantic segmentation has a wide range of applications, such as urban planning, resource exploration, intelligent transportation, and other scenarios. Although UNetFormer performs well by introducing the self-attention mechanism of Transformer, it still faces challenges arising from relatively low segmentation accuracy and significant edge segmentation errors. To this end, this paper proposes DEUFormer by employing a special weighted sum method to fuse the features of the encoder and the decoder, thus capturing both local details and global context information. Moreover, an Enhanced Feature Refinement Head is designed to finely re-weight features on the channel dimension and narrow the semantic gap between shallow and deep features, thereby enhancing multi-scale feature extraction. Additionally, an Edge-Guided Context Module is introduced to enhance edge areas through effective edge detection, which can improve edge information extraction. Experimental results show that DEUFormer achieves an average Mean Intersection over Union (mIoU) of 53.8% on the LoveDA dataset and 69.1% on the UAVid dataset. Notably, the mIoU of buildings in the LoveDA dataset is 5.0% higher than that of UNetFormer. The proposed model outperforms methods such as UNetFormer on multiple datasets, which demonstrates its effectiveness.",computer vision,convolutional neural nets,,,,,,,,,,"Chen, Niansheng",,,,,,,,,
Row_976,"Deng, Guohui","Wu, Zhaocong","Xu, Miaozhong","Wang, Chengjun","Wang, Zhiye",Crisscross-Global Vision Transformers Model for Very High Resolution Aerial Image Semantic Segmentation,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,5,"Semantic segmentation is a key means for understanding very high resolution (VHR) aerial imagery. With the explosive development of deep learning, deep learning methods are being applied to the segmentation of VHR images, with convolutional neural networks (CNNs) as the basic framework; however, owing to the highly complex details present in VHR images and the high spatial dependence of geographical objects, CNN-based methods are inadequate. This is because the inherent locality of CNNs limits the size of the receptive field, thus limiting the ability to obtain long-range context information. To solve this problem, in this article, we propose a transformer-based novel deep learning model called crisscross-global vision transformers (CGVTs). CGVT exploits the transformer's inherent ability to obtain long-range context information to solve the restricted receptive field problem. Specifically, we redesign the self-attention (SA) mechanism in the transformer and call it crisscross-global attention. It consists of two parts: a crisscross transformer encoder block (CC-TEB) and a global squeeze transformer encoder block (GS-TEB). CC-TEB overcomes the limitation of the traditional SA design (specifically, difficulty applying it to VHR aerial image segmentation) and further increases the local feature representation ability of the model. GS-TEB increases the global feature representation ability of the model. The results of experiments conducted on the popular ISPRS Vaihingen, IEEE GRSS data fusion contest Zeebrugge, and LoveDA semantic segmentation challenge datasets verify the effectiveness and superiority of our proposed method. Specifically, it achieved state-of-the-art performance on both Zeebrugge and LoveDA datasets and is currently ranked second in the Vaihingen dataset.",Aerial imagery,remote sensing,semantic segmentation,transformers,,,,,,,,"Lu, Zhongyuan",,,,,,,,,
Row_977,"Hu, Haiyang","Yang, Linnan","Chen, Jiaojiao","Luo, Shuang",,The remote sensing image segmentation of land cover based on multi-scale attention features,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,0,"Segmentation of land cover in remote sensing images is a task that involves interpreting remote sensing data using machine vision. Satisfying segmentation results in agriculture and forestry regions can guide land resource management, natural environment protection, urban construction, and the distribution of agricultural products. However, the performance of the widely used deep learning segmentation model on high-resolution remote sensing segmentation datasets in agriculture and forestry regions needs to be improved. To solve the problems of poor accuracy and loss of context information in remote sensing image semantic segmentation, this paper proposes an improved semantic segmentation network architecture. The model utilizes multi-scale feature extraction, deploys a multi-layer attention feature fusion module and an up-sampling fusion module to capture high-quality multi-scale context information, correctly handle scale changes, and help narrow the semantic gap between different levels. Finally, the proposed MLP decoder refers to the dynamic up-sampling operator to aggregate the information at different levels to achieve pixel segmentation. To verify the effectiveness of our proposed model, the researchers conducted experiments on two land cover segmentation datasets. The training process specifically designs data augmentation strategies for remote sensing segmentation tasks to enhance the model's generalization ability. The final model achieved an mIoU (mean Intersection over Union) of 65.05% on the self-built rural land cover datasets, surpassing the benchmark network UPerNet by 5.92%. On the LoveDA dataset, our model achieved state-of-the-art performance with an mIoU of 53.39%, demonstrating its versatility.",land cover,remote sensing image,neural network,attention mechanism,multi-scale feature,,,,,,,,,,,,,,,,
Row_978,"Saxena, Nidhi","Babu, N. Kishore","Raman, Balasubramanian",,,Semantic Segmentation of Multispectral Images using Res-Seg-net Model,,2020,12,"Semantic segmentation is pixel-wise labeling of the image. Recently deep convolutional neural network (DCNN) providing progressive results in semantic segmentation. However, in remote sensing multispectral imagery very limited work has been done due to lack of training dataset. In this paper, a Res-eg-net model is proposed for the semantic segmentation which is motivated by the existing Resnet and Segnet models. This model consists of encoder-decoder parts in which residual mapping is followed. For validation and testing of the proposed model, the RIT-18 dataset of multispectral imagery is used. The comparison results of the experiment on a multispectral imagery dataset have demonstrated the effectiveness of the proposed model.",Convolutional neural network,Multispectral images,Semantic Segmentation,,,,,,,,,,,,,,,,,2020 IEEE 14TH INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING (ICSC 2020),
Row_979,"Dong, Shan","Zhuang, Yin","Chen, He","Zhang, Tong","Li, Lianlin",Full Semantic Constructed Network for Urban Use Classification From Very High-Resolution Optical Remote Sensing Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,0,"Recently, semantic segmentation technology has been a research hotspot in optical remote sensing urban use classification. However, because of coupled semantic relations in very high-resolution and complex urban scenes, a more effective semantic description for pixelwise urban use interpretation has become a challenge. Then, aiming to set up a more effective semantic description, the effective receptive field (ERF) is analyzed in general convolutional neural networks. The unreasonable ERF distribution in the stacked convolutional layers of the encoder would lead to a large amound of small ERFs and fewer not large enough ERFs that form a naive semantic description in decoder. Therefore, in this article, a novel full semantic constructed network (FSCNet) is proposed to improve the naive semantic description and set up an effective semantic description. First, to avoid noise from shallow feature layers, a residual refinement convolution is designed to optimize the full-scale skip connections based on the U-shaped encoder-decoder. Second, an interscale fusion module is newly designed for multiscale feature fusion, which can generate three initial semantic modalities that are prepared for redefining the full semantic description. Third, a multiscale local context spatial attention module and boundary supervision are designed for an initial shallow semantic modality to capture the pure boundary information, and then, pyramid spatial pooling is employed for an initial deep semantic modality to further enlarge the ERF and obtain more abstract global information. Next, a self-calibration convolution combined with the atrous spatial pyramid pooling is designed to rectify and enrich an initial middle semantic modality, which can improve the naive semantic description and bridge the semantic gap between the redefined shallow and deep semantic modalities to advance the full semantic feature fusion. Finally, extensive experiments are carried out on three benchmarks (e.g., ISPRS Vaihingen, Potsdam, and DLRSD), and comparative results show that the proposed FSCNet can get remarkable performance compared to state-of-the-art (SOTA) methods. Besides, the code is available at https://github.com/DorisCV/FSCNet.",Semantics,Location awareness,Feature extraction,Convolution,Remote sensing,Optical sensors,Decoding,Full semantic description,optical remote sensing,urban use classification,very high resolution (VHR),"Long, Teng",,,,,,,,,
Row_980,"Fan, Runyu","Li, Fengpeng","Han, Wei","Yan, Jining","Li, Jun",Fine-Scale Urban Informal Settlements Mapping by Fusing Remote Sensing Images and Building Data via a Transformer-Based Multimodal Fusion Network,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2022,25,"Urban informal settlements (UISs) are high-density population settlements with low standards of living and supply. UIS semantic segmentation, which identifies pixels corresponding to informal settlements in remote sensing images, is crucial to the estimation of poor communities, urban management, resource allocation, and future planning, particularly in megacities. However, most studies on informal settlement mapping are either based on parcels (image classification) or pixels (semantic segmentation). Few studies utilize object information to improve UIS mapping. Since informal settlements are formed by buildings (objects), utilizing object information can improve UIS semantic segmentation. Furthermore, current UIS mapping studies mainly focus on using single-modality remote sensing images, and there is a lack of related research on using multimodal data. Due to the spatial heterogeneity of informal settlements, using only a single modality of remote sensing image features limits the effectiveness and accuracy of informal settlements semantic segmentation. Aiming at achieving fine-scale UIS mapping results, this article proposes a UIS semantic segmentation method, namely UisNet, that utilizes a transformer-based block to receive multimodal data, including high-spatial-resolution remote sensing images (parcel- and pixel-level) and building polygon data (object-level) to identify UIS. The experiments were conducted in Shenzhen City, and they confirmed the superior performance of UisNet, which achieved an overall accuracy (OA) of 94.80% and a mean intersection over union (mIoU) of 85.51% in the testing set of the manually labeled UIS semantic segmentation dataset (UIS-Shenzhen dataset) and outperformed the best models on semantic segmentation tasks. Besides, we add a set of experiments on a public dataset [gaofen image dataset (GID) dataset] and compare our method with the current state-of-the-art semantic segmentation methods. Experiments show that the proposed UisNet improves mIoU by 1.64% to 7.58% compared to other methods. This work will be available at https://github.com/RunyuFan/.",Semantics,Image segmentation,Buildings,Remote sensing,Task analysis,Urban areas,Transformers,Deep learning,multimodal,remote sensing,semantic segmentation,"Wang, Lizhe",,,,,urban informal settlements (UISs),,,,
Row_981,"Song, Pengfei","Li, Jinjiang","An, Zhiyong","Fan, Hui","Fan, Linwei",CTMFNet: CNN and Transformer Multiscale Fusion Network of Remote Sensing Urban Scene Imagery,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2023,38,"Semantic segmentation of remotely sensed urban scene images is widely demanded in areas such as land cover mapping, urban change detection, and environmental protection. With the development of deep learning, methods based on convolutional neural networks (CNNs) have been dominant due to their powerful ability to represent hierarchical feature information. However, the limitations of the convolution operation itself limit the network's ability to extract global contextual information. With the successful use of transformer in computer vision in recent years, transformer has shown great potential for modeling global contextual information. However, transformer is not sufficiently capable of capturing local detailed information. In this article, to explore the potential of the joint CNN and transformer mechanism for semantic segmentation of remotely sensed urban scenes, we propose a CNN and transformer multiscale fusion network (CTMFNet) based on encoding-decoding for urban scene understanding. To couple local-global context information more efficiently, we designed a dual backbone attention fusion module (DAFM) to couple the local and global context information of the dual-branch encoder. In addition, to bridge the semantic gap between scales, we built a multi-layer dense connectivity network (MDCN) as our decoder. The MDCN enables the full flow of semantic information between multiple scales to be fused with each other through upsampling and residual connectivity. We conducted extensive subjective and objective comparison experiments and ablation experiments on both the International Society of Photogrammetry and Remote Sensing (ISPRS) Vaihingen and ISPRS Potsdam datasets. Numerous experimental results have proven the superiority of our method compared to currently popular methods.",Feature extraction,Transformers,Semantics,Remote sensing,Decoding,Semantic segmentation,Convolutional neural networks,Convolutional neural network (CNN),multiscale fusion,semantic segmentation,transformer,,,,,,,,,,
Row_982,"Li, Yuxia","Peng, Bo","He, Lei","Fan, Kunlong","Tong, Ling",Road Segmentation of Unmanned Aerial Vehicle Remote Sensing Images Using Adversarial Network With Multiscale Context Aggregation,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,JUL 2019,40,"Semantic segmentation using adversarial networks has proved to be effective in image processing fields. However, two problems need to be solved in the field of the road segmentation of unmanned aerial vehicle (UAV) remote sensing images. One is the occupied proportion of road area in UAV remote sensing images; the other is that the constant size of convolutional kernel cannot deal with multiscale feature very well. To solve these two problems, this paper proposed a road segmentation model that combined the adversarial networks with multiscale context aggregation. First, the output feature-maps of three scales (0.5n, 1n, 2n) were obtained, based on an end-to-end training from image segmentation network. Second, after the convolution and deconvolution operations, the processed images were unified to the size scale of original images. Third, with the pixel-by-pixel addition method, the three scales of image feature (0.5n, 1n, 2n) were merged together, then inputted into the discriminative network. Finally, the errors were obtained and propagated backwards compared with the label, and then the parameters of a generative network and a discriminative network could be updated. Further, the segmented results were compared with those from normal adversarial networks, Linknet and D-linknet, and were developed with the morphological operation. The research results show that the proposed model can improve the precision of road segmentation from UAV images with multiscale context aggregation and the regularization property of adversarial networks.",Adversarial network,image processing,multiscale context aggregation,road segmentation,unmanned aerial vehicle (UAV) image,,,,,,,,,,,,,,,,
Row_983,"Liu, Wei","Wang, He","Qiao, Yicheng","Liang, Bin","Yang, Junli",DLAFNET: A DIRECT FUSION METHOD OF 2D AERIAL IMAGE AND 3D LIDAR POINT CLOUD FOR SEMANTIC SEGMENTATION,,2023,1,"Semantic segmentation of high-resolution remote sensing images (RSIs) is developing rapidly. Multispectral images can provide rich spectral information for semantic segmentation, while 3D LiDAR point cloud data can provide depth information. Thus, semantic segmentation accuracy could be improved by fusing multispectral images and 3D LiDAR point cloud. In this paper, we propose a method titled Direct LiDAR-Aerial Fusion Network (DLAFNet) which directly uses RSIs and LiDAR point cloud for semantic segmentation tasks. In particular, owing to the fact that sparse features extracted from the KPConv branch are not as essential as features from RSIs, we design LiDAR Assisted Attention Module (L-AAM). Our experiments on the modified GRSS18 dataset prove that our method is proper and can obtain the best results by comparing with its components and other methods.",Semantic segmentation,data fusion,,,,,,,,,,"Zhang, Haopeng",,,,,,,,IGARSS 2023 - 2023 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,
Row_984,"Niu, Binglin",,,,,Semantic Segmentation of Remote Sensing Image Based on Convolutional Neural Network and Mask Generation,MATHEMATICAL PROBLEMS IN ENGINEERING,JUN 2 2021,9,"High-resolution remote sensing images usually contain complex semantic information and confusing targets, so their semantic segmentation is an important and challenging task. To resolve the problem of inadequate utilization of multilayer features by existing methods, a semantic segmentation method for remote sensing images based on convolutional neural network and mask generation is proposed. In this method, the boundary box is used as the initial foreground segmentation profile, and the edge information of the foreground object is obtained by using the multilayer feature of the convolutional neural network. In order to obtain the rough object segmentation mask, the general shape and position of the foreground object are estimated by using the high-level features in the process of layer-by-layer iteration. Then, based on the obtained rough mask, the mask is updated layer by layer using the neural network characteristics to obtain a more accurate mask. In order to solve the difficulty of deep neural network training and the problem of degeneration after convergence, a framework based on residual learning was adopted, which can simplify the training of those very deep networks and improve the accuracy of the network. For comparison with other advanced algorithms, the proposed algorithm was tested on the Potsdam and Vaihingen datasets. Experimental results show that, compared with other algorithms, the algorithm in this article can effectively improve the overall precision of semantic segmentation of high-resolution remote sensing images and shorten the overall training time and segmentation time.",,,,,,,,,,,,,,,,,,,,,
Row_985,"Zheng, Chen","Zhang, Yun","Wang, Leiguang",,,Multilayer semantic segmentation of remote-sensing imagery using a hybrid object-based Markov random field model,INTERNATIONAL JOURNAL OF REMOTE SENSING,DEC 2016,5,"High spatial resolution (HR) remote-sensing image usually contains hierarchical semantic information. Many supervised methods have been developed to interpret this information through data training. In this article, without data training, a hybrid object-based Markov random field (HOMRF) model is proposed for multi-layer semantic segmentation of remote-sensing images. In this method, label fields of different semantic layers are defined on the same region adjacency graph (RAG) of a given image, and a hybrid framework is suggested to capture and utilize the interactions within and between semantic layers by label fields. Namely a new transition probability matrix is introduced into the energy functions of label fields for describing the semantic context between layers, and the multilevel logistic model is employed to describe the interactions within the same layer. A principled probabilistic inference is developed to determine the optimal solution of the proposed method by iteratively updating each label field until convergence. The computational complexity of the proposed model is O(knt), where k is the number of classes in all of the layers, n is the number of sites in the probability graph of the MRF model, and t is the number of iterations. Experimental results from various remote-sensing images demonstrate that the proposed method can produce higher segmentation accuracy than state-of-the-art MRF-based methods.",,,,,,,,,,,,,,,,,,,,,
Row_986,Zhu Ying,Zhao Ming,,,,Registration of Laser Point Cloud and Optical Image in Urban Area Based on Semantic Segmentation,ACTA PHOTONICA SINICA,JAN 2021,4,"The collaborative application of point cloud data and optical remote sensing image has been widely concerned in the field of remote sensing. In order to accurately register two kind of data and better integrate their advantages, an automatic registration method of point cloud and optical remote sensing image in urban scene is proposed. Firstly, the depth image is generated from point cloud data, that is, 3D data is converted into 2D image. Secondly, the Unet model is used to train the depth image and the optical remote sensing image respectively and get building segmentations. Thirdly, the minimum circumscribed rectangles of buildings are constructed based on the contour set of building segmentation, and the length-width ratio of rectangle is taken as the constraint condition to find Corresponding Points (CPs). Then, we use the similar triangle principle to find CPs of the rectangle' s center point. Finally , the coordinate of the CPs are substituted into the transformation model to calculate the model parameters, thus the registration is achieved. The experimental results show that the proposed method can achieve better registration effect when it is difficult to match with traditional point feature method, and it is resistant to image translation, rotation and scaling.",Remote sensing,Registration,Semantic segmentation,Point cloud,Optical,,,,,,,,,,,,,,,,
Row_987,"Wang, Jin","Ding, Ning","He, Guangjun",,,A boundary enhancement loss function for semantic segmentation of land cover,INTERNATIONAL JOURNAL OF REMOTE SENSING,JUN 18 2023,1,"With the rapid development of UAV remote sensing, satellite remote sensing and computer vision, the semantic segmentation of remote sensing images has also developed rapidly and is widely used in research on land utilization classification, ecology, urban planning and other problems. Large differences in spatial and temporal scales, different image resolutions, insufficient model robustness to the data domain, and blurred object boundaries are the main problems for existing semantic segmentation models based on deep learning. This paper studies the problem of blurred target boundaries after semantic segmentation and propose a boundary enhancement loss function that highlights the importance of target edges. Compared to other models used in investigating higher boundary accuracy, the proposed model can be trained without boundary-labelled data, and no additional inference time is consumed. This loss function is applied to some other deep learning networks as a plug-and-play module on two different datasets and show that the IoU has an improvement of 2-5% with better clear boundary and continuity, which is more prominent on buildings and roads.",Semantic segmentation,boundary enhancement loss,land utilization,,,,,,,,,,,,,,,,,,
Row_988,"Qian, Xiaoliang","Li, Chao","Wang, Wei","Yao, Xiwen","Cheng, Gong",Semantic segmentation guided pseudo label mining and instance re-detection for weakly supervised object detection in remote sensing images,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,MAY 2023,28,"Weakly supervised object detection (WSOD) in remote sensing images (RSIs) has good practical value because it only requires the image-level annotations. The existing methods usually have two problems. The first problem is that many methods mine the pseudo ground truth (PGT) instances solely depending on the class confidence score (CCS), however, the reliability of CCS is not enough because of the inter-class similarity and intra-class diversity in RSIs, consequently, the reliability of corresponding PGT instances is limited, in addition, the most discriminative part with high CCS rather than the whole object is easily selected as the PGT instance. The second problem is that the object localization solely relies on the candidate proposals generated by the selective search or edge boxes algorithm, however, the localization accuracy of the candidate proposals is not enough because of the cluttered background in RSIs. To address the first problem, a semantic segmentation guided pseudo label mining (SGPLM) module is proposed, which uses a novel metric named class-specific object confidence score (COCS) to mine high-quality PGT instances. The COCS is made up of the CCS and class-specific object overlap score (COOS) which is calculated through the weakly supervised semantic segmentation. The mined PGT instances are more robust and incline to cover the whole object by combining the COOS. To handle the second problem, an instance re-detection (IR) module is proposed for improving the localization accuracy of the WSOD model, in which an enhanced PGT instance generation strategy is designed to obtain the enhanced PGT instances on the basis of the candidate proposals, and the enhanced PGT instances are used to train the instance re-classification and re-localization branches which are jointly utilized to infer the final results. The ablation studies validate the effectiveness of the SGPLM and IR modules. The comprehensive comparisons with other advanced methods show that the performance of the proposed method is state-of-the-art on two RSI datasets.",Weakly supervised object detection (WSOD),Remote sensing images (RSIs),Semantic segmentation guided pseudo label,mining (SGPLM),Instance re-detection (IR),,,,,,,,,,,,,,,,
Row_989,"Zhuang, Zhenrong","Shi, Wenzao","Sun, Wenting","Wen, Pengyu","Wang, Lei",Multi-class remote sensing change detection based on model fusion,INTERNATIONAL JOURNAL OF REMOTE SENSING,FEB 1 2023,3,"Change detection in remote sensing images has an important impact in various application fields. In recent years, great progress has been made in the change detection methods of multiple types of ground objects, but there are still limited recognition capabilities of the extracted features, resulting in unclear boundaries, and the accuracy rate needs to be improved. To address these issues, we use a high-resolution network (HRNet) to generate high-resolution representations and add new data augmentation methods to improve its accuracy. Secondly, we introduce the model of Transformer structure -- CSWin and HRNet to fuse to improve the performance and effect of the model. In order to enhance the model's ability to perceive ground objects at different scales, a feature fusion network suitable for multi-class semantic segmentation is designed, named A-FPN. This feature fusion network is introduced between the CSWin backbone network and the semantic segmentation network. The experimental results show that the fusion method greatly improves the accuracy to 89.31% on the SECOND dataset, significantly reduces false detections, and recognizes the edges of objects more clearly. And achieved good results in the three evaluation indicators of precision, recall, and F1-score.",Deep learning,change detection,semantic segmentation,remote sensing image,model integration,,,,,,,"Yang, Weiqi","Li, Tian",,,,,,,,
Row_990,"Doi, Kento","Iwasaki, Akira",,,,THE EFFECT OF FOCAL LOSS IN SEMANTIC SEGMENTATION OF HIGH RESOLUTION AERIAL IMAGE,,2018,20,"The semantic segmentation of High Resolution Remote Sensing (HRRS) images is the fundamental research area of the earth observation. Convolutional Neural Network(CNN), which has achieved superior performance in computer vision task, is also useful for semantic segmentation of HRRS images. In this work, focal loss is used instead of cross-entropy loss in training of CNN to handle the imbalance in training data. To evaluate the effect of focal loss, we train SegNet and FCN with focal loss and con firm improvement in accuracy in ISPRS 2D Semantic Labeling Contest dataset, especially when gamma is 0.5 in SegNet.",deep learning,semantic segmentation,CNN,focal loss,,,,,,,,,,,,,,,,IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,
Row_991,"Zhao, Sijie","Chen, Hao","Zhang, Xueliang","Xiao, Pengfeng","Bai, Lei",RS-Mamba for Large Remote Sensing Image Dense Prediction,IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,2024,4,"Context modeling is critical for remote sensing image dense prediction tasks. Nowadays, the growing size of very-high-resolution (VHR) remote sensing images poses challenges in effectively modeling context. While transformer-based models possess global modeling capabilities, they encounter computational challenges when applied to large VHR images due to their quadratic complexity. The conventional practice of cropping large images into smaller patches results in a notable loss of contextual information. To address these issues, we propose the remote sensing Mamba (RSM) for dense prediction tasks in large VHR remote sensing images. RSM is specifically designed to capture the global context of remote sensing images with linear complexity, facilitating the effective processing of large VHR images. Considering that the land covers in remote sensing images are distributed in arbitrary spatial directions due to characteristics of remote sensing over-head imaging, the RSM incorporates an omnidirectional selective scan module (OSSM) to globally model the context of images in multiple directions, capturing large spatial features from various directions. We designed simple yet effective models based on RSM, achieving state-of-the-art performance on dense prediction tasks in VHR remote sensing images without fancy training strategies. Extensive experiments on semantic segmentation (SS) and change detection (CD) tasks across various land covers demonstrate the effectiveness of the proposed RSM. Leveraging the linear complexity and global modeling capabilities, RSM achieves better efficiency and accuracy than transformer-based models on large remote sensing images. Interestingly, we also demonstrated that our model generally performs better with a larger image size on dense prediction tasks.",Remote sensing,Task analysis,Context modeling,Transformers,Feature extraction,Predictive models,Complexity theory,Change detection (CD),deep learning,dense prediction,large remote sensing images,"Ouyang, Wanli",,,,,semantic segmentation (SS),state space model (SSM),very high resolution (VHR),,
Row_992,"Zhang, Tony","Dick, Robert P.",,,,SPATIAL-FREQUENCY NETWORK FOR SEGMENTATION OF REMOTE SENSING IMAGES,,2023,0,We describe a deep learning system for satellite image segmentation. Our CNN model embeds contextual feature dependencies in both spatial and frequency domains. Its Spatial Weighting Module uses a multi-scale pooling layer to represent correlations at longer length scales in the spatial domain. Its Frequency Weighting Module uses frequency-domain information to better discriminate between object classes. Experimental results on the Potsdam dataset demonstrate that our model has a 1.9% higher average F1 accuracy than previous methods.,Remote sensing segmentation,spatial,frequency,,,,,,,,,,,,,,,,,"2023 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",
Row_993,"Sun, Shuting","Mu, Lin","Wang, Lizhe","Liu, Peng","Liu, Xiaolei",Semantic Segmentation for Buildings of Large Intra-Class Variation in Remote Sensing Images with O-GAN,REMOTE SENSING,FEB 2021,25,"Remote sensing building extraction is of great importance to many applications, such as urban planning and economic status assessment. Deep learning with deep network structures and back-propagation optimization can automatically learn features of targets in high-resolution remote sensing images. However, it is also obvious that the generalizability of deep networks is almost entirely dependent on the quality and quantity of the labels. Therefore, building extraction performances will be greatly affected if there is a large intra-class variation among samples of one class target. To solve the problem, a subdivision method for reducing intra-class differences is proposed to enhance semantic segmentation. We proposed that backgrounds and targets be separately generated by two orthogonal generative adversarial networks (O-GAN). The two O-GANs are connected by adding the new loss function to their discriminators. To better extract building features, drawing on the idea of fine-grained image classification, feature vectors for a target are obtained through an intermediate convolution layer of O-GAN with selective convolutional descriptor aggregation (SCDA). Subsequently, feature vectors are clustered into new, different subdivisions to train semantic segmentation networks. In the prediction stages, the subdivisions will be merged into one class. Experiments were conducted with remote sensing images of the Tibet area, where there are both tall buildings and herdsmen's tents. The results indicate that, compared with direct semantic segmentation, the proposed subdivision method can make an improvement on accuracy of about 4%. Besides, statistics and visualizing building features validated the rationality of features and subdivisions.",building extraction,GF-2,orthogonal generative adversarial networks,subdivision,,,,,,,,"Zhang, Yuwei",,,,,,,,,
Row_994,"Liu, Guoying","Zhou, Hongyu",,,,Semantic Segmentation Based on Multi-stage Region-level Clustering,,2013,1,"In the field of remote-sensed image segmentation, it is very important to obtain semantic results. However, in high resolution remote sensed images, different complex patterns always have components with the same spectrum, which makes it rather difficult to extract such patterns only through traditional clustering methods. In this paper, a novel multi-stage region-level clustering method is proposed to solve this problem. Firstly, the initial oversegmentation is obtained by using the Mean Shift algorithm, based on which a region adjacent graph (RAG) is built; Then, FCM is employed to get the spectral-based segmentation result; After that, the context clues for each region is calculated according to the label and size of neighboring regions, followed by the second FCM clustering on each set of regions with the same label to distinct regions with the same spectrum but belongs to different objects; Rearranging all of these clustering results to form the finial processing unit, this algorithm goes a step further to calculate more accurate context clues, and use the third FCM to obtain the final segmentation result. Experiments on the high resolution remote-sensed images have shown the superiority to the competitions.",image segmentation,region adjacent graph,fuzzy c-means clustering,remote sensing,,,,,,,,,,,,,,,,"MIPPR 2013: MULTISPECTRAL IMAGE ACQUISITION, PROCESSING, AND ANALYSIS",
Row_995,"Li, Xiaolong","Li, Yuyin","Ai, Jinquan","Shu, Zhaohan","Xia, Jing",Semantic segmentation of UAV remote sensing images based on edge feature fusing and multi-level upsampling integrated with Deeplabv3+,PLOS ONE,JAN 20 2023,13,"Deeplabv3+ currently is the most representative semantic segmentation model. However, Deeplabv3+ tends to ignore targets of small size and usually fails to identify precise segmentation boundaries in the UAV remote sensing image segmentation task. To handle these problems, this paper proposes a semantic segmentation algorithm of UAV remote sensing images based on edge feature fusing and multi-level upsampling integrated with Deeplabv3+ (EMNet). EMNet uses MobileNetV2 as its backbone and adds an edge detection branch in the encoder to provide edge information for semantic segmentation. In the decoder, a multi-level upsampling method is designed to retain high-level semantic information (e.g., the target's location and boundary information). The experimental results show that the mIoU and mPA of EMNet improved over Deeplabv3+ by 7.11% and 6.93% on the dataset UAVid, and by 0.52% and 0.22% on the dataset ISPRS Vaihingen.",,,,,,,,,,,,"Xia, Yuanping",,,,,,,,,
Row_996,"Li, Rui","Zheng, Shunyi","Zhang, Ce","Duan, Chenxi","Wang, Libo",ABCNet: Attentive bilateral contextual network for efficient semantic segmentation of Fine-Resolution remotely sensed imagery,ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,NOV 2021,163,"Semantic segmentation of remotely sensed imagery plays a critical role in many real-world applications, such as environmental change monitoring, precision agriculture, environmental protection, and economic assessment. Following rapid developments in sensor technologies, vast numbers of fine-resolution satellite and airborne remote sensing images are now available, for which semantic segmentation is potentially a valuable method. However, because of the rich complexity and heterogeneity of information provided with an ever-increasing spatial resolution, state-of-the-art deep learning algorithms commonly adopt complex network structures for segmentation, which often result in significant computational demand. Particularly, the frequently-used fully convolutional network (FCN) relies heavily on fine-grained spatial detail (fine spatial resolution) and contextual information (large receptive fields), both imposing high computational costs. This impedes the practical utility of FCN for real-world applications, especially those requiring real-time data processing. In this paper, we propose a novel Attentive Bilateral Contextual Network (ABCNet), a lightweight convolutional neural network (CNN) with a spatial path and a contextual path. Extensive experiments, including a comprehensive ablation study, demonstrate that ABCNet has strong discrimination capability with competitive accuracy compared with stateof-the-art benchmark methods while achieving significantly increased computational efficiency. Specifically, the proposed ABCNet achieves a 91.3% overall accuracy (OA) on the Potsdam test dataset and outperforms all lightweight benchmark methods significantly. The code is freely available at https;//github.com./lironui/ABCNet.",Semantic Segmentation,Attention Mechanism,Bilateral Architecture,Convolutional Neural Network,Deep Learning,,,,,,,"Atkinson, Peter M.",,,,,,,,,
Row_997,"Yang, Zenan","Niu, Haipeng","Wang, Xiaoxuan","Huang, Liang","Yang, Kui",An unsupervised semantic segmentation method that combines the ImSE-Net model with SLICm superpixel optimization,INTERNATIONAL JOURNAL OF DIGITAL EARTH,DEC 31 2024,1,"In the field of remote sensing, using a large amount of labeled image data to supervise the training of fully convolutional networks for the semantic segmentation of images is expensive. However, using a small amount of labeled data can lead to reduced network performance. This paper proposes an unsupervised semantic segmentation method that combines the ImSE-Net model with SLICm superpixel optimization. First, the ImSE-Net model is used to extract semantic features from the image to obtain rough semantic segmentation results. Then, the SLICm superpixel segmentation algorithm is used to segment the input image into superpixel images. Finally, an unsupervised semantic segmentation model (UGLS) is used to combine high-level abstract semantic features with detailed information on superpixels to obtain edge-optimized semantic segmentation results. Experimental results show that compared with other semantic segmentation algorithms, our method more effectively handles unbalanced areas, such as object boundaries, and achieves better segmentation results, with higher semantic consistency.",High-spatial-resolution remote sensing images,semantic segmentation,ImSE-Net model,SLICm superpixel optimization model,unsupervised semantic segmentation model,,,,,,,,,,,,,,,,
Row_998,"Veljanovski, Tatjana","Kanjir, Ursa","Ostir, Kristof",,,OBJECT-BASED IMAGE ANALYSIS OF REMOTE SENSING DATA,GEODETSKI VESTNIK,DEC 2011,7,"Remote sensing has developed various methods and technologies for con tactless and cost-effective mapping of large area land cover/land use maps and other thematic maps. The key factor for the availability and reliability of these maps for use in Earth sciences is the development of effective procedures for satellite data analysis and classification. The most appropriate approach for classifying low and medium resolution satellite images (pixel size is coarser than, or at best similar to, the size of geographical objects) is pixel-based classification in which an individual pixel is classified into the closest class based on its spectral similarityWith increasing spatial resolution, pixel-based classification methods became less effective, since the relationship between the pixel size and the dimension of the observed objects on the Earth's surface has changed significantly Therefore object-oriented classification has become increasingly popular over the past decade. This combines segmentation (which is a fundamental phase of the approach) and contextual classification. Segmentation divides the image into homogeneous pixel groups (segments), which are during the semantic classification process - arranged into classes based on their spectral, geometric, textural and other features during. The intent of this paper is to present the theoretical argumentation and methodology of object-based image analysis of remote sensing data, provide all overview of the field and point out certain restrictions as regards the current operational solutions.",remote sensing,object-based image analysis,segmentation,object-based classification,semantic classification,,,,,,,,,,,,,,,,
Row_999,"Veljanovski, Tatiana","Kanjir, Ursa","Ostir, Kristof",,,OBJECT-BASED IMAGE ANALYSIS OF REMOTE SENSING DATA,GEODETSKI VESTNIK,DEC 2011,6,"Remote sensing has developed various methods and technologies for con tactless and cost-effective mapping of large area land cover/land use maps and other thematic maps. The key factor for the availability and reliability of these maps for use in Earth sciences is the development of effective procedures for satellite data analysis and classification. The most appropriate approach for classifying low and medium resolution satellite images (pixel size is coarser than, or at best similar to, the size of geographical objects) is pixel-based classification in which an individual pixel is classified into the closest class based on its spectral similarity.With increasing spatial resolution, pixel-based classification methods became less effective, since the relationship between the pixel size and the dimension of the observed objects on the Earth's surface has changed significantly. Therefore object-oriented classification has become increasingly popular over the past decade. This combines segmentation (which is a fundamental phase of the approach) and contextual classification. Segmentation divides the image into homogeneous pixel groups (segments), which are - during the Semantic classification process - arranged into classes based on their spectral, geometric, textural and other features during. The intent of this paper is to present the theoretical argumentation and methodology of object-bused image analysis of remote sensing data, provide an overview of the field and point out certain restrictions as regards the current operational solutions.",remote sensing,object-based image analysis,segmentation,object-based classification,semantic classification,,,,,,,,,,,,,,,,
Row_1000,"Liu, Bin","Li, Bing","Liu, Haiming","Li, Shuofeng",,ST-MDAMNet: Swin transformer combines multi-dimensional attention mechanism for semantic segmentation of high-resolution earth surface images,ADVANCES IN SPACE RESEARCH,OCT 15 2024,0,"In the derection of remote sensing (RS) image analysis, semantic segmentation, as an important technology, is of key significance for the identification and analysis of land surface cover types. In recent years, applying deep learning models to tasks such as road extraction, water distribution extraction, building classification and building segmentation from RS images has become an important research hot- spot. Due to its limited receptive field, traditional convolutional neural networks (CNN) cannot effectively capture global context information. Transformer uses the multi-head self-attention mechanism to capture a wide range of information and can solve this problem well. Therefore, we proposed ST-MDAMNet based on Swin Transformer and combined with the multi-dimensional attention mechanism. First, a feature enhancement module (FAM) is introduced after each stage of the Swin Transformer encoder to effectively enhance the model's proficiency in identifying essential information. Secondly, a feature fusion module (FFM) is proposed to effectively fuse the multi-scale information of the encoder part. It further improves the expression ability of different dimensional features and effectively improves the detection effect of small targets. Ultimately, the fused features are input into the multi-dimensional attention module (MDAM) to carefully optimize the features, which greatly increases the effect of semantic segmentation of RS images. We demonstrate the effectiveness of each module through ablation experiments. Comparative experiments are completed on two publicly large-scale data- sets, and the proposed method shows excellent results compared with state-of-the-art methods. (c) 2024 COSPAR. Published by Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",Swin Transfromer,Semantic segmentation,Remote sensing,Attention mechanism,,,,,,,,,,,,,,,,,
